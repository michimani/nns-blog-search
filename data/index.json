[
  {
    "contents": "Lambda 関数に対して URL を発行して外部から HTTP で実行できるようにする Function URLs ですが、そのままだと自動で発行されるドメインに対してリクエストを送ることになります。今回は、任意のドメインで Function URLs を利用する構成を AWS CDK を使って構築してみた話です。\n構成については構成図を見ればわかるので、記事の内容としては AWS CDK での実装にコメントを足していくような感じになります。\nまえがき 構成図 AWS CDK で作成するリソース Route 53 Certificate Manager CloudFront Lambda まとめ まえがき タイトルにもある通り、 Lambda Function URLs をカスタムドメインで使う構成を AWS CDK v2 (Go) で構築してみます。\nAWS CDK: 2.64.0 Go: 1.20 Function URLs を使って Lambda 関数に対して URL を発行すると、ランダムな英数字を含んだ下記のような URL が発行されます。\nhttps://{ランダムな英数字}.lambda-url.{リージョン}.on.aws/ 今回は、 前段に CloudFront を配置することで任意のドメインで Function URLs による Lambda 関数の実行ができるようにします。\nソースコードは下記に置いてあるので、お手元でも試していただけます。\naws-cdk-go-examples/lambda-function-urls-with-custom-domain at main · michimani/aws-cdk-go-examples 構成図 構成図としては下記のようになります。\nC4Component Container_Boundary(r53, \"Route 53\") { Component(h, \"HostedZone\", \"hosted zone\",\"imported by HostedZoneID\") Component(r, \"RecordSet\", \"custom domain record\",\"e.g) api.example.com -\u003e xxxxxx.cloudfront.net\") Rel(r, h, \"\") } Container_Boundary(acm, \"Certificate Manager\") { Component(c, \"Certificate\", \"SSL Certificate\", \"imported by ARN\") } Container_Boundary(cf,\"CloudFront\") { Component(oap, \"Origin Access Policy\", \"\", \"\") Component(dist, \"Distribution\", \"\", \"\") Component(cp, \"Cache Policy\", \"\", \"\") Rel_L(dist, cp, \"\", \"\") Rel_L(dist, oap,\"\", \"\") } Container_Boundary(lambda, \"Lambda\") { Component(url1, \"Function URL\", \"\", \"for default behavior\") Component(url2, \"Function URL\", \"\", \"for hello behavior\") Component(url3, \"Function URL\", \"\", \"for bye behavior\") Component(fn1, \"Function\", \"\", \"simple-response-default\") Component(fn2, \"Function\", \"\", \"simple-response-hello\") Component(fn3, \"Function\", \"\", \"simple-response-bye\") Rel(url1, fn1, \"\",\"\") Rel(url2, fn2, \"\",\"\") Rel(url3, fn3, \"\",\"\") } Rel_D(dist, c, \"\", \"\") Rel_D(r, dist, \"\", \"\") Rel_D(dist, url1, \"\",\"default\") Rel_D(dist, url2, \"\",\"/hello pattern\") Rel_D(dist, url3, \"\",\"/bye pattern\") UpdateElementStyle(h, $bgColor=\"grey\") UpdateElementStyle(c, $bgColor=\"grey\") UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\") Lambda 関数を 3 つ用意してそれぞれに Function URL を発行、CloudFront の Behavior でそれぞれの Function URL にリクエストが分配されるようにします。\n背景が水色になっているリソースを AWS CDK で構築します。灰色になっている Route53:HostedZone と CertificateManager:Certificate は、あらかじめ作成されているものを HostedZoneID と ZoneName および ARN でそれぞれインポートして利用します。\nAWS CDK で作成するリソース 今回は Route 53, Certificate Manager, CloudFront, Lambda に関連するリソースを作成します。\nRoute 53 Route 53 関連では、 任意のドメインから CloudFront のドメインに対する A レコードとなる RecordSet を作成します。その際、 RecordSet を紐付かせる HostedZone が必要になりますが、これについては既に存在している HostedZone を HostedZoneID と ZoneName をもとにしてインポートします。\nhostedZone := awsroute53.HostedZone_FromHostedZoneAttributes(scope, jsii.String(\u0026#34;MyHostedZone\u0026#34;), \u0026amp;awsroute53.HostedZoneAttributes{ HostedZoneId: \u0026amp;hostedZoneID, ZoneName: \u0026amp;zoneName, }) インポートした HostedZone を使って、 CloudFront に対する A レコードを作成します。 CloudFront に対する A レコードのターゲットについては awsroute53targets.NewCloudFrontTarget() を使って作成できます。\n// scope constructs.Construct // dist awscloudfront.Distribution // subDomain string // このドメインで Lambda 関数を実行できるようにする aliasTarget := awsroute53.RecordTarget_FromAlias( awsroute53targets.NewCloudFrontTarget(dist), ) props := \u0026amp;awsroute53.RecordSetProps{ RecordName: jsii.String(subDomain), RecordType: awsroute53.RecordType_A, Zone: hostedZone, Target: aliasTarget, } awsroute53.NewRecordSet(scope, jsii.String(\u0026#34;RecordSet\u0026#34;), props) aws-cdk-go-examples/route53.go at main · michimani/aws-cdk-go-examples Certificate Manager Certificate Manager については、既に存在している Certificate を ARN でインポートして利用します。\nceritificate := awscertificatemanager.Certificate_FromCertificateArn( stack, jsii.String(\u0026#34;ImportedCertificate\u0026#34;), jsii.String(\u0026#34;arn:aws:acm:us-east-1:000000000000:certificate/hoge\u0026#34;)) CloudFront CloudFront 関連では、 CachePolicy, OriginRequestPolicy, Distribution のリソースを作成します。 それぞれ特に難しいところはない1 のですが、 Distribution のカスタムオリジンとして Function URL の ドメイン をしていするところで小細工が必要でした。 Distribution のカスタムオリジンに Function URL を利用する場合、設定するのは URL ではなく ドメイン部分のみ となります。\nCloudFront ディストリビューションのオリジンとして Lambda 関数 URL を使用するには、オリジンドメインとして Lambda 関数 URL の完全なドメイン名を指定します。\nCloudFront ディストリビューションでのさまざまなオリジンの使用 - Amazon CloudFront AWS CDK において Function URL については awslambda.FunctionUrl.Url() で参照できるのですが、 Distribution と同じ scope 内で FunctionURL を作成している場合には DistributionProps 内で利用する時点ではまだ値が確定していません。なので、 strings.Replace などを使って https:// を取り除くことができません。そのため、 CloudFormation の Fn を利用して URL からドメイン部分のみを抜き出す必要があります。具体的には、下記のような関数を用意して加工しました。\nvar slash string = \u0026#34;/\u0026#34; func functionURLDomain(furl awslambda.FunctionUrl) *string { // function url format: https://hoge.lambda-url.ap-northeast-1.on.aws/ // split: [\u0026#34;https:\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;hoge.lambda-url.ap-northeast-1.on.aws\u0026#34;, \u0026#34;\u0026#34;] splitURL := awscdk.Fn_Split(\u0026amp;slash, furl.Url(), jsii.Number(4)) return (*splitURL)[2] } AWS CDK には CloudFormation の Fn を再現するための関数が要されており、今回は文字列を分割する Fn_Split を使っています。\nこのようにすることで、 cdk synth を実行した際の CFn テンプレートは下記のような形で生成されます。\nAWSCDKGoExampleFunctionURLFunctionCFDistribution6BF356B9: Type: AWS::CloudFront::Distribution Properties: DistributionConfig: Origins: - ConnectionAttempts: 1 ConnectionTimeout: 5 CustomOriginConfig: OriginProtocolPolicy: https-only OriginSSLProtocols: - TLSv1.2 DomainName: Fn::Select: - 2 - Fn::Split: - / - Fn::GetAtt: - AwsCdkGoExampleSimpleResponseDefaultUrlB01107C8 - FunctionUrl Id: LambdaFunctionUrlsWithCustomDomainStackAWSCDKGoExampleFunctionURLFunctionCFDistributionOrigin181CAA1CC OriginCustomHeaders: - HeaderName: x-aws-cdk-go-example-from HeaderValue: aws-cdk-go-example-cf また、任意の Origin Custom Header を設定しておくことで、後述する Lambda 関数の実装にて直接 Function URL の URL を叩かれたときの対策とすることができます。今回は x-aws-cdk-go-example-from というヘッダに aws-cdk-go-example-cf という値を設定するようにしています。\ncustomHeaderForFunction := map[string]*string{ \u0026#34;x-aws-cdk-go-example-from\u0026#34;: jsii.String(\u0026#34;aws-cdk-go-example-cf\u0026#34;), } props := \u0026amp;awscloudfront.DistributionProps{ Enabled: jsii.Bool(true), DefaultBehavior: \u0026amp;awscloudfront.BehaviorOptions{ CachePolicy: cachePolicy, OriginRequestPolicy: originRequestPolicy, Origin: awscloudfrontorigins.NewHttpOrigin(defaultFnURLDomain, \u0026amp;awscloudfrontorigins.HttpOriginProps{ ConnectionAttempts: jsii.Number(1), ConnectionTimeout: awscdk.Duration_Seconds(jsii.Number(5)), CustomHeaders: \u0026amp;customHeaderForFunction, ProtocolPolicy: awscloudfront.OriginProtocolPolicy_HTTPS_ONLY, OriginSslProtocols: \u0026amp;[]awscloudfront.OriginSslPolicy{ awscloudfront.OriginSslPolicy(\u0026#34;TLS_V1_2\u0026#34;), }, }), ViewerProtocolPolicy: awscloudfront.ViewerProtocolPolicy_HTTPS_ONLY, }, HttpVersion: awscloudfront.HttpVersion_HTTP2, PriceClass: awscloudfront.PriceClass_PRICE_CLASS_200, EnableLogging: jsii.Bool(false), } aws-cdk-go-examples/cloudfront.go at main · michimani/aws-cdk-go-examples Lambda Lambda 関連では Function と Function URL のリソースを作成しますが、それぞれ特筆するべき点はないのでリソースの定義については割愛します。一方で、 Lambda 関数の実装については工夫すべき点がありました。 CloudFront のところでも書きましたが、今回は Lambda 関数をFunction URL ではなく任意のドメインで実行できるようにするため、 Function URL を直接叩かれたときにはエラーにしたいです。これを実現するためには、 Lambda 関数内で CloudFront からのリクエスト時に付与されるカスタムヘッダを検査します。\nfunc handleRequest(ctx context.Context, httpRequest events.APIGatewayProxyRequest) (response, error) { lctx, ok := lambdacontext.FromContext(ctx) if !ok { return jsonResponse(http.StatusInternalServerError, errorBody{Error: \u0026#34;failed to parse lambda context\u0026#34;}, nil) } if !isAvailableAccess(httpRequest) { return jsonResponse(http.StatusForbidden, errorBody{Error: \u0026#34;forbidden\u0026#34;}, nil) } body := okBody{ RequestID: lctx.AwsRequestID, Message: message, Time: time.Now().Format(time.RFC3339Nano), } customHeader := map[string]string{ \u0026#34;x-aws-cdk-example\u0026#34;: \u0026#34;lambda-function-urls-with-custom-domain\u0026#34;, } return jsonResponse(http.StatusOK, body, customHeader) } func isAvailableAccess(req events.APIGatewayProxyRequest) bool { // invoke from specified CloudFront if h, ok := req.Headers[customHeaderKeyFromCloudFront]; !ok { fmt.Printf(\u0026#34;custom header %s does not exists. req:%v\u0026#34;, customHeaderKeyFromCloudFront, req) return false } else if h != customHeaderValueFromCloudFront { fmt.Printf(\u0026#34;custom header value is invalid. req:%v\u0026#34;, req) return false } return true } ハンドラ関数で events.APIGatewayProxyRequest を受け取るようにしておくことで、リクエストヘッダを検査することができます。\n上記の実装であれば、 Function URL を直接叩かれたとき (= 特定のリクエストヘッダが含まれないとき) は 403 のレスポンスが返ることになります。\naws-cdk-go-examples/main.go at main · michimani/aws-cdk-go-examples まとめ Lambda Function URL を任意のドメインで実行できるようにする構成を AWS CDK v2 (Golang) で構成してみた話でした。\nFunction URL が発表されたときは 「カスタムドメイン対応してないのかー」と思っていましたが、前段に CloudFront を置けば実現可能であるということがわかりました。\n前段に CloudFront を置くということは、キャッシュの恩恵を受けられるということになります。また、 WAF を利用することで DDoS 攻撃や悪意のある bot 等からのアクセスについても対策することができます。\n今回のように新たに CloudFront を用意するところからとなるとちょっと大変ですが、既に CloudFront を利用したアプリケーションがある状態であれば Behavior の追加だけで実現できることになるので、サーバサイドの実装を Lambda で作って API として使うというのがサクッとできてしまいそうです。\nDistributionProps の構造体がデカすぎる問題はありますが、これは CloudFront 扱う上での宿命なので諦めます\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-lambda-function-urls-with-custom-domain/",
    "title": "Lambda Function URLs でカスタムドメインを使う構成を AWS CDK v2 (Go) で構築する"
  },
  {
    "contents": "何も書かないのはあれなので、雑に 2022 年の振り返りをしておきます。\n去年の振り返りの最後に書いていたこと 来年こそは SAP 合格します。それではまた来年👋\n具体的な目標はこれだけでした。\nAWS 認定まわり 2019 年の 7 月に SAA、 2019 年の 12 月に SOA と DVA を取得していたので、それらの有効期限が 2022 に切れることになっていました。\n認定を継続するためにはそれぞれ再取得するか、上位資格となる SAP と DOP を取得する必要がありました。\n結論、 2022 年 6 月に SAP、 11 月に DOP を無事に取得できたので下位資格となる SAA/SOA/DVA も延命することができました。なので、去年の終わりに掲げた目標は達成です。\nこんにちは。私がギリギリを生きるソリューションアーキテクトのプロフェッショナルです。\n今月末で失効する予定だった SAA も延命できたのでヨシ。 pic.twitter.com/DFNyRQpMNm\n\u0026mdash; michimani Lv.870 (@michimani210) June 13, 2022 こんにちは。私がギリギリを生きる DevOps Engineer のプロフェッショナルです。\n来月失効する予定だった DVA と SOA も延命できたのでヨシ。 pic.twitter.com/fOb7pWKFes\n\u0026mdash; michimani Lv.870 (@michimani210) November 29, 2022 それぞれスコアはギリギリでしたが、合格できたのでヨシ。\nこれで AWS 認定は 5 つになったわけです。\n全部取ろうとかは思ってないですが、来年は Specialty の認定一つくらいは挑戦してみたいです。\n開発まわり (ざっくり) 去年の振り返り記事で、ふんわりと 「Go も勉強します」みたいなことを書いてました。\n去年作った Twitter API v2 用ライブラリもスター数が 66 まで増えて、 「盆栽を育てるように細々とメンテしていこうと思います。」 というのは達成かなと。利用者から issue 報告や PR が飛んできたりもして、あー使われてんなー感を感じることができました。\n今年は、 Go 以外だと Rust を触り始めてみたり、 AWS リソースを定義するのに Terraform と仲良くなったり、というのが新しい領域だったと思います。この辺については、今年新たに GitHub に作ったリポジトリを見ていって振り返ろうと思います。\n今年 GitHub に作ったリポジトリ 対象リポジトリの取得には、 gh コマンドで取得しました。\ngh repo list \\ --json name,createdAt,url \\ -q \u0026#39;.[] | select(.createdAt | test(\u0026#34;2022-\u0026#34;))\u0026#39; \\ --visibility public \\ --source \\ | jq --slurp \u0026#39;. | sort_by(.createdAt)\u0026#39; programming-Rust-2nd michimani/programming-Rust-2nd これは Rust の勉強を始めるにあたって O\u0026rsquo;Reilly の 「 プログラミング言語 Rust 第2版 」 を読みながら基本構文の使い方とかを実際にコードで書いていたやつです。\nとりあえず一周して、その後は Project Euler の問題を Rust でやったりしてました。が、まだまだ Rust なにもわからない。\nmichimani/Project-Euler-Solutions: My solutions for Project Euler. (Python, Go, Rust) go-esa michimani/go-esa: Unofficial esa SDK for the Go programming language. Twitter API v2 用ライブラリを作った勢いとその反省を活かそうとして作った、Go 製の esa API 用ライブラリです。\nesa は無料期間が 3ヶ月あるので、その間に作りきりました。無料期間終了後は、個人では使っていません。\ntext-detector michimani/text-detector: This is a library for the Go language to detect text from images. It uses Amazon Rekognition or Google Cloud Vision for text detection. 画像に含まれているテキストを抽出するためのパッケージです。抽出処理には Amazon Rekognition もしくは Google Cloud Vision のどちらかを選べるようになっています。\nREADME が TBD になっていることからも、突発的に作って放置していることがわかります。\nelasticsearch-get-started, get-started-opensearch michimani/elasticsearch-get-started michimani/get-started-opensearch: Hello OpenSearch. Elasticsearch と OpenSearch に入門しようと思って触ってみたやつです。\nOpenSearch については完全に理解した気でいましたがもう完全に忘れてしまったので、やはり継続は大事。\namazon-aws michimani/amazon-aws: The quiz \u0026amp;ldquo;Amazon\u0026amp;rdquo; or \u0026amp;ldquo;AWS\u0026amp;rdquo; or neither? なんとなく React.js に入門するかーと思って作った簡単なクイズアプリです。\nこちらから遊べます。\nQuiz Amazon or AWS aws-cdk-go-examples michimani/aws-cdk-go-examples: This is a repository of example implementations of using AWS CDK with Go language. AWS CDK v2 の Go 言語サポートが GA されたので、実装例を集めてみるか、と思って作ったものです。\nGA されたもののまだまだ情報が少ないので、自分が困ったときの参考になれば十分で、もし他の人の参考になればとても嬉しいです。\nget-started-cdk-terraform michimani/get-started-cdk-terraform CDK for Terraform 入門用に作ったやつです。\n触ってみた結果、管理するリソースが AWS リソースに限られるのであれば AWS CDK のほうが良いな、と思いました。\njsc michimani/jsc: Joins multiple slack channels to display posts in chronological order. Slack で複数チャンネルの TL をマージして見たいな、と思って作ったツールです。\nsqs-lambda-example michimani/sqs-lambda-example: Example of AWS Lambda functions that invoked by message from Amazon SQS. SQS をトリガーとする Lambda 関数について、あまりわかっていなかったで実際に簡単な構成を作ってみたやつです。リソースは Terraform で定義、 Event Source Mapping の Filter Pattern を使ってメッセージごとに異なる Lambda 関数を実行するような構成になっています。\ncontainer-lambda-cicd michimani/container-lambda-cicd: コンテナイメージを利用する Lambda 関数の CI/CD のサンプル。 コンテナイメージを使用する Lambda 関数について、 GitHub Actions を使った CI/CD を整備してみたサンプルです。\nlambda-parameters-extension michimani/lambda-parameters-extension: Sample code of using Parameter and Secret Lambda Extension. Parameter Store および Secret Manager へのリクエスト/レスポンスをキャッシュする Lambda Extension が AWS 公式からリリースされたので、それを試すために作ったやつです。\ngenerative-art michimani/generative-art: 『数学から創るジェネラティブアート - Processing で学ぶ かたちのデザイン』で紹介されているコードを Rust で書いたもの。 リポジトリの description にも書いてますが 「 数学から創るジェネラティブアート - Processing で学ぶ かたちのデザイン 」という本を興味本位で買ったので、底に書かれている実装を Rust でやってみようと思って作ったものです。とりあえずひとつは実装してみましたが、それ以降は進捗がありません。\naws-lambda-api-go michimani/aws-lambda-api-go: This is a client library for Go language to use AWS Lambda\u0026amp;rsquo;s Runtime API, Extension API, Telemetry API, and Logs API. Lambda の Runtime API, Extension API, Telemetry API 用 Go 製ライブラリです。Go で Lambda Extension を作るときに便利になる想定のライブラリです。\nhttp-client-mock michimani/http-client-mock: This is a package to generate a mock of http.Client in Go language. You can easily generate a http. Go で API 用のライブラリを書いていると http.Client をモック化したテストをたくさん書く機会があり、その部分をいい感じに書きやすくするためのパッケージを作りました。\nlambda-api-example michimani/lambda-api-example michimani/aws-lambda-api-go を使った実装例です。大した内容ではないので、作った理由がわかりません。\ninvocation-history-extension michimani/invocation-history-extension: This is a Extension for AWS Lambda Function that records history of invocation at the same runtime environment. Lamdba の Telemetry API を使う Lambda Extension です。\n詳しくはアドベントカレンダーの記事で書いています。\nTelemetry API を使う Lambda Extension を作ってみた - michimani.net その他 数学系の本をいくつか読んだ Google Analytics 4 に移行した オンライン勉強会はあまり参加できなかった (継続) 登壇はした 体重は減らせなかった (継続) 結婚した まとめ 雑に振り返り、と言いつつ GitHub のリポジトリをベースに振り返っただけになってしまいました。\n去年よりは学びはあった気はしつつも、結果 (収入) には結びついていないので、 2023 年は結果 (収入) にも拘っていけたらいいなと思います。\n具体的な目標も書いておかないと振り返るときに不便なのでいくつか。\nAWS 認定の Specialty 系の試験を受ける 数研準一級を受ける 2023 年もがんばりましょう 👋\n",
    "permalink": "https://michimani.net/post/other-retrospect-in-2022/",
    "title": "2022 年を雑に振り返る"
  },
  {
    "contents": "AWS Lambda の Runtime API について調べていたら Extension API、 Telemetry API というものの存在を知り、 ひとつ前の記事 ではそれを使って簡単な Lambda Extension を自作してみました。今回は、 Telemetry API を使う Lambda Extension を作ってみて、 Telemetry API でどのような情報が取得できるのかを確認してみます。\nはじめに Telemetry API とは Telemetry API を使う Lambda Extension の概要 Telemetry API の詳細 1. Subscribe API - PUT /telemetry について 2. Subscriber に対して送信されるイベントについて Telemetry API を使う Extension を作ってみる Telemetry API の Subscriber を起動 Subscribe API をコールして Subscriber を登録 イベントのハンドリング Lambda 関数で使ってみる 準備 Extension と それを使う関数を作成 実行してログを確認 まとめ はじめに この記事は 弁護士ドットコム Advent Calendar 2022 (と AWS LambdaとServerless Advent Calendar 2022 (その2) ) の 12 日目の記事です。\n昨日は @talog による 「textlint のルールを作って仕組みを理解した💪 」 でした。このブログにも linter を導入したい\u0026hellip;\n今回の記事ではタイトルの通り Telemetry API を使った Lambda Extension について書きますが、下記の記事で Lambda Extension そのものについて書いているので合わせて読んでいただくとよいかもしれません。\nAWS Lambda の Extension API を使いたいがために Go で Lambda Extension を自作してみた - michimani.net Telemetry API とは 2022年 11月にリリースされた機能で、 Lambda の実行環境、拡張機能、関数で起こっていることをイベントとして受け取ることができるような API のことです。\nAWS Lambda において、新しい AWS Lambda Telemetry API がリリースされました。これにより、関数の実行に関する強化されたモニタリングデータおよびオブザーバビリティデータを、Lambda 拡張機能で収集できるようになりました。 Telemetry API は、ログ、プラットフォームのトレース、関数呼び出しレベルのメトリクスを Lambda から直接受け取るための、拡張機能用のシンプルなインターフェイスを提供します。\n引用元: AWS Lambda が Telemetry API を導入したことで Lambda 拡張機能のモニタリングとオブザーバビリティがさらに向上 Telemetry API のリリース以前には Logs API というものが提供されており、この API では関数でログが出力されたイベントを受け取ることができていました。 Telemetry API は Logs API の上位互換のような存在で、今後は Logs API の代わりとして Telemetry API を使用することが推奨されています。\nThe Lambda Telemetry API supersedes the Lambda Logs API. While the Logs API remains fully functional, we recommend using only the Telemetry API going forward.\n引用元: Lambda Telemetry API - AWS Lambda Telemetry API を使う Lambda Extension の概要 Lambda Extension で Telemetry API を使う場合、 Extension の実装としては下記のようになります。\nHTTP または TCP でリクエストを受け取るサーバを所定のアドレス (ポートは任意) で起動する 1 で起動したサーバのアドレスを Telemetry API の Subscribe API (PUT /telemetry) を使って登録する 何かしらのイベントが発生した際に Telemetry API から 1 のサーバに対してリクエストが送信されるので、ハンドリングを行う シーケンス図で描くと下記のようなイメージです。\nsequenceDiagram participant ls as Lambda Service participant eapi as Extension API participant tapi as Telemetry API participant main as Extension (main) participant sub as Extension (subscriber) alt Init ls -\u003e\u003e main: start (init extensions in /ops/extensions) main -\u003e\u003e eapi: POST /extension/register eapi -\u003e\u003e main: OK + ExtensionIdentifier main -\u003e\u003e sub: Start HTTP (or TCP) server sub -\u003e\u003e main: OK + listenAddress main -\u003e\u003e tapi: PUT /telemetry with ExtensionIdentifier + listenAddress tapi -\u003e\u003e main: OK end loop Repeat main -\u003e\u003e eapi: GET /extension/event/next eapi -\u003e\u003e main: INVOKE/SHUTDOWN event tapi -\u003e\u003e sub: platform/extension/function event sub -\u003e\u003e sub: handle event end 上記の通り、Extension からコールする Telemetry API としては PUT /telemetry の Subscribe API のみです。\nTelemetry API の詳細 ここでは Telemetry API の詳細として、下記の 2 点について整理します。\nSubscribe API - PUT /telemetry について Subscriber に対して送信されるイベントについて 1. Subscribe API - PUT /telemetry について Subscribe API は、 Extension からコールする Telemetry API として唯一公開されている API で、各イベントを受け取るための Subscriber の登録を行う API です。\n仕様については下記公式ドキュメントに記載されていますが、一部載っていない項目もあったのであらためてここで整理します。\nLambda Telemetry API reference - AWS Lambda メソッドとエンドポイント\nPUT http://${AWS_LAMBDA_RUNTIME_API}/2022-07-01/telemetry リクエストヘッダ\nContent-Type: application/json Lambda-Extension-Identifier: POST /extension/register のレスポンスヘッダに含まれる ExtensionIdentifier リクエストボディ\nschemaVersion: 2022-07-01 (required) destination: Subscriber として起動しているサーバの情報 (required) protocol: プロトコル (HTTP or TCP) URI: アドレス (\u0026ldquo;URI\u0026rdquo; は小文字じゃなくて大文字) types: 受信したいイベントの種類の配列 (platform, function, extension) (required) buffering: Telemetry API 側でのイベントのバッファリング設定 (option) maxItems: 最大イベント数 maxBytes: 最大バイト数 timeoutMs: 最大秒数 (ミリ秒) 例 { \u0026#34;schemaVersion\u0026#34;: \u0026#34;2022-07-01\u0026#34;, \u0026#34;types\u0026#34;: [ \u0026#34;platform\u0026#34;, \u0026#34;function\u0026#34;, ], \u0026#34;buffering\u0026#34;: { \u0026#34;maxItems\u0026#34;: 1000, \u0026#34;maxBytes\u0026#34;: 262144, \u0026#34;timeoutMs\u0026#34;: 100 }, \u0026#34;destination\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;HTTP\u0026#34;, \u0026#34;URI\u0026#34;: \u0026#34;http://sandbox.localdomain:8080\u0026#34; } } Subscribe API をコールする際に注意したいのは、まず destination.URI の値としてはドメインが sandbox.localdomain である必要があります。それ以外のドメインで指定した場合、下記のようなエラーになります。\nstatusCode:403\nerrType:Telemetry.AccessDenied\nerrMessage:Only \u0026lsquo;sandbox.localdomain\u0026rsquo; hostname is supported.\nまた、上記の公式ドキュメントでは Lambda-Extension-Identifier に関する記述がありませんが、設定しない もしくは値が不正な場合は下記のようなエラーになります。\nstatusCode:403\nerrType:Extension.InvalidExtensionIdentifier\nerrMessage:Invalid Lambda-Extension-Identifier\n2. Subscriber に対して送信されるイベントについて Telemetry API から送信されるイベントは、大きく分けて Platform event, Function logs, Extension logs の 3 つのカテゴリに分けられ、各カテゴリ内で更にいつかの Event type に分類されています。\nPlatform event platform.initStart platform.initRuntimeDone platform.initReport platform.start platform.runtimeDone platform.report platform.restoreStart platform.restoreRuntimeDone platform.restoreReport platform.extension platform.telemetrySubscription platform.logsDropped Function logs function Extension logs extension 各 Event type ごとに schema が定義されていますが、 Event 全体の schema としては下記のようになっています。\n{ \u0026#34;time\u0026#34;: \u0026#34;イベントの発生日時\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Event type\u0026#34;, \u0026#34;record\u0026#34;: \u0026#34;各 Event type の schema\u0026#34; } 例えば、 Extension が登録されて起動状態になったことを示すイベント platform.extension は下記のようなオブジェクトになります。\n{ \u0026#34;time\u0026#34;: \u0026#34;2022-10-12T00:02:15.000Z\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;platform.extension\u0026#34;, \u0026#34;record\u0026#34;: { \u0026#34;events\u0026#34;: [ \u0026#34;INVOKE\u0026#34;, \u0026#34;SHUTDOWN\u0026#34; ], \u0026#34;name\u0026#34;: \u0026#34;my-telemetry-extension\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;Ready\u0026#34; } } その他、各 Event type の schema の詳細については下記の公式ドキュメントを参照してください。\nLambda Telemetry API Event schema reference - AWS Lambda Telemetry API を使う Extension を作ってみる では実際に Telemetry API を使う Extension を作ってみます。前回と同様に Go で実装します。\n今回作る Extension でやることは下記のとおりです。\nplatform と function のイベントを Subscribe する 受け取ったイベントの内容をログに出力する 以上です。\nExtension のメイン処理については前回と大きく変わらないので、 Telemetry API に関連するところだけ掻い摘んで説明します。\n実装の詳細については下記の GitHub リポジトリを参照してください。\naws-lambda-api-go/_examples/extension-using-telemetry-api at main · michimani/aws-lambda-api-go Telemetry API の Subscriber を起動 まずは、 Telemetry API からのイベントを受信してハンドリングするための Subscriber を起動します。\naws-lambda-api-go/subscriber.go at v0.2.0 · michimani/aws-lambda-api-go const defaultSubscriberPort = \u0026#34;1210\u0026#34; const address = \u0026#34;sandbox.localdomain:\u0026#34; + defaultSubscriberPort type TelemetryAPISubscriber struct { httpServer *http.Server logger *logger.Logger } func NewTelemetryAPISubscriber(l *logger.Logger) *TelemetryAPISubscriber { return \u0026amp;TelemetryAPISubscriber{ httpServer: nil, logger: l, } } func (s *TelemetryAPISubscriber) Start() (string, error) { s.logger.Info(\u0026#34;Starting on address:%s\u0026#34;, address) s.httpServer = \u0026amp;http.Server{Addr: address} http.HandleFunc(\u0026#34;/\u0026#34;, s.telemetryEventHandler) go func() { err := s.httpServer.ListenAndServe() if err != http.ErrServerClosed { s.logger.Error(\u0026#34;Unexpected stop on Http Server. err:%v\u0026#34;, err) s.Shutdown() } else { s.logger.Info(\u0026#34;Http Server closed. err:%v\u0026#34;, err) } }() return fmt.Sprintf(\u0026#34;http://%s/\u0026#34;, address), nil } TelemetryAPISubscriber を返す NewTelemetryAPISubscriber と、 TelemetryAPISubscriber のメソッドとして Start を実装しています。\nTelemetryAPISubscriber.Start で、 sandbox.localdomain:1210 のアドレスで HTTP サーバを起動しています。HTTP サーバでは / に対するリクエストを受けて、リクエスト (Telemetry API から送信されるイベント) をハンドリングする TelemetryAPISubscriber.telemetryEventHandler を実行するようにしています。\nサーバのアドレスはこのあとの Subscribe API をコールする際に必要になるので呼び出し元 (= Extension のメイン処理) に返します。\nSubscribe API をコールして Subscriber を登録 続いて、起動した Subscriber の情報と受信したいイベントを Subscribe API を使って登録します。\naws-lambda-api-go/client.go at v0.2.0 · michimani/aws-lambda-api-go func (c *Client) Subscribe(ctx context.Context, exId string, httpURI string) error { var bufTimeoutMs uint64 = 100 out, err := telemetry.Subscribe(ctx, c.alagoClient, \u0026amp;telemetry.SubscribeInput{ LambdaExtensionIdentifier: exId, DestinationProtocol: telemetry.DestinationProtocolHTTP, DestinationURI: httpURI, TelemetryTypes: []telemetry.TelemetryType{ telemetry.TelemetryTypeFunction, telemetry.TelemetryTypePlatform, }, BufferTimeoutMs: \u0026amp;bufTimeoutMs, }) if err != nil { c.logger.Error(\u0026#34;An error occurred at telemetrySubscribe. err:%v\u0026#34;, err) return err } if out.StatusCode != http.StatusOK { return fmt.Errorf(\u0026#34;An error occurred at extension registration. statusCode:%d errType:%s errMessage:%s\u0026#34;, out.StatusCode, out.Error.ErrorType, out.Error.ErrorMessage) } return nil } 今回も、クライントライブラリとして aws-lambda-api-go を使っています。\n受信するイベントは、 platform と function にしています。 (extension はイベントの数が多すぎたので除外しました)\nエラーがあれば呼び出し元に返す、ただそれだけの処理になっています。\nイベントのハンドリング 最後に、Telemetry API から送信されたイベントのハンドリング処理です。\naws-lambda-api-go/subscriber.go at v0.2.0 · michimani/aws-lambda-api-go type telemetryAPIEvent struct { Record any `json:\u0026#34;record\u0026#34;` Type string `json:\u0026#34;type\u0026#34;` Time time.Time `json:\u0026#34;time\u0026#34;` } func (s *TelemetryAPISubscriber) telemetryEventHandler(w http.ResponseWriter, r *http.Request) { body, err := io.ReadAll(r.Body) if err != nil { s.logger.Error(\u0026#34;Failed to reading body. err:%v\u0026#34;, err) return } var events []telemetryAPIEvent _ = json.Unmarshal(body, \u0026amp;events) s.logger.Info(\u0026#34;Received %d events.\u0026#34;, len(events)) for i, e := range events { s.logger.Info(\u0026#34;%d: Time:%s Type:%s Record:%v\u0026#34;, i, e.Time.Format(time.RFC3339Nano), e.Type, e.Record) } events = nil } 今回はイベントの大枠となる schema に対応する構造体 telemetryAPIEvent だけ用意して、各イベントごとの schema に対応する構造体の定義については省略しています。\nまた、ハンドリングと言っても今回はログに出力するだけなので、特に難しいことはしていません。\nLambda 関数で使ってみる 前回と同様に RIE を使ってローカルで試そうと思ったんですが、どうやら Telemetry API はまだ RIE (が含まれたコンテナイメージ) では対応していないようだった1ので実際の環境にデプロイして試してみました。\n準備 実際に Lambda で実行して確認するまでの準備として、 Extension と関数をビルドします。\nExtension については Lambda Layer として作成するので、下記コマンドで build -\u0026gt; zip 化します。\nGOOS=linux GOARCH=amd64 go build -o ../bin/extensions/telemetry-api-extension main.go chmod +x bin/extensions/telemetry-api-extension cd bin \u0026amp;\u0026amp; zip -r extension.zip extensions/ Lambda 関数についてはただログを出力する処理だけあればよさそうなので、下記のような内容で Python で実装しました。\nimport json import logging def lambda_handler(event, context): logger = logging.getLogger() logger.setLevel(logging.INFO) logger.info(\u0026#39;[%s] Hello Telemetry API!\u0026#39;, context.aws_request_id) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } こちらも zip 化しておきます。\nzip function.zip main.py また、 Lambda 関数の作成時にアタッチする IAM Role が必要になるので、下記コマンドで作成しておきます。\n# IAM Role の作成 aws iam create-role \\ --role-name telemetry-api-function-role \\ --assume-role-policy-document \u0026#39;{\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: {\u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34;}, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;}]}\u0026#39; \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text # IAM Policy をアタッチ aws iam attach-role-policy \\ --role-name telemetry-api-function-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole Extension と それを使う関数を作成 準備ができたので、 Extension (Lambda Layer) と Lambda 関数のリソースを作成します。\nExtension は Lambda Layer として作成するので、下記コマンドでデプロイします。\naws lambda publish-layer-version \\ --layer-name \u0026#39;telemetry-api-extension\u0026#39; \\ --zip-file \u0026#39;fileb://bin/extension.zip\u0026#39; \\ --region ap-northeast-1 Lambda 関数については、下記コマンドで作成します。中で AWS CLI コマンドを叩いていてややこしく見えますが、各 ARN 記述する必要がなく、アカウント ID に依存しない形になっています。\naws lambda create-function \\ --function-name \u0026#39;function-with-telemetry-api-extension\u0026#39; \\ --runtime \u0026#39;python3.9\u0026#39; \\ --handler \u0026#39;main.lambda_handler\u0026#39; \\ --role $( aws iam get-role \\ --role-name \u0026#39;telemetry-api-function-role\u0026#39; \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text) \\ --zip-file fileb://function.zip \\ --layers $( aws lambda list-layer-versions \\ --layer-name \u0026#39;telemetry-api-extension\u0026#39; \\ --query \u0026#39;LayerVersions[0].LayerVersionArn\u0026#39; \\ --output text) \\ --region ap-northeast-1 実行してログを確認 Extension を設定した Lambda 関数が作成できたので、下記コマンドで実行してみます。\naws lambda invoke \\ --function-name \u0026#39;function-with-telemetry-api-extension\u0026#39; \\ --invocation-type \u0026#39;RequestResponse\u0026#39; \\ --cli-binary-format \u0026#39;raw-in-base64-out\u0026#39; \\ --region \u0026#39;ap-northeast-1\u0026#39; \\ --log-type \u0026#39;Tail\u0026#39; \\ /dev/stdout \\ | jq -sr \u0026#39;.[1] | .LogResult\u0026#39; \\ | base64 -d 上記コマンドを実行すると、 Lambda 関数実行時のログが手元で確認できます。\n1[Telemetry API Extension Client] INFO: Succeeded to register extension. 2[Telemetry API Subscriber] INFO: Starting on address:sandbox.localdomain:1210 3TELEMETRY Name: telemetry-api-extension State: Subscribed Types: [function,platform] 4[Telemetry API Subscriber] INFO: Received 1 events. 5[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:21:53.222Z Type:platform.initStart Record:map[initializationType:on-demand phase:init] 6[Telemetry API Extension Client] INFO: Waiting for next event... 7[Telemetry API Subscriber] INFO: Received 1 events. 8[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:21:53.277Z Type:platform.telemetrySubscription Record:map[name:telemetry-api-extension state:Subscribed types:[function platform]] 9EXTENSION Name: telemetry-api-extension State: Ready Events: [SHUTDOWN,INVOKE] 10START RequestId: eded6372-e614-4305-8516-f2697c3acfbc Version: $LATEST 11[Telemetry API Extension Client] INFO: Received invoke event. awsRequestId:eded6372-e614-4305-8516-f2697c3acfbc invokedAt:2022-12-11 10:21:53.381772081 +0000 UTC 12[Telemetry API Extension Client] INFO: Waiting for next event... 13[INFO] 2022-12-11T10:21:53.382Z eded6372-e614-4305-8516-f2697c3acfbc [eded6372-e614-4305-8516-f2697c3acfbc] Hello Telemetry API! 14END RequestId: eded6372-e614-4305-8516-f2697c3acfbc 15REPORT RequestId: eded6372-e614-4305-8516-f2697c3acfbc Duration: 17.38 ms Billed Duration: 18 ms Memory Size: 128 MB Max Memory Used: 45 MB Init Duration: 157.98 ms なんとなく冒頭に書いたような順番で処理がされてそうなことがわかると思います。\n例えば、 1 行目の\n[Telemetry API Extension Client] INFO: Succeeded to register extension.\nでは、メッセージが示すとおり Extension の登録が完了したことがわかります。\nまた、 4-5 行目の\n[Telemetry API Subscriber] INFO: Received 1 events. [Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:21:53.222Z Type:platform.initStart Record:map\nでは、 Telemetry API から Platform に関するイベントを受け取っていることがわかります。\nとは言え、一度関数を実行しただけでは雰囲気が掴みづらいと思うので、何度か invoke-function コマンドを実行した後に、下記のコマンドで CloudWatch Logs に出力されたログを確認してみます。\naws logs get-log-events \\ --log-group-name /aws/lambda/function-with-telemetry-api-extension \\ --log-stream-name \u0026#34;$( aws logs describe-log-streams \\ --log-group-name /aws/lambda/function-with-telemetry-api-extension \\ --query \u0026#39;max_by(logStreams[], \u0026amp;lastEventTimestamp).logStreamName\u0026#39; \\ --output text)\u0026#34; \\ --limit 100 \\ --query \u0026#39;events[].message\u0026#39; \\ --output text 得られる出力は下記のようになります。\n1[Telemetry API Extension Client] INFO: Succeeded to register extension. 2[Telemetry API Subscriber] INFO: Starting on address:sandbox.localdomain:1210 3TELEMETRY\tName: telemetry-api-extension\tState: Subscribed\tTypes: [function,platform] 4[Telemetry API Extension Client] INFO: Waiting for next event... 5[Telemetry API Subscriber] INFO: Received 1 events. 6[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:32:46.331Z Type:platform.initStart Record:map[initializationType:on-demand phase:init] 7[Telemetry API Subscriber] INFO: Received 1 events. 8[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:32:46.389Z Type:platform.telemetrySubscription Record:map[name:telemetry-api-extension state:Subscribed types:[function platform]] 9EXTENSION\tName: telemetry-api-extension\tState: Ready\tEvents: [SHUTDOWN,INVOKE] 10START RequestId: e63295fe-c7a1-48c2-ada2-c7044c697c12 Version: $LATEST 11[Telemetry API Extension Client] INFO: Received invoke event. awsRequestId:e63295fe-c7a1-48c2-ada2-c7044c697c12 invokedAt:2022-12-11 10:32:46.510767828 +0000 UTC 12[Telemetry API Extension Client] INFO: Waiting for next event... 13[INFO]\t2022-12-11T10:32:46.511Z\te63295fe-c7a1-48c2-ada2-c7044c697c12\t[e63295fe-c7a1-48c2-ada2-c7044c697c12] Hello Telemetry API! 14END RequestId: e63295fe-c7a1-48c2-ada2-c7044c697c12 15REPORT RequestId: e63295fe-c7a1-48c2-ada2-c7044c697c12\tDuration: 16.30 ms\tBilled Duration: 17 ms\tMemory Size: 128 MB\tMax Memory Used: 46 MB\tInit Duration: 177.38 ms 16[Telemetry API Subscriber] INFO: Received 7 events. 17[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:32:46.507Z Type:platform.initRuntimeDone Record:map[initializationType:on-demand phase:init status:success] 18[Telemetry API Subscriber] INFO: 1: Time:2022-12-11T10:32:46.508Z Type:platform.extension Record:map[events:[SHUTDOWN INVOKE] name:telemetry-api-extension state:Ready] 19[Telemetry API Subscriber] INFO: 2: Time:2022-12-11T10:32:46.508Z Type:platform.initReport Record:map[initializationType:on-demand metrics:map[durationMs:176.689] phase:init] 20[Telemetry API Subscriber] INFO: 3: Time:2022-12-11T10:32:46.51Z Type:platform.start Record:map[requestId:e63295fe-c7a1-48c2-ada2-c7044c697c12 version:$LATEST] 21[Telemetry API Subscriber] INFO: 4: Time:2022-12-11T10:32:46.511Z Type:function Record:[INFO]\t2022-12-11T10:32:46.511Z\te63295fe-c7a1-48c2-ada2-c7044c697c12\t[e63295fe-c7a1-48c2-ada2-c7044c697c12] Hello Telemetry API! 22[Telemetry API Subscriber] INFO: 5: Time:2022-12-11T10:32:46.526Z Type:platform.runtimeDone Record:map[metrics:map[durationMs:15.686 producedBytes:53] requestId:e63295fe-c7a1-48c2-ada2-c7044c697c12 spans:[map[durationMs:1.26 name:responseLatency start:2022-12-11T10:32:46.510Z] map[durationMs:0.021 name:responseDuration start:2022-12-11T10:32:46.511Z]] status:success] 23[Telemetry API Subscriber] INFO: 6: Time:2022-12-11T10:32:46.532Z Type:platform.report Record:map[metrics:map[billedDurationMs:17 durationMs:16.295 initDurationMs:177.377 maxMemoryUsedMB:46 memorySizeMB:128] requestId:e63295fe-c7a1-48c2-ada2-c7044c697c12 status:success] 24START RequestId: 29fffe63-ae9a-4dce-ad28-0a2bd117e5fe Version: $LATEST 25[Telemetry API Extension Client] INFO: Received invoke event. awsRequestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe invokedAt:2022-12-11 10:32:49.885648337 +0000 UTC 26[Telemetry API Extension Client] INFO: Waiting for next event... 27[INFO]\t2022-12-11T10:32:49.885Z\t29fffe63-ae9a-4dce-ad28-0a2bd117e5fe\t[29fffe63-ae9a-4dce-ad28-0a2bd117e5fe] Hello Telemetry API! 28END RequestId: 29fffe63-ae9a-4dce-ad28-0a2bd117e5fe 29REPORT RequestId: 29fffe63-ae9a-4dce-ad28-0a2bd117e5fe\tDuration: 1.30 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 46 MB 30[Telemetry API Subscriber] INFO: Received 4 events. 31[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:32:49.885Z Type:platform.start Record:map[requestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe version:$LATEST] 32[Telemetry API Subscriber] INFO: 1: Time:2022-12-11T10:32:49.886Z Type:function Record:[INFO]\t2022-12-11T10:32:49.885Z\t29fffe63-ae9a-4dce-ad28-0a2bd117e5fe\t[29fffe63-ae9a-4dce-ad28-0a2bd117e5fe] Hello Telemetry API! 33[Telemetry API Subscriber] INFO: 2: Time:2022-12-11T10:32:49.886Z Type:platform.runtimeDone Record:map[metrics:map[durationMs:1.065 producedBytes:53] requestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe spans:[map[durationMs:0.791 name:responseLatency start:2022-12-11T10:32:49.885Z] map[durationMs:0.032 name:responseDuration start:2022-12-11T10:32:49.886Z]] status:success] 34[Telemetry API Subscriber] INFO: 3: Time:2022-12-11T10:32:49.887Z Type:platform.report Record:map[metrics:map[billedDurationMs:2 durationMs:1.3 maxMemoryUsedMB:46 memorySizeMB:128] requestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe status:success] 35[Telemetry API Extension Client] INFO: Received shutdown event. reason:spindown Telemetry API から送信されるイベントは Subscribe API をコールするときにリクエストボディにセットする buffering の設定に従ってバッファリングされるため実際のイベント発生時とは多少のラグがあります。ただ、 24-34 行目あたりは普段の Lambda 関数実行時に出力されるログと、Telemetry API のイベントとがわかりやすいかと思います。\n24START RequestId: 29fffe63-ae9a-4dce-ad28-0a2bd117e5fe Version: $LATEST 25[Telemetry API Extension Client] INFO: Received invoke event. awsRequestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe invokedAt:2022-12-11 10:32:49.885648337 +0000 UTC 26[Telemetry API Extension Client] INFO: Waiting for next event... 27[INFO]\t2022-12-11T10:32:49.885Z\t29fffe63-ae9a-4dce-ad28-0a2bd117e5fe\t[29fffe63-ae9a-4dce-ad28-0a2bd117e5fe] Hello Telemetry API! 28END RequestId: 29fffe63-ae9a-4dce-ad28-0a2bd117e5fe 29REPORT RequestId: 29fffe63-ae9a-4dce-ad28-0a2bd117e5fe\tDuration: 1.30 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 46 MB 30[Telemetry API Subscriber] INFO: Received 4 events. 31[Telemetry API Subscriber] INFO: 0: Time:2022-12-11T10:32:49.885Z Type:platform.start Record:map[requestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe version:$LATEST] 32[Telemetry API Subscriber] INFO: 1: Time:2022-12-11T10:32:49.886Z Type:function Record:[INFO]\t2022-12-11T10:32:49.885Z\t29fffe63-ae9a-4dce-ad28-0a2bd117e5fe\t[29fffe63-ae9a-4dce-ad28-0a2bd117e5fe] Hello Telemetry API! 33[Telemetry API Subscriber] INFO: 2: Time:2022-12-11T10:32:49.886Z Type:platform.runtimeDone Record:map[metrics:map[durationMs:1.065 producedBytes:53] requestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe spans:[map[durationMs:0.791 name:responseLatency start:2022-12-11T10:32:49.885Z] map[durationMs:0.032 name:responseDuration start:2022-12-11T10:32:49.886Z]] status:success] 34[Telemetry API Subscriber] INFO: 3: Time:2022-12-11T10:32:49.887Z Type:platform.report Record:map[metrics:map[billedDurationMs:2 durationMs:1.3 maxMemoryUsedMB:46 memorySizeMB:128] requestId:29fffe63-ae9a-4dce-ad28-0a2bd117e5fe status:success] 各行のログは下記のような情報を示しています。\nLambda 関数が Invoke された Extension API の GET /extension/event/next のレスポンスとして INVOKE イベントを取得 再度 Extension API の GET /extension/event/next をコールして次のイベントを待機 Lambda 関数の logger.info() が出力しているログ Lambda 関数の終了 Lambda 関数の実行レポート Subscriber が Telemetry API から、バッファリングされていた 4 つのイベントを受信した platform.start イベント (24 行目: Lambda 関数の Invoke に対応) function イベント (27 行目: Lambda 関数内でのログ出力に対応) platform.runtimeDone イベント (28 行目: Lambda 関数の終了に対応) platform.report イベント (29 行目: Lambda 関数の実行レポートに対応) 以上で、 Telemetry API によってどのようなイベントが取得できるのか、実際の Lambda 関数の実行と合わせて確認ができました。\nまとめ AWS Lambda の Telemetry API を使った Lambda Extension を作ってみた話でした。\nTelemetry API のリリース情報を見たときは「お、なんか凄そう？」という感想だけでしたが、実際にこれを使う Extension を作ってみてどのような挙動をするものなのか、理解ができた気がします。\n今回はただ受け取ったイベントをログとして出力するだけでしたが、例えば function イベントを取得して Lambda 関数が出力しているログの文字列を解析して独自のレポートを出力する みたいなことをやると面白そうだなと思います。\n弁護士ドットコム Advent Calendar 2022 、明日の担当は @shellme です。お楽しみに。\nSubscribe API をコールするところで 404 Page Not Found が返ってきました\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-lambda-extension-using-telemetry-api/",
    "title": "Telemetry API を使う Lambda Extension を作ってみた"
  },
  {
    "contents": "この記事は AWS LambdaとServerless Advent Calendar 2022 7 日目の記事です。\n最近 Lambda の Runtime API について調べていたところで Extension API というものもある事に気づきました。どうやら Lambda Extension を自作するときに使う API のようだったので、その API を使いたいがために実際に Lambda Extension を自作してみました。\nLambda Extension とは 実行環境のライフサイクルと Extension Init Invoke Shutdown Go で Lambda Extension を作ってみる 自作する Extension の概要 実装 Extension のメイン処理 POST /extension/register で実行環境に Extension を登録 GET /extension/event/next で次のイベントを取得・ハンドリング 自作した Extension を Lambda 関数で使う Extension の実行可能ファイルを作成 Lambda 関数のコード Dockerfile ローカルで起動して invoke まとめ Lambda Extension とは Lambda Extension (以下、 Extension) は、 Lambda 関数の Runtime の一部、もしくは実行環境内で独立したプロセスとして様々な機能を実行できる機能です。それぞれ、前者は内部拡張機能、後者は外部拡張機能と呼ばれ、この記事内で触れる Extension は後者の外部拡張機能を指すものとします。\nExtension は、 AWS が提供しているもの、 AWS Lambda パートナー 1 が提供しているもの、および今回紹介するように独自の Extension を作成して使用することができます。 最近だと AWS からリリースされた AWS Parameters and Secrets Lambda Extension 2 3 が注目されていました。\nLambda は実行環境上で関数を呼び出しますが、 Extension はその実行環境が作成される際に開始されます。\nそして、関数と同じ実行環境上で起動するため Extension 自体のパフォーマンス (初期化にかかる時間、処理時間、メモリ消費量など) は関数のパフォーマンスにも影響します。そのため、 Extension を自作する際には、そのあたりにも注意が必要です。\n参考: Lambda extensions - AWS Lambda 実行環境のライフサイクルと Extension Lambda の実行環境のライフサイクルには、 Init 、 Invoke 、 Shutdown という 3 つのフェーズがあります。各フェーズでは実行環境の作成や関数の Invoke、実行環境の破棄などを行い、 Extension は各フェーズで Extension API を使用して Runtime への登録や実行環境で発生するイベントの検知を行い、 Extension としての処理を実行します。\nsequenceDiagram participant c as Client participant ls as Lambda Service participant r as Runtime participant e as Extension alt Init ls -\u003e\u003e e: init extensions in /ops/extensions e -\u003e\u003e ls: POST /extension/register e -\u003e\u003e ls: GET /extension/event/next end loop alt Invoke c -\u003e\u003e ls: invoke ls -\u003e\u003e r: invoke ls -\u003e\u003e e: response of GET /extension/event/next (INVOKE event) r -\u003e\u003e ls: /response (Runtime API) ls -\u003e\u003e c: response e -\u003e\u003e ls: GET /extension/event/next end end alt Shutdown ls -\u003e\u003e r: SIGTERM ls -\u003e\u003e e: response of GET /extension/event/next (SHUTDOWN event) end Init 実行環境の作成 関数のハンドラ外部にあるコードの実行 所謂　\u0026ldquo;コールドスタート\u0026rdquo;　時に実行されるフェーズ Extension との関係 /opt/extensions ディレクトリ配下にある実行可能ファイルを Extension として解釈し、それらを並列で実行 (開始) 開始された Extension は Extension POST /extension/register Extension API をコールして Runtime に Extension を登録 Invoke 関数のハンドラを呼び出す = 関数の実行 Extension との関係 GET /extension/event/next Extension API をコールして Invoke イベントを取得 Shutdown 一定期間 Invoke されなかったときに実行されるフェーズ 独立したプロセスとして動く Extension を停止させた上で、実行環境を破棄 Extension との関係 GET /extension/event/next Extension API をコールして Shutdown イベントを取得 参考: Lambda Extensions API - AWS Lambda Go で Lambda Extension を作ってみる これまでの内容を踏まえて、実際に Extension を作ってみます。\nExtension は関数とは独立したプロセスとして実行されるため、実装する際に使用する言語は関数と合わせる必要はありません。今回は下記リポジトリの実装例を参考にして Go で実装してみます。\naws-samples/aws-lambda-extensions: A collection of sample extensions to help you get started with AWS Lambda Extensions 自作する Extension の概要 今回実装する Extension でやりたいことは、下記のとおりです。\n同一実行環境で Invoke された関数に対応する Request ID をメモリ上に保持する 関数から任意のタイミングで Extension に対して GET リクエストを送ることで、保持している Request ID の一覧を取得できる 1 は、 Lambda の Invoke フェーズにて Invoke イベントを検知して、そのイベント内に含まれる Request ID をメモリ上に保持します。\n2 は、 Extension のサブプロセス (goroutine) として HTTP API サーバを起動し、 特定のエンドポイントに対するリクエストのレスポンスとしてメモリ上に保持している Request ID のリストを JSON 形式で返します。\nシーケンス図にするとこんな感じです。\nsequenceDiagram participant c as Client participant ls as Lambda Service participant r as Runtime participant f as Function participant e as Extension participant m as On Memory participant s as Extension's HTTP API Server alt Init ls -\u003e\u003e e: init e -\u003e\u003e ls: POST /extension/register e -\u003e\u003e s: start e -\u003e\u003e m: init invocation history e -\u003e\u003e ls: GET /extension/event/next end loop alt Invoke c -\u003e\u003e ls: invoke ls -\u003e\u003e r: invoke ls -\u003e\u003e e: response of GET /extension/event/next (INVOKE event) e -\u003e\u003e m: save Request ID to history e -\u003e\u003e ls: GET /extension/event/next r -\u003e\u003e f: execute main handler f -\u003e\u003e s: GET localhost:1203/invocations s -\u003e\u003e m: read history m -\u003e\u003e s: s -\u003e\u003e f: {\"invocations\": [{\"awsRequestId\":\"id1\", \"invocatedAt\":\"2022-12-04T00:15:06.992783032Z\"}]} f -\u003e\u003e r: r -\u003e\u003e ls: /response (Runtime API) ls -\u003e\u003e c: response end end alt Shutdown ls -\u003e\u003e r: SIGTERM ls -\u003e\u003e e: response of GET /extension/event/next (SHUTDOWN event) end 実装 実装したものは下記リポジトリに置いています。\nmichimani/invocation-history-extension: This is a Extension for AWS Lambda Function that records history of invocation at the same runtime environment. この記事内では、主に Extension API を利用するところについて触れます。\nExtension のメイン処理 main.go#main では下記の処理を順に実行しています。\nExtension API をコールするための Context を生成 シグナルハンドリング用の goroutine を起動 Extension API POST /extension/register をコールして実行環境に Extension を登録 Invocation の履歴を返す HTTP API サーバを起動 Extension API GET /extension/event/next をコールして各イベントをハンドリングする処理を開始 1,2,4 は特に目新しいポイントはないので、 3 と 4 について 各 Extension API の仕様とともに実装を見ていきます。\nPOST /extension/register で実行環境に Extension を登録 実行環境のライフサイクルと Extension の関係のところで書いたとおり、 Extension のコードが実行されたときに Extension API の /extension/register エンドポイントに POST リクエストを送ることで Extension を登録する必要があります。\nこの API ではリクエストヘッダおよびリクエストボディに必須の項目があります。\nまず、リクエストヘッダの Lambda-Extension-Name には Extension の名前をセットする必要があります。セットする値は、 Extension の実行可能ファイルのファイル名です。今回の Extension の場合、 invocation-history-extension というファイル名で /opt/extensions 配下に実行ファイルを配置するようにしているので invocation-history-extension を値としてセットします。\n次にリクエストボディですが、この Extension でハンドリングしたいイベントを配列で指定します。 イベントの種類は INVOKE と SHUTDOWN のみで、今回は両方のイベントをハンドリングしたいので両方指定します。\nAPI のレスポンスヘッダ Lambda-Extension-Identifier には登録が完了した結果として一意な値が設定されています。この値は後にコールする API のリクエストヘッダにセットする必要があるので保持しておきます。\nExtension API を実行するには単純に HTTP リクエストを送信すればよいのですが、今回はその部分を aws-lambda-api-go というクライアントライブラリを 自作 使用して実装します。\nmichimani/aws-lambda-api-go: This is a client library for Go language to use AWS Lambda\u0026amp;rsquo;s Runtime API, Extension API, Telemetry API, and Logs API. import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/michimani/aws-lambda-api-go/alago\u0026#34; \u0026#34;github.com/michimani/aws-lambda-api-go/extension\u0026#34; ) type Client struct { alagoClient *alago.Client logger *Logger extensionIdentifier string } func (c *Client) Register(ctx context.Context, extensionName string) error { out, err := extension.Register(ctx, c.alagoClient, \u0026amp;extension.RegisterInput{ LambdaExtensionName: extensionName, Events: []extension.EventType{ extension.EventTypeInvoke, extension.EventTypeShutdown, }, }) if err != nil { return fmt.Errorf(\u0026#34;An expected error occurred at extension registration. err:%v\u0026#34;, err) } if out.StatusCode != http.StatusOK { return fmt.Errorf(\u0026#34;An error occurred at extension registration. statusCode:%d errType:%s errMessage:%s\u0026#34;, out.StatusCode, out.Error.ErrorType, out.Error.ErrorMessage) } c.logger.Info(\u0026#34;Succeeded to register extension.\u0026#34;) c.extensionIdentifier = out.LambdaExtensionIdentifier return nil } クライアントライブラリの extension.Register 関数に対して extension.RegisterInput に Extension の名前とハンドリングしたいイベントをセットして渡すだけで POST /extension/register API をコールすることができます。簡単ですね。\nAPI のレスポンスは extension.RegisterOutput として取得できるので、 RegisterOutput.LambdaExtensionIdentifier の値を保持しておきます。\n参考: Lambda Extensions API | Register - AWS Lambda GET /extension/event/next で次のイベントを取得・ハンドリング 実行環境のライフサイクルと Extension の関係のところで書いたとおり、 Lambda の実行環境は関数の invoke および実行環境の破棄時にそれぞれ INVOKE と SHUTDOWN のイベントを発行します。発行します、と書きましたが、実際には GET /extension/event/next API がリクエストをブロックして、実行環境からイベントが発行されるタイミングでレスポンスが返るという動きになります。なので、この API をコールするための http.Client のタイムアウトは長め (もしくは 0) にしておく必要があります。\nこの API は、 Register API のレスポンスヘッダ Lambda-Extension-Identifier に含まれていた値をリクエストヘッダ Lambda-Extension-Identifier にセットしてコールします。リクエストボディは不要です。\nこちらも Register と同じクライアントライブラリを使って、イベントのハンドリングと合わせて下記のように実装しています。\nfunc (c *Client) PollingEvent(ctx context.Context) (bool, error) { c.logger.Info(\u0026#34;Waiting for next event...\u0026#34;) out, err := extension.EventNext(ctx, c.alagoClient, \u0026amp;extension.EventNextInput{ LambdaExtensionIdentifier: c.extensionIdentifier, }) if err != nil { return false, err } if out.StatusCode != http.StatusOK { return false, fmt.Errorf(\u0026#34;An error occurred at calling /extension/event/next API. statusCode:%d errType:%s errMessage:%s\u0026#34;, out.StatusCode, out.Error.ErrorType, out.Error.ErrorMessage) } switch out.EventType { case eventTypeInvoke: now := time.Now().UTC() saveInvocationHistory(out.RequestID, \u0026amp;now) c.logger.Info(\u0026#34;Succeeded to save new history. awsRequestId:%s invokedAt:%v\u0026#34;, out.RequestID, now) case eventTypeShutdown: c.logger.Info(\u0026#34;Received shutdown event. reason:%s\u0026#34;, out.ShutdownReason) c.logger.Info(\u0026#34;Truncate invocation history.\u0026#34;) for _, h := range History.Invocations { c.logger.Info(\u0026#34;%+v\u0026#34;, *h) } return false, nil default: return false, fmt.Errorf(\u0026#34;Cannot handle event. eventType:%s\u0026#34;, out.EventType) } return true, nil } イベントの種類が INVOKE のときは、レスポンスに含まれる Request ID をメモリ上に保存しています。 一方、イベントの種類が SHUTDOWN のときは、それまでに保存していた Request ID をすべてログに出力するようにしています。\nメソッド名にもあるように、実質イベントをポーリングしているような処理になるので、このメソッドの返り値としては bool と error を返すようにしていて、 bool が true であれば呼び出し元 (今回であれば main.go#processEvents) で再度このメソッドを実行してポーリングを始めるような形にしています。\n参考: Lambda Extensions API | Next - AWS Lambda 自作した Extension を Lambda 関数で使う では、自作した Extension を実際に使ってみます。\nExtension の利用方法としては下記の 3 通りがあります。\nLambda Layer として publish して使う Lambda 関数のコードと一緒に zip に同梱して使う コンテナイメージに同梱して使う 今回はコンテナイメージに同梱してローカル環境で自作 Extension を使ってみます。コンテナイメージにすることで RIE を使ってローカル環境でも実際の Lambda の実行環境に近い形で動作確認ができます。4\nExtension の実行可能ファイルを作成 まずは Extension を実行可能ファイルとして用意します。\n今回は Go で実装したので、下記コマンドでビルドして実行可能ファイルを生成し、 zip 化します。\nGOOS=linux GOARCH=amd64 go build -o bin/extensions/invocation-history-extension main.go \\ \u0026amp;\u0026amp; chmod +x bin/extensions/invocation-history-extension \\ \u0026amp;\u0026amp; cd bin \\ \u0026amp;\u0026amp; zip -r extension.zip extensions/ もしくは、今回実装した Extension については GitHub の Releases から zip でダウンロードできるので、そちらから取得してきます。\nReleases · michimani/invocation-history-extension Lambda 関数のコード 今回実装した Extension は localhost:1203 で HTTP API サーバを起動しているので、そこにリクエストを送ってそのレスポンスを関数のレスポンスとするような Lambda 関数を用意します。\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; runtime \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambdacontext\u0026#34; ) type Response struct { Message string `json:\u0026#34;message\u0026#34;` Invocations []invocation `json:\u0026#34;invocations\u0026#34;` } func handleRequest(ctx context.Context) (*Response, error) { log.Println(\u0026#34;start handler\u0026#34;) defer log.Println(\u0026#34;end handler\u0026#34;) lc, ok := lambdacontext.FromContext(ctx) if !ok { return nil, fmt.Errorf(\u0026#34;Failed to get Lambda context from context.\u0026#34;) } history, err := getInvocationHistory() if err != nil { return nil, err } return \u0026amp;Response{ Message: fmt.Sprintf(\u0026#34;Current request ID is %s\u0026#34;, lc.AwsRequestID), Invocations: history, }, nil } func init() { log.Println(\u0026#34;cold start\u0026#34;) } func main() { runtime.Start(handleRequest) } // Struct of response from Invocation History Extension - GET /invocations API type resultFromExtension struct { Invocations []invocation `json:\u0026#34;invocations\u0026#34;` } type invocation struct { AWSRequestID string `json:\u0026#34;awsRequestId\u0026#34;` InvocatedAt *time.Time `json:\u0026#34;invocatedAt\u0026#34;` } const invocationsEndpoint = \u0026#34;http://localhost:1203/invocations\u0026#34; // Get invocation history from Invocation History Extension IPC. func getInvocationHistory() ([]invocation, error) { req, err := http.NewRequestWithContext(context.Background(), \u0026#34;GET\u0026#34;, invocationsEndpoint, nil) if err != nil { return nil, err } // call Extension API client := http.Client{} res, err := client.Do(req) if err != nil { return nil, err } defer res.Body.Close() buf := new(bytes.Buffer) if _, err := buf.ReadFrom(res.Body); err != nil { return nil, err } bodyString := buf.String() if res.StatusCode != http.StatusOK { return nil, fmt.Errorf(\u0026#34;Failed to get invocation history by using extension. statusCode:%d body:%s\u0026#34;, res.StatusCode, bodyString) } exRes := resultFromExtension{} if err := json.Unmarshal([]byte(bodyString), \u0026amp;exRes); err != nil { return nil, err } return exRes.Invocations, nil } 別プロセスで起動している Extension の HTTP API サーバに GET リクエストを送信し、そのレスポンスに現在の Request ID を付与したものを関数のレスポンスとして返すような実装になっています。具体的なレスポンスの内容はこのあと確認します。\nDockerfile Extension をコンテナイメージに同梱する場合は Extension の実行可能なファイルをイメージ内の /opt/extensions 以下に配置すればよいので、下記のような Dockerfile を作成することで Extension を同梱したイメージを作成することができます。\nFROM public.ecr.aws/lambda/provided:al2 as build RUN yum install -y golang unzip RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main RUN mkdir -p /opt ## ここは GitHub から直接取得するようにしてもよい # ADD https://github.com/michimani/invocation-history-extension/releases/download/v0.2.0/extension.zip ./ ADD ./bin/extension.zip ./ RUN unzip extension.zip -d /opt RUN rm extension.zip FROM public.ecr.aws/lambda/provided:al2 COPY --from=build /main /main COPY entry.sh / RUN chmod 755 /entry.sh RUN mkdir -p /opt/extensions WORKDIR /opt/extensions COPY --from=build /opt/extensions . ENTRYPOINT [ \u0026#34;/entry.sh\u0026#34; ] CMD [\u0026#34;/main\u0026#34;] AWS やパートナーから提供されている Extension で実行可能ファイルそのものが公開されていない場合、コンテナイメージとして公開されているものがあればそれを使えばよくて、 Lambda Layer として公開されているものについては下記の AWS CLI コマンドを実行することで zip 形式で Extension をダウンロードすることができます。 (AWS Parameters And Secrets Lambda Extension の場合)\ncurl $( aws lambda get-layer-version-by-arn \\ --arn \u0026#39;arn:aws:lambda:ap-northeast-1:133490724326:layer:AWS-Parameters-and-Secrets-Lambda-Extension:2\u0026#39; \\ --query \u0026#39;Content.Location\u0026#39; \\ --output text ) --output extension.zip その後 unzip して実行可能なファイルのみをコンテナイメージに同梱すれば、自作した Extension と同様にコンテナイメージで使用することができます。\nローカルで起動して invoke Dockerfile の準備もできたので、あとは docker build して docker run すればローカルで Lambda の実行環境が起動します。\ndocker build -t invocation-history-ex-func:local . \\ \u0026amp;\u0026amp; docker run \\ --rm \\ -p 9000:8080 \\ invocation-history-ex-func:local これで localhost:9000 で Lambda の実行環境が起動しているので、そこに対して curl でリクエストを送って invoke します。\ncurl \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ http://localhost:9000/2015-03-31/functions/function/invocations すると下記のようなレスポンスが得られます。\n{ \u0026#34;message\u0026#34;: \u0026#34;Current request ID is e65d6f55-2335-47e9-88f9-a851e7d539d5\u0026#34;, \u0026#34;invocations\u0026#34;: [ { \u0026#34;awsRequestId\u0026#34;: \u0026#34;e65d6f55-2335-47e9-88f9-a851e7d539d5\u0026#34;, \u0026#34;invocatedAt\u0026#34;: \u0026#34;2022-12-06T13:25:52.447847893Z\u0026#34; } ] } 続けて invoke すると\n{ \u0026#34;message\u0026#34;: \u0026#34;Current request ID is 4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f\u0026#34;, \u0026#34;invocations\u0026#34;: [ { \u0026#34;awsRequestId\u0026#34;: \u0026#34;e65d6f55-2335-47e9-88f9-a851e7d539d5\u0026#34;, \u0026#34;invocatedAt\u0026#34;: \u0026#34;2022-12-06T13:25:52.447847893Z\u0026#34; }, { \u0026#34;awsRequestId\u0026#34;: \u0026#34;4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f\u0026#34;, \u0026#34;invocatedAt\u0026#34;: \u0026#34;2022-12-06T13:27:17.244096344Z\u0026#34; } ] } それまでに同じ実行環境で invoke された際の Request ID が履歴となって保持されていることがわかります。\nLambda の実行環境が出力しているログは下記のようになっています。\n1START RequestId: e65d6f55-2335-47e9-88f9-a851e7d539d5 Version: $LATEST 206 Dec 2022 13:25:52,433 [INFO] (rapid) External agent invocation-history-extension (c96c9db2-076e-4ac7-9572-f98d4091d0e8) registered, subscribed to [INVOKE SHUTDOWN] 3[Invocation History Extension] INFO: Succeeded to register extension. 4[Invocation History Extension] INFO: Waiting for next event... 52022/12/06 13:25:52 cold start 62022/12/06 13:25:52 start handler 7[Invocation History Extension] INFO: Succeeded to save new history. awsRequestId:e65d6f55-2335-47e9-88f9-a851e7d539d5 invokedAt:2022-12-06 13:25:52.447847893 +0000 UTC 8[Invocation History Extension] INFO: Waiting for next event... 92022/12/06 13:25:52 end handler 10END RequestId: e65d6f55-2335-47e9-88f9-a851e7d539d5 11REPORT RequestId: e65d6f55-2335-47e9-88f9-a851e7d539d5 Init Duration: 0.22 ms Duration: 28.23 ms Billed Duration: 29 ms Memory Size: 3008 MB Max Memory Used: 3008 MB 12START RequestId: 4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f Version: $LATEST 132022/12/06 13:27:17 start handler 14[Invocation History Extension] INFO: Succeeded to save new history. awsRequestId:4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f invokedAt:2022-12-06 13:27:17.244096344 +0000 UTC 15[Invocation History Extension] INFO: Waiting for next event... 162022/12/06 13:27:17 end handler 17END RequestId: 4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f 18REPORT RequestId: 4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f Duration: 1.53 ms Billed Duration: 2 ms Memory Size: 3008 MB Max Memory Used: 3008 MB 1 〜 11 行目が 1 回目の invoke によるログで、このときのみ Register API を実行したログが出力されていることがわかります。\nこの状態から ctr + C で実行環境のプロセスを終了させると下記のようなログが出力されます。\n^C06 Dec 2022 13:31:54,898 [INFO] (rapid) Received signal signal=interrupt 06 Dec 2022 13:31:54,898 [INFO] (rapid) Shutting down... 06 Dec 2022 13:31:54,898 [WARNING] (rapid) Reset initiated: SandboxTerminated [Invocation History Extension] INFO: Received shutdown event. reason:SandboxTerminated [Invocation History Extension] INFO: Truncate invocation history. [Invocation History Extension] INFO: {AWSRequestID:e65d6f55-2335-47e9-88f9-a851e7d539d5 InvocatedAt:2022-12-06 13:25:52.447847893 +0000 UTC} [Invocation History Extension] INFO: {AWSRequestID:4a61a5dc-7bbf-43cb-abd0-67ed0388cf5f InvocatedAt:2022-12-06 13:27:17.244096344 +0000 UTC} 06 Dec 2022 13:31:54,899 [INFO] (rapid) runtime exited ローカル環境ではプロセス終了 = 実行環境の破棄となります。出力されているログからも、 SHUTDOWN イベントを受け取ってハンドリングされていることがわかります。\nまとめ AWS Lambda の Extension API を使ってみたいがために Lambda Extension を自作してみた話でした。\nLambda Extension を自作してみて Extension API がやっていること、 Lambda の実行環境上での Extension のライフサイクル、 Lambda そのもののライフサイクルについて完全に理解できた気がします。\nと言っても Extension API にはまだ Init Error API と Exit Error API があり、今回はそれらを使っていません。また、 Lambda には Extension API の他にも Runtime API、 Telemetry API (Logs API) もあり、まだまだ何もわからない部分がたくさんありそうです。\n個人的に一時期 Lambda 熱が冷めていたんですが、最近また温まってきたので今後はこのあたりの API 群を追っていこうかなと思います。\nAWS Lambda 拡張機能パートナー - AWS Lambda \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAWS Parameters and Secrets Lambda Extension を発表 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSSM Parameter Store および Secrets Manager への問い合わせをキャッシュすることで再取得時のレイテンシーおよびコストを削減できるという機能\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nコンテナイメージを使った Lambda 関数のあれこれ - michimani.net \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-lambda-extension-written-in-go/",
    "title": "AWS Lambda の Extension API を使いたいがために Go で Lambda Extension を自作してみた"
  },
  {
    "contents": "Lambda 関数のパッケージタイプとしてコンテナイメージを指定できるようになってからもう 2 年が経とうとしています。最近になってちゃんと触るようになって色々わかってきたことがあるので、やったことをまとめておきます。\nコンテナイメージを使った Lambda 関数の概要 Dockerfile AWS ベースイメージを使う 代替ベースイメージを使う ローカル環境での実行 RIE によるローカルでの実行 イベントの再現 コールドスタートとウォームスタート Lambda Runtime API の利用 Lambda Layer (Extension) の利用 CI/CD 基本的な更新方法 CI/CD を構築する際の注意点 GitHub Actions による CI/CD Lambda Layer (Extension) の使い方 概要 Parameter and Secret Lambda Extension を使ったサンプル Extension のバイナリ取得 コンテナイメージの作成 実装 Extension を使った場合のログ まとめ コンテナイメージを使った Lambda 関数の概要 まずは概要です。\nと言っても難しいことはなく、\nパッケージタイプとして Image を、 ImageURI として ECR Repository のイメージ URI を指定します コンテナイメージを使う場合はランタイムは指定しません 指定できるコンテナイメージの最大サイズは 10 GB となっています くらいです。\nその他の Lambda 関数の機能 (環境変数、 Layer (Extension) など) については、従来の Lambda 関数と同等です。\nDockerfile Lambda 関数用のコンテナイメージを作成する際、ベースとなるイメージは AWS が提供しているベースイメージを使う方法と、 alpine などの代替イメージを使う方法があります。それぞれメリット・デメリットがあるので簡単にまとめておきます。\nまた、今回は前提として Go で実装されたコードをもとにイメージを作成します。なので、下記ディレクトリ構成と main.go が存在していることとします。\n. ├── Dockerfile ├── entry.sh ├── go.mod ├── go.sum └── main.go package main import ( \u0026#34;log\u0026#34; runtime \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) type Response struct { Message string `json:\u0026#34;message\u0026#34;` } func handleRequest() (Response, error) { log.Println(\u0026#34;start handler\u0026#34;) defer log.Println(\u0026#34;end handler\u0026#34;) return Response{ Message: \u0026#34;Hello AWS Lambda\u0026#34;, }, nil } func init() { log.Println(\u0026#34;cold start\u0026#34;) } func main() { runtime.Start(handleRequest) } AWS ベースイメージを使う AWS ベースイメージを使う場合は下記のような Dockerfile になります。\nFROM public.ecr.aws/lambda/provided:al2 as build RUN yum install -y golang RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main FROM public.ecr.aws/lambda/provided:al2 COPY --from=build /main /main ENTRYPOINT [\u0026#34;/main\u0026#34;] AWS ベースイメージは ECR Public で公開されているので、 docker build するには ECR にログインしている必要があります。\nAWS ベースイメージには、 Lambda Layer (Extension) を使用するための機構やローカルで実行するための RIE (Runtime Interface Emulator) も含まれており、ビルド後のイメージサイズは大きくなります。\n後述しますが、 RIE だけをイメージに含めることもできるので、 AWS ベースイメージを使うシチュエーションとしては Lambda Layer (Extension) を使いたい場面となりそうです。\n代替ベースイメージを使う 代替のベースイメージ (今回は alpine) を使う場合は下記のような Dockerfile になります。\nFROM alpine as build RUN apk add go git RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main FROM alpine COPY --from=build /main /main ENTRYPOINT [ \u0026#34;/main\u0026#34; ] ベースに使うイメージのサイズにもよりますが、必要最低限の構成となるため AWS ベースイメージを使う場合と比較してビルド後のイメージサイズは小さくなります。\nイメージサイズが小さくなる分、 Lambda Layer (Extension) は使用できず、ローカルでの実行もできません。\nLambda Layer (Extension) を使用しない場合、AWS ベースイメージの代わりに代替となるベースイメージを使うことでイメージサイズを小さくできます。\nとはいえローカルでの実行はできたほうが嬉しいので、その場合は下記のように RIE だけを追加するような Dockerfile を用意します。\nFROM alpine as build RUN apk add go git RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main FROM alpine COPY --from=build /main /main ADD https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie /usr/bin/aws-lambda-rie RUN chmod 755 /usr/bin/aws-lambda-rie COPY entry.sh / RUN chmod 755 /entry.sh ENTRYPOINT [ \u0026#34;/entry.sh\u0026#34; ] CMD [ \u0026#34;/main\u0026#34; ] ここで出てくる entry.sh は下記の内容とします。\n#!/bin/sh if [ -z \u0026#34;${AWS_LAMBDA_RUNTIME_API}\u0026#34; ]; then exec /usr/bin/aws-lambda-rie \u0026#34;$@\u0026#34; else exec \u0026#34;$@\u0026#34; fi ローカル環境での実行 コンテナイメージを使った Lambda 関数はローカル環境での実行も容易です。\nRIE によるローカルでの実行 ローカル環境での実行には RIE (Runtime Interface Emulator) を使用します。\n前項で書いたように AWS ベースイメージを使用するか、代替ベースイメージに RIE を追加することでローカル環境で Lambda 関数を実行することができます。\nコンテナイメージ内に RIE が含まれている場合、下記の手順でローカルでの起動・実行が可能です。\ndocker build\ndocker build -t container-lambda-function:local . docker run\ndocker run \\ --rm \\ -p 9000:8080 \\ container-lambda-function:local (ポートは任意)\nコンテナを起動すると、 Lambda 関数が実行されるのではなく、実行可能な状態になります。\n標準出力には下記のようなログが出力されます。\n03 Nov 2022 13:39:12,623 [INFO] (rapid) exec \u0026#39;/main\u0026#39; (cwd=/var/task, handler=) invoke\n実際に Lambda 関数を実行するには、\nhttp://localhost:9000/2015-03-31/functions/function/invocations に対して GET もしくは POST リクエストを送ります。\ncurl \u0026#39;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#39; curl のレスポンスとしては下記のような出力が得られます。\n{\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda\u0026#34;} また、起動しているコンテナイメージの標準出力では、下記のような出力が得られます。\nSTART RequestId: b8a7e4ed-381d-4925-8a7a-48a90a4dd2e4 Version: $LATEST 2022/11/03 13:44:08 cold start 2022/11/03 13:44:08 start handler 2022/11/03 13:44:08 end handler END RequestId: b8a7e4ed-381d-4925-8a7a-48a90a4dd2e4 REPORT RequestId: b8a7e4ed-381d-4925-8a7a-48a90a4dd2e4 Init Duration: 0.34 msDuration: 10.43 ms Billed Duration: 11 ms Memory Size: 3008 MB Max Memory Used: 3008 MB これは実際の Lambda 関数が出力するログと同等の内容になっています。\n一方で、 RIE はイメージ内に含めずにローカル環境にインストールして使用することもできます。1 ただ、開発者のローカル環境に RIE をインストールしてもらうのは面倒なので、コンテナイメージ内に含めてしまうのが良いかなと思います。\nローカル環境に RIE をインストールしてそれを使う場合、コンテナ起動時のコマンドが下記のようになります。\ndocker run -d -v ~/.aws-lambda-rie/aws-lambda \\ --entrypoint /aws-lambda/aws-lambda-rie \\ -p 9000:8080 \\ container-lambda-function:local イベントの再現 Lambda 関数の実行時には API Gateway からの Payload、 S3 や SQS 等からのイベントを受け取ることがあります。\nローカル環境でそれらの受け取りを再現するには、 curl でのリクエスト時にメソッドを POST に、イベントの JSON をリクエストボディに入れることで再現できます。\n例えば、 SQS 駆動の Lambda 関数を再現したい場合は、下記のようなリクエストを送ります。\ncurl -X POST \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;Records\u0026#34;: [{\u0026#34;messageId\u0026#34;: \u0026#34;message-id\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;event-source\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;key\\\u0026#34;: \\\u0026#34;value\\\u0026#34;}\u0026#34;}]}\u0026#39; \\ \u0026#39;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#39; SQS 駆動の Lambda 関数の実装例はこちら。\nsqs-lambda-example/lambda at main · michimani/sqs-lambda-example コールドスタートとウォームスタート Lambda 関数の特徴として、コールドスタートとウォームスタートがあります。\nコンテナイメージを使った Lambda 関数のローカル実行ではこれらの再現も可能です。\n具体的な動作としては、 docker run でコンテナを起動してから最初に curl でリクエスト送った際の実行はコールドスタートとなり、それ以降はウォームスタートの挙動となります。起動中のコンテナを停止して起動しなおせば、またコールドスタートの挙動を確認できます。\n例であげている Go のコードでは、コールドスタート時のみ init() 関数が実行され、それ以降は実行されないことが確認できます。\nコールドスタート時のログ。\nSTART RequestId: b8a7e4ed-381d-4925-8a7a-48a90a4dd2e4 Version: $LATEST 2022/11/03 13:44:08 cold start 2022/11/03 13:44:08 start handler 2022/11/03 13:44:08 end handler END RequestId: b8a7e4ed-381d-4925-8a7a-48a90a4dd2e4 REPORT RequestId: b8a7e4ed-381d-4925-8a7a-48a90a4dd2e4 Init Duration: 0.34 msDuration: 10.43 ms Billed Duration: 11 ms Memory Size: 3008 MB Max Memory Used: 3008 MB ウォームスタート時のログ。\nSTART RequestId: d5016f63-3648-423c-bb7a-597548b83da1 Version: $LATEST 2022/11/03 14:00:20 start handler 2022/11/03 14:00:20 end handler END RequestId: d5016f63-3648-423c-bb7a-597548b83da1 REPORT RequestId: d5016f63-3648-423c-bb7a-597548b83da1 Duration: 1.41 ms Billed Duration: 2 ms Memory Size: 3008 MB Max Memory Used: 3008 MB コールドスタート時には init() 関数内で出力しているログがあり、さらに Init Duration の情報も出力されていますが、ウォームスタート時にはそれらがありません。\nLambda 関数のこの挙動によって DB へのコネクションが増え続ける罠がありますが、それについてもローカル環境で再現することができます。\nmichimani/lambda-rdb-test: In this repository, you can try to see how the number of DB connections changes depending on the implementation method when connecting to RDB from Lambda functions implemented in Go. Lambda Runtime API の利用 Lambda には Lambda Runtime API 2 というものが用意されており、実行中の Lambda 関数に関する情報を取得したり、動作を与えることができます。 RIE を使ったローカル実行では、Lambda Runtime API の挙動についても確認することができます。\nLambda Runtime API のエンドポイントは、Lambda 関数実行時に自動的に設定される AWS_LAMBDA_RUNTIME_API という環境変数に設定されており、 RIE を使ったローカル実行時にもこの環境変数は設定されています。\nGo では下記のような実装によって、実行中の Lambda 関数の Request ID を取得することができます。\nconst ( runtimeAPIEnvKey = \u0026#34;AWS_LAMBDA_RUNTIME_API\u0026#34; runtimeRequestIDHeaderName = \u0026#34;Lambda-Runtime-Aws-Request-Id\u0026#34; ) func getRequestID(client *http.Client) (string, error) { host := os.Getenv(runtimeAPIEnvKey) if host == \u0026#34;\u0026#34; { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;host is empty\u0026#34;) } url := fmt.Sprintf(\u0026#34;http://%s/2018-06-01/runtime/invocation/next\u0026#34;, host) req, err := http.NewRequestWithContext(context.Background(), \u0026#34;GET\u0026#34;, url, nil) if err != nil { return \u0026#34;\u0026#34;, err } res, err := client.Do(req) if err != nil { return \u0026#34;\u0026#34;, err } rHeader := res.Header rIds, exists := rHeader[runtimeRequestIDHeaderName] if !exists { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;\u0026#39;%s\u0026#39; header does not exists.\u0026#34;, runtimeRequestIDHeaderName) } if len(rIds) == 0 { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;Value of \u0026#39;%s\u0026#39; header is empty.\u0026#34;, runtimeRequestIDHeaderName) } return rIds[0], nil } 関数全体の実装はこちら。\nbase-of-lambda-container-image/main.go at main · michimani/base-of-lambda-container-image Lambda Layer (Extension) の利用 Lambda には Lambda Layer (Extension) という機能があります。ローカル実行ではこの機能の挙動も確認することができます。これに関しては後述します。\nCI/CD コンテナイメージを使った Lambda 関数の CI/CD について考えてみます。\n従来の Lambda 関数であれば、該当バージョンのコード一式を zip 化して S3 Bucket にアップロードし、 lambda update-function-code にてソースとなるコードをしてい\n基本的な更新方法 従来の Lambda 関数であれば、該当バージョンのコード一式を zip 化して更新、もしくは zip を S3 Bucket にアップロードして更新していましたが、コンテナイメージを使った Lambda の場合はフローが異なります。\nまず、変更後のコードでコンテナイメージを作成し、 ECR Repository に push します。 そして、 lambda update-function-code で ImageURI を更新します。\nこのときの URI はハッシュ付き URI でもよいですが、 latest 等のタグ付き URI でも可です。むしろ、タグ付き URI を使って更新したほうが、リソースを CFn や Terraform で管理している場合には差分が発生しないのでおすすめです。\nタグ付き URI で更新した場合でも、実際に利用されるイメージのハッシュ付き URI については lambda get-function で確認することができます。\naws lambda get-function \\ --function-name \u0026#39;lambda-parameters-extension-function\u0026#39; \\ --query \u0026#39;Code\u0026#39; | grep Uri \u0026#34;ImageUri\u0026#34;: \u0026#34;000000000000.dkr.ecr.ap-northeast-1.amazonaws.com/container-lambda-function:latest\u0026#34;, \u0026#34;ResolvedImageUri\u0026#34;: \u0026#34;000000000000.dkr.ecr.ap-northeast-1.amazonaws.com/container-lambda-function@sha256:c0b63302fa2ad2a78e6da8b37b8d4394c1b231a0963d82abd06d37dd4fd66277\u0026#34; 更新したあとは、実行基盤が変わってコールドスタートとなるタイミングから新しいコードでの実行となります。\nCI/CD を構築する際の注意点 コンテナイメージを使った Lambda 関数関連のリソースを定義する際には、いくつか依存関係がありリソース作成に関しては下記の順序で実施する必要があります。\nECR Repository を作成 Lambda 関数のコードを実装 ECR Repository に push Lambda 関数のリソースを作成 (対象となる ECR Repository にイメージが存在している必要があるため) CI/CD パイプラインを作成する際にも上記に注意しながら順番にリソースを定義していく必要があります。具体的には、まずは ECR に push するところまでのパイプラインを作って一度実行し、その後に関数を更新するステップを追加する、といった段階を踏むことになります。\nGitHub Actions による CI/CD コンテナイメージを使った Lambda 関数の更新を GitHub Actions で実施する場合の実装例です。\ndev ブランチへの push で dev 環境用の Lambda 関数を、 main ブランチへの push で production 環境用の Lambda 関数を、それぞれ更新するようなサンプルになっています。\nmichimani/container-lambda-cicd: コンテナイメージを利用する Lambda 関数の CI/CD のサンプル。 認証情報については、予め作成しておいた IAM Role の ARN のリポジトリの Secrets に設定しておいて、 yaml 内では ${{ secrets.ASSUME_ROLE_ARN }} で使用します。外部から設定が必要な情報はそれだけです。\nLambda Layer (Extension) の使い方 コンテナイメージを使用した Lambda 関数での Lambda Layer (Extension) の使い方についてです。\n概要 従来の Lambda 関数では、マネジメントコンソールから Layer を選択、もしくは Lambda リソース内の Layers に使用したいレイヤーを指定すればよかったですが、コンテナイメージを使用した Lambda 関数ではこの方法では使えません。\n方法としては、使用したい Layer (Extension) のバイナリをコンテナイメージに含めることで実現します。 配置するディレクトリも決まっており、 /opt/extensions 配下にバイナリを配置します。\nLayer (Extension) については AWS 公式/3rd party 問わずコンテナイメージとして公開されているものもありますが、公開されていないものもあります。コンテナイメージとして公開されていない AWS 公式の Layer (Extension) に関しては、対処の Layer (Extension) の ARN をもとに lambda get-layer-version-by-arn で取得できます。\n他の注意点として、 Dockerfile のところでも触れましたが、 Layer (Extension) を使用したい場合は AWS が提供しているベースイメージを使用する必要があります。\n具体的な方法については下記の AWS 公式ブログに記載されています。\nコンテナイメージ内でLambda レイヤーと拡張機能を動作させる | Amazon Web Services ブログ Parameter and Secret Lambda Extension を使ったサンプル 例として、先日発表されていた Parameter and Secret Lambda Extension をコンテナイメージを使用する Lambda 関数で使う場合を考えます。\nExtension のバイナリ取得 まずは Extension のバイナリを取得します。\nParameter and Secret Lambda Extension の ARN については下記の公式ドキュメントに記載されています。\nUse AWS Secrets Manager secrets in AWS Lambda functions - AWS Secrets Manager 東京 (ap-northeast-1) リージョンの ARN は\narn:aws:lambda:ap-northeast-1:133490724326:layer:AWS-Parameters-and-Secrets-Lambda-Extension:2\nなので、下記コマンドで zip 形式の Extension を取得します。\ncurl $( aws lambda get-layer-version-by-arn \\ --arn \u0026#39;arn:aws:lambda:ap-northeast-1:133490724326:layer:AWS-Parameters-and-Secrets-Lambda-Extension:2\u0026#39; \\ --query \u0026#39;Content.Location\u0026#39; --output text ) --output ps-ex.zip AWS 公式ブログ内では Dockerfile 内に記述されていましたが、 アクセスキーやシークレットを Dockerfile 内に記述したり環境変数で渡したりするのが面倒だった (直球) ので、ローカル環境にダウンロードしてくる方法をとっています。\nコンテナイメージの作成 下記のような Dockerfile でコンテナイメージを作成します。まずはローカルでの実行を想定しているため entry.sh を含めるようにしています。\n1FROM public.ecr.aws/lambda/provided:al2 as build 2RUN yum install -y golang unzip 3RUN go env -w GOPROXY=direct 4ADD go.mod go.sum ./ 5RUN go mod download 6ADD . . 7RUN go build -o /main 8RUN mkdir -p /opt 9ADD ./ps-ex.zip ./ 10RUN unzip ps-ex.zip -d /opt 11RUN rm ps-ex.zip 12 13FROM public.ecr.aws/lambda/provided:al2 14COPY --from=build /main /main 15COPY entry.sh / 16RUN chmod 755 /entry.sh 17RUN mkdir -p /opt/extensions 18WORKDIR /opt/extensions 19COPY --from=build /opt/extensions . 20ENTRYPOINT [ \u0026#34;/entry.sh\u0026#34; ] 21CMD [\u0026#34;/main\u0026#34;] 8-11 行目で、ローカルにダウンロードした Extension の zip をビルド用のステージに追加し unzip しています。\nそして 18-19 行目で、最終的に作成されるイメージの /opt/extensions ディレクトリに Extension のバイナリを配置しています。\n実装 Parameter and Secret Lambda Extension は http://localhost:2773 でリクエストを受け付けているので、下記のような実装で利用します。\nconst ( // Endpoint for getting parameter by Parameters and Secrets Lambda Extension. exGetParameterEndpoint = \u0026#34;http://localhost:2773/systemsmanager/parameters/get\u0026#34; // Header key of secret token secretTokenHeaderKey = \u0026#34;X-Aws-Parameters-Secrets-Token\u0026#34; // Query parameter key queryParameterKeyForName = \u0026#34;name\u0026#34; queryParameterKeyForVersion = \u0026#34;version\u0026#34; ) // Struct of response from AWSParametersAndSecretsLambdaExtension API type resultFromExtension struct { Parameter struct { ARN string DateType string LastModifiedDate time.Time Name string Selector string SourceResult *string Type string Value string Version int } ResultMetadata any } // Get a value using Parameters and Secrets Lambda Extension. func getValueByUsingExtension(key string, version int) (string, error) { // Get a value from extension // https://docs.aws.amazon.com/systems-manager/latest/userguide/ps-integration-lambda-extensions.html query := url.Values{} query.Add(queryParameterKeyForName, key) query.Add(queryParameterKeyForVersion, fmt.Sprintf(\u0026#34;%d\u0026#34;, version)) queryStr := query.Encode() url := fmt.Sprintf(\u0026#34;%s?%s\u0026#34;, exGetParameterEndpoint, queryStr) req, err := http.NewRequestWithContext(context.Background(), \u0026#34;GET\u0026#34;, url, nil) if err != nil { return \u0026#34;\u0026#34;, err } // set X-Aws-Parameters-Secrets-Token header req.Header.Add(secretTokenHeaderKey, os.Getenv(\u0026#34;AWS_SESSION_TOKEN\u0026#34;)) // call Extension API client := http.Client{} res, err := client.Do(req) if err != nil { return \u0026#34;\u0026#34;, err } defer res.Body.Close() buf := new(bytes.Buffer) if _, err := buf.ReadFrom(res.Body); err != nil { return \u0026#34;\u0026#34;, err } bodyString := buf.String() if res.StatusCode != http.StatusOK { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;Failed to get parameter by using extension. statusCode:%d body:%s\u0026#34;, res.StatusCode, bodyString) } exRes := resultFromExtension{} if err := json.Unmarshal([]byte(bodyString), \u0026amp;exRes); err != nil { return \u0026#34;\u0026#34;, err } return exRes.Parameter.Value, nil } Extension を使った場合のログ Extension を使った場合、コールドスタート時に下記のようなログが確認できます。\n[AWS Parameters and Secrets Lambda Extension] 2022/11/03 15:20:32 PARAMETERS_SECRETS_EXTENSION_LOG_LEVEL is not present. Log level set to info. [AWS Parameters and Secrets Lambda Extension] 2022/11/03 15:20:32 INFO Systems Manager Parameter Store and Secrets Manager Lambda Extension 1.0.94 [AWS Parameters and Secrets Lambda Extension] 2022/11/03 15:20:32 INFO Serving on port 2773 詳細な実装サンプルについてはこちら。\nmichimani/lambda-parameters-extension: Sample code of using Parameter and Secret Lambda Extension. まとめ コンテナイメージを使った Lambda 関数について、最近触っていてわかったことなどをまとめてみました。\nLambda 関数は便利で利用できる場面も多い反面、ローカルでの動作確認が (SAM 等のツールを別途インストールする必要があったりで) 面倒な印象でした。\nそれが、コンテナイメージを使うことで (実際には RIE を使用することで) 実際の環境とほぼ同じような挙動を確認することができるという点が、個人的には一番嬉しいです。\nこの一点だけでもコンテナイメージ Lambda を使う価値はあるかなと思っています。\n他には、CI/CD に関しても イメージをビルドして ECR に push するところまでは ECS タスクの場合と同じなので、そのあたりの共通化 (もしくはほぼ流用) ができるというのも嬉しいポイントです。\nということで、コンテナイメージを使った Lambda 関数はいいぞ。\nコンテナイメージを使用して Go Lambda 関数をデプロイする - AWS Lambda \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAWS Lambda ランタイム API - AWS Lambda \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-lambda-function-with-container-image/",
    "title": "コンテナイメージを使った Lambda 関数のあれこれ"
  },
  {
    "contents": "CloudFront から S3 へのアクセスを制御する方法として Origin Access Identity (OAI) という機能がありましたが、それに代わる機能として Origin Access Control (OAC) という機能が発表されました。今回は、 AWS CLI を使って OAI を利用していたものを OAC に移行する方法について書きます。\n移行方法 AWS CLI でやっていく 1. 対象の S3 Bucket の Bucket Policy に、 Origin Access Identity の Statement を残したまま Origin Access Control の Statement を追記する 現在の Bucket Policy ドキュメントを取得する Origin Access Control による制御の Statement を追記する Bucket Policy を更新する 2. Origin Access Control のリソースを作成する always (推奨設定) never no-override 3. 対象の Distribution に設定する 4. 対象の S3 Bucket の Bucket Policy から Origin Access Identity の Statement を削除する 5. 不要になった Origin Access Identity のリソースを削除する まとめ 移行方法 基本的には下記の公式ドキュメントに沿って移行していきます。\nRestricting access to an Amazon S3 origin - Amazon CloudFront 具体的な手順としては、\n対象の S3 Bucket の Bucket Policy に、 Origin Access Identity の Statement を残したまま Origin Access Control の Statement を追記する Origin Access Control のリソースを作成する 対象の Distribution に設定する 対象の S3 Bucket の Bucket Policy から Origin Access Identity の Statement を削除する 不要になった Origin Access Identity のリソースを削除する となります。\nAWS CLI でやっていく 今回は、上記の手順を AWS CLI を使ってやってみます。\n必要な AWS CLI のバージョンは、 v1 は 1.25.60 以降、 v2 は 2.7.27 以降です。今回は v2 を使います。\n❯ aws --version aws-cli/2.7.27 Python/3.9.11 Darwin/21.6.0 exe/x86_64 prompt/off 作業するにあたっては、対象の S3 Bucket の名前と CloudFront の Distribution ID が必要なので、それぞれ環境変数に設定しておきます。\nBUCKET_NAME=\u0026#39;your-bucket-name\u0026#39; DIST_ID=\u0026#39;your-distribution-id\u0026#39; 1. 対象の S3 Bucket の Bucket Policy に、 Origin Access Identity の Statement を残したまま Origin Access Control の Statement を追記する 現在の Bucket Policy ドキュメントを取得する aws s3api get-bucket-policy \\ --bucket \u0026#34;${BUCKET_NAME}\u0026#34; \\ --output text \\ | jq . \u0026gt; bucket-policy.json OAI による制御の Statement として下記のような設定がされています。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;OriginAccessIdentityStatement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity your-distribution-id\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34;, \u0026#34;arn:aws:s3:::your-bucket-name\u0026#34; ] } ] } Origin Access Control による制御の Statement を追記する 上記の JSON を下記のように編集します。(Distribution ARN のアカウント ID は読み替えてください)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ + { + \u0026#34;Sid\u0026#34;: \u0026#34;OriginAccessControlStatement\u0026#34;, + \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, + \u0026#34;Principal\u0026#34;: { + \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; + }, + \u0026#34;Action\u0026#34;: [ + \u0026#34;s3:GetObject\u0026#34;, + \u0026#34;s3:ListBucket\u0026#34; + ], + \u0026#34;Resource\u0026#34;: [ + \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34;, + \u0026#34;arn:aws:s3:::your-bucket-name\u0026#34; + ], + \u0026#34;Condition\u0026#34;: { + \u0026#34;StringEquals\u0026#34;: { + \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::000000000000:distribution/your-distribution-id\u0026#34; + } + } + }, { \u0026#34;Sid\u0026#34;: \u0026#34;OriginAccessIdentityStatement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity your-origin-access-identity-id\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34;, \u0026#34;arn:aws:s3:::your-bucket-name\u0026#34; ] } ] } Bucket Policy を更新する aws s3api put-bucket-policy \\ --bucket \u0026#34;${BUCKET_NAME}\u0026#34; \\ --policy file://bucket-policy.json 問題ないはずですが、一応この時点でちゃんとアクセスできることを確認します。\n2. Origin Access Control のリソースを作成する Origin Access Control リソースの作成には、新たに追加された cloudfront コマンドの create-origin-access-control サブコマンドを使います。\n❯ aws cloudfront create-origin-access-control help ... SYNOPSIS create-origin-access-control --origin-access-control-config \u0026lt;value\u0026gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--debug] [--endpoint-url \u0026lt;value\u0026gt;] [--no-verify-ssl] [--no-paginate] [--output \u0026lt;value\u0026gt;] [--query \u0026lt;value\u0026gt;] [--profile \u0026lt;value\u0026gt;] [--region \u0026lt;value\u0026gt;] [--version \u0026lt;value\u0026gt;] [--color \u0026lt;value\u0026gt;] [--no-sign-request] [--ca-bundle \u0026lt;value\u0026gt;] [--cli-read-timeout \u0026lt;value\u0026gt;] [--cli-connect-timeout \u0026lt;value\u0026gt;] [--cli-binary-format \u0026lt;value\u0026gt;] [--no-cli-pager] [--cli-auto-prompt] [--no-cli-auto-prompt] ... パラメータがいくつかありますが、今回は yaml ファイルを作って --cli-input-yaml で渡してリソースを作ってみます。\nそのために、ベースとなる yaml を生成します。\naws cloudfront create-origin-access-control \\ --generate-cli-skeleton yaml-input \u0026gt; oac.yaml 中身はこんな感じで、必須のパラメータのみ記載されています。\nOriginAccessControlConfig: # [REQUIRED] Contains the origin access control. Name: \u0026#39;\u0026#39; # [REQUIRED] A name to identify the origin access control. Description: \u0026#39;\u0026#39; # [REQUIRED] A description of the origin access control. SigningProtocol: sigv4 # [REQUIRED] The signing protocol of the origin access control, which determines how CloudFront signs (authenticates) requests. Valid values are: sigv4. SigningBehavior: never # [REQUIRED] Specifies which requests CloudFront signs (adds authentication information to). Valid values are: never, always, no-override. OriginAccessControlOriginType: s3 # [REQUIRED] The type of origin that this origin access control is for. Valid values are: s3. なので、中身を埋めます。\nOriginAccessControlConfig: Name: \u0026#39;OriginAccessControlForMyBlog\u0026#39; Description: \u0026#39;Origin Access Control for my blog.\u0026#39; SigningProtocol: sigv4 SigningBehavior: always OriginAccessControlOriginType: s3 Name, Description については任意の文字列を入れます。\nSigningProtocol にはそのまま sigv4 を、 OriginAccessControlOriginType にもそのまま s3 を入れます。\nSigningBehavior については、下記公式ドキュメントの Advanced settings for origin access control の部分を確認し、 always, never, no-override のいずれかを入れます。\nRestricting access to an Amazon S3 origin | Advanced settings for origin access control - Amazon CloudFront それぞれの設定について、公式ドキュメントの内容を要約してみます。\nalways (推奨設定) CloudFront が Origin (S3 Bucket) に送信するすべてのリクエストに署名します。\nnever CloudFront が Origin (S3 Bucket) に送信するすべてのリクエストに署名しません。これはつまり、この Origin Access Control を使用する Distribution のすべての Origin に対して アクセスコントロールをオフにします。\nこの設定をする場合、 Originとなる S3 Bucket には Public に公開されている必要があります。公開されていない Bucket に対してこの設定を行うと、 CloudFront は Origin にアクセスできず、 Viewer にはエラーを返します。\nno-override Viewer リクエストに Authorization ヘッダが存在する場合はそれを Origin リクエストに使用し、含まれていない場合は CloudFront が署名します。\nこの設定を使用する場合、 Viewer からの Authorization ヘッダを Origin リクエストで使用するためにキャッシュポリシーで Authorization ヘッダを許可する必要があります。\n以上から、基本的には always で設定しておけば良さそうです。\n作った yaml をもとに、 Origin Access Control のリソースを作成します。作成結果として ID が後に必要になるので控えておきます。\naws cloudfront create-origin-access-control \\ --cli-input-yaml file://oac.yaml \\ --query \u0026#39;OriginAccessControl.Id\u0026#39; \\ --output text 3. 対象の Distribution に設定する 続いて、作成した Origin Access Control を Distribution に設定します。 これは cloudfront の update-distribution という、どデカイ更新を行うサブコマンドで実施します。\nまずは現状の Distribution の設定を yaml で取得し、中身を変更して update-distribution で投げます。\naws cloudfront get-distribution-config \\ --id \u0026#34;${DIST_ID}\u0026#34; \\ --output yaml \\ \u0026gt; dist-config.yaml 100 行を超える yaml が生成されるので、その中の Origins の設定を下記のように変更します。このとき、 Origin Access Identity のリソースは後に削除するので、 your-oai-id の部分を環境変数に設定しておきます。 (OAI_ID='your-oai-id')\n+ OriginAccessControlId: your-oac-id S3OriginConfig: - OriginAccessIdentity: origin-access-identity/cloudfront/your-oai-id + OriginAccessIdentity: \u0026#39;\u0026#39; また、 ETag フィールドの 値はそのままで フィールド名を IfMatch に変更します。\n- ETag: HOGEHOGEVALUE + IfMatch: HOGEHOGEVALUE 下記コマンドで更新します。\naws cloudfront update-distribution \\ --id \u0026#34;${DIST_ID}\u0026#34; \\ --cli-input-yaml file://dist-config.yaml この時点でアクセスできていれば、既に Origin Access Control による制御への移行は成功しています。\n4. 対象の S3 Bucket の Bucket Policy から Origin Access Identity の Statement を削除する Bucket Policy 内の Origin Access Identity の Statement は不要になったので削除します。\n1 の手順で使った bucket-policy.json を下記のように変更し、 put-bucket-policy で更新します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;OriginAccessControlStatement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34;, \u0026#34;arn:aws:s3:::your-bucket-name\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::000000000000:distribution/your-distribution-id\u0026#34; } } - }, - { - \u0026#34;Sid\u0026#34;: \u0026#34;OriginAccessIdentityStatement\u0026#34;, - \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, - \u0026#34;Principal\u0026#34;: { - \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity your-origin-access-identity-id\u0026#34; - }, - \u0026#34;Action\u0026#34;: [ - \u0026#34;s3:GetObject\u0026#34;, - \u0026#34;s3:ListBucket\u0026#34; - ], - \u0026#34;Resource\u0026#34;: [ - \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34;, - \u0026#34;arn:aws:s3:::your-bucket-name\u0026#34; - ] } ] } aws s3api put-bucket-policy \\ --bucket \u0026#34;${BUCKET_NAME}\u0026#34; \\ --policy file://bucket-policy.json 5. 不要になった Origin Access Identity のリソースを削除する 不要になった Origin Access Identity のリソースは cloudfront コマンドの delete-cloud-front-origin-access-identity コマンドで削除します。\naws cloudfront delete-cloud-front-origin-access-identity \\ --id \u0026#34;${OAI_ID}\u0026#34; \\ --if-match $( aws cloudfront get-cloud-front-origin-access-identity \\ --id ${OAI_ID} \\ --query \u0026#39;ETag\u0026#39; \\ --output text) まとめ Amazon CloudFront の新しいアクセスコントロール機能である Origin Access Control への移行を AWS CLI でやってみた話でした。\nTerraform も次の 4.29.0 で対応されそうなので、対応されたら試してみます。\n",
    "permalink": "https://michimani.net/post/aws-migrate-cloudfront-oai-to-oac/",
    "title": "AWS CLI で CloudFront の OAI を OAC に移行する"
  },
  {
    "contents": "最近 Go 言語で AWS CDK を使っているのですが、 L1 Constructs (low-level constructs) での記述に関する情報が見つけられず苦戦したので、メモとして残しておきます。\n目次 前提 L1 Constructs が必要な理由 リソース指定しない、または固定値で指定する場合 特定のプレフィックスを持つ Stack を対象リソースとする場合 L1 Constructs を使用したリソースの定義方法 まとめ 前提 今回は、先日リリースされた CloudFormation Stack のイベントを SNS 経由でメール通知する構成を AWS CDK v2 (Go) で構築してみます。\n作成する主なリソースは下記のとおりです。\nSNS::Topic SNS::TopicPolicy Events::Rule また、通知されるのは 特定のプレフィックスを持つ Stack のみ を対象にしたいと思います。\nL1 Constructs が必要な理由 今回の構成で L1 Constructs が必要になる理由は、 EventPattern で Resources を指定する際に、 prefix 演算子を使用したいからです。\nEventPattern ではワイルドカード (*) による値のマッチができず1、今回の要件である 特定のプレフィックスを持つ Stack のみ を対象としたい場合は prefix 演算子を使用する必要があります。2 この prefix 演算子を使用するか否かで、 L1 Constructs による実装が必要か否かが決まります。\n具体的にどのような違いになるのか見ていきます。\nリソース指定しない、または固定値で指定する場合 まず、 EventPattern として指定する JSON のスキーマおよび記述方法は下記のようになります。\n{ \u0026#34;detail-type\u0026#34;: [ \u0026#34;CloudFormation Resource Status Change\u0026#34;, \u0026#34;CloudFormation Stack Status Change\u0026#34;, \u0026#34;CloudFormation Drift Detection Status Change\u0026#34; ], \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:cloudformation:ap-northeast-1:000000000000:stack/TestStack_Red\u0026#34;, \u0026#34;arn:aws:cloudformation:ap-northeast-1:000000000000:stack/TestStack_Green\u0026#34;, \u0026#34;arn:aws:cloudformation:ap-northeast-1:000000000000:stack/TestStack_Blue\u0026#34; ] } 一方、 AWS CDK (Go) の L2 Constructs で EventPattern の定義に使用する awsevents.EventPattern 構造体の定義は下記のとおりです。(上記の JSON と対応する部分のみ。全体は pkg.go.dev を参照。)\ntype EventPattern struct { DetailType *[]*string `field:\u0026#34;optional\u0026#34; json:\u0026#34;detailType\u0026#34; yaml:\u0026#34;detailType\u0026#34;` Resources *[]*string `field:\u0026#34;optional\u0026#34; json:\u0026#34;resources\u0026#34; yaml:\u0026#34;resources\u0026#34;` } Resources は *string のスライスとして定義されているので、設定したい JSON のスキーマと一致しています。なので、固定のリソースを対象とする、もしくはリソースを指定しない場合は L2 Constructs で定義することができます。 (対象のリソースを指定しない場合は EventPattern.Resources を省略すればよいだけです)\n特定のプレフィックスを持つ Stack を対象リソースとする場合 特定のプレフィックスを持つ Stack を対象リソースとする場合、 EventPattern の JSON は prefix 演算子を使用して下記のような記述になります。\n{ \u0026#34;detail-type\u0026#34;: [ \u0026#34;CloudFormation Resource Status Change\u0026#34;, \u0026#34;CloudFormation Stack Status Change\u0026#34;, \u0026#34;CloudFormation Drift Detection Status Change\u0026#34; ], \u0026#34;resources\u0026#34;: [ { \u0026#34;prefix\u0026#34;: \u0026#34;arn:aws:cloudformation:ap-northeast-1:000000000000:stack/TestStack\u0026#34; } ] } prefix 演算子を使わない場合とは異なり、 resources 配列の中身が文字列ではなくオブジェクトになっています。なので、 L2 Constructs ではこの JSON を表現することができず、 L1 Constructs を使用する必要が出てきます。\nL1 Constructs を使用したリソースの定義方法 ではここで本題の、 AWS CDK (Go) で L1 Constructs を使用してリソースを定義する方法について見ていきます。\n結論として、 *map[string]interface{} 型で定義します。\n先ほどの prefix 演算子を使用した JSON を表現するには、下記のように EventPattern を定義します。\neventPattern := \u0026amp;map[string]interface{}{ \u0026#34;detail-type\u0026#34;: \u0026amp;[]*string{ jsii.String(\u0026#34;CloudFormation Resource Status Change\u0026#34;), jsii.String(\u0026#34;CloudFormation Stack Status Change\u0026#34;), jsii.String(\u0026#34;CloudFormation Drift Detection Status Change\u0026#34;), }, \u0026#34;resources\u0026#34;: \u0026amp;[]interface{}{ \u0026amp;map[string]*string{ \u0026#34;prefix\u0026#34;: jsii.String(\u0026#34;arn:aws:cloudformation:ap-northeast-1:000000000000:stack/TestStack\u0026#34;), }, }, } L1 Constructs だと awsevents.CfnRuleProps.EventPattern が interface{} 型になるので、 *map[string]interface{} 型のスライスとして実装することで単純な文字列や prefix などの演算子を使用したオブジェクト型での指定が可能になります。\nこれを awsevents.CfnRule の定義と合わせると、下記のようになります。\nawsevents.NewCfnRule(scope, jsii.String(\u0026#34;CloudFormationEventsRule\u0026#34;), \u0026amp;awsevents.CfnRuleProps{ Name: jsii.String(util.ToKebabCase(\u0026#34;cloud-formation-events-rule\u0026#34;)), EventBusName: jsii.String(\u0026#34;default\u0026#34;), State: jsii.String(\u0026#34;ENABLED\u0026#34;), EventPattern: \u0026amp;map[string]interface{}{ \u0026#34;detail-type\u0026#34;: \u0026amp;[]*string{ jsii.String(\u0026#34;CloudFormation Resource Status Change\u0026#34;), jsii.String(\u0026#34;CloudFormation Stack Status Change\u0026#34;), jsii.String(\u0026#34;CloudFormation Drift Detection Status Change\u0026#34;), }, \u0026#34;region\u0026#34;: \u0026amp;[]*string{ region, }, \u0026#34;resources\u0026#34;: \u0026amp;[]interface{}{ \u0026amp;map[string]*string{ \u0026#34;prefix\u0026#34;: jsii.String(\u0026#34;arn:aws:cloudformation:ap-northeast-1:000000000000:stack/TestStack\u0026#34;), }, }, }, Targets: \u0026amp;[]interface{}{ \u0026amp;map[string]*string{ \u0026#34;arn\u0026#34;: jsii.String(\u0026#34;arn:aws:sns:ap-northeast-1:000000000000:TestTopic\u0026#34;), \u0026#34;id\u0026#34;: jsii.String(\u0026#34;Target0\u0026#34;), }, }, }) まとめ AWS CDK v2 で Go 言語を使用した場合の L1 Constructs によるリソースの定義方法について書きました。\nv2 で Go がサポートされたとはいえ TypeScript での利用が大多数のようで、なかなか Go を利用した場合の情報がなく大変だったんですが、完全に理解した気がします。\n今回の内容を使ったその他のリソースも合わせた実装例は下記リポジトリに置いていますので、参考になれば幸いです。\naws-cdk-go-examples/cloudformation-events-to-slack at main · michimani/aws-cdk-go-examples EventBridge ルール用のカスタムイベントパターンを作成する \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n他にも数値の範囲によるマッチなどが可能 Amazon EventBridge のイベントパターン - Amazon EventBridge \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-use-aws-cdk-l1-constructs-with-golang/",
    "title": "AWS CDK v2 (Go) で L1 Constructs を使う"
  },
  {
    "contents": "Terraform とOpenSearch CLI で OpenSearch Service に入門してみたので、そのメモです。\n目次 やること とりあえず試したい Terraform で OpenSearch Service のドメインを定義する ドメイン ドメインポリシー variables.tf を作成して apply OpenSearch CLI でリクエストを投げる OpenSearch CLI のインストール プロファイルの設定 Read Only なロールでリクエストを投げてみる Write も許可されてたロールでリクエストを投げてみる まとめ やること Terraform で OpenSearch Service のドメインを定義する エンジンは OpenSearch v1.x ドメインポリシーで OpenSearch に対してのリクエストを制限する 構築した OpenSearch クラスタに OpenSearch CLI でリクエストを投げる 異なる IAM ロールで認証して、ドメインポリシーの挙動を確認する とりあえず試したい 以降は↑の内容を順番に書いていくだけなので、もし実際に試したいという方は下記リポジトリを clone して試してみてください。 terraform/README.md を読んでもらえれば諸々試せます。\nmichimani/get-started-opensearch: Hello OpenSearch. Terraform で OpenSearch Service のドメインを定義する 下記ドキュメントを参考に、 OpenSearch Service のドメインを定義します。\naws_opensearch_domain | Resources | hashicorp/aws | Terraform Registry ドメイン 最小構成だとこんな感じになります。\nresource \u0026#34;aws_opensearch_domain\u0026#34; \u0026#34;first_opensearch\u0026#34; { domain_name = \u0026#34;first-opensearch\u0026#34; engine_version = \u0026#34;OpenSearch_1.2\u0026#34; cluster_config { instance_type = \u0026#34;t3.small.search\u0026#34; } ebs_options { ebs_enabled = true volume_size = 10 } } OpenSearch Service で指定できるエンジンは aws opensearch list-versions で確認できます。2022/07/04 現在だと下記のエンジンが指定可能です。\n{ \u0026#34;Versions\u0026#34;: [ \u0026#34;OpenSearch_1.2\u0026#34;, \u0026#34;OpenSearch_1.1\u0026#34;, \u0026#34;OpenSearch_1.0\u0026#34;, \u0026#34;Elasticsearch_7.10\u0026#34;, \u0026#34;Elasticsearch_7.9\u0026#34;, \u0026#34;Elasticsearch_7.8\u0026#34;, \u0026#34;Elasticsearch_7.7\u0026#34;, \u0026#34;Elasticsearch_7.4\u0026#34;, \u0026#34;Elasticsearch_7.1\u0026#34;, \u0026#34;Elasticsearch_6.8\u0026#34;, \u0026#34;Elasticsearch_6.7\u0026#34;, \u0026#34;Elasticsearch_6.5\u0026#34;, \u0026#34;Elasticsearch_6.4\u0026#34;, \u0026#34;Elasticsearch_6.3\u0026#34;, \u0026#34;Elasticsearch_6.2\u0026#34;, \u0026#34;Elasticsearch_6.0\u0026#34;, \u0026#34;Elasticsearch_5.6\u0026#34;, \u0026#34;Elasticsearch_5.5\u0026#34;, \u0026#34;Elasticsearch_5.3\u0026#34;, \u0026#34;Elasticsearch_5.1\u0026#34;, \u0026#34;Elasticsearch_2.3\u0026#34;, \u0026#34;Elasticsearch_1.5\u0026#34; ] } ドメインポリシー 今回は、構築する OpenSearch クラスターに対して Read (GET アクセス) のみ可能な IAM ロールと Write (POST, PUT, DELETE アクセス) も可能な IAM ロールを作成し、それらによるアクセス制御を行うためのドメインポリシーを設定します。 Terraform での定義は下記のようになります。\nresource \u0026#34;aws_opensearch_domain_policy\u0026#34; \u0026#34;first_domain_policy\u0026#34; { domain_name = aws_opensearch_domain.first_opensearch.domain_name access_policies = \u0026lt;\u0026lt;POLICIES { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AccessByAdministrator\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/demo/opensearch-admin\u0026#34; ] }, \u0026#34;Action\u0026#34;: [ \u0026#34;es:*\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: [ \u0026#34;${var.my_ip}\u0026#34; ] } }, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:es:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:domain/first-opensearch/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AccessByReadOnlyUser\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/demo/opensearch-read-only\u0026#34; ] }, \u0026#34;Action\u0026#34;: [ \u0026#34;es:ESHttpGet\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: [ \u0026#34;${var.my_ip}\u0026#34; ] } }, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:es:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:domain/first-opensearch/*\u0026#34; } ] } POLICIES } opensearch-admin というロールには es:* ですべてのアクションを許可し、 opensearch-read-only ロールには es:ESHttpGet で GET アクセスのみ許可します。また、今回は Condition で特定の IP からのアクセスのみ許可するようにしています。\nIAM ロールの方の定義はこちら。\nresource \u0026#34;aws_iam_role\u0026#34; \u0026#34;opensearch_ro_role\u0026#34; { name = \u0026#34;opensearch-read-only\u0026#34; path = \u0026#34;/demo/\u0026#34; assume_role_policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;${var.iam_user_arn}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } POLICY } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;opensearch_admin_role\u0026#34; { name = \u0026#34;opensearch-admin\u0026#34; path = \u0026#34;/demo/\u0026#34; assume_role_policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;${var.iam_user_arn}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } POLICY } 特定の IAM ユーザーからスイッチロールして使うことを想定しています。 ${var.iam_user_arn} には、実際にこれらのロールにスイッチする IAM ユーザーの ARN を variable で指定します。\nvariables.tf を作成して apply variables.tf は下記のような形で作成しておきます。\nvariable \u0026#34;my_ip\u0026#34; { default = \u0026#34;203.0.113.0\u0026#34; // Your machine\u0026#39;s global IP address. } variable \u0026#34;iam_user_arn\u0026#34; { default = \u0026#34;arn:aws:iam::000000000000:user/hoge\u0026#34; // ARN of the IAM User for which AWS CLI credentials have been set up. } 自身の IP アドレスの確認には、\ncurl -X GET \u0026#39;https://checkip.amazonaws.com/\u0026#39; がシンプルなのでおすすめです。\nあとは terraform apply でリソースを作成します。初回の apply には 15 分程度かかります。\nOpenSearch CLI でリクエストを投げる OpenSearch のクラスタが構築できたら、 OpenSearch CLI を使ってリクエストを投げてみます。\nOpenSearch CLI のインストール 下記ページからダウンロードします。\nOpensearch 2.0.1 · OpenSearch macOS の場合はインストーラ (pkg) 形式なので、ダウンロードしてインストーラの指示に従ってインストールします。\nプロファイルの設定 OpenSearch CLI はいくつかプロファイルを作成して使い分ける事ができます。 AWS CLI でいう --profile オプションみたいなのがあります。\nプロファイルの作成には OpenSearch クラスタの URL と、認証方法を指定する必要があります。\nまず、 OpenSearch クラスタの URL を取得します。\n今回は first-opensearch という名前でドメインを作成したので、下記の AWS CLI コマンドで URL (エンドポイント) を取得します。\nENDPOINT=$( aws opensearch describe-domain \\ --domain-name \u0026#39;first-opensearch\u0026#39; \\ --query \u0026#39;DomainStatus.join(``,[`https://`,Endpoint])\u0026#39; \\ --output text ) \u0026amp;\u0026amp; echo \u0026#34;${ENDPOINT}\u0026#34; 続いて認証方法についてですが、 OpenSearch CLI では下記の 3 の認証方法が指定できます。\ndisabled basic cert aws-iam 今回は、 IAM ロールでの認証を行うので aws-iam を認証方法として指定します。\nプロファイルの作成には下記コマンドを実行します。\nopensearch-cli profile create \\ --auth-type aws-iam \\ --endpoint \u0026#34;${ENDPOINT}\u0026#34; \\ --name env-role aws-iam を指定した場合、認証に使用する AWS のプロファイルと AWS のサービス名を聞かれます。\nプロファイルに関しては特定のプロファイルを指定する事もできますが、指定しない場合は実行環境の環境変数をもとに認証に使用するプロファイルが選択されます。今回は環境変数によって使用するプロファイルを使い分けることにします。\nAWS のサービス名は、 es または ec2 で指定します。今回は es を指定します。 (OpenSearch Service ですが es です)\nRead Only なロールでリクエストを投げてみる まずは Read Only なロール opensearch-read-only にスイッチしてリクエストを投げてみます。\nスイッチロールするには下記のコマンドを実行します。\nSWITCH_CMD=$( \\ aws sts assume-role \\ --role-arn $(aws iam list-roles \\ --path-prefix \u0026#39;/demo\u0026#39; \\ --query \u0026#39;Roles[?contains(to_string(RoleName), `opensearch-read-only`)].Arn\u0026#39; \\ --output text) \\ --role-session-name \u0026#39;opensearch-read-only\u0026#39; \\ --query \u0026#39;Credentials.join(``,[`export AWS_ACCESS_KEY_ID=\\\u0026#34;`,AccessKeyId,`\\\u0026#34; AWS_SECRET_ACCESS_KEY=\\\u0026#34;`,SecretAccessKey,`\\\u0026#34; AWS_SESSION_TOKEN=\\\u0026#34;`,SessionToken,`\\\u0026#34;`])\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; eval ${SWITCH_CMD} \\ \u0026amp;\u0026amp; aws sts get-caller-identity ちなみに、このコマンドを AWS CLI のエイリアスとしていい感じに設定しておくことで、 AWS CLI でのスイッチロールがめちゃくちゃ簡単になるよ というのを別記事で書いているので参考にしてみてください。\nAWS CLI のエイリアスを使ってメチャクチャ簡単にスイッチロールできるようにしてみた - michimani.net opensearch-read-only ロールにスイッチできたら、 GET のリクエストを投げてみます。\n$ opensearch-cli curl get \\ --path \u0026#39;_cluster/health\u0026#39; \\ --output-format yaml \\ --profile env-role --- cluster_name: \u0026#34;000000000000:first-opensearch\u0026#34; status: \u0026#34;green\u0026#34; timed_out: false number_of_nodes: 1 number_of_data_nodes: 1 discovered_master: true active_primary_shards: 1 active_shards: 1 relocating_shards: 0 initializing_shards: 0 unassigned_shards: 0 delayed_unassigned_shards: 0 number_of_pending_tasks: 0 number_of_in_flight_fetch: 0 task_max_waiting_in_queue_millis: 0 active_shards_percent_as_number: 100.0 続いて PUT リクエストを投げてみます。\n$ opensearch-cli curl put \\ --path \u0026#39;demoindex\u0026#39; \\ --profile env-role { \u0026#34;Message\u0026#34;: \u0026#34;User: arn:aws:sts::000000000000:assumed-role/opensearch-read-only/opensearch-read-only is not authorized to perform: es:ESHttpPut because no identity-based policy allows the es:ESHttpPut action\u0026#34; } こちらはエラーになりました。\nここまで終わったらスイッチロールを解除するために下記コマンドを実行します。\nunset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN Write も許可されてたロールでリクエストを投げてみる 次に Write も許可されたロール opensearch-admin にスイッチしてリクエストを投げてみます。\nopensearch-admin にスイッチするには下記コマンドを実行します。\nSWITCH_CMD=$( \\ aws sts assume-role \\ --role-arn $(aws iam list-roles \\ --path-prefix \u0026#39;/demo\u0026#39; \\ --query \u0026#39;Roles[?contains(to_string(RoleName), `opensearch-admin`)].Arn\u0026#39; \\ --output text) \\ --role-session-name \u0026#39;opensearch-admin\u0026#39; \\ --query \u0026#39;Credentials.join(``,[`export AWS_ACCESS_KEY_ID=\\\u0026#34;`,AccessKeyId,`\\\u0026#34; AWS_SECRET_ACCESS_KEY=\\\u0026#34;`,SecretAccessKey,`\\\u0026#34; AWS_SESSION_TOKEN=\\\u0026#34;`,SessionToken,`\\\u0026#34;`])\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; eval ${SWITCH_CMD} \\ \u0026amp;\u0026amp; aws sts get-caller-identity Read Only のときに失敗した PUT リクエストを投げてみます。\n$ opensearch-cli curl put \\ --path \u0026#39;demoindex\u0026#39; \\ --profile env-role {\u0026#34;acknowledged\u0026#34;:true,\u0026#34;shards_acknowledged\u0026#34;:true,\u0026#34;index\u0026#34;:\u0026#34;demoindex\u0026#34;} 今度は成功しました。続いて DELETE リクエストも投げてみます。\n$ opensearch-cli curl delete \\ --path \u0026#39;demoindex\u0026#39; \\ --profile env-role {\u0026#34;acknowledged\u0026#34;:true} こちらも成功しました。\nここまで終わったら、忘れずにスイッチロールを解除しておきます。\nunset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN まとめ Terraform で OpenSearch Service のドメイン、ドメインポリシーを定義した OpenSearch CLI で OpenSearch のクラスタにリクエストを投げてみた 副作用として AWS CLI でスイッチロールするワンライナーが生まれた ",
    "permalink": "https://michimani.net/post/aws-get-started-opensearch-service-via-terraform/",
    "title": "Terraform と OpenSearch CLI で OpenSearch Service に入門する"
  },
  {
    "contents": "AWS CLI でスイッチロールする場合、 sts assume-role で認証情報を取得し、それを環境変数なり ~/.aws/credentials に追記したりする必要があります。さくっとスイッチしたいだけなのに少々手間がかかるので、 AWS CLI のエイリアスを使ってコマンド一つでスイッチロールできるようにしてみました。\n結論 これです。\nawscli-aliases/alias at 9edd908f2d27c8a4acf07a6dc31ce7d19dc6ced3 · michimani/awscli-aliases AWS CLI でスイッチロールする流れ そもそものスイッチロールの仕組みと AWS CLI でスイッチロールする方法については下記の AWS Blog の記事に書かれています。\nIAM チュートリアル: AWS アカウント間の IAM ロールを使用したアクセスの委任 - AWS Identity and Access Management AWS CLI での流れを要約すると下記の通りとなります。\naws sts assume-role コマンドで、スイッチしたいロールの一時認証情報を取得する 取得した認証情報から、必要な値を下記の環境変数にそれぞれ設定する AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN たったこれだけですが、 2 の部分が多少手間がかかります。というのも、普通にやると sts assume-role の実行結果から必要な情報をコピペしてくる必要があるからです。\nこの手順 1, 2 を --query オプションを使っていい感じにしてみます。\nquery オプションを使ってワンライナーで書く まずは、手順 1 で実行するコマンドから。\n今回は、例として下記のような IAM ロールにスイッチしたいとします。 (iam get-role の出力の一部を抜粋)\n{ \u0026#34;Role\u0026#34;: { \u0026#34;Path\u0026#34;: \u0026#34;/demo/\u0026#34;, \u0026#34;RoleName\u0026#34;: \u0026#34;target-role\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;AROA2XXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::00000000:role/demo/target-role\u0026#34;, } } 上記のロールにスイッチするためには下記コマンドを実行します。\naws sts assume-role \\ --role-arn \u0026#39;arn:aws:iam::00000000:role/demo/target-role\u0026#39; --role-session-name \u0026#39;switch-to-target-role\u0026#39; 正常に実行できれば、下記のような出力が得られます。\n{ \u0026#34;Credentials\u0026#34;: { \u0026#34;AccessKeyId\u0026#34;: \u0026#34;ASIAXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;FbNIxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;SessionToken\u0026#34;: \u0026#34;IQxxxxx...............xxxxx+4w==\u0026#34;, \u0026#34;Expiration\u0026#34;: \u0026#34;2022-06-30T16:05:13+00:00\u0026#34; }, \u0026#34;AssumedRoleUser\u0026#34;: { \u0026#34;AssumedRoleId\u0026#34;: \u0026#34;AROA2XXXXXXXXXXXXXXXX:switch-to-target-role\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::00000000:assumed-role/target-role/switch-to-target-role\u0026#34; } } この中から、 AccessKeyId 、 SecretAccessKey 、SessionToken をそれぞれ環境変数 AWS_ACCESS_KEY_ID 、 AWS_SECRET_ACCESS_KEY 、 AWS_SESSION_TOKEN に設定することでスイッチしたロールで AWS CLI のコマンドを実行することができます。\nLinux または macOS では環境変数の設定には export コマンドを使用するので、 --query オプションを使って環境変数設定用のコマンドを生成します。具体的には下記のように実行します。\naws sts assume-role \\ --role-arn \u0026#39;arn:aws:iam::00000000:role/demo/target-role\u0026#39; --role-session-name \u0026#39;switch-to-target-role\u0026#39; --query \u0026#39;Credentials.join(``,[`export AWS_ACCESS_KEY_ID=\\\u0026#34;`,AccessKeyId,`\\\u0026#34; AWS_SECRET_ACCESS_KEY=\\\u0026#34;`,SecretAccessKey,`\\\u0026#34; AWS_SESSION_TOKEN=\\\u0026#34;`,SessionToken,`\\\u0026#34;`])\u0026#39; \\ --output text 出力としては下記のようになります。\nexport AWS_ACCESS_KEY_ID=\u0026#34;FbNIxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; AWS_SECRET_ACCESS_KEY=\u0026#34;IQxxxxx...............xxxxx+4w==\u0026#34; AWS_SESSION_TOKEN=\u0026#34;IQxxxxx...............xxxxx+4w==\u0026#34; これをコピペして実行すればいいのですが、それもコマンドでやってしまいます。\nこの出力を一旦変数に入れ、それを eval で展開して実行します。\nSWITCH_CMD=$( \\ aws sts assume-role \\ --role-arn $( \\ aws iam get-role \\ --role-name \u0026#39;target-role\u0026#39; \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text) \\ --role-session-name \u0026#39;switch-to-target-role\u0026#39; \\ --query \u0026#39;Credentials.join(``,[`export AWS_ACCESS_KEY_ID=\\\u0026#34;`,AccessKeyId,`\\\u0026#34; AWS_SECRET_ACCESS_KEY=\\\u0026#34;`,SecretAccessKey,`\\\u0026#34; AWS_SESSION_TOKEN=\\\u0026#34;`,SessionToken,`\\\u0026#34;`])\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; eval ${SWITCH_CMD} \\ \u0026amp;\u0026amp; aws sts get-caller-identity eval で export コマンドを実行したあとは、正常にスイッチできているかを確認するために aws sts get-caller-identity を実行しています。\n実行結果としては下記のようになります。\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AROA2XXXXXXXXXXXX:switch-to-target-role\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;000000000000\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::000000000000:assumed-role/target-role/switch-to-target-role\u0026#34; } 以上で、ワンライナーでスイッチロールできるようになりました。最後に、これを AWS CLI のエイリアスコマンドとして設定し、より簡単にスイッチロールできるようにしてみます。\nAWS CLI のエイリアスコマンドを設定する AWS CLI では、ユーザが任意のエイリアスコマンドを設定できるようになっています。\nAWS CLI エイリアスの作成と使用 - AWS Command Line Interface 例えば、下記のようなエイリアスを設定しておけば aws id と実行するだけでアカウント ID を確認することができます。\nid = sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text エイリアスには引数で値を渡すこともできます。今回であれば可変なのはロール名のみなので、エイリアスの引数にロール名を渡すようにすればよいので、下記のようなエイリアスを設定することになります。\nswitch-role = !f() { SWITCH_CMD=$( \\ aws sts assume-role \\ --role-arn $( \\ aws iam get-role \\ --role-name \u0026#34;${1}\u0026#34; \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text) \\ --role-session-name \u0026#34;switch-to-${1}\u0026#34; \\ --query \u0026#39;Credentials.join(``,[`export AWS_ACCESS_KEY_ID=\\\u0026#34;`,AccessKeyId,`\\\u0026#34; AWS_SECRET_ACCESS_KEY=\\\u0026#34;`,SecretAccessKey,`\\\u0026#34; AWS_SESSION_TOKEN=\\\u0026#34;`,SessionToken,`\\\u0026#34;`])\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; eval ${SWITCH_CMD} \\ \u0026amp;\u0026amp; aws sts get-caller-identity }; f あとは aws switch-role 'target-role' と実行するだけでスイッチロールできます。\nさいごに AWS CLI のエイリアスいいぞ ワンライナー最高 どんどんスイッチしたくなる ちなみに、元のロールに戻りたい場合は下記コマンドを実行して各環境変数の値をクリアすれば OK です。\nunset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN ",
    "permalink": "https://michimani.net/post/aws-switch-role-via-aws-cli/",
    "title": "AWS CLI のエイリアスを使ってメチャクチャ簡単にスイッチロールできるようにしてみた"
  },
  {
    "contents": "Google Analytics のユニバーサルアナリティクス (UA) が 2023年 7月に廃止され、それ以降は Google Analytics 4 (GA4) を使う必要があります。まだ UA 廃止までの猶予はありますが、PV 等の各値を比較したくなったときに 1 年前のデータが無いと困るので、なるはやで移行しておく必要があります。今回は、とりあえずこのブログを UA から GA4 に移行してみます。\nユニバーサル アナリティクスのサポートは終了します - アナリティクス ヘルプ やること GA4 に切り替えていない状態で対象のプロパティの画面を開くと、ページ上部に下記のようなメッセージが表示されています。\nここの [開始] から GA4 のプロパティを設定していきます。\nプロパティの設定ができたら、発行されたスクリプトをブログの \u0026lt;head\u0026gt; タグ内に配置します。\n今回は、細かい設定はスキップして とりあえず GA4 のプロパティで PV だけでも計測できるようになることをゴールとします。\nGA4 プロパティの設定 先ほどの [開始] を押すと下記のような画面に遷移します。\n今回は新たに GA4 のプロパティを作成します。\n新しい Google アナリティクス 4 プロパティを作成する GA4 設定アシスタントの画面から 新しい Google アナリティクス 4 プロパティの作成 を開始すると、下記のようなウィザードが表示されます。\nここで [作成] を押すと、 GA4 設定アシスタントの画面に戻り、下記のような表示になる。\nGA4 のプロパティが作成された。\nGA4 プロパティの機能の確認と設定を行う [GA4 プロパティを確認] を押すと GA4 の設定画面に遷移します。\n正直、設定項目が多すぎてよくわからん。ので、欲を出さずに PV だけでも計測できるようになること を目指す。\nデータストリーム の項目を見ると、下記のようにまだデータが受信できていません。(それはそう)\nストリームの一覧から対象の行をクリックすると、下記のような詳細が表示されます。\nタグの設定手順 にある通り、データストリームで各値を計測するにはいずれかの方法でページ上にタグを埋め込む必要があります。\n今回は 新しいページ上のタグを追加する 、且つ GTM を使っていないため gtag.js を追加する方法で進めます。\n埋め込むタグは下記のような形式です。\n\u0026lt;!-- Global site tag (gtag.js) - Google Analytics --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id=G-XXXXXXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;G-XXXXXXXXX\u0026#39;); \u0026lt;/script\u0026gt; これをこのブログの \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; 内に配置します。\nGA4 のプロパティタグをブログに設置する このブログは Hugo で生成しており、テーマには Simplog を使っています。\nSimplog は UA のタグ埋め込みには対応しています (UA-XXXXXX を config.toml に追記するだけで OK) が、 GA4 の埋め込みには対応していません。ただ、 partials/additional-custom-head.html を作成すれば head 内にその内容が埋め込まれるようになっているので、 partials/additional-custom-head.html にプロパティタグを記述する方法で対応します。\nsimplog/head.html at c6940745f5b0751a408f3fc959f231b9ec624e5a · michimani/simplog ゆくゆくは G-XXXXXXXXX のタグを config.toml に記述するだけでタグが埋め込まれるようにしたい。(してくれる方がいたら PR お待ちしています)\n設置して様子を見る GA4 のプロパティタグを設置したらしばらく様子を見て、先ほどのデータストリームを確認します。\n他にやること GA4 の設定を見る限りだと\nGoogle Search Console との統合 Google AdSense との統合 あたりは設定が必要そうです。\n",
    "permalink": "https://michimani.net/post/development-migrate-google-analytics-gav4/",
    "title": "Hugo で生成しているブログをユニバーサルアナリティクスから Google Analytics 4 に移行する"
  },
  {
    "contents": "CloudFormation で Lambda の Function URLs が管理できるようになっていたので試してみます。\n対応するリソース Function URLs に対応するリソースは AWS::Lambda::Url。 AWS::Lambda::FunctionUrl になるのでは？と勝手に予想していましたが、違いました。\nプロパティは下記のとおりです。\nAuthType: String Cors: Cors AllowCredentials: Boolean AllowHeaders: List of String AllowMethods: List of String AllowOrigins: List of String ExposeHeaders: List of String MaxAge: Integer Qualifier: String TargetFunctionArn: String !Ref が返す値は、Function URLs の設定対象となる Lambda 関数の ARN です。\n!GetAtt では FunctionArn および FunctionUrl を指定できます。\n公式ドキュメント AWS::Lambda::Url - AWS CloudFormation CFn テンプレート作成 前回1と同様に、 うみほたるの風速・風向を返す Lambda 関数 umiwind に対して Functions URLs を設定してみます。\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;Example of Function URL Config\u0026#34; Resources: UmiwindURL: Type: AWS::Lambda::Url Properties: TargetFunctionArn: arn:aws:lambda:ap-northeast-1:000000000000:function:umiwind AuthType: NONE Cors: AllowCredentials: false AllowMethods: - GET AllowOrigins: - \u0026#34;*\u0026#34; Outputs: UmiwindPublicURL: Description: Public endpoint for invoking umiwind Value: !GetAtt UmiwindURL.FunctionUrl 生成された URL をスタックの出力としてしています。\nデプロイと確認 デプロイは使い古された下記のシェルスクリプト (function_url.sh) を使って実施します。\n#!/bin/bash CHANGESET_OPTION=\u0026#34;--no-execute-changeset\u0026#34; if [ $# = 1 ] \u0026amp;\u0026amp; [ $1 = \u0026#34;deploy\u0026#34; ]; then echo \u0026#34;deploy mode\u0026#34; CHANGESET_OPTION=\u0026#34;\u0026#34; fi readonly CFN_TEMPLATE=\u0026#34;$(dirname $0)/function_url.yml\u0026#34; readonly CFN_STACK_NAME=UmiwindURL echo \u0026#34;CFN_TEMPLATE = ${CFN_TEMPLATE}\u0026#34; echo \u0026#34;CFN_STACK_NAME = ${CFN_STACK_NAME}\u0026#34; aws cloudformation deploy \\ --stack-name \u0026#34;${CFN_STACK_NAME}\u0026#34; \\ --template-file \u0026#34;${CFN_TEMPLATE}\u0026#34; ${CHANGESET_OPTION} sh ./function_url.sh \u0026#39;deploy\u0026#39; 生成された URL は下記コマンドで確認します。\naws cloudformation describe-stacks \\ --stack-name UmiwindURL \\ --query \u0026#34;Stacks[0].Outputs[?contains(to_string(OutputKey),\\`UmiwindPublicURL\\`)].OutputValue\u0026#34; \\ --output text まとめ Lambda の Function URLs が CloudFormation で構築できるようになっていたので試した それ以上でもそれ以下でもない Lambda 関数に個別の URL を設定できる AWS Lambda Function URLs を試す - michimani.net \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-manage-lambda-functions-urls-by-cloudformation/",
    "title": "CloudFormation で Lambda Function URLs を管理する"
  },
  {
    "contents": "AWS からとあるメールが届いていました。\nWe are contacting you as we have identified that your AWS Account currently has one or more Lambda functions using Python 3.6 runtime.\nWe are ending support for Python 3.6 in AWS Lambda. This follows Python 3.6 End-Of-Life (EOL) reached on December 23, 2021 [1].\nAs described in the Lambda runtime support policy [2], end of support for language runtimes in Lambda happens in two stages. Starting July 18, 2022, Lambda will no longer apply security patches and other updates to the Python 3.6 runtime used by Lambda functions, and functions using Python 3.6 will no longer be eligible for technical support. In addition, you will no longer be able to create new Lambda functions using the Python 3.6 runtime. Starting August 17, 2022, you will no longer be able to update existing functions using the Python 3.6 runtime.\nWe recommend that you upgrade your existing Python 3.6 functions to Python 3.9 before August 17, 2022.\nPython 3.6 の EOL に従って、 Lambda でランタイムに Python 3.6 を使っている関数は 2022年8月17日 以降は更新できなくなるというものです。\n個人の環境なので特に問題ないが更新できなくなると困るので、とりあえず全部 Python 3.9 に更新してしまいます。\nPython 3.6 を使っている Lambda 関数を特定する まずは Python 3.6 をランタイムに使っている Lambda 関数を特定します。下記 AWS CLI のコマンドを実行して、各リージョンに対して対象の Lambda 関数が存在するかどうか調べます。\nfor region in $(aws ec2 describe-regions --query \u0026#39;Regions[].RegionName\u0026#39; --output text); do echo \u0026#34;Lambda function in ${region} with Python 3.6 runtime\u0026#34; aws lambda list-functions \\ --query \u0026#39;Functions[?contains(to_string(Runtime),`python3.6`)].FunctionArn\u0026#39; \\ --region \u0026#34;${region}\u0026#34; done 結果。\nLambda function in eu-north-1 with Python 3.6 runtime [] Lambda function in ap-south-1 with Python 3.6 runtime [] Lambda function in eu-west-3 with Python 3.6 runtime [] Lambda function in eu-west-2 with Python 3.6 runtime [] Lambda function in eu-west-1 with Python 3.6 runtime [] Lambda function in ap-northeast-3 with Python 3.6 runtime [] Lambda function in ap-northeast-2 with Python 3.6 runtime [] Lambda function in ap-northeast-1 with Python 3.6 runtime [ \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:StartStopEc2\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:KeyakizakaBlogUpdateCheck\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:ResizeS3BacketOperation\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:DeployTestDev\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:toSlack\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:iot1click_onclick_sms_20190710142349\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:HinatazakaBlogUpdateCheck\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:DeployTest\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:PutItemToDynamoDbFromJson\u0026#34;, \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:iot1click_onclick_sms_20190710143217\u0026#34; ] Lambda function in sa-east-1 with Python 3.6 runtime [] Lambda function in ca-central-1 with Python 3.6 runtime [] Lambda function in ap-southeast-1 with Python 3.6 runtime [] Lambda function in ap-southeast-2 with Python 3.6 runtime [] Lambda function in eu-central-1 with Python 3.6 runtime [] Lambda function in us-east-1 with Python 3.6 runtime [] Lambda function in us-east-2 with Python 3.6 runtime [] Lambda function in us-west-1 with Python 3.6 runtime [] Lambda function in us-west-2 with Python 3.6 runtime [ \u0026#34;arn:aws:lambda:us-west-2:000000000000:function:iot1click_onclick_email_20190710142349\u0026#34;, \u0026#34;arn:aws:lambda:us-west-2:000000000000:function:iot1click_onclick_email_20190710143217\u0026#34;, \u0026#34;arn:aws:lambda:us-west-2:000000000000:function:iot1click_onclick_sms_20190705234051\u0026#34; ] そこそこありました。\nまとめて更新する 3.6 → 3.9 のアップデートなので特に気にするところはなさそうなので、且つ個人の環境なのでとりあえず動作確認は置いておいてランタイムだけ更新します。\n下記 AWS CLI コマンドでまとめて更新します。\nfor region in $(aws ec2 describe-regions --query \u0026#39;Regions[].RegionName\u0026#39; --output text); do for arn in $(aws lambda list-functions \\ --query \u0026#39;Functions[?contains(to_string(Runtime),`python3.6`)].FunctionArn\u0026#39; \\ --region \u0026#34;${region}\u0026#34; \\ --output text); do echo \u0026#34;update ${arn}...\u0026#34; aws lambda update-function-configuration \\ --function-name \u0026#34;${arn}\u0026#34; \\ --runtime \u0026#39;python3.9\u0026#39; \\ --region \u0026#34;${region}\u0026#34; \\ --query \u0026#39;Runtime\u0026#39; done done 結果。\nupdate arn:aws:lambda:ap-northeast-1:000000000000:function:StartStopEc2... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:KeyakizakaBlogUpdateCheck... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:ResizeS3BacketOperation... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:DeployTestDev... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:toSlack... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:iot1click_onclick_sms_20190710142349... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:HinatazakaBlogUpdateCheck... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:DeployTest... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:PutItemToDynamoDbFromJson... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:ap-northeast-1:000000000000:function:iot1click_onclick_sms_20190710143217... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:us-west-2:000000000000:function:iot1click_onclick_email_20190710142349... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:us-west-2:000000000000:function:iot1click_onclick_email_20190710143217... \u0026#34;python3.9\u0026#34; update arn:aws:lambda:us-west-2:000000000000:function:iot1click_onclick_sms_20190705234051... \u0026#34;python3.9\u0026#34; 一応再確認 for region in $(aws ec2 describe-regions --query \u0026#39;Regions[].RegionName\u0026#39; --output text); do echo \u0026#34;Lambda function in ${region} with Python 3.6 runtime\u0026#34; aws lambda list-functions \\ --query \u0026#39;Functions[?contains(to_string(Runtime),`python3.6`)].FunctionArn\u0026#39; \\ --region \u0026#34;${region}\u0026#34; done 結果。\nLambda function in eu-north-1 with Python 3.6 runtime [] Lambda function in ap-south-1 with Python 3.6 runtime [] Lambda function in eu-west-3 with Python 3.6 runtime [] Lambda function in eu-west-2 with Python 3.6 runtime [] Lambda function in eu-west-1 with Python 3.6 runtime [] Lambda function in ap-northeast-3 with Python 3.6 runtime [] Lambda function in ap-northeast-2 with Python 3.6 runtime [] Lambda function in ap-northeast-1 with Python 3.6 runtime [] Lambda function in sa-east-1 with Python 3.6 runtime [] Lambda function in ca-central-1 with Python 3.6 runtime [] Lambda function in ap-southeast-1 with Python 3.6 runtime [] Lambda function in ap-southeast-2 with Python 3.6 runtime [] Lambda function in eu-central-1 with Python 3.6 runtime [] Lambda function in us-east-1 with Python 3.6 runtime [] Lambda function in us-east-2 with Python 3.6 runtime [] Lambda function in us-west-1 with Python 3.6 runtime [] Lambda function in us-west-2 with Python 3.6 runtime [] さようなら、 Python 3.6 👋\n注意点 あくまでも手動で作った Lambda 関数に対してのみの変更を想定しています CFn や Terraform などで管理されている Lambda 関数をこの方法で編集した場合、それらの次回以降のデプロイでエラーになります ",
    "permalink": "https://michimani.net/post/aws-update-lambda-functions-python36-to-python39/",
    "title": "AWS CLI で Lambda 関数のランタイムを一括で更新する"
  },
  {
    "contents": "AWS Lambda の各関数に対して個別の URL を割り当てる機能 AWS Lambda Function URLs がリリースされたので試してみます。\n例のごとく、諸々の操作は AWS CLI v1 (1.22.90 以降) で既に対応している (v2 は多分 数日後に対応) ので CLI で操作します。\n追記 (2022/04/09) v2 に関しても 2.5.4 でサポートされました。\naws-cli/CHANGELOG.rst at 2.5.4 · aws/aws-cli 追記 2 (2022/04/22) CloudFormation でも構築できるようになっていたので試しました。\nCloudFormation で Lambda Function URLs を管理する - michimani.net AWS Lambda Function URLs の概要 詳細は下記の AWS 公式ブログを参照してください。\nAnnouncing AWS Lambda Function URLs: Built-in HTTPS Endpoints for Single-Function Microservices | AWS News Blog AWS Lambda の各関数に個別の URL を割り当て、 HTTPS で Lambda 関数を invoke できるようになる API Gateway の設定が不要になる URL は Lambda 関数のバージョンに対して設定する 複数設定でき、例えば開発中の確認用を $LATEST に設定し、安定版やステージング用を他の任意のエイリアスに設定する といったことが可能 Functions URLs の設定自体に料金は発生せず、 Lambda 関数の実行に対して発生する料金に含まれる 認可方法としては IAM もしくは 無し (完全にパブリック) が選択できる CORS も設定可 リクエストヘッダの Content-type は application/json もしくは text/* を指定する必要がある 指定しない場合、 base64-encoded な値として渡されるので Lambda 関数側で decode が必要になる API Gateway との使い分け 下記のような API Gateway の機能を使いたいかどうか リクエスト時のバリデーション スロットリング カスタムオーソライザー カスタムドメイン 使用プラン キャッシュ webhook のハンドラなどであれば Functions URLs がよさそう 追加されたリソース Functions URLs のリリースに伴って FunctionUrlConfig という新しいリソースが増えています。1\nFunctionUrl (string) 発行された HTTP URL。\nFunctionArn (string) 設定されている Lambda 関数の ARN\nAuthType (string) 認可タイプ。 AWS_IAM または NONE\nCors (structure) CORS の設定。詳細は下記の項目。\nAllowCredentials (boolean) Cookie やその他のクレデンシャルを許可するかどうか。デフォルト false 。\nAllowHeaders (list) リクエスト時に許可するヘッダ (string) のリスト。\nAllowMethods (list) 許可するメソッド (string) のリスト。\nAllowOrigins (list) 許可する origin (string) のリスト。\nExposeHeaders (list) オリジンに対して公開したいレスポンスヘッダ (string) のリスト。\nMaxAge (integer) ブラウザの preflight request をキャッシュする時間 (秒)。デフォルト 0 。\nCreationTime (string) FunctionUrlConfig が作成された日時。ISO-8601 (YYYY-MM-DDThh:mm:ss.sTZD).\nLastModifiedTime (string) FunctionUrlConfig が更新された日時。ISO-8601 (YYYY-MM-DDThh:mm:ss.sTZD).\nAWS CLI に追加されたサブコマンド AWS CLI にも、そのリソースに対する create, delete, get, list, update のサブコマンドが増えています。\naws lambda help | grep function-url o create-function-url-config o delete-function-url-config o get-function-url-config o list-function-url-configs o update-function-url-config 例えば create-function-url-config のパラメータは下記のとおりです。\nSYNOPSIS create-function-url-config --function-name \u0026lt;value\u0026gt; [--qualifier \u0026lt;value\u0026gt;] --auth-type \u0026lt;value\u0026gt; [--cors \u0026lt;value\u0026gt;] [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] function-name URL を発行したい Lambda 関数の名前 または ARN を指定します。\nqualifier URL を発行したい Lambda 関数のエイリアス。指定しない場合 $LATEST になります。\nauth-type 認可方法。 AWS_IAM または NONE を指定します。\ncors CORS の設定。下記内容で設定します。\n{ \u0026#34;AllowCredentials\u0026#34;: true, \u0026#34;AllowHeaders\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;AllowMethods\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;AllowOrigins\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;ExposeHeaders\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;MaxAge\u0026#34;: 0 } 設定してみる だいぶ前に作って放置していた umiwind-lambda という関数があったので、それに対して設定してみます。\n今回はとりあえず試してすぐ消すので認可はなし、 CORS も * で設定して完全なパブリック状態で試す。この状態で URL が割れたらと思うと怖いですね。\naws lambda create-function-url-config \\ --function-name \u0026#39;umiwind-lambda\u0026#39; \\ --auth-type \u0026#39;NONE\u0026#39; \\ --cors \u0026#39;AllowCredentials=false,AllowMethods=GET,AllowOrigins=*\u0026#39; 出力。\n{ \u0026#34;FunctionUrl\u0026#34;: \u0026#34;https://ouws7qavn54XXXXXXXXXXXXXXXXXXXXX.lambda-url.ap-northeast-1.on.aws/\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:ap-northeast-1:000000000000:function:umiwind-lambda\u0026#34;, \u0026#34;AuthType\u0026#34;: \u0026#34;NONE\u0026#34;, \u0026#34;Cors\u0026#34;: { \u0026#34;AllowCredentials\u0026#34;: false, \u0026#34;AllowMethods\u0026#34;: [ \u0026#34;GET\u0026#34; ], \u0026#34;AllowOrigins\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;CreationTime\u0026#34;: \u0026#34;2022-04-06T23:32:56.130866Z\u0026#34; } curl で叩いてみます。\ncurl https://ouws7qavn54XXXXXXXXXXXXXXXXXXXXX.lambda-url.ap-northeast-1.on.aws/ | jq 出力。\n{ \u0026#34;ja\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022/04/07\u0026#34;, \u0026#34;vector\u0026#34;: \u0026#34;北北東\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;08:55\u0026#34;, \u0026#34;velocity\u0026#34;: \u0026#34;8 m\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;[2022/04/07 08:55 JST] 現在の風向は 北北東 風速は 8 m です\u0026#34; }, \u0026#34;en\u0026#34;: { \u0026#34;date\u0026#34;: \u0026#34;2022/04/07\u0026#34;, \u0026#34;vector\u0026#34;: \u0026#34;NNE\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;08:55\u0026#34;, \u0026#34;velocity\u0026#34;: \u0026#34;8 m\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;[2022/04/07 08:55 JST] The current wind direction is NNE and the wind speed is 8 m.\u0026#34; }, \u0026#34;message\u0026#34;: \u0026#34;ok\u0026#34; } 無事にうみほたるの風向・風速が取れました。簡単。\n削除 delete-function-url-config サブコマンドで削除します。\naws lambda delete-function-url-config \\ --function-name umiwind-lambda まとめ Lambda 関数を HTTP URL で invoke できるようになった AWS CLI は、慣習通り今はまだ v1 のみ利用可 v1 は 1.22.90 以降、 v2 は 2.5.4 以降でサポート aws lambda get-function-url-config help の出力から確認\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-ger-started-lambda-functions-urls/",
    "title": "Lambda 関数に個別の URL を設定できる AWS Lambda Function URLs を試す"
  },
  {
    "contents": "Amazon CloudFront が Server Timing に対応したみたいなので、既存の distribution に対して設定してどんな情報が得られるのか試してみます。\nServer Timing とは This specification enables a server to communicate performance metrics about the request-response cycle to the user agent. It also standardizes a JavaScript interface to enable applications to collect, process, and act on these metrics to optimize application delivery.\nServer Timing : https://w3c.github.io/server-timing/ W3C で定義されている仕様で、サーバに対するリクエスト/レスポンスに関する情報をレスポンスヘッダの Server-Timing フィールドを通じてクライアントに伝えることができるというものです。\nこれにより、キャッシュまたはオリジンサーバーからレスポンスを取得するのにかかった時間、リクエストがどのようにルーティングされてどこで時間が費やされたかを知ることができるようです。\nAmazon CloudFront でのサポート Starting today, you can configure your CloudFront distributions to include Server Timing headers to monitor CloudFront behavior and performance. Server Timing headers provide detailed performance information, such as whether content was served from cache when a request was received, how the request was routed to the CloudFront edge location, and how much time elapsed during each stage of the connection and response process.\nAmazon CloudFront now supports Server Timing headers CloudFront では、 ResponseHeadersPolicy の設定項目として Server Timing が追加され、 有効/無効 とサンプリングレートを設定します。\n現時点 (2022/04/01) では AWS CLI の最新バージョン (1.22.86, 2.5.1) でも対応しておらず、 create-response-headers-policy および update-response-headers-policy で設定できず、 get-response-headers-policy および list-response-headers-policies のレスポンスにも設定項目の情報は含まれていません。なので、マネジメントコンソール上でのみ設定・閲覧が可能です。\n設定して確認 ResponseHeadersPolicy の項目としては 有効/無効 とサンプリングレートのみで、作成した ResponseHeadersPolicy を任意の distribution の任意の behavior の Response headers policy にて設定して使います。\n手順については特に新しい要素もないので割愛、とりあえずサンプリングレートは 100% で設定して確認します。\nレスポンスの確認には httpie を使います。(curl もいいけど httpie のオプションが使いやすい。出力も見やすい。)\nhttp -p hH https://michimani.net/ CLI で http リクエストするなら HTTPie が便利 - michimani.net 出力の確認 CloudFront でキャッシュヒットした場合としなかった場合とで、それぞれ Server-Timing ヘッダに含まれる値を確認してみます。\nServer-Timing フィールドの値の詳細な説明については下記を参照してください。\nUnderstanding response headers policies #server-timing-header - Amazon CloudFront キャッシュヒットした場合 HTTP/1.1 200 OK Accept-Ranges: bytes Age: 1 Cache-Control: no-store Connection: keep-alive Content-Length: 10871 Content-Type: text/html;charset=UTF-8 Date: Fri, 01 Apr 2022 14:34:31 GMT ETag: \u0026#34;a9105ce3fc62d088f4c991060d32d77f\u0026#34; Last-Modified: Thu, 17 Mar 2022 09:41:20 GMT Server: AmazonS3 Server-Timing: cdn-cache-hit,cdn-pop;desc=\u0026#34;NRT12-C3\u0026#34;,cdn-rid;desc=\u0026#34;KwMMqgb8SoVYsOfXg7IFyIfE0nTpjdjNfwpPzDPQPdv7ldjaNqTJGw==\u0026#34;,cdn-hit-layer;desc=\u0026#34;EDGE\u0026#34; Via: 1.1 e72e0d477a3b173c0d7c54332be184a4.cloudfront.net (CloudFront) X-Amz-Cf-Id: KwMMqgb8SoVYsOfXg7IFyIfE0nTpjdjNfwpPzDPQPdv7ldjaNqTJGw== X-Amz-Cf-Pop: NRT12-C3 X-Cache: Hit from cloudfront x-amz-server-side-encryption: AES256 カンマ区切りで情報が含まれています。\ncdn-cache-hit, cdn-pop;desc=\u0026#34;NRT12-C3\u0026#34;, cdn-rid;desc=\u0026#34;KwMMqgb8SoVYsOfXg7IFyIfE0nTpjdjNfwpPzDPQPdv7ldjaNqTJGw==\u0026#34;, cdn-hit-layer;desc=\u0026#34;EDGE\u0026#34; cdn-cache-hit キャッシュヒットしたことがわかります。\ncdn-pop;desc=\u0026ldquo;NRT12-C3\u0026rdquo; CloudFront のどの POP (point of presence) がリクエストを処理したか。\nNRT は成田空港の IATA コードなので、そのあたりのエッジが処理したと思われる。\ncdn-rid;desc=\u0026ldquo;KwMMqgb8SoVYsOfXg7IFyIfE0nTpjdjNfwpPzDPQPdv7ldjaNqTJGw==\u0026rdquo; リクエスト ID。トラブルシュート用。\ncdn-hit-layer;desc=\u0026ldquo;EDGE\u0026rdquo; キャッシュヒットしたレイヤー。CloudFront がオリジンにリクエストを行わずにキャッシュからのレスポンスを返す場合に含まれる値。\nこの場合 EDGE なので POP のキャッシュにヒットしていることがわかります。\nキャッシュミスヒットの場合 HTTP/1.1 200 OK Accept-Ranges: bytes Cache-Control: no-store Connection: keep-alive Content-Length: 10871 Content-Type: text/html;charset=UTF-8 Date: Fri, 01 Apr 2022 14:34:06 GMT ETag: \u0026#34;a9105ce3fc62d088f4c991060d32d77f\u0026#34; Last-Modified: Thu, 17 Mar 2022 09:41:20 GMT Server: AmazonS3 Server-Timing: cdn-upstream-layer;desc=\u0026#34;EDGE\u0026#34;,cdn-upstream-dns;dur=0,cdn-upstream-connect;dur=9,cdn-upstream-fbl;dur=78,cdn-cache-miss,cdn-pop;desc=\u0026#34;NRT12-C3\u0026#34;,cdn-rid;desc=\u0026#34;0_6wUpeXAmA-0HC9ZWaMp9MnPjjlhtC4PmP7bHkfXXqYt-bMIxz5tw==\u0026#34; Via: 1.1 823128cacec2b9d382c65187bf76768e.cloudfront.net (CloudFront) X-Amz-Cf-Id: 0_6wUpeXAmA-0HC9ZWaMp9MnPjjlhtC4PmP7bHkfXXqYt-bMIxz5tw== X-Amz-Cf-Pop: NRT12-C3 X-Cache: Miss from cloudfront x-amz-server-side-encryption: AES256 cdn-upstream-layer;desc=\u0026#34;EDGE\u0026#34;, cdn-upstream-dns;dur=0, cdn-upstream-connect;dur=9, cdn-upstream-fbl;dur=78, cdn-cache-miss, cdn-pop;desc=\u0026#34;NRT12-C3\u0026#34;, cdn-rid;desc=\u0026#34;0_6wUpeXAmA-0HC9ZWaMp9MnPjjlhtC4PmP7bHkfXXqYt-bMIxz5tw==\u0026#34; cdn-upstream-layer;desc=\u0026ldquo;EDGE\u0026rdquo; CloudFront がオリジンに対してレスポンスをリクエストしたときに含まれる値。\nこの場合、 POP がオリジンに対してリクエストを送信したことがわかります。\ncdn-upstream-dns;dur=0 オリジンの DNS レコードの取得にかかった時間 (ミリ秒) 。\n0 の場合、 CloudFront がキャッシュしている DNS を結果を使用したか、既存の接続を再利用したことを示します。\ncdn-upstream-connect;dur=9 オリジンの DNS 解決が完了してからオリジンへの TCP 接続が完了するまでにかかった時間 (ミリ秒) 。\ncdn-upstream-fbl;dur=78 オリジンへの HTTP リクエストが完了してから、最初のバイト列が返ってくるまでにかかった時間 (ミリ秒) 。 (fbl = first byte latency)\ncdn-cache-miss キャッシュヒットせず、オリジンに対してリクエストしています。\ncdn-pop;desc=\u0026ldquo;NRT12-C3\u0026rdquo; CloudFront のどの POP (point of presence) がリクエストを処理したか。\nNRT は成田空港の IATA コードなので、そのあたりのエッジが処理したと考えられます。\ncdn-rid;desc=\u0026ldquo;0_6wUpeXAmA-0HC9ZWaMp9MnPjjlhtC4PmP7bHkfXXqYt-bMIxz5tw==\u0026rdquo; リクエスト ID。トラブルシュート用。\nまとめ CloudFront が ServerTiming に対応して、諸々の情報がレスポンスヘッダから取得できるようになりました。\n特に、キャッシュヒットしなかった場合にオリジンとの接続にかかった時間、オリジンからレスポンスが返ってくるまでにかかった時間がわかるようになったことで、パフォーマンス改善のための一つの情報として役に立ちそうです。\nAWS CLI/AWS SDK/Terraform で ResponseHeadersPolicy の設定項目を定義・閲覧できるようになったらまた試します。\n参考 Server Timing : https://w3c.github.io/server-timing/ Amazon CloudFront now supports Server Timing headers Understanding response headers policies - Amazon CloudFront Amazon CloudFrontがCDN-オリジン間のパフォーマンスを計測するServer Timingに対応しました | DevelopersIO ",
    "permalink": "https://michimani.net/post/aws-cloudfront-support-server-timing/",
    "title": "Amazon CloudFront が Server Timing に対応したみたいなので試してみる"
  },
  {
    "contents": "タイトルの通り、 macOS 12.3 にアップデートした直後から MacBook Pro (Intel) で外部モニターが 1 枚しか認識しなくなりました。\nMac mini と LG ディスプレイでの似たような挙動はいくつか報告されているようですが、 MacBook Pro での事象はあまり見ないので何かの参考になれば。(何の)\n解決策 事象についても把握していて、とにかく解決策が知りたいというかはここだけ読んで試してみてもらえればと思います。\n結論、DisplayPort 1.4 での接続ではなく、 HDMI で接続すれば 2 枚のモニターを認識してくれました。\n対象 MacBook Pro 13-inch, 2018, Four Thunderbolt 3 Ports 外部モニター BenQ EL2870U BenQ EL3270U 起こった事象 外部モニターが 1 枚しか認識しなくなった 2 枚接続した状態で再起動すると、認識はしているっぽい (システム環境設定のディスプレイには出てこない) が片方のモニターはブラックアウトしている 接続しているケーブルを抜き差しすると、それ以降はブラックアウトすらしない いつから macOS 12.2 から macOS 12.3 にアップデートした直後から。\n試したこと MacBook Pro 側 SMC リセット NVRAM リセット 外部モニターを接続した状態で再起動 外部モニターを接続していない状態で再起動、起動後に接続 システム終了 → 起動 接続まわり 普段は下記のように接続しています。\n[MacBook Pro] -- [CalDigit USB Pro Dock] -- DisplayPort -- [BenQ EL2870U] | `-- DisplayPort -- [BenQ EL3270U] この状態でダメだったので、下記の接続方法を試しました。\na. 1 枚は Dock 経由、もう 1 枚は USB Type-C で MBP と直接 [MacBook Pro] -- [CalDigit USB Pro Dock] -- DisplayPort -- [BenQ EL2870U] | `-- USB Type-C -- [BenQ EL3270U] これもだめ。\nb. 1 枚は USB Type-C で MBP と直接、1 枚は Apple 純正アダプタ経由で HDMI 接続 [MacBook Pro] -- [Apple USB Digital Multi Port] -- HDMI -- [BenQ EL2870U] | `-- USB Type-C -- [BenQ EL3270U] これもだめ。\nc. 2 枚とも Apple 純正アダプタ経由で HDMI 接続 [MacBook Pro] -- [Apple USB Digital Multi Port] -- HDMI -- [BenQ EL2870U] | `-- [Apple USB Digital Multi Port] -- HDMI -- [BenQ EL3270U] これは OK でした。\nちなみに会社から貸与されている MacBook Pro (13-inch, 2020, Four Thunderbolt 3 Ports) ではこの接続方法をとっており、 macOS 12.2 で問題なく 2 枚認識されています。\nDisplayPort のバージョンが問題？ DisplayPort 1.4 のディスプレイが原因という記事もあり、今回の事象と試した結果からもおそらくそうなのかなという見解です。\nひとこと せっかく大きいモニターが 2 枚あるのに 1 枚死んでるのは辛いですね。\n",
    "permalink": "https://michimani.net/post/gadget-macos-12-3-issue-for-intel-mac/",
    "title": "macOS 12.3 にアップデートしたら MacBook Pro (Intel) で外部モニターが 1 枚しか認識しなくなった"
  },
  {
    "contents": "何も書かないのはアレなので、簡単に 2021 年の振り返りをしておきます。\n去年の最後に書いていたこと 今年は 11 月に転職して環境が大きく変わったので、来年はもっとアプリケーション寄り、というかアーキテクチャとかの内容、あとはプロジェクトの進め方とかそのへんの知見を深めたいなと思います。あとは、 Go 言語をメインで触ることになったのでもっと Go の勉強します。\nhttps://michimani.net/post/other-retrospect-in-2020/ ここに書いていたことを中心に振り返ってみます。\nアプリケーション寄り、というかアーキテクチャとかの内容 AWS Lambda × RDS 辛さを身をもって感じた データ思考アプリケーションデザイン を読んで分散システムの辛さを知った 最後のほうでサービス分割されたアプリケーションのアーキテクチャについて設計した ざっくり書くとこんなくらいでしょうか。(少ない)\n全体的には AWS を使ったアプリケーションの運用について (ざっくり) は色々経験値がたまったかなと思います。ただ、その勢いで受験した SAP は 12 点足らずに不合格だったので、まだまだ初心者です。SAA の期限が切れる来年の 6 月までには合格したいところ。(ここで言質警察がくる)\nプロジェクトの進め方 半年くらいスクラムマスターやってみた (進行形) PdM 不在のプロジェクトで各所と仕様とかリリース日の調整とかした 項目としては少ないですが、この部分が大きいかなと。\nスクラムマスターをやる機会が転がってきたので、何もわからない状態でそのボールを拾って半年くらいスクラムマスターやってみました。現在進行形でやってます。おそらく世間一般に期待されるスクラムマスターとしての動きは出来ていない気がしますが、とにかくチームの障害を排除する動きを心がけて動きました。(動いています)\n各スクラムイベントの準備 (miro のボード作ったり Jira のボード作ったりなんやかんや) と進行、他部署・他チームとの調整などなどをやりました。このへんをやっているとまとまった時間を確保するのが難しく、自分が手を動かして実装したりすることは少なくなりました。やり始めてから数ヶ月は、手動かせないの辛いなと思っていましたが、自分が着手したタスクの進捗が芳しくなくてボトルネックになる場面もあって、障害を排除するための人間が障害になってしまっているな\u0026hellip;と思うこともありました。そうなっては本末転倒なので、一番の役割は障害の排除であると言い聞かせて最近はちょっと諦めというか気持ちの整理がついてきた感じです。\n難しい部分は多々ありますが、レトロスペクティブで使用する振り返りの手法が色々あったり、Jira 力が上がったり、そもそもプロジェクトってどうやったら上手く進むんや？というところを考えるようになったのは収穫かなと思います。\n他の会社さんがどんな感じでスクラム回してるのか気になるところです。\nGo の勉強します コードレビューを通じて日々勉強 CLI ツールとか Twitter API 用のライブラリ作ったりした Go に関しては、他のエンジニアが書いたコード読んだりコードレビューしてもらいながら日々勉強中です。\nスクラムマスターになって仕事で手を動かせない時期には、勉強もかねて Twitter API v2 用のライブラリを作ってみました。\nmichimani/gotwi 勢いで作ってみた割にはスターが 15 も付いたり (多くはないけど嬉しい)、 Twitter の developer ページ に載ったりしました。(嬉しい)\nTwitter API v2 はまだ続々と API が追加されていっているので、盆栽を育てるように細々とメンテしていこうと思います。\nその他 Chrome 拡張作って公開した Google Workspaces 用のアドオン作って公開した オンライン勉強会はあまり参加できなかった 体重は減らせなかった バイク乗り換えた まとめ 全体通してあまり進捗のない一年だったなと、主観的には思う。もう少し細かい期間でフィードバックを得られると嬉しいなと感じる場面があったり、今やってることが正解なのかがわかる日がいつなんだ？という不安さもある。ただ、今までやったことがないことをやっているのでその辺も手探りなのかなーと思ったり。でも何かしら評価、フィードバック欲しいなと思ったり。そんな年末です。(とは)\n来年こそは SAP 合格します。それではまた来年👋\n",
    "permalink": "https://michimani.net/post/other-retrospect-in-2021/",
    "title": "2021 年を雑に振り返ってみる"
  },
  {
    "contents": "はじめに この記事は 弁護士ドットコム Advent Calendar 2021 の 9 日目の記事です。\n昨日は @takky さんの 「 チームで内部API開発をするときのツール 」 の話でした。ぜひこちらの記事もご覧ください。\nちなみに去年は AWS CLI だけで Hugo のホスティング環境を構築するハンズオン的な記事 を書いてました。\n目次 今回書くこと 作ったもの ざっくり解説 CDK のディレクトリ構成 Go で Lambda 関数を実装する CDK で Go で実装した Lambda 関数を定義する CDK で Step Functions のステートマシンを定義する CDK のテスト デプロイした Lambda 関数、ステートマシンを手動で実行する まとめ 今回書くこと 今回は、この 1 年間で新たに触れた技術要素と、今までほとんど触っていなかった AWS のサービスを無理やり組み合わせて何かを作ってみた話です。\nこの 1 年間で新たに触れた技術要素は、 Go 言語です。\nGo 言語を本格的に実務で使うようになったのは弁護士ドットコムに転職してからなので、歴としてはちょうど 1 年くらいです。今回は、Go をランタイムとした Lambda 関数を実装します。Lambda 関数では、 宣伝も兼ねて 自作した Twitter API v2 の Go のライブラリを使って特定の文字列をツイートします。\n今までほとんど触っていなかった AWS のサービスとしては、 AWS Step Functions (以下、Step Functions (半角スペース警察に注意です)) を使ってみます。\nStep Functions に関しては、最近リリースされた AWS SDK Integration も試してみます。\nGo で Lambda 関数を書きたい方、 AWS CDK を使って Step Functions のステートマシンを構築してみたい方の参考になれば幸いです。\n作ったもの 作ったものは、 Step Functions のステートマシンとして AWS SDK Integration と Lambda 関数を繋げただけの簡素な翻訳ワークフローです。\nステートマシンは下記のような JSON を input として受け取ります。\n{ \u0026#34;sourceLang\u0026#34;: \u0026#34;ja\u0026#34;, \u0026#34;targetLang\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;inputText\u0026#34;: \u0026#34;こんにちは\u0026#34; } AWS SDK Integration では、受け取った input をもとに Amazon Translate の TranslateText を実行します。\n実行した結果の JSON から翻訳済みのテキストを下記のような JSON に埋め込めこんで、次の Lambda 関数に input として渡します。 { \u0026#34;inputText\u0026#34;: \u0026#34;Hello\u0026#34; } Lambda 関数は受け取った JSON の inputeText フィールドの文字列を、 Twitter API v2 の POST /v2/tweets にリクエストしてツイートします。\nレスポンスとしてツイートされてたツイートの ID が返ってくるので、その値を output します。 そして、これらの構成は AWS CDK (以下、CDK) で管理します。\nCDK については、最近 \u0026ldquo;RC\u0026rdquo; が取れてリリースされた v2.0.0 を TypeScript で使います。\nv2 使うなら CDK も Go 使えばいいのでは？ となりますが、 Go についてはまだ developer release なのでここでは TypeScript を使うことにします。\n作ったものは GitHub に置いています。以降はほぼこのリポジトリ内の説明になるので、中身だけ見たいという方はリポジトリの方をご覧ください。\nざっくり解説 ここからは作ったもの ( michimani/honyakutter-ts ) の解説をしていきます。\n解説の前提 解説するにあたって、下記の内容を前提とします。\nAWS CDK のバージョンは 2.0.0 デプロイ対象のリージョンは ap-northeast-1 (東京) AWS CLI のバージョンは v2.4.5 および 1.22.21 Go のバージョンは 1.17 ローカルの環境は macOS 11.6 (Intel) CDK のディレクトリ構成 まずは CDK で管理するアプリケーションのディレクトリ構成です。ドキュメント類、 Makefile、 Git 管理されないものなど、直接アプリケーションに関係ないものは省略しています。\nhonyakutter-ts ├── README.md ├── bin │ └── honyakutter-ts.ts ├── cdk.json ├── jest.config.js ├── lib │ └── honyakutter-ts-stack.ts ├── package-lock.json ├── package.json ├── resources │ ├── lambda.ts │ ├── lambdaFunctions │ │ └── tweet │ │ ├── go.mod │ │ ├── go.sum │ │ └── main.go │ ├── logs.ts │ └── stepFunctions.ts ├── test │ └── honyakutter-ts.test.ts ├── testdata │ ├── statemachine_input.json │ └── tweet_lambda_payload.json └── tsconfig.json cdk init --language=typescript で初期化したときに生成されるファイル群以外に追加しているのは resources ディレクトリと testdata ディレクトリです。\nresources reosurces ディレクトリでは、デプロイする AWS の各種リソースの定義をサービスごとに分けて配置しています。\nlambda.ts \u0026hellip; ツイートするための Lambda 関数を定義しています logs.ts \u0026hellip; Step Functions のステートマシンの実行結果を出力する CloudWatch Logs の LogGroup を定義しています stepFunctions.ts \u0026hellip; Step Functions のステートマシンを定義しています また、 Lambda 関数の実体 (コード) については lambdaFunctions ディレクトリに配置しています。\n今回は Go で実装したので、 lambdaFunctions/tweet の下に main.go と go.mod および go.sum があります。\ntestdata testdata ディレクトリには、デプロイした Lambda 関数を単体で起動、およびステートマシンを起動するときに input として渡す JSON を置いています。Lambda 関数およびステートマシンを手動で実行する際には AWS CLI を使います。(後述します)\nGo で Lambda 関数を実装する 今回は event (= input) として受け取った文字列に、実行時の時刻を付与してツイートする Lambda 関数を Go で実装しました。 main.go はこんな感じです。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; \u0026#34;github.com/michimani/gotwi\u0026#34; \u0026#34;github.com/michimani/gotwi/tweets\u0026#34; \u0026#34;github.com/michimani/gotwi/tweets/types\u0026#34; ) const ( OAuthTokenEnvKeyName = \u0026#34;GOTWI_ACCESS_TOKEN\u0026#34; OAuthTokenSecretEnvKeyName = \u0026#34;GOTWI_ACCESS_TOKEN_SECRET\u0026#34; ) type TweetEvent struct { Text TweetText `json:\u0026#34;inputText\u0026#34;` } type TweetText string func (t TweetText) withCurrentTime() string { now := time.Now() return fmt.Sprintf(\u0026#34;[%s] %s\u0026#34;, now, t) } func handleRequest(ctx context.Context, event TweetEvent) (string, error) { fmt.Printf(\u0026#34;%#+v\\n\u0026#34;, event) c, err := newTiwtterClient() if err != nil { return \u0026#34;\u0026#34;, err } tweetText := event.Text.withCurrentTime() tweetID, err := tweet(c, tweetText) if err != nil { return \u0026#34;\u0026#34;, err } return tweetID, nil } func newTiwtterClient() (*gotwi.GotwiClient, error) { in := \u0026amp;gotwi.NewGotwiClientInput{ AuthenticationMethod: gotwi.AuthenMethodOAuth1UserContext, OAuthToken: os.Getenv(OAuthTokenEnvKeyName), OAuthTokenSecret: os.Getenv(OAuthTokenSecretEnvKeyName), } return gotwi.NewGotwiClient(in) } func tweet(c *gotwi.GotwiClient, text string) (string, error) { p := \u0026amp;types.ManageTweetsPostParams{ Text: gotwi.String(text), } res, err := tweets.ManageTweetsPost(context.Background(), c, p) if err != nil { return \u0026#34;\u0026#34;, err } return gotwi.StringValue(res.Data.ID), nil } func main() { lambda.Start(handleRequest) } ツイートは Twitter API v2 の POST /v2/tweet にリクエストします。その際、 gotwi というライブラリを使用しています。Go で Twitter API v2 を使える良い感じのライブラリがなかったので、 Go の勉強も兼ねて自作しました。\nTwitter Developer で取得した API Key/API Key Secret および Access Token/Access Token Secret を環境変数およびクライアント生成時のパラメータとして使用することで、 Twitter の各種 API を実行できるようになっています。\nStream 系 API についてはまだサポートできていないのですが、それ以外の v2 で利用可能な API については対応しています。star ください\n少し話が逸れました。\nGo で Lambda 関数を実装する際には、 Context と Event を受け取る関数 (今回だと handleRequest()) を定義して、 main 関数内でその関数を lambda.Start に渡すようにします。\nEvent に関しては JSON で渡されるので、パースできるように構造体を定義しておきます。(今回であれば TweetEvent)\nLambda 関数のレスポンスとして、今回は単純に String を返していますが、 JSON を返したい場合は別途 JSON タグをつけたフィールドを持つ構造体をレスポンス用に定義して、ハンドラー関数でそれを返すようにします。レスポンス用の構造体を定義しているサンプルも置いておきます。\nGo で実装した Lambda 関数は、ビルドしてから zip として、または S3 にアップロードしてからデプロイする必要があります。ビルドする際、 OS は linux を、アーキテクチャは amd64 を、それぞれ指定します。\nGOOS=linux GOARCH=amd64 go build -o bin/main ハンドラー関数とメイン関数の定義、およびビルド時の OS とアーキテクチャに気をつければ、あとは普通に Go のアプリケーションとして実装するだけです。\nCDK で Go で実装した Lambda 関数を定義する Go で実装した Lambda を CDK で定義するには、下記のようにします。\n1import { Duration } from \u0026#34;aws-cdk-lib\u0026#34;; 2import { Construct } from \u0026#34;constructs\u0026#34;; 3import * as lambda from \u0026#34;aws-cdk-lib/aws-lambda\u0026#34;; 4 5const memorySize = 128; 6const timeout = 60; 7 8/** 9 * Lambda function that tweet a text. 10 * @param scope 11 * @returns lambda.Function 12 */ 13function tweetLambdaFunction(scope: Construct): lambda.Function { 14 return new lambda.Function(scope, \u0026#34;TweetLambdaFunction\u0026#34;, { 15 functionName: \u0026#34;honyakutter-ts-tweet-function\u0026#34;, 16 description: \u0026#34;Tweet text.\u0026#34;, 17 code: new lambda.AssetCode(\u0026#34;./resources/lambdaFunctions/tweet/bin/\u0026#34;), 18 handler: \u0026#34;main\u0026#34;, 19 runtime: lambda.Runtime.GO_1_X, 20 memorySize: memorySize, 21 timeout: Duration.seconds(timeout), 22 environment: { 23 GOTWI_API_KEY: process.env.TWITTER_API_KEY!, 24 GOTWI_API_KEY_SECRET: process.env.TWITTER_API_KEY_SECRET!, 25 GOTWI_ACCESS_TOKEN: process.env.TWITTER_ACCESS_TOKEN!, 26 GOTWI_ACCESS_TOKEN_SECRET: process.env.TWITTER_ACCESS_TOKEN_SECRET! 27 } 28 }); 29} 30 31export { tweetLambdaFunction }; ポイントになるのは Lambda 関数の実体 (= コード) をどうやって渡すか、です。\nPython や Node.js であれば new lambda.InlineCode() でインラインでコードを指定できますが、 Go の場合は new lambda.AssetCode() でビルド後のバイナリが配置されたディレクトリを指定します。(17 行目)\nちなみに CDK v1 には @aws-cdk/aws-lambda-go というモジュールがあり、より簡単に Go で実装した Lambda 関数を定義してデプロイすることができます。(ローカルマシンに Dokcer がインストールされている必要があります)\nCDK v2 にも同様のモジュールはありますが、この記事を書いている時点 (2021-12-09 時点) では α 版でした。\n@aws-cdk/aws-lambda-go-alpha CDK で Step Functions のステートマシンを定義する 今回は resources/stepFunctions.ts で下記のように定義しています。\n1import { Duration } from \u0026#34;aws-cdk-lib\u0026#34;; 2import { Construct } from \u0026#34;constructs\u0026#34;; 3import * as lambda from \u0026#34;aws-cdk-lib/aws-lambda\u0026#34;; 4import * as stepfunctions from \u0026#34;aws-cdk-lib/aws-stepfunctions\u0026#34;; 5import * as tasks from \u0026#34;aws-cdk-lib/aws-stepfunctions-tasks\u0026#34;; 6import { StateMachineLogGroup } from \u0026#34;./logs\u0026#34;; 7 8const stateMachineTimeout = 300; 9const taskTimeout = 60; 10 11function translateTweetStateMaschine( 12 scope: Construct, 13 tweetFunc: lambda.Function 14) { 15 const initState = new stepfunctions.Pass(scope, \u0026#34;init\u0026#34;, { 16 comment: \u0026#34;init state\u0026#34; 17 }); 18 19 // Translate state 20 const translateResultSelector: { [key: string]: string } = { 21 \u0026#34;inputText.$\u0026#34;: \u0026#34;$.TranslatedText\u0026#34; 22 }; 23 const callAWSServiceProps: tasks.CallAwsServiceProps = { 24 service: \u0026#34;Translate\u0026#34;, 25 action: \u0026#34;translateText\u0026#34;, 26 iamResources: [\u0026#34;*\u0026#34;], 27 iamAction: \u0026#34;translate:TranslateText\u0026#34;, 28 parameters: { 29 SourceLanguageCode: stepfunctions.JsonPath.stringAt(\u0026#34;$.sourceLang\u0026#34;), 30 TargetLanguageCode: stepfunctions.JsonPath.stringAt(\u0026#34;$.targetLang\u0026#34;), 31 Text: stepfunctions.JsonPath.stringAt(\u0026#34;$.inputText\u0026#34;) 32 }, 33 resultSelector: translateResultSelector 34 }; 35 const translateState = new tasks.CallAwsService( 36 scope, 37 \u0026#34;TranslateByAmazonTranslate\u0026#34;, 38 callAWSServiceProps 39 ); 40 41 // Tweet state 42 const tweetState = lambdaFunctionToTask(scope, tweetFunc, {}); 43 44 const definition = initState.next(translateState).next(tweetState); 45 46 const logGroup = StateMachineLogGroup( 47 scope, 48 \u0026#34;TranslateTweetStateMachineLogGroup\u0026#34; 49 ); 50 new stepfunctions.StateMachine(scope, \u0026#34;TranslateTweetStateMaschine\u0026#34;, { 51 stateMachineName: \u0026#34;honyakutter-ts-translate-tweet-state-maschine\u0026#34;, 52 stateMachineType: stepfunctions.StateMachineType.EXPRESS, 53 timeout: Duration.seconds(stateMachineTimeout), 54 definition: definition, 55 logs: { 56 destination: logGroup, 57 level: stepfunctions.LogLevel.ALL 58 } 59 }); 60} 61 62function lambdaFunctionToTask( 63 scope: Construct, 64 func: lambda.Function, 65 resultSelector: { [key: string]: string } 66): tasks.LambdaInvoke { 67 const props: tasks.LambdaInvokeProps = { 68 lambdaFunction: func, 69 invocationType: tasks.LambdaInvocationType.REQUEST_RESPONSE, 70 timeout: Duration.seconds(taskTimeout), 71 resultSelector: resultSelector 72 }; 73 74 return new tasks.LambdaInvoke(scope, func.functionName, props); 75} 76 77export { translateTweetStateMaschine }; ハイライトしている箇所がステートマシンの各ステートになっています。ひとつずつ見ていきます。\n何もしない最初のステート 15const initState = new stepfunctions.Pass(scope, \u0026#34;init\u0026#34;, { 16 comment: \u0026#34;init state\u0026#34; 17}); これは一番最初に置くステート (必ず置くわけではない) で、Type は Pass です。Pass は input をそのまま output として次のステートに渡します。\nAWS SDK Integration で翻訳を実行するステート 20 const translateResultSelector: { [key: string]: string } = { 21 \u0026#34;inputText.$\u0026#34;: \u0026#34;$.TranslatedText\u0026#34; 22 }; 23 const callAWSServiceProps: tasks.CallAwsServiceProps = { 24 service: \u0026#34;Translate\u0026#34;, 25 action: \u0026#34;translateText\u0026#34;, 26 iamResources: [\u0026#34;*\u0026#34;], 27 iamAction: \u0026#34;translate:TranslateText\u0026#34;, 28 parameters: { 29 SourceLanguageCode: stepfunctions.JsonPath.stringAt(\u0026#34;$.sourceLang\u0026#34;), 30 TargetLanguageCode: stepfunctions.JsonPath.stringAt(\u0026#34;$.targetLang\u0026#34;), 31 Text: stepfunctions.JsonPath.stringAt(\u0026#34;$.inputText\u0026#34;) 32 }, 33 resultSelector: translateResultSelector 34 }; 35 const translateState = new tasks.CallAwsService( 36 scope, 37 \u0026#34;TranslateByAmazonTranslate\u0026#34;, 38 callAWSServiceProps 39 ); 続いては、input 内の文字列を翻訳するステートで、 Type は Task です。Task では、 Lambda 関数を実行したり、 ECS タスクを起動したり、 AWS SDK Integration で AWS の各サービスのアクションを実行することができます。\nここでは AWS SDK Integration で Amazon Translate の TranslateText を実行しています。\nAWS SDK Integration で各サービスのアクションを実行する場合は、 tasks.CallAwsService() でタスクを生成します。その際の props で、 \u0026ldquo;どのサービスの\u0026rdquo;、 \u0026ldquo;どのアクションを\u0026rdquo;、 \u0026ldquo;どんなパラメータで\u0026rdquo; 実行するかを指定します。実行時のパラメータとしてステートへの input 内の値を使用する場合は JsonPath.stringAt() を使用します。\n今回、このステートには下記のような JSON が input として渡されるので、それぞれ \u0026quot;$.sourceLang\u0026quot; のようにして取得し、 TranslteText の各パラメータにセットしています。\n{ \u0026#34;sourceLang\u0026#34;: \u0026#34;ja\u0026#34;, \u0026#34;targetLang\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;inputText\u0026#34;: \u0026#34;こんにちは\u0026#34; } また、このステートでは ResultSelector として \u0026quot;inputText.$\u0026quot;: \u0026quot;$.TranslatedText\u0026quot; を指定しています。これは、この Task の実行結果をステートの output として次のステートに渡すために良い感じに加工するための設定です。今回であれば、ResultSelector を指定しない場合は Amazon Translate の TranslateText の実行結果である下記のような JSON がそのままこのステートの output となり、次のステートに渡されます。\n{ \u0026#34;SourceLanguageCode\u0026#34;: \u0026#34;ja\u0026#34;, \u0026#34;TargetLanguageCode\u0026#34;: \u0026#34;en\u0026#34;, \u0026#34;TranslatedText\u0026#34;: \u0026#34;Hello\u0026#34; } 次のステートである Lambda 関数には {\u0026quot;inputText\u0026quot;: \u0026quot;hogehoge\u0026quot;} という JSON を event (= input) として渡す必要があります。なので、この翻訳ステートの output を ResultSelector で加工しています。\nResultSelector を \u0026quot;inputText.$\u0026quot;: \u0026quot;$.TranslatedText\u0026quot; のように定義することで、このステートの output は下記のような JSON になります。\n{ \u0026#34;inputText\u0026#34;: \u0026#34;Hello\u0026#34; } ツイートする Lamdba 関数を実行するステート 42const tweetState = lambdaFunctionToTask(scope, tweetFunc, {}); 続いては、 input 内の文字列をツイートする Lambda 関数を実行するステートで、 Type はこちらも Task です。このステートでは ResultSelector は指定していないので、 Lambda 関数からのレスポンスがそのままステートの output になります。\nステートマシンの定義を生成 最後に、これまで定義したステートを繋げてステートマシンの定義を生成します。\n44const definition = initState.next(translateState).next(tweetState); 「CDK で Step Fucntions のステートマシンを定義するの、めっちゃ楽！」 というのは以前から聞いていたのですが、その楽さを身を以て感じました。\nやっているのは、各ステートを next() で繋いでいるだけです。直感的でわかりやすくてめっちゃ楽ですねこれは。\n例えば、 terraform でこれと同じ定義をしようと思うと、下記のような記述をする必要があります。\nresource \u0026#34;aws_sfn_state_machine\u0026#34; \u0026#34;sfn_state_machine\u0026#34; { name = \u0026#34;honyakutter-ts-translate-tweet-state-maschine\u0026#34; type = \u0026#34;EXPRESS\u0026#34; definition = \u0026lt;\u0026lt;EOF { \u0026#34;StartAt\u0026#34;: \u0026#34;init\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;init\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Pass\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;init state\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;TranslateByAmazonTranslate\u0026#34; }, \u0026#34;TranslateByAmazonTranslate\u0026#34;: { \u0026#34;Next\u0026#34;: \u0026#34;honyakutter-ts-tweet-function\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;ResultSelector\u0026#34;: { \u0026#34;inputText.$\u0026#34;: \u0026#34;$.TranslatedText\u0026#34; }, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:translate:translateText\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;SourceLanguageCode.$\u0026#34;: \u0026#34;$.sourceLang\u0026#34;, \u0026#34;TargetLanguageCode.$\u0026#34;: \u0026#34;$.targetLang\u0026#34;, \u0026#34;Text.$\u0026#34;: \u0026#34;$.inputText\u0026#34; } }, \u0026#34;honyakutter-ts-tweet-function\u0026#34;: { \u0026#34;End\u0026#34;: true, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;Lambda.ServiceException\u0026#34;, \u0026#34;Lambda.AWSLambdaException\u0026#34;, \u0026#34;Lambda.SdkClientException\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 2, \u0026#34;MaxAttempts\u0026#34;: 6, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;TimeoutSeconds\u0026#34;: 60, \u0026#34;ResultSelector\u0026#34;: {}, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;arn:aws:lambda:ap-northeast-1:XXXXXXXXXXXX:function:honyakutter-ts-tweet-function\u0026#34;, \u0026#34;Payload.$\u0026#34;: \u0026#34;$\u0026#34;, \u0026#34;InvocationType\u0026#34;: \u0026#34;RequestResponse\u0026#34; } } }, \u0026#34;TimeoutSeconds\u0026#34;: 300 } EOF } この definition の部分は手書きです。辛いですねこれは。\nCDK のテスト CDK のテストとは、定義したリソースが正しく構築されるかをテストします。今回であれば、 Lambda 関数、 Step Functions のステートマシンが定義した通りに構築されるかをテストします。\nと言っても実際にデプロイして構築されるかをチェックするわけではなく、 TypeScript で記述した定義から CloudFormation のテンプレートを生成し、そのテンプレートに対して期待するリソースが存在するか、というテストになります。具体的には、 Lambda 関数が構築されることのテストコードは下記のようになります。\n1import * as cdk from \u0026#34;aws-cdk-lib\u0026#34;; 2import { Template } from \u0026#34;aws-cdk-lib/assertions\u0026#34;; 3import * as HonyakutterTs from \u0026#34;../lib/honyakutter-ts-stack\u0026#34;; 4 5test(\u0026#34;Tweet Lambda function Created\u0026#34;, () =\u0026gt; { 6 // set environment values for test 7 process.env.TWITTER_API_KEY = \u0026#34;twitter_api_key_for_test\u0026#34;; 8 process.env.TWITTER_API_KEY_SECRET = \u0026#34;twitter_api_key_secret_for_test\u0026#34;; 9 process.env.TWITTER_ACCESS_TOKEN = \u0026#34;twitter_access_token_for_test\u0026#34;; 10 process.env.TWITTER_ACCESS_TOKEN_SECRET = \u0026#34;twitter_access_token_secret_for_test\u0026#34;; 11 12 const app = new cdk.App(); 13 const stack = new HonyakutterTs.HonyakutterTsStack(app, \u0026#34;StackForTest\u0026#34;); 14 const template = Template.fromStack(stack); 15 16 template.hasResourceProperties(\u0026#34;AWS::Lambda::Function\u0026#34;, { 17 FunctionName: \u0026#34;honyakutter-ts-tweet-function\u0026#34;, 18 Description: \u0026#34;Tweet text.\u0026#34;, 19 Runtime: \u0026#34;go1.x\u0026#34;, 20 Handler: \u0026#34;main\u0026#34;, 21 MemorySize: 128, 22 Timeout: 60, 23 Environment: { 24 Variables: { 25 GOTWI_API_KEY: \u0026#34;twitter_api_key_for_test\u0026#34;, 26 GOTWI_API_KEY_SECRET: \u0026#34;twitter_api_key_secret_for_test\u0026#34;, 27 GOTWI_ACCESS_TOKEN: \u0026#34;twitter_access_token_for_test\u0026#34;, 28 GOTWI_ACCESS_TOKEN_SECRET: \u0026#34;twitter_access_token_secret_for_test\u0026#34; 29 } 30 } 31 }); 32}); 12, 13 行目でスタックを生成し、そのスタックをもとに 14 行目で CloudFormation のテンプレートを生成しています。\nそして 16 行目で、生成したテンプレートが期待するプロパティを持つリソースを含んでいるかをテストしています。\nデプロイした Lambda 関数、ステートマシンを手動で実行する 今回デプロイされるメインのリソースとしては Lambda 関数と Step Functions のステートマシンです。(その他 IAM Role/IAM Policy/CloudWatch Logs の LogGroup 等ももちろんあります)\nデプロイ後の動作確認方法として、 Lambda 関数単体の実行、ステートマシンとしての実行をそれぞれ手動でできるような準備をしました。冒頭で testdata ディレクトリについて触れましたが、そこに配置した JSON を使ってそれぞれ手動実行します。\n実行するにあたっては AWS CLI を使用します。使用するバージョンは v2.4.5 および 1.22.21 です。 v2 と v1 では若干コマンドに違いがあるので、それについては後述します。\nLambda 関数単体の実行 Lambda 関数の実行には下記コマンドを実行します。(v2 で実行する場合のコマンド)\naws lambda invoke \\ --function-name honyakutter-ts-tweet-function \\ --invocation-type Event \\ --region ap-northeast-1 \\ --payload fileb://testdata/tweet_lambda_payload.json \\ out このコマンドに関して、 v1 で実行する場合は下記のように変更します。\naws lambda invoke \\ --function-name honyakutter-ts-tweet-function \\ --invocation-type Event \\ --region ap-northeast-1 \\ - --payload fileb://testdata/tweet_lambda_payload.json \\ + --payload file://testdata/tweet_lambda_payload.json \\ out これは AWS CLI の v2 と v1 でバイナリパラメータの扱いが違うためです。詳細については今回の記事の内容からはズレるので、下記ドキュメントを参照してください。\n破壊的変更 - AWS CLI バージョン 1 からバージョン 2 への移行 - AWS Command Line Interface ステートマシンの開始 Step Functions のステートマシンを開始するには、まずは下記コマンドを実行します。\nSTATEMACHINE_ARN=$( aws stepfunctions list-state-machines \\ --query \u0026#34;stateMachines[?name==\u0026#39;honyakutter-ts-translate-tweet-state-maschine\u0026#39;].stateMachineArn\u0026#34; \\ --output text ) \u0026amp;\u0026amp; echo \u0026#34;${STATEMACHINE_ARN}\u0026#34; 続いて、取得した ARN をもとにステートマシンを開始します。\naws stepfunctions start-execution \\ --state-machine-arn \u0026#34;${STATEMACHINE_ARN}\u0026#34; \\ --input file://testdata/statemachine_input.json うまくいくと英語に翻訳された文章がツイートされます。\n[2021-12-07 17:40:17.516840688 +0000 UTC m=+0.036532198] Hello. This is translated from Japanese to English in Amazon Translate (AWS SDK Task State) using Step Functions and tweeted with Lambda functions It is a sentence that was made.\n\u0026mdash; michimani Lv.861 (@michimani210) December 7, 2021 まとめ 弁護士ドットコム Advent Calendar 2021 の 9 日目の記事として、Go と AWS を無理やり混ぜ込んだ話でした。\n去年のハンズオン形式の記事 とは違って単純なやってみた記事担ってしまいましたが、冒頭に書いた通り、Go で Lambda 関数を書きたい方、 AWS CDK を使って Step Functions のステートマシンを構築してみたい方の参考になれば幸いです。(早く AWS CDK (golang) で触ってみた話も書きたい)\n明日の担当は @takapiya さんです。お楽しみに！\n",
    "permalink": "https://michimani.net/post/aws-deploy-step-functions-state-machine-using-cdk-v2/",
    "title": "AWS Step Functions を使った翻訳ワークフローを AWS CDK v2 (TypeScript) で構築してみた話"
  },
  {
    "contents": "AWS のサービス条件 (AWS Service Terms) が 2021年 12月 3日に更新されていました。毎年 re:Invent のこの時期になると話題(?)になる Beta 版とプレビュー版の扱いについて、関連する条項を確認してみました。\n注意 これは 2021 年 12 月 6 日時点の情報をもとに書いています\n目次 Beta 版とプレビュー版に関するこれまで 2019年の夏頃 2020年の秋頃 2021年 12月 3日現在のサービス条件 結局どうなの？ Beta 版とプレビュー版に関するこれまで 2019年の夏頃 2019年の 7月頃、当時はまだ Beta 版だった AWS Chatbot の記事をブログに書いたところ、Twitter で指摘を受けて非公開にする ということがありました。当時は Beta 版に関する情報公開について何も考えていなかったので、指摘していただいた方には感謝しています。\nその時に自分なりに調べたり AWS のサポートに問い合わせたときのブログは下記です。\n結論、その当時は Beta 版および プレビュー とされているサービスに関する情報はすべて秘密情報であり、個人のブログ等で公開することは NG という結果でした。\n2020年の秋頃 それから約一年経った 2020年の秋頃、クラスメソッドさんのブログで下記のような記事が公開されました。\nこの記事によると、\nいつ時点の情報であるかを明記 不具合や改善リクエストはAWSへフィードバックを行う これらを明記すればブログでのアウトプットは問題ないとのことでした。\nただし、これはあくまでも AWS からクラスメソッドさんへの回答であり、一個人もこの流れにのってしまっていいのか不安だったので、個人的には Beta 版およびプレビュー版についてはひっそり触っていました。\n2021年 12月 3日現在のサービス条件 サービス条件の最新は 2021年 12月 3日、つい最近です。1\n正直、 2019年の一件以来ちゃんとサービス条件の更新を追っていたわけではないのですが、この最新版での Beta 版およびプレビュー版に関する記述を再度確認しておきます。\nBeta 版およびプレビュー版に関する扱いについては、 2. Betas and Previews の 2.5 で書かれています。\n2.5. Test Observations, Suggestions concerning a Beta Service or Beta Region, and any other information about or involving (including the existence of) any Beta Service or Beta Region are considered AWS Confidential Information.\nとりあえず、 AWS の秘密情報である ということは書かれていますが、公開してはダメ ということは 2.5 および 2 の他の項にも書かれていません。\nでは、ここで言う AWS Confidential Information とは何なのか。それは 1. Universal Service Terms (Applicable to All Services) の 1.7 に書かれています。\n1.7. If your Agreement does not include a provision on AWS Confidential Information, and you and AWS do not have an effective non-disclosure agreement in place, then you agree that you will not disclose AWS Confidential Information (as defined in the AWS Customer Agreement), except as required by law.\nAWS との契約書に秘密情報に関する条項が含まれておらず、有効な秘密保持契約を結んでいない場合は、法律で要求される場合を除いて AWS 秘密情報を開示しないことに同意する。 という内容です。つまり、何かしら AWS との契約時に秘密情報に関する条項を含んで契約しない場合は Beta 版およびプレビュー版に関する情報は開示できない、と読みとれます。\n結局どうなの？ クラメソさんの 2020 年の記事では条件付きで公開可となっていましたが、サービス条件を読む限りではやはり公開はできないのかなと。\n結局よくわからんので、 Beta 版およびプレビュー版についてはひっそり触ってひっそりフィードバックを送ろうと思います。\nこの記事を書いている時点で、日本語版のサービス条件の最終更新は 2021 年 10 月 5 日です。英語版で内容確認しましょう。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-service-terms-update-at-20211203/",
    "title": "AWS のサービス条件がつい最近更新されていたので Beta 版とプレビュー版の扱いについて確認してみた"
  },
  {
    "contents": "Application Load Balancer のパスベースルーティングにて、パスをワイルドカード指定しなかった場合のクエリパラメータがどのような扱いになるのかを確かめてみました。\n目次 結論 やったこと 手順 Application Load Balancer 作成 リスナーを追加 パスベースのルーティングを追加 クエリパラメータを付与してリクエスト 掃除 セキュリティグループのインバウンドルールを削除 ALB を削除 結論 The path pattern is applied only to the path of the URL, not to its query parameters. It is applied only to visible ASCII characters; control characters (0x00 to 0x1f and 0x7f) are excluded.\nListeners for your Application Load Balancers - Elastic Load Balancing ざっくり日本語訳。\nパスパターンは、URLのパスにのみ適用され、クエリパラメーターには適用されません。また、制御文字（0x00～0x1f、0x7f）を除く、可視のASCII文字にのみ適用されます。\nやったこと ALB を作成 固定レスポンスを返すリスナーを追加 パスベースのルーティングを追加 各パスへのリクエストとレスポンスを確認 クエリパラメータを付与した場合のレスポンスを確認 手順 Application Load Balancer 作成 SUBNET_ID_1=\u0026#34;subnet-0561f45xxxxxxxxxx\u0026#34; SUBNET_ID_2=\u0026#34;subnet-036c672xxxxxxxxxx\u0026#34; \\ ALB_ARN=$( aws elbv2 create-load-balancer \\ --name path-routeing-lb \\ --subnets \u0026#34;${SUBNET_ID_1}\u0026#34; \u0026#34;${SUBNET_ID_2}\u0026#34; \\ --region ap-northeast-1 \\ --query \u0026#34;LoadBalancers[0].LoadBalancerArn\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${ALB_ARN}\u0026#34; 出力\narn:aws:elasticloadbalancing:ap-northeast-1:000000000000:loadbalancer/app/path-routeing-lb/aac5a9xxxxxxxxxx リスナーを追加 LISTENER_ARN=$( aws elbv2 create-listener \\ --load-balancer-arn \u0026#34;${ALB_ARN}\u0026#34; \\ --protocol HTTP \\ --port 80 \\ --default-actions \u0026#39;{\u0026#34;Type\u0026#34;:\u0026#34;fixed-response\u0026#34;,\u0026#34;FixedResponseConfig\u0026#34;:{\u0026#34;MessageBody\u0026#34;: \u0026#34;{\\\u0026#34;message\\\u0026#34;:\\\u0026#34;This is the default action.\\\u0026#34;}\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;200\u0026#34;,\u0026#34;ContentType\u0026#34;:\u0026#34;application/json\u0026#34;}}\u0026#39; \\ --query \u0026#34;Listeners[0].ListenerArn\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${LISTENER_ARN}\u0026#34; 出力\narn:aws:elasticloadbalancing:ap-northeast-1:000000000000:listener/app/path-routeing-lb/aac5a9xxxxxxxxxx/8fea4fxxxxxxxxxx 追加したリスナーが正しく動作するか確認します。\nまずは作成した ALB の DNS 名を取得しておきます。\nDNS_NAME=$( aws elbv2 describe-load-balancers \\ --load-balancer-arns \u0026#34;${ALB_ARN}\u0026#34; \\ --query \u0026#34;LoadBalancers[0].DNSName\u0026#34; \\ --output text ) この時点ではセキュリティグループのインバウンド設定によってアクセスできないので、自分のマシンの IP からのみ 80 ポートにアクセスできるようにします。\nまず セキュリティグループ ID を取得します。\nSG_ID=$( aws elbv2 describe-load-balancers \\ --load-balancer-arns \u0026#34;${ALB_ARN}\u0026#34; \\ --query \u0026#34;LoadBalancers[0].SecurityGroups[0]\u0026#34; \\ --output text ) 続いて、自分のマシンの IP を取得します。\nMY_IP=$(curl http://checkip.amazonaws.com/) それらの値を使用してインバウンドルールを作成します。\naws ec2 authorize-security-group-ingress \\ --group-id \u0026#34;${SG_ID}\u0026#34; \\ --protocol tcp \\ --port 80 \\ --cidr \u0026#34;${MY_IP}\u0026#34;/32 これで ALB に対して DNS 名でアクセスできるようになったので、 httpie1 でリクエストを送信してみます。\n$ http \u0026#34;${DNS_NAME}\u0026#34; HTTP/1.1 200 OK Connection: keep-alive Content-Length: 41 Content-Type: application/json; charset=utf-8 Date: Mon, 23 Aug 2021 14:06:56 GMT Server: awselb/2.0 { \u0026#34;message\u0026#34;: \u0026#34;This is the default action.\u0026#34; } 設定した固定値のレスポンスが返ってきました。\nパスベースのルーティングを追加 続いて、パスベースのルーティングを追加します。\n今回は、下記のようなルーティングを追加してみます。\n/not-found は 404 レスポンスを返却 /error は 500 レスポンスを返却 aws elbv2 create-rule --listener-arn \u0026#34;${LISTENER_ARN}\u0026#34; --priority 1 \\ --conditions Field=path-pattern,Values=\u0026#39;/not-found\u0026#39; \\ --actions \u0026#39;{\u0026#34;Type\u0026#34;:\u0026#34;fixed-response\u0026#34;,\u0026#34;FixedResponseConfig\u0026#34;:{\u0026#34;MessageBody\u0026#34;: \u0026#34;{\\\u0026#34;message\\\u0026#34;:\\\u0026#34;This is 404 path.\\\u0026#34;}\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;404\u0026#34;,\u0026#34;ContentType\u0026#34;:\u0026#34;application/json\u0026#34;}}\u0026#39; \\ \u0026amp;\u0026amp; aws elbv2 create-rule --listener-arn \u0026#34;${LISTENER_ARN}\u0026#34; --priority 2 \\ --conditions Field=path-pattern,Values=\u0026#39;/error\u0026#39; \\ --actions \u0026#39;{\u0026#34;Type\u0026#34;:\u0026#34;fixed-response\u0026#34;,\u0026#34;FixedResponseConfig\u0026#34;:{\u0026#34;MessageBody\u0026#34;: \u0026#34;{\\\u0026#34;message\\\u0026#34;:\\\u0026#34;This is 500 path.\\\u0026#34;}\u0026#34;,\u0026#34;StatusCode\u0026#34;:\u0026#34;500\u0026#34;,\u0026#34;ContentType\u0026#34;:\u0026#34;application/json\u0026#34;}}\u0026#39; それぞれのパスにアクセスしてみます。\n$ http \u0026#34;${DNS_NAME}/not-found\u0026#34; HTTP/1.1 404 Not Found Connection: keep-alive Content-Length: 31 Content-Type: application/json; charset=utf-8 Date: Mon, 23 Aug 2021 14:15:16 GMT Server: awselb/2.0 { \u0026#34;message\u0026#34;: \u0026#34;This is 404 path.\u0026#34; } $ http \u0026#34;${DNS_NAME}/error\u0026#34; HTTP/1.1 500 Internal Server Error Connection: keep-alive Content-Length: 31 Content-Type: application/json; charset=utf-8 Date: Mon, 23 Aug 2021 14:15:32 GMT Server: awselb/2.0 { \u0026#34;message\u0026#34;: \u0026#34;This is 500 path.\u0026#34; } ワイルドカードで指定していないので、上記から始まるパスであってもそれらに続くパスへのリクエストに対しては、ルールにマッチせずにデフォルトのレスポンスが返ります。\n$ http \u0026#34;${DNS_NAME}/error-error\u0026#34; HTTP/1.1 200 OK Connection: keep-alive Content-Length: 41 Content-Type: application/json; charset=utf-8 Date: Mon, 23 Aug 2021 14:16:58 GMT Server: awselb/2.0 { \u0026#34;message\u0026#34;: \u0026#34;This is the default action.\u0026#34; } クエリパラメータを付与してリクエスト では、クエリパラメータが付与されている場合はどうなるでしょうか。\n/error に ?errorType=internal-server-error というクエリパラメータを付与してリクエストしてみます。\n$ http \u0026#34;${DNS_NAME}/error?errorType=internal-server-error\u0026#34; HTTP/1.1 500 Internal Server Error Connection: keep-alive Content-Length: 31 Content-Type: application/json; charset=utf-8 Date: Mon, 23 Aug 2021 14:19:04 GMT Server: awselb/2.0 { \u0026#34;message\u0026#34;: \u0026#34;This is 500 path.\u0026#34; } 500 のレスポンスが返ります。\nつまり、パスベースのルーティングでは 指定したパスにクエリパラメータの部分は含まれまれず、ルールの判定にはクエリパラメータを除くパス部分のみで判定されます。逆に言うと、特定のパスに対してクエリパラメータを考慮したパスルーティングをしたい場合であっても、ワイルドカードで /error* のように指定する必要はありません。\n掃除 セキュリティグループのインバウンドルールを削除 aws ec2 revoke-security-group-ingress \\ --group-id \u0026#34;${SG_ID}\u0026#34; \\ --protocol tcp \\ --port 80 \\ --cidr \u0026#34;${MY_IP}\u0026#34;/32 ALB を削除 aws elbv2 delete-load-balancer \\ --load-balancer-arn \u0026#34;${ALB_ARN}\u0026#34; CLI で http リクエストするなら HTTPie が便利 - michimani.net \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-alb-path-routing-with-query-parameters/",
    "title": "ALB のパスベースルーティングとクエリパラメータの関係"
  },
  {
    "contents": "Go で実装した Lambda 関数から RDB に接続した際に、DB 接続数の増え方が実装方法によってどのように変わるのかを確認してみました。\n目次 前置き 試したいこと 検証環境 やってみる 1. handler 内で DB への接続を確立し、 DB 接続を明示的に Close しない場合 2. handler 内で DB への接続を確立し、 handler の処理の最後で DB 接続を明示的に Close する 3. handler 外で DB 接続インスタンスを定義し、 handler 内では接続の存在をチェックして無ければ確立する まとめ 前置き Lambda + RDS はアンチパターンとして語られることも多いです。その理由としてよく話題にあがるのは、Lambda 関数が大量に起動することによるコネクションの枯渇問題です。今回は、その課題に対する実装側での対処法方法を実際に試してみました。\n試したいこと コネクション枯渇への対処方法としては、 RDS Proxy の利用や実装方法による対応がありますが、今回は実装方法による対応を試します。\n具体的には、同じ実行環境での起動 (ウォームスタート) が連続したときに、 DB 接続数がどのように増えるのか、または増えないのかを確認します。実装方法のパターンとしては下記の 3 つです。\nhandler 内で DB への接続を確立し、 DB 接続を明示的に Close しない handler 内で DB への接続を確立し、 handler の処理の最後で DB 接続を明示的に Close する handler 外で DB 接続インスタンスを定義し、 handler 内では接続の存在をチェックして無ければ確立する 検証環境 今回はローカルで Lambda と RDB (MySQL 8.0) のコンテナを立てて検証しました。\n実際に Lambda + RDS の環境を構築するのは面倒だったのと、ローカルだと Lambda のコールドスタート/ウォームスタートを再現しやすいこともあって、ローカルでの検証としています。\n具体的には下記のようなファイル群を用意して、 docker-compose でまとめて起動します。\n. ├── docker-compose.yml ├── lambda-close-db │ ├── Dockerfile │ ├── entry.sh │ ├── go.mod │ ├── go.sum │ └── main.go ├── lambda-not-close-db │ ├── Dockerfile │ ├── entry.sh │ ├── go.mod │ ├── go.sum │ └── main.go ├── lambda-reuse-db │ ├── Dockerfile │ ├── entry.sh │ ├── go.mod │ ├── go.sum │ └── main.go └── mysql ├── Dockerfile └── init.sql ソースコードは GitHub に置いてあるので、 clone して docker-compose up -d を叩けばお手元で検証してもらえるはずです。\nmichimani/lambda-rdb-test | GitHub やってみる まずは必要なコンテナを起動します。\n$ docker-compose up -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------- lambda-rdb docker-entrypoint.sh mysqld Up 0.0.0.0:8000-\u0026gt;3306/tcp,:::8000-\u0026gt;3306/tcp, 33060/tcp lambda-rdb-func-1 /entry.sh /main Up 0.0.0.0:8001-\u0026gt;8080/tcp,:::8001-\u0026gt;8080/tcp lambda-rdb-func-2 /entry.sh /main Up 0.0.0.0:8002-\u0026gt;8080/tcp,:::8002-\u0026gt;8080/tcp lambda-rdb-func-3 /entry.sh /main Up 0.0.0.0:8003-\u0026gt;8080/tcp,:::8003-\u0026gt;8080/tcp 3 つの Lambda 関数と RDB のコンテナが起動しました。\n別セッションで RDB のコンテナに入っておいて、 Lambda 関数を invoke するたびに接続数がどのように変化するかを見ていきます。\n$ docker exec -it lambda-rdb bash root@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 1 | +-------------------+-------+ Lambda 関数を invoke する前の時点での接続数は 1 です。(この SQL を実行するための接続です)\n1. handler 内で DB への接続を確立し、 DB 接続を明示的に Close しない場合 まずは handler 内で毎回 DB 接続を確立するパターンで試してみます。実装はこんな感じです。\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; runtime \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) type Response struct { Message string `json:\u0026#34;message\u0026#34;` StatusCode int `json:\u0026#34;statusCode\u0026#34;` } func handleRequest() (Response, error) { // Create DB connection each invocation, and do not close it. db, err := initDB() if err != nil { fmt.Println(err.Error()) return Response{Message: err.Error(), StatusCode: http.StatusInternalServerError}, err } title, err := getTitle(db) if err != nil { fmt.Println(err.Error()) return Response{Message: err.Error(), StatusCode: http.StatusInternalServerError}, err } return Response{ Message: fmt.Sprintf(\u0026#34;title: %s\u0026#34;, *title), StatusCode: http.StatusOK, }, nil } func initDB() (*sql.DB, error) { dbName := os.Getenv(\u0026#34;DB_DATABASE\u0026#34;) dbUser := os.Getenv(\u0026#34;DB_USER\u0026#34;) dbPass := os.Getenv(\u0026#34;DB_PASS\u0026#34;) d, err := sql.Open(\u0026#34;mysql\u0026#34;, fmt.Sprintf(\u0026#34;%s:%s@tcp(db:3306)/%s\u0026#34;, dbUser, dbPass, dbName)) if err != nil { return nil, err } if err := d.Ping(); err != nil { return nil, err } return d, nil } func getTitle(db *sql.DB) (*string, error) { var title string sql := \u0026#34;SELECT title FROM sample_table LIMIT 1\u0026#34; err := db.QueryRow(sql).Scan(\u0026amp;title) if err != nil { return nil, err } return \u0026amp;title, nil } func main() { runtime.Start(handleRequest) } この Lambda 関数は 8001 ポートで起動しているので、下記コマンドで invoke します。\n$ curl \u0026#34;http://localhost:8001/2015-03-31/functions/function/invocations\u0026#34; 出力としては下記の通りです。実行時に特にエラーがなければ statusCode: 200 でのレスポンスが出力されます。\n{\u0026#34;message\u0026#34;:\u0026#34;title: This is a sample record\u0026#34;,\u0026#34;statusCode\u0026#34;:200} 1 回実行した時点で DB 接続数を確認してみます。\nroot@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 2 | +-------------------+-------+ 2 になっています。続いて、 10 回実行してから接続数を確認してみます。\n$ for i in `seq 10`; do curl \u0026#34;http://localhost:8001/2015-03-31/functions/function/invocations\u0026#34; done root@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 12 | +-------------------+-------+ 実行回数分だけ増えてます。\nこの結果から、 Lambda 関数が同じ実行環境で実行されている間は、一度接続した DB 接続が残り続けているのがわかります。\n実行環境を作り直す場合、ローカル環境では Lambda 関数のコンテナを再起動すれば OK です。(実際の Lambda では、実行環境が入れ替わるタイミングはわかりません)\n$ docker-compose restart lambda1 再起動後に接続数を確認してみます。\nroot@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 1 | +-------------------+-------+ 1 に戻りました。\nこのパターンが、いわゆるコネクションの枯渇につながる実装方法です。\n2. handler 内で DB への接続を確立し、 handler の処理の最後で DB 接続を明示的に Close する 続いては、 1 のパターンと同じく毎回 handler 内で DB 接続を確立するが、 handler 内で明示的に接続を Close するようにします。 1 との差分は下記のとおりで、 defer db.Close() を追加しています。\n--- ./lambda-not-close-db/main.go 2021-08-19 23:47:39.000000000 +0900 +++ ./lambda-close-db/main.go 2021-08-19 23:47:47.000000000 +0900 @@ -16,13 +16,16 @@ } func handleRequest() (Response, error) { - // Create DB connection each invocation, and do not close it. + // Create DB connection each invocation. db, err := initDB() if err != nil { fmt.Println(err.Error()) return Response{Message: err.Error(), StatusCode: http.StatusInternalServerError}, err } + // Close DB connection at end of each invocation. + defer db.Close() + title, err := getTitle(db) if err != nil { fmt.Println(err.Error()) この Lambda 関数は 8002 ポートで起動しているので、下記コマンドで invoke します。(出力は 1 と同じなので割愛します。)\n$ curl \u0026#34;http://localhost:8002/2015-03-31/functions/function/invocations\u0026#34; 実行後に接続数を確認します。\nroot@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 1 | +-------------------+-------+ 1 のままです。\n先ほどと同じように 10 回実行してから確認してみます。\n$ for i in `seq 10`; do curl \u0026#34;http://localhost:8002/2015-03-31/functions/function/invocations\u0026#34; done root@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 1 | +-------------------+-------+ かわらず 1 のままです。\nこれがコネクション枯渇への実装方法による対処法のひとつです。毎回 明示的に DB 接続を Close することで、同じ実行環境での実行であっても DB の接続数が増えることはありません。\n3. handler 外で DB 接続インスタンスを定義し、 handler 内では接続の存在をチェックして無ければ確立する 最後に、 DB 接続インスタンスを handler 外 で定義、つまりグローバル変数として定義しておいて、 handler 内では接続の存在チェックをして無ければ確立するような実装で試します。これは、コールドスタート時に接続が確立され、それ以降は同じコネクションを使いまわすような実装です。 1 との差分は下記のとおりです。\n--- ./lambda-not-close-db/main.go 2021-08-19 23:47:39.000000000 +0900 +++ ./lambda-reuse-db/main.go 2021-08-19 23:49:43.000000000 +0900 @@ -15,12 +15,19 @@ StatusCode int `json:\u0026#34;statusCode\u0026#34;` } +// Define the DB connection as a global variable. +var db *sql.DB + func handleRequest() (Response, error) { - // Create DB connection each invocation, and do not close it. - db, err := initDB() - if err != nil { - fmt.Println(err.Error()) - return Response{Message: err.Error(), StatusCode: http.StatusInternalServerError}, err + // Create a DB connection only if it is nil. + if db == nil || db.Ping() != nil { + d, err := initDB() + if err != nil { + fmt.Println(err.Error()) + return Response{Message: err.Error(), StatusCode: http.StatusInternalServerError}, err + } + + db = d } title, err := getTitle(db) この Lambda 関数は 8003 ポートで起動しているので、下記コマンドで invoke します。 (出力は 1 と同じなので割愛します。)\n$ curl \u0026#34;http://localhost:8003/2015-03-31/functions/function/invocations\u0026#34; 接続数を確認します。\nroot@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 2 | +-------------------+-------+ 2 になっています。これまでと同様に 10 回実行してから確認してみます。\n$ for i in `seq 10`; do curl \u0026#34;http://localhost:8003/2015-03-31/functions/function/invocations\u0026#34; done root@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 2 | +-------------------+-------+ 初回に確立された接続を再利用するので、 2 のままです。\nLamdba のコンテナを再起動 (または停止) すれば、初回に確立された 1 つの接続は解放され、 1 に戻ります。\n$ docker-compose restart lambda3 root@c97cb098b401:/# mysql -proot -e\u0026#39;show status like \u0026#34;Threads_connected\u0026#34;;\u0026#39; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_connected | 1 | +-------------------+-------+ まとめ Go で実装した Lambda 関数から RDB に接続する際に、実装方法によって接続数がどのように変化するのかを試してみました。\nGo の database/sql パッケージには明示的に Close する必要はない旨の記載がありますが、 Lambda 関数の実装をする際には注意が必要そうです。\nThe returned DB is safe for concurrent use by multiple goroutines and maintains its own pool of idle connections. Thus, the Open function should be called just once. It is rarely necessary to close a DB.\nsql · database/sql · pkg.go.dev Lambda + RDS のアンチパターンが語られる際のメインの項目であろうコネクション枯渇の問題ですが、仮に RDS Proxy を使うにしても不要な接続を増やすのは良くないので、実装レベルでの対応は必須かなと思いました。\n",
    "permalink": "https://michimani.net/post/aws-how-connections-increase-rdb-from-lambda/",
    "title": "Go で実装した Lambda 関数から RDB に接続したときのコネクションの増え方"
  },
  {
    "contents": "以前 Fargate でのコンテナ起動までの時間を計測しましたが、どうやら最近その時間が短くなっているという話を見かけたので実際に計測して前回と比較してみます。\n目次 前回 計測する条件 やってみる まとめ 前回 前回計測したのは今年の 2月でした。\nAWS SDK for Go V2 を使って Fargate for ECS でタスクを実行してコンテナが起動するまでの時間を計測してみた - michimani.net そこから約半年たった今、 Fargate でのコンテナ起動までの時間が短くなったらしいので、あらためて計測してみます。\n計測する条件 条件は前回同様に下記の内容で実施します。\nイメージサイズ: 約 256 MB CPU: 256 MB メモリ: 512 MB ❯ docker images fargate-speed-test REPOSITORY TAG IMAGE ID CREATED SIZE fargate-speed-test latest fc479f1d67e2 5 months ago 270MB ❯ cat task-definition.json | jq . { \u0026#34;family\u0026#34;: \u0026#34;fargate-speed-test\u0026#34;, \u0026#34;networkMode\u0026#34;: \u0026#34;awsvpc\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;fargate-speed-test-app\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;000000000000.dkr.ecr.ap-northeast-1.amazonaws.com/fargate-speed-test\u0026#34;, \u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;awslogs\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;awslogs-group\u0026#34;: \u0026#34;/dev/fargate-speed-test-task\u0026#34;, \u0026#34;awslogs-region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;awslogs-stream-prefix\u0026#34;: \u0026#34;dev\u0026#34; } } } ], \u0026#34;requiresCompatibilities\u0026#34;: [ \u0026#34;FARGATE\u0026#34; ], \u0026#34;executionRoleArn\u0026#34;: \u0026#34;arn:aws:iam::000000000000:role/fargate-speed-test-task-exec-role\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;512\u0026#34; } やってみる では計測してみます。ちなみに、前回の結果は 37.879 sec でした。残念ながら単発の実行時間しか残していなかったのでなんとも言えないですが、今回は 10 回くらい実行してその平均を出して比較してみます。\nコードは前回と同じこれを使います。\nThis is a script using the AWS SDK for Go V2 and Go 1.16. Measure the time it takes to launch AWS Fargate for ECS. とりあえず 1回。\n❯ go run main.go Task Created. TaskARN: arn:aws:ecs:ap-northeast-1:000000000000:task/fargate-speed-test-cluster/000000000000000000000000e628db6d aws ecs describe-tasks --cluster fargate-speed-test-cluster --tasks arn:aws:ecs:ap-northeast-1:000000000000:task/fargate-speed-test-cluster/000000000000000000000000e628db6d Task Stopped. TaskARN: arn:aws:ecs:ap-northeast-1:000000000000:task/fargate-speed-test-cluster/000000000000000000000000e628db6d CreatedAt: 2021-07-29 13:34:58.037 +0000 UTC StartedAt: 2021-07-29 13:35:38.626 +0000 UTC StoppedAt: 2021-07-29 13:36:23.11 +0000 UTC TakenTimeToStart: 40.589s 🤔\nとりあえず、ループで実行するようにコード修正してあと 9回計測してみます。\nfunc main() { - if err := runECSTask(); err != nil { - fmt.Println(err.Error()) + var wg sync.WaitGroup + + for i := 0; i \u0026lt; 9; i++ { + wg.Add(1) + go func() { + defer wg.Done() + if err := runECSTask(); err != nil { + fmt.Println(err.Error()) + } + }() } + wg.Wait() } ❯ go run main.go TakenTimeToStart: 25.07 TakenTimeToStart: 25.278 TakenTimeToStart: 25.16 TakenTimeToStart: 24.759 TakenTimeToStart: 30.34 TakenTimeToStart: 46.153 TakenTimeToStart: 47.481 TakenTimeToStart: 37.311 TakenTimeToStart: 55.649 最初の 1 回と合わせて平均とると、 35.779 sec 。\nうーん\u0026hellip;。もう一度 10 回実行してみます。\n❯ go run main.go TakenTimeToStart: 24.852s TakenTimeToStart: 24.819s TakenTimeToStart: 26.999s TakenTimeToStart: 25.865s TakenTimeToStart: 26.792s TakenTimeToStart: 24.165s TakenTimeToStart: 24.585s TakenTimeToStart: 24.914s TakenTimeToStart: 43.41s TakenTimeToStart: 36.45s 平均 28.285 sec 。早くなってる感が出てきました。\nという感じで合計 100 回実行した平均値は 33.087 sec でした。\n前回が 37.879 sec だったので、絶対値だと 4.792 sec 、割合にすると 12.65 % 短くなってます。\nまとめ Fargate でのコンテナ起動までの時間が短くなったらしいので試してみた話でした。結果としては、 10% ほど短縮されたかな？くらいの印象でした。他のイメージサイズ、CPU、メモリの組み合わせだと違った結果が出てくるかもしれません。\n現時点で Fargate ではイメージキャッシュが利用できないので、そこをなんとか\u0026hellip;という思いは変わらずです。前回載せた issue もコメントは増えてますが、実現は難しそうなのでは？という意見もあります。\n[Fargate/ECS] [Image caching]: provide image caching for Fargate. · Issue #696 · aws/containers-roadmap どうなるんでしょうか。\n",
    "permalink": "https://michimani.net/post/aws-how-long-does-fargate-take-to-start-2nd/",
    "title": "Fargate でコンテナ起動までの時間が短くなったらしいのであらためて計測してみた"
  },
  {
    "contents": "AWS Lambda にはコールドスタートとウォームスタートの 2 パターンありますが、それぞれのパターンで Go の init() 関数の振る舞いが異なるようなので実際に試してみました。\n結論 AWS Lambda のウォームスタート時には Go の init() 関数は実行されません。\n手元で確認されたい場合は、下記のリポジトリを clone してお試しください。\nmichimani/base-of-lambda-container-image | GitHub 試してみる 実際に AWS 上に作った Lambda のコールドスタート/ウォームスタートはこちらでは制御できないので、 Lambda のコンテナイメージをローカルで実行して試します。\n試すために用意した Go のコードは下記です。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; runtime \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) type Response struct { Message string `json:\u0026#34;message\u0026#34;` StatusCode int `json:\u0026#34;statusCode\u0026#34;` } var count int func handleRequest() (Response, error) { log.Println(\u0026#34;start handler\u0026#34;) defer log.Println(\u0026#34;end handler\u0026#34;) count++ message := \u0026#34;Hello AWS Lambda\u0026#34; for i := 0; i \u0026lt; count; i++ { message = message + \u0026#34;!\u0026#34; } return Response{ Message: message, StatusCode: http.StatusOK, }, nil } func init() { count = 0 log.Println(\u0026#34;init function called\u0026#34;) } func main() { runtime.Start(handleRequest) } このコード、実行するとどんなログ出力、レスポンスになるでしょうか。\n下記のような Dockerfile を用意してイメージをビルドします。\nFROM public.ecr.aws/lambda/provided:al2 as build # install compiler RUN yum install -y golang RUN go env -w GOPROXY=direct # cache dependencies ADD go.mod go.sum ./ RUN go mod download # build ADD . . RUN go build -o /main # copy artifacts to a clean image FROM public.ecr.aws/lambda/provided:al2 COPY --from=build /main /main COPY entry.sh / RUN chmod 755 /entry.sh ENTRYPOINT [ \u0026#34;/entry.sh\u0026#34; ] docker build -t base-of-lambda-container-image . コンテナを起動します。\ndocker run \\ --rm \\ -p 9000:8080 \\ base-of-lambda-container-image:latest /main 別のターミナルで Lambda を invoke します。\ncurl \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; すると、コンテナを起動している方のターミナルでは下記のような出力が得られます。\nSTART RequestId: 7910c8d7-1394-455a-b253-64f6322044bc Version: $LATEST time=\u0026#34;2021-07-01T14:34:23.324\u0026#34; level=info msg=\u0026#34;extensionsDisabledByLayer(/opt/disable-extensions-jwigqn8j) -\u0026gt; stat /opt/disable-extensions-jwigqn8j: no such file or directory\u0026#34; time=\u0026#34;2021-07-01T14:34:23.324\u0026#34; level=warning msg=\u0026#34;Cannot list external agents\u0026#34; error=\u0026#34;open /opt/extensions: no such file or directory\u0026#34; 2021/07/01 14:34:23 init function called 2021/07/01 14:34:23 start handler 2021/07/01 14:34:23 end handler END RequestId: 7910c8d7-1394-455a-b253-64f6322044bc REPORT RequestId: 7910c8d7-1394-455a-b253-64f6322044bc Init Duration: 0.18 ms Duration: 6.83 ms Billed Duration: 100 ms Memory Size: 3008 MB Max Memory Used: 3008 MB REPORT の中に Init Duration の情報が含まれているので、コールドスタートであることがわかります。 init() が実行されていることもわかります。\n一方、 invoke した方のターミナルでは、レスポンスとして下記の出力が得られます。\n{\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} では、続けてもう一度実行するとどうなるでしょうか。\nコンテナ側と invoke 側、それぞれの出力は下記のようになります。\nSTART RequestId: 28c51c4b-cd7a-43b7-a2ae-fa8ef9017015 Version: $LATEST 2021/07/01 14:37:01 start handler 2021/07/01 14:37:01 end handler END RequestId: 28c51c4b-cd7a-43b7-a2ae-fa8ef9017015 REPORT RequestId: 28c51c4b-cd7a-43b7-a2ae-fa8ef9017015 Duration: 1.17 ms Billed Duration: 100 ms Memory Size: 3008 MB Max Memory Used: 3008 MB REPORT の中に Init Duration の情報が含まれておらず、ウォームスタートであることがわかります。そして、 init function called のログがないため init() は実行されていないことがわかります。\n{\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} message の語尾の ! が 2 つになっています。このようになるのは下記が原因です。\ninit() が実行されないため count の値が 0 で初期化されなかった count はグローバル変数 (ハンドラ外で定義した変数) のため、前回実行時の 1 が格納されていた このまま連続で実行していくと ! の数は増えていきます。\n$ for i in `seq 5`; do curl \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; \u0026amp;\u0026amp; echo \u0026#34;\\n\u0026#34; done {\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!!!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} {\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!!!!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} {\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!!!!!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} {\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!!!!!!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} {\u0026#34;message\u0026#34;:\u0026#34;Hello AWS Lambda!!!!!!!\u0026#34;,\u0026#34;statusCode\u0026#34;:200} コンテナを一旦停止して再度起動すれば、最初の一回だけはコールドスタートで実行されます。\nまとめ AWS Lambda のウォームスタート時には Go の init() 関数は実行されません。\n実行ごとに初期化したい変数がある場合は、グローバル変数で定義せずにハンドラ内で宣言・初期化する、もしくは beforeHandler() みたいな初期化用の関数を作って、ハンドラの最初でそれを実行するみたいなことをする必要がありそうです。\n下記のリポジトリに今回の挙動を試すことができる一式を置いているので、手元で確認したい場合は clone して試してみてください。\nmichimani/base-of-lambda-container-image: This is a base of the AWS Lambda function in Go and deploys it as a container image. ",
    "permalink": "https://michimani.net/post/aws-behavior-of-go-init-func-on-lambda/",
    "title": "AWS Lambda における Go の init() の振る舞い"
  },
  {
    "contents": "ブラウザのタブって気付いたらめちゃくちゃ開いてることってよくありますよね？(圧力) ということで、大量に開かれたタブをページのドメインごとにグループ化する Chrome 拡張を作ってストアに公開しました。この記事は、公開までの記録と拡張機能そのものの宣伝です。\n散らかったタブをドメインごとにグループ化する Chrome 拡張作った。 pic.twitter.com/NB4NnCVxZw\n\u0026mdash; michimani Lv.859 (@michimani210) June 10, 2021 目次 概要 Chrome 拡張の実装 Getting started の概要 Chrome 拡張用の API を使ってみる chrome.tabs chrome.tabGroups ストアへの公開 デベロッパー登録 必要な素材 プライバシーへの取り組み 申請から公開まで まとめ 概要 Google Chrome の拡張機能を自作して Chrome ウェブストアに公開しました。Chrome 拡張を作るのは初めてだったので、どんな感じで作ったのか、ストアに公開するために何をしたのかについて書きます。\nちなみに、作って公開したのは Group Tabs という拡張機能です。その名の通り、開かれているタブをグループ化する拡張機能です。\nできることとしては、次の 3 つです。\nアクティブなウィンドウ内のタブ (固定されたものを除く) を URL でソートする アクティブなウィンドウ内のタブ (固定されたものを除く) をドメイン名でグループ化する タブのグループ化を解除する Group Tabs - Chrome Web Store ぜひインストールして使ってみてください。\nちなみにタブのグループ化は Chrome 89 以降で利用可能なので、必然的にこの拡張機能も Chrome 89 以降で利用可能です。\nChrome 拡張の実装 Chrome 拡張の実装ですが、めちゃくちゃ簡単でした。 JavaScript での実装になるので、 js での実装経験がある方ならより簡単に感じられると思います。\nChrome 拡張では chrome モジュールを使って諸々の要素を操作していきます。例えば今回のようにタブを操作する場合は、 chrome.tabs に含まれる API を使います。\nChrome Developers のサイトには各 API のリファレンスと Chrome 拡張機能に関するドキュメント1が用意されています。 Getting started では、とても丁寧な手順が書かれているので、サンプルの実装まで特に詰まることなく進めることができます。\nGetting started - Chrome Developers Getting started の概要 Getting started では、サンプルとして 「開いているページの背景色を変更する」 拡張機能を実装します。\n手順としては次のような内容です。\nmanifest.json の作成 バックグランドで動作するスクリプトの実装 拡張機能のアイコンを押したときに出るメニュー用の HTML の実装 メニュー内のボタンなどを押したときに発火する処理の実装 ＋α 1 〜 4 の手順で拡張機能の大枠は完成するので、あとは 3 と 4 の実装を目的の処理を達成するためにいろんな API を使って実装していきます。\nChrome 拡張用の API を使ってみる 今回作成したのはタブを操作する拡張機能です。なので、使用した API は chrome.tabs と chrome.tabGroups のみです。\nそれぞれの API について、どのように使ったか書いておきます。これを見てもらえれば他の API の使い方もイメージできると思います。\nchrome.tabs chrome.tabs API では、タブを作ったり移動させたりリロードしたりといった操作ができます。\nchrome.tabs - Chrome Developers 例えば、アクティブなウィンドウ内のタブのリストを取得する場合は query メソッドを使って下記のように実装します。\nconst targetTabConditions = { currentWindow: true, pinned: false, url: [\u0026#39;http://*/*\u0026#39;, \u0026#39;https://*/*\u0026#39;], groupId: chrome.tabGroups.TAB_GROUP_ID_NONE }; const tabs = await chrome.tabs.query(targetTabConditions); tabs.forEach(tab =\u0026gt; { console.log(`${tab.title}: ${tab.url}`); }); // Group Tabs - Chrome ウェブストア: https://chrome.google.com/webstore/detail/group-tabs/cnmcnafaccboidemenkpfnlgfcejijgm/ // DeepL API: https://www.deepl.com/docs-api/translating-text/large-volumes/ // aws-cli/CHANGELOG.rst at develop · aws/aws-cli: https://github.com/aws/aws-cli/blob/develop/CHANGELOG.rst // oapi-codegen/README.md at master · deepmap/oapi-codegen: https://github.com/deepmap/oapi-codegen/blob/master/README.md タブをグループ化する group メソッドも chrome.tabs API に含まれています。\nグループ化したいタブの ID を配列にして渡すだけです。\nconst tabs = await chrome.tabs.query(targetTabConditions); // tab ID の配列を作る const tabIdList = Array(); tabs.forEach(tab =\u0026gt; { tabIdList.push(tab.id); }); // ID の配列を渡す const groupId = await chrome.tabs.group({tabIds: tabIdList}); chrome.tabGroups chrome.tabGroups API では、タブグループのタイトル変更、色変更、移動、などの操作ができます。\nchrome.tabGroups - Chrome Developers 今回の拡張機能では、 chrome.tabs.group で作成したタブグループのタイトルなどを更新するために update メソッドを使っています。\nconst groupId = await chrome.tabs.group({tabIds: tabIdList}); await chrome.tabGroups.update(groupId, { title: \u0026#39;hoge group\u0026#39;, color: \u0026#39;green\u0026#39; }) ストアへの公開 Google Workspace のアドオンと違い、 Chrome ウェブストアへの公開も非常に簡単でした。\nデベロッパー登録 Chrome ウェブストアで拡張機能を公開するには、まずは Chrome ウェブストアのデベロッパー登録をする必要があります。\nChrome ウェブストア - デベロッパー契約 デベロッパー登録には、登録時に 1 回のみ 5ドルの登録料を支払う必要があります。\nデベロッパー登録をしたあとは、ダッシュボードから拡張機能のパッケージ (ZIP ファイル) をアップロードして、必要事項を入力して申請します。\n必要な素材 入力項目以外では、下記の素材が必要になります。\n1280x800 または 640x400 JPEG または 24 ビット PNG (アルファなし) のスクリーンショット 1 枚 128x128 のアイコン画像 プライバシーへの取り組み 入力項目として多少迷うとしたら、 プライバシーへの取り組み かと思います。\nプライバシーへの取り組みとしては下記の項目について入力、またはチェックボックスによるチェックをします。\n単一用途 公開する拡張機能が、単一の、限られた範囲の用途であることを説明します。(日本語で OK でした) 権限が必要な理由 manifest.json の permission で指定している権限が必要な理由を説明します。 (日本語で OK でした) データ使用 ユーザーのデータ (個人を特定できる情報、健康に関する情報、位置情報、など) にアクセスするかどうかをチェックボックスで選択します。 取得したデータを目的外で使用しない旨の宣言を、チェックボックスにチェックを入れることで宣言します。 申請から公開まで 必要事項を入力して申請すると、審査が開始します。\n今回の場合、申請から 2 日程度でストアに公開されました。ただし、公開された際にこちら側への通知はなかったので、適宜ダッシュボードで進捗を確認する必要があります。\n初回の申請後、拡張機能のバージョンを上げて再申請した際にも、 2 日程度で更新がストアに反映されました。このあたりの期間はどれくらいが相場なのかはわかりませんが、シンプルな機能であれば公開までの期間は数日なのでは？と思っています。\nまとめ 散らかったタブを良い感じに整理してくれる Chrome 拡張を作ってストアに公開した話でした。\n以前に Google Workspace のアドオンを作って公開したときと比べると、あっけなく公開までできてしまったなという印象です。\nGoogle スプレッドシートを Markdown 形式のテーブルに変換するアドオンを作って公開しました - michimani.net 今回作った拡張機能のソースコードは GitHub で公開しているので、バグ報告や要望などあればぜひ issue 作っていただけると嬉しいです。\nmichimani/chrome-tab-ex: This is a Chrome extension that sorts and groups tabs. ストアのレビューも励みになりますので、ぜひインストールしていただいて、忌憚のないご意見をいただければと思います。\nGroup Tabs - Chrome Web Store Welcome - Chrome Developers \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/development-publish-chrome-extension-grouping-tabs-by-domain/",
    "title": "散らかった Chrome のタブをドメインごとにグループ化する拡張機能を作った"
  },
  {
    "contents": " What\u0026rsquo;s this? This is a Google Chrome Extension that sort, group and ungroup tabs.\nFeatures Sort all tabs in the current window by its URL Group tabs in the current window by its domain Ungroup all tab groups in the current window Usage Install this extension from Chrome Web Store.\nGroup Tabs - Chrome Web Store Click extension icon, and select an action.\nSource Code michimani/chrome-tab-ex: This is a Chrome extension that sorts and groups tabs. If you have any comments or bug reports, please create an issue in the above repository.\nLogo ",
    "permalink": "https://michimani.net/projects/group-tabs/about/",
    "title": "About Group Tabs"
  },
  {
    "contents": "去年の 3 月頃からテレワークになって以降 自宅の作業環境を整えてきましたが、この春に引っ越しを機に諸々良い感じに再構築したのでメモ程度に書いておきます。\n目次 前回書いた記事 再構築のモチベーション デスクの改善 FLEXISPOT E3B かなでもの 天板 ラバーウッド ナチュラル 会社 PC と個人 PC とのスムーズな切り替え ディスプレイ 周辺機器 配線 まとめ 前回書いた記事 去年の 4 月頃、クラメソの社長である横田さんの記事を読んで、自分も当時の作業環境についてブログを書きました。(一般的 ってなんだ？って感じですが)\n一般的なエンジニアとしてそれっぽいテレワーク環境を整えてみた - michimani.net これを書いてからも色々買い足して環境を整備していたのですが、今回引っ越しを機に諸々を一新して良い感じに再構築できたので、今回は再構築後の環境について書きます。\n横田さんの記事はこちら\n社長として最低限のテレワーク環境を整えてみた | DevelopersIO 再構築のモチベーション 環境を再構築するにあたって達成したかったのは下記の内容です。\nデスクの改善 スタンディングデスクの導入 奥行きのある天板 会社 PC と個人 PC とのスムーズな切り替え 配線 それぞれについて書いていきます。\nデスクの改善 まずはデスクの改善です。といってもこれがメインです。\nこれまでは IKEA の LINNMON / ADILS という 奥行き 60 cm 幅 200 cm のデスクを使っていました。幅が広いのは良かったのですが、奥行きがもう少しほしかったのと、脚が華奢でディスプレイ 2 枚を置いていると片側に重さがかかって安定しないという不満ポイントがありました。\nということで今回の改善では 奥行き と 不安定さ の改善と、あらゆるテレワーク環境構築ブログで紹介されていた 昇降デスクの導入 を目的としました。\n結果として下の写真のような環境になりました。\nデスクの脚は FLEXISPOT - E3B で、天板は かなでもの - THE BOARD / ラバーウッド ナチュラル (幅 180 / 奥行き 80) にしました。それぞれ所感を書いておきます。\nFLEXISPOT E3B FLEXISPOT の昇降デスクについてはいろんなブログでレビューが書かれていて、社内でも使ってる人がいて良さそうだなとは思っていました。\nAmazon のレビュー内では、取り付けに必要な穴が空いていなかったり、昇降の誤動作などのレビューもいくつかありましたが、まあ、それにあたったら仕方ないと思って購入しました。\nまず、梱包重量が約 30 kg と結構な重さがあるので非力な自分には扱うのが大変でした。\n部品自体はこれだけです。\n組立自体は非常に簡単で、脚自体の組み立てには特に別途工具が必要になることはありませんでした。ただし、昇降する部分のパーツは特に重たいので、組み立ての際には扱いに注意が必要です。\n今回は天板別売りのものを購入しましたが、天板と組み付けるためのネジは付属しています。ネジ穴は 22 〜 26 箇所 (脚の幅による) なので、電動ドリルおよび電動ドライバーが無いと天板の取り付けはしんどいです。\nリリーフ(RELIFE) 六角軸ドリル 木工・樹脂用ドリル刃 10本組 26808 アイリスオーヤマ 電動ドライバー インパクトドライバー 10.8V 充電式 軽量 LEDライト 正逆転切替 JID80 ビット10種・バッテリー・充電器付き 【共通バッテリーシリーズ】 昇降については、特に不具合はなかったです。\n天板と、ディスプレイ 2 枚 (27 インチ と 32 インチ)、その他 PC 周辺機器などを載せた状態でも安定して昇降してくれます。\n諸々セッティングし終わった状態での昇降 pic.twitter.com/qU1K8tgbrX\n\u0026mdash; michimani Lv.859 (@michimani210) April 25, 2021 メモリ機能もあり、座って作業するときの高さと立って作業するときの高さを設定しておくことで、ボタン一つで設定した高さまで昇降してくれます。\nメモリ機能をつかた昇降で一つ気になるのが、設定した高さで ドンッ て感じで急に止まる点です。と言っても机の上のものが倒れたりモニターアームが揺れたりする程ではないのですが。上下ボタンで昇降させる場合には、ボタンを離してから数秒は昇降スピードが緩やかになって止まるので、それに比べると急に止まる感じはあります。\n購入する前に気になっていたのは、高くした状態での安定度です。が、これに関しては杞憂でした。\n立って作業しているときはある程度体重を預けてもびくともしないですし、前後のブレはほとんどないので安定感は抜群です。ただし構造上左右の揺れには強くないので、その点は頭に入れておいたほうが良さそうです。と言っても、心配になるくらい揺れるわけではないので、神経質になるほどではないです。\nスタンディングデスク、想像以上に良い環境で、最近では業務の半分以上はたった状態で作業しているくらいです。\nフローリングで立ったまま作業しているとどうしても足の裏が痛くなるので、疲労軽減用のマットを使うとだいぶ楽になります。\n八幡ねじ 疲労軽減マット ブラック 900X450 1枚入り こちらのモデルには障害物検知機能があり、昇降中に上や下から負荷がかかった場合には昇降が自動で止まります。\nこの機能で一つ気をつけたいのが、わりとちょっとした負荷も検知されるという点です。例えば、デスクから飛び出した諸々の配線が昇降時に壁と擦れたりする場合、壁との引っかかり具合によっては障害物検知機能が作動して昇降が自動で止まってしまうこともあります。これに関しては実際に何度か起こっていて気になっていたので、後述する配線の整理で対応しました。\nなので、特に障害物がないのに自動停止が作動する場合はこのあたりも確認するとよさそうです。\nFLEXISPOT スタンディングデスク 電動式 昇降デスク 高さ調節デスク 人間工学 メモリー機能付き オフィスワークテーブル パソコンデスク ゲーミングデスク 学習机 勉強机 ブラック E3B（天板別売り） かなでもの 天板 ラバーウッド ナチュラル かなでもの については、たまに広告で出てきていて、良い感じの机売ってるなという認識はありました。ただ、今回に関しては脚は不要なので全く検討していませんでした。が、社内の人に天板のみでも売っているのを教えていただいて、最終的にかなでもので天板を購入することになりました。最近は天板を自作するエンジニアさんを自分の観測範囲でよくお見かけするのですが、今回は既製品を選びました。 (いつか DIY したい)\nかなでものの良いとろこは、木の材質と天板のサイズを選べるのと、オプションで穴あけ加工もしてくれる点です。\n木の材質については、かなでもので取り扱っている木材の中でも一番硬い ラバーウッド で、色はナチュラルにしました。\n天板のサイズは 幅が 100 cm 〜 180 cm 、奥行きが 45 cm 〜 80 cm の範囲で指定可能で、今回はそれぞれの最大サイズである 180 cm x 80 cm にしました。天板の厚さは 2.5 cm です。\nそしてデスクの左奥にあたる箇所に丸型の配線孔を空けてもらいました。\n天板自体の重さは木の材質とサイズによって変わりますが、今回の ラバーウッド x 180 x 80 だと 約 24 kg でした。\n天板自体の重さについては下記で計算方法が書かれています。\n天板の重量を教えてください : by KANADEMONO ヘルプセンター 天板自体も重量がかなりあるので、 FLEXISPOT の脚への組付けには苦労しました。本来であれば天板を下において、脚をその上に乗せて組み付けるとよいのですが、そうすると一人でひっくり返すことはほぼ不可能です。今回は一人での作業だったため脚の上に天板を置いて、上向きにネジを締めて組み付けました。上向きへのネジ締めは体重をかけられない分力が必要になるので、あらかじめドリルで穴を開けておくことをおすすめします。\nTHE BOARD / ラバーウッド ナチュラル– by KANADEMONO 会社 PC と個人 PC とのスムーズな切り替え 現在所属している会社では業務用の PC (MacBook Pro) が貸与されており仕事中はそれを使っています。プライベートで使っている MacBook Pro は別にあるので、 ディスプレイやキーボードなどの周辺機器は同じものを使いたいし、ただし各ケーブルの抜き差しなどは極力減らしたい というのが今回の目的です。\n結論から言うと、下図のような構成で落ち着きました。\n大きくわけてディスプレイと周辺機器とで接続のコンセプト (?) が異なるので、それぞれについて詳細を書きます。\nディスプレイ ディスプレイは 2 枚 (BenQ の 27 インチと 32 インチ) あります。\nまず会社 PC からは Apple の USB-C Digital AV Multiport (USB-C, USB-A, HDMI) を 2 つ使って、HDMI でそれぞれのディスプレイに接続しています。\n一方の USB-C Digital AV Multiport には USB-C - Ethernet アダプタを接続して有線 LAN に繋いでおり、もう一方の USB-C Digital AV Multiport には電源ケーブルと、後述する USB 切替器を接続しています。\nUSB-C Digital AV Multiportアダプタ 個人 PC からは、 CalDigit の USB Dock を使って、 DisplayPort でそれぞれのディスプレイに接続しています。\nCalDigit の USB Dock には Ethernet と USB-A ポートがあり、このポートは後述する USB 切替器を接続しています。こちらの MacBook Pro は USB Dock 経由で給電されています。\nCalDigit USB-C Pro Dock Thunderbolt 3ケーブル (0.7m)付き USB-Cドッキングステーション DisplayPort 1.2 x 2 ディスプレイについてはそれぞれの PC から各ディスプレイに接続することで、ケーブルの抜き差しは不要になります。\n周辺機器 周辺機器として接続したいのは下記のデバイスです。\nキーボード Web カメラ オーディオインターフェース これらの接続を会社 PC と個人 PC でスムーズに切り替えることができると嬉しいなと思っていたわけです。\nそこで見つけたのが、 ELECOM - KVMスイッチ というアイテムです。\nエレコム KVMスイッチ キーボード・マウス用 手元スイッチ付 ブラック KM-A22BBK パッケージはダサいのですが、これがめちゃくちゃ良いアイテムでした。\nこれは USB-A のオスが 2 つ、メスが 2 つ、切り替えスイッチ がひとつになっているもので、切り替えたい 2 つの PC それぞれにオスを接続し、メスには切り替えたいデバイスを接続します。\n今回は、前述したとおりオスの一方を会社 PC に接続した USB-C Digital AV Multiport に、もう一方を個人 PC に接続した CalDigit の USB Dock に、それぞれ接続します。\n切り替えて使いたいデバイスが 3 つあるので、メスの一方には USB-A のハブを接続して 3 つのデバイスを接続しています。\nあとは、切り替えスイッチを押すことで各デバイスの接続先が会社 PC と個人 PC とで切り替えることができます。\n普段の運用としては、作業している PC をスリープさせて、スイッチを切り替えてからキーボード操作することで他方の PC がスリープから復帰する という感じです。\n切り替えスイッチにはインジケーターランプがついていて、オレンジ色と緑色で接続されている PC を認識することができます。\nこれによって、各種ケーブルの抜き差しは一切不要で 2 つの PC 間で共通の周辺機器をスムーズに切り替えながら使うことができています。\n私の場合はそれぞれの PC に別々のトラックパッドを Bluetooth 接続していますが、この方法であれば一つのマウスを有線接続して使うこともできそうです。\n前回からのアップデート 周辺機器のアップデートとしては、マイクとオーディオインターフェースです。\nマイクは marantz - MPM-1000 です。\n【Amazon限定ブランド】888M マランツプロ コンデンサーマイクロホン ウィンドスクリーン・スタンド・XLRケーブル付属 MPM-1000 めちゃくちゃ音質に拘っているわけでもないので、比較的安価なコンデンサマイクである MPM-1000 にしました。\n以前使っていたのは TASCAM の DR-07X です。本来はレコーダーとして使用するもので、 USB 接続でオーディオインターフェースとしても使えるというデバイスでした。基本的には普通に使えていたのですが、大きめのノイズが乗ることが増えてきた (特に金曜になると頻発していた) ので ちゃんとしたオーディオインターフェースとセットでマイクを接続することにしました。\nそして、オーディオインターフェースとして選んだのが YAMAHA - AG03 です。\nヤマハ YAMAHA ウェブキャスティングミキサー オーディオインターフェース 3チャンネル AG03 インターネット配信に便利な機能付き 音楽制作アプリケーションCubasis LE対応 正直、 Web 会議等で会話するためだけに導入するのはスペック過多だとは思ったのですが、もし今後 DTM とかやりたくなったときにも使えるようにと思ってこれにしました。 (可能性はゼロではない)\n配線 最後に配線です。\nあらためてデスク周りのデバイス関係の図を貼ります。\n図にするとシンプルに見えますが、実際には配置の関係から各ケーブルの取り回しが難しく、適当に接続しているとカオスになります。また、前述したとおり昇降デスクの障害物検知に引っかかる可能性もあるため、見た目の問題だけでなく実用面でも配線の整理は重要なわけです。\nということで、今回の配線整理の目的としては、スムーズな昇降ができるようにすることと見た目の改善です。\n天板の上にあるのはキーボードのケーブルのみで、それ以外の配線は机の下に全部集めています。具体的には、クランプ式のケーブルトレーを設置してそこに電源タップを置いています。\nプラス Garage 配線ケーブルトレー 幅40cm チャコールグレー 434664 各ケーブルはそのケーブルトレーのクランプやモニターアームのクランプの内側を通すようにして、デスクの外に極力はみ出さないようにしています。それが下の写真です。\nディスプレイに接続されているケーブルはできる限りアームに這わせるようにしてアームを可動しやすくしています。\n電源タップ自体のケーブルと LAN ケーブルについては、蛇のおもちゃみたいなケーブルダクトを使ってまとめています。\nFLEXISPOT ケーブルダクト 配線ダクト 配線収納 パソコンアクセサリー ケーブルチューブ ケーブル収納 オフィス収納 その他、余ったケーブルをまとめるためにマジックテープ式の結束バンドを使ったり、たるんでいるケーブルを天板裏に這わせるために小さいクリップを使ったりしています。\nTrilancerケーブル結束バンド, バンド 収納バンド ストラップ, コード配線収納 (黒, 3 サイズ) 3M コマンド フック キレイにはがせる 両面テープ コード用 クリア Sサイズ 20個 CMG-S-CL20 まとめ テレワーク環境を再構築した話でした。\n昨年 3 月頃から続いている在宅勤務ですが、終りが見えない (もうこのままでも良い気はしている) ので、思い切って諸々良いものを揃えてみました。結果としてはかなり快適な空間が完成して満足してます。特にスタンディングデスクを導入してからは座りっぱなしになることがなくなり、健康面でも良い生活になってきていると思います。\n環境構築するといってもどこから手を付けたらよいかわからないという方もいるかも知れませんが、まずは電動ドリルと電動ドライバーを準備するところから始めるのをおすすめします。これらがあると何をするにも作業が捗ります。\n",
    "permalink": "https://michimani.net/post/gadget-update-for-working-from-home/",
    "title": "テレワーク開始から 1 年以上経ったので作業環境を良い感じ再構築した"
  },
  {
    "contents": "CloudFront Functions でサポートされているランタイムは現時点で JavaScript (ECMAScript 5.1 準拠) となっていますが、「ECMAScript 5.1 !? いつの規格やねん、解散。」 となる前にあらためて公式ドキュメントを読んで詳細を確認してみます。\nなお、内容については 2021/05/13 時点のものです。\n目次 はじめに ECMAScript 5.1 compliant Core features 式と演算子 文と宣言 関数 Primitive objects Object String Number Built-in objects Date Function Regular expressions Array Typed arrays Built-in modules Crypto (hash and HMAC) Query string まとめ はじめに 今回の内容は、下記の公式ドキュメントの内容を自分なりにまとめたものになります。公式ドキュメント以上の内容はありません。\nJavaScript runtime features for CloudFront Functions - Amazon CloudFront ECMAScript 5.1 compliant CloudFront Functions の発表に歓喜したものの、サポートされているランタイムが JavaScript のみで且つ ECMAScript 5.1 compliant となっていて、残念に思った方は少なくないと思います。ただし、公式ドキュメントには下記のように書かれています。\nThe CloudFront Functions JavaScript runtime environment is compliant with ECMAScript (ES) version 5.1 and also supports some features of ES versions 6 through 9.\nJavaScript runtime features for CloudFront Functions - Amazon CloudFront つまり ベースは ECMAScript 5.1 (2011/06)1 ですが、 ES 6 (2015)2 から ES 9 (2018)3 に含まれる一部の機能もサポートしており、さらに ES の仕様に含まれない非標準のメソッドも提供されています。\nなので、 ES 5.1 以降の仕様で何が含まれているのか、非標準のメソッドって具体的に何が含まれているのか、気になったところを見ていきます。\nちなみに、 コンソールや AWS CLI などからランタイムを指定する場合は cloudfront-js-1.0 という名前になっています。\nCore features ES のコアとなる仕様についてです。\n式と演算子 ES 5.1 のすべての演算子に加えて、 ES 7 に含まれる べき乗演算子 ** がサポートされています。\nvar side = 4; var square = side ** 2; console.log(square); // 16 文と宣言 下記のステートメントがサポートされています。\n宣言 var ※ let および const はサポートされていません (ES 6)\n制御フロー break continue if...else switch try...catch [finally] throw 反復処理 for for...in while do...while その他 label 関数 ES 5.1 のすべての機能に加えて、 ES 6 に含まれる arrow function 、 rest parameter (残余引数) がサポートされています。\nvar sum = (...args) =\u0026gt; { var total = 0; for (var i in args) { total += args[i]; } return total; } console.log(sum(2,4,6)); // 12 Primitive objects Object ES 5.1 の Object メソッド (一部) に加えて、 ES 6 および ES 8 に含まれる下記のメソッドがサポートされています。\nassign (ES 6)\nvar curry = {source: \u0026#39;ポーク\u0026#39;, spicy: \u0026#39;2辛\u0026#39;}; var rice = {origin: \u0026#39;新潟\u0026#39;, quantity: \u0026#39;400g\u0026#39;}; var curryRice = Object.assign(rice, curry); console.log(curry); console.log(rice); console.log(curryRice); // {source:\u0026#39;ポーク\u0026#39;,spicy:\u0026#39;2辛\u0026#39;} // {origin:\u0026#39;新潟\u0026#39;,quantity:\u0026#39;400g\u0026#39;,source:\u0026#39;ポーク\u0026#39;,spicy:\u0026#39;2辛\u0026#39;} // {origin:\u0026#39;新潟\u0026#39;,quantity:\u0026#39;400g\u0026#39;,source:\u0026#39;ポーク\u0026#39;,spicy:\u0026#39;2辛\u0026#39;} is (ES 6)\nvar rice = {origin: \u0026#39;新潟\u0026#39;, quantity: \u0026#39;400g\u0026#39;}; var thaiRice = {origin: \u0026#39;タイ\u0026#39;, quantity: \u0026#39;400g\u0026#39;}; console.log(Object.is(rice, thaiRice)); console.log(Object.is(rice.quantity, thaiRice.quantity)); // false // true prototype.setPrototypeOf (ES 6)\nentries (ES 8)\nvar curryRice = {origin: \u0026#39;新潟\u0026#39;, quantity: \u0026#39;400g\u0026#39;, source: \u0026#39;ポーク\u0026#39;, spicy: \u0026#39;2辛\u0026#39;}; console.log(Object.entries(curryRice)); // [[\u0026#39;origin\u0026#39;,\u0026#39;新潟\u0026#39;],[\u0026#39;quantity\u0026#39;,\u0026#39;400g\u0026#39;],[\u0026#39;source\u0026#39;,\u0026#39;ポーク\u0026#39;],[\u0026#39;spicy\u0026#39;,\u0026#39;2辛\u0026#39;]] values (ES 8)\nvar curryRice = {origin: \u0026#39;新潟\u0026#39;, quantity: \u0026#39;400g\u0026#39;, source: \u0026#39;ポーク\u0026#39;, spicy: \u0026#39;2辛\u0026#39;}; console.log(Object.values(curryRice)); // [\u0026#39;新潟\u0026#39;,\u0026#39;400g\u0026#39;,\u0026#39;ポーク\u0026#39;,\u0026#39;2辛\u0026#39;] String ES 5.1 の String メソッド (一部) に加えて、 ES 6、 ES 8、 ES 9 に含まれる下記のメソッドがサポートされています。\nfromCodePoint\nprototype.codePointAt (ES 6) prototype.endsWith (ES 6) prototype.includes (ES 6) prototype.repeat (ES 6) prototype.startsWith (ES 6) prototype.padStart (ES 8) prototype.padEnd (ES 8) prototype.trimStart (ES 9) prototype.trimEnd (ES 9) 更に、下記の非標準メソッドもサポートされています。\nprototype.bytesFrom(array | string, encoding) prototype.fromBytes(start[, end]) prototype.fromUTF8(start[, end]) prototype.toBytes(start[, end]) prototype.toUTF8(start[, end]) 用途がパッと思いつかなかったんですが、公式の example では JWT の検証として String.bytesFrom() を使ってました。\nValidate a simple token in the request - Amazon CloudFront Number ES 5.1 のすべての Number メソッドと、 ES 6 に含まれる下記のメソッド/定数がサポートされています。\nisFinite isInteger isNaN isSafeInteger parseFloat parseInt prototype.toExponential prototype.toFixed prototype.toPrecision EPSILON MAX_SAFE_INTEGER MAX_VALUE MIN_SAFE_INTEGER MIN_VALUE NEGATIVE_INFINITY NaN POSITIVE_INFINITY Built-in objects Date ES 5.1 の Date の機能がすべてサポートされていますが、下記の制限があります。\nセキュリティの観点から、 Date のメソッドで返却される現在時刻は実行される Function のライフタイム内で常に一定で、その値は Function が起動した時点の時刻となります。\nつまり、関数の開始と終了時点でそれぞれの時刻を取得して、その差分から関数の実行時間を取得するようなことはできません。\nvar start = Date.now(); /** * 何かしらの処理 */ var end = Date.now(); console.log(`${start}\\n${end}`); // 1620857555194 // 1620857555194 Function Function constructors はサポートされていません。\nvar hello = new Function(\u0026#39;return \u0026#34;Hello\u0026#34;;\u0026#39;); console.log(hello()); // Error Message: The CloudFront function associated with the CloudFront distribution is invalid or could not run. TypeError: function constructor is disabled in \u0026#34;safe\u0026#34; mode var bye = function() {return \u0026#39;Bye\u0026#39;;}; console.log(bye()); // bye (safe じゃないモードなら使えるのか\u0026hellip;？)\nRegular expressions ES 5.1 のすべての RegExp の機能に加えて、 ES 9 に含まれる named capture groups がサポートされています。\nvar CURRY_RICE_RE = /ソース:(?\u0026lt;source\u0026gt;.*) 辛さ:(?\u0026lt;spicy\u0026gt;.*) ご飯の量:(?\u0026lt;rice\u0026gt;.*)/; var menu = \u0026#39;ソース:ポーク 辛さ:2辛 ご飯の量:300g\u0026#39;; var matched = CURRY_RICE_RE.exec(menu); console.log(matched.groups.source); console.log(matched.groups.spicy); console.log(matched.groups.rice); // ポーク // 2辛 // 300g Array ES 5.1 の Array メソッド (一部) に加えて、 ES 6 および ES 7 に含まれる下記のメソッドがサポートされています。\nof (ES 6) prototype.copyWithin (ES 6) prototype.fill (ES 6) prototype.find (ES 6) prototype.findIndex (ES 6) prototype.includes (ES 7) var currySource = Array.of(\u0026#39;ポーク\u0026#39;, \u0026#39;ビーフ\u0026#39;); console.log(currySource); // [\u0026#39;ポーク\u0026#39;,\u0026#39;ビーフ\u0026#39;] console.log(currySource.copyWithin(0, 1, 2)); // [\u0026#39;ビーフ\u0026#39;,\u0026#39;ビーフ\u0026#39;] console.log(currySource.fill(\u0026#39;ハヤシ\u0026#39;, 1, 2)); // [\u0026#39;ビーフ\u0026#39;,\u0026#39;ハヤシ\u0026#39;] console.log(currySource.find(element =\u0026gt; element = \u0026#39;ビーフ\u0026#39;)); // ビーフ console.log(currySource.findIndex(element =\u0026gt; element = \u0026#39;ビーフ\u0026#39;)); // 0 console.log(currySource.includes(\u0026#39;キーマ\u0026#39;)); // false Typed arrays ES 6 に含まれる 型付き配列 がサポートされています。\nInt8Array Uint8Array Uint8ClampedArray Int16Array Uint16Array Int32Array Uint32Array Float32Array Float64Array var int8array = new Int8Array(2); int8array[0] = 0; console.log(int8array); int8array[1] = \u0026#39;string\u0026#39;; console.log(int8array); int8array[1] = 1; console.log(int8array); // Int8Array [0,0] // Int8Array [0,0] // Int8Array [0,1] Built-in modules Built-in modules として crypto と querystring がサポートされており、それぞれ require で読み込むことで使用できます。\nCrypto (hash and HMAC) crypto モジュールでは、標準的なハッシュおよびハッシュベースのメッセージ認証コード (HMAC) のヘルパが提供されています。下記のメソッドがサポートされており、それぞれ Node.js の対応するメソッド4 と同じ挙動となります。\nHashing methods crypto.createHash(algorithm) hash.update(data) hash.digest([encoding]) var crypto = require(\u0026#39;crypto\u0026#39;); var text = \u0026#39;I love curry.\u0026#39;; var hash = crypto.createHash(\u0026#39;sha256\u0026#39;); // or md5, sha1 hash.update(text); var digest = hash.digest(\u0026#39;hex\u0026#39;); // or base64, base64url console.log(`${text}\\n${digest}`); // I love curry. // 74dc57666d14c957cf622ad110f1c8d88c0f3390ee057333e97bcaf0c50ac82a HMAC methods crypto.createHmac(algorithm, secret key) hmac.update(data) hmac.digest([encoding]) var crypto = require(\u0026#39;crypto\u0026#39;); var text = \u0026#39;I love curry.\u0026#39;; var hmac = crypto.createHmac(\u0026#39;sha256\u0026#39;, \u0026#39;secret key hoge\u0026#39;); // or md5, sha1 hmac.update(text); var digest = hmac.digest(\u0026#39;hex\u0026#39;); // or base64, base64url console.log(`${text}\\n${digest}`); // I love curry. // 2e3ef82b9c4265d77b977ab9d778663e240da3737e0059ca82c0f93b49f4ddd1 Query string querystring モジュールでは、 URL 内のクエリ文字列を解析して操作するためのメソッドが提供されています。下記のメソッドがサポートされており、それぞれ Node.js の対応するメソッド5 と同じ挙動となります。\nただ、 CloudFront Functions に渡されるイベントオブジェクトは自動的にパースされるので、このモジュールを使う場面はほぼないです。\nquerystring.escape(string) 通常は querystring.stringify() 経由で実行されるため、直接使用することはない querystring.parse(string[, separator[, equal[, options]]]) querystring.stringify(object[, separator[, equal[, options]]]) querystring.unescape(string) 通常は querystring.parse() 経由で実行されるため、直接使用することはない var qs = require(\u0026#39;querystring\u0026#39;); var queryString = \u0026#39;source=beef\u0026amp;rice=300\u0026amp;topping=egg\u0026amp;topping=garlic\u0026#39;; var parsed = qs.parse(queryString); console.log(JSON.stringify(parsed, null, 2)); // { // \u0026#34;source\u0026#34;: \u0026#34;beef\u0026#34;, // \u0026#34;rice\u0026#34;: \u0026#34;300\u0026#34;, // \u0026#34;topping\u0026#34;: [ // \u0026#34;egg\u0026#34;, // \u0026#34;garlic\u0026#34; // ] // } var params = { source: \u0026#39;pork\u0026#39;, rice: \u0026#39;500\u0026#39;, topping: [ \u0026#39;seafood\u0026#39; ] } var paramsString = qs.stringify(params); console.log(paramsString); // source=pork\u0026amp;rice=500topping=seafood まとめ CloudFront Functions でサポートされているランタイムについて、公式ドキュメントの内容を読んでみた話でした。\nベースは ECMAScript 5.1 ですが それ以降の ES 9 までの内容も含まれているので、 ECMAScript 5.1 という文字だけで気を落とすのは早そうです。そもそも CloudFront Functions では外部ネットワーク接続できなかったりメモリや実行時間の制限が厳しいこともあり、古い仕様でもなんとかなっているのかなと思いました。とはいえ var で変数宣言するのはちょっと気持ち悪いです。\n今後ランタイムは更新されているかもしれませんが、現状では諸々の制限の中で処理を実装する上では ECMAScript 5.1 準拠であっても困ることはなさそうです。\nECMAScript Language Specification - ECMA-262 Edition 5.1 \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nECMAScript 2015 Language Specification – ECMA-262 6th Edition \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nECMAScript® 2018 Language Specification \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCrypto | Node.js v16.1.0 Documentation \u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQuery string | Node.js v16.1.0 Documentation \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-about-runtime-of-cloudfront-functions/",
    "title": "CloudFront Functions でサポートされているランタイムについて調べてみた"
  },
  {
    "contents": "CloudFront Functions という新しい機能がリリースされたので AWS CLI で触ってみます。ついでに、 Lambda@Edge でやっているこのブログの URL 正規化処理を CloudFront Functions に移行してみました。\n目次 CloudFront Functions とは Lambda@Edge でやってたことをそのままやる CloudFront Functions の要件に合うのか AWS CLI でやってみる CloudFront Functions 用のスクリプトを作成 Function を作成 Function をテスト Function を Publish Distribution との紐付け まとめ CloudFront Functions とは Amazon CloudFront announces the general availability of CloudFront Functions, a new serverless edge compute capability. You can use this new CloudFront feature to run JavaScript functions across 225+ CloudFront edge locations in 90 cities across 47 countries. CloudFront Functions is built for lightweight HTTP(S) transformations and manipulations, allowing you to deliver richer, more personalized content with low latency to your customers.\nAmazon CloudFront announces CloudFront Functions, a lightweight edge compute capability CloudFront のエッジロケーションで、 JavaScript で実装された関数を実行できるというものです。\nCloudFront には似たような機能として Lambda@Edge というのがありますが、本当にざっくりとしたイメージでは同じような機能だという認識です。が、もちろん違いはあり、詳細については既にクラメソさんの記事で解説されています。\nエッジで爆速コード実行！CloudFront Functionsがリリースされました！ | DevelopersIO ちなみに、今回 触る範囲で気になる差は下記の部分です。\nCloudFront Functions Lambda@Edge ランタイム JavaScript Node.js, Python 最大パッケージサイズ 10 KB 1 MB1 最大メモリ 2 MB 128 MB2 最大実行時間 1ms 未満 5 秒3 CloudFront Functions は、略して CF2 と呼ぶようです。\nAnnouncing CloudFront Functions (CF2), a new serverless edge compute capability for lightweight customizations. CF2 runs custom Javascript code at all of CloudFront’s 225+ edge locations with minimal latency and will cost ~1/6th that of Lambda@Edge.. https://t.co/tlD5dvwpmj\n\u0026mdash; Amazon CloudFront (@cloudfront) May 3, 2021 Lambda@Edge でやってたことをそのままやる 今回やるのは、 このブログに対して既に Lambda@Edge でやっている URL の正規化を、 CloudFront Functions に移行します。\nLambda@Edge での正規化については下記の記事で書いています。\nLambda@Edge で静的サイトの URL を正規化する - michimani.net Viewer Request をトリガーに、 / 無しのリクエストを / にリダイレクトしたり、 / でのリクエストに /index..html を補完してオリジンにリクエストを流したり、ということをやっています。\nCloudFront Functions の要件に合うのか 前述したとおり Lambda@Edge と CloudFront Functions とでは違いがいくつかあるので、そこをクリアできるか確認しておきます。\nランタイム これは同じ処理を JavaScript で書き換えればよいだけなので、作業は発生するもの移行は可能です。ちなみに今回対象となっている Lambda@Edge は Node.js で実装しているものなので、大きな変更はなさそうです。\n最大パッケージサイズ Lambda@Edge で使っている Lambda 関数のサイズは 991.0 byte なので、 10 KB 以内に収まっています。\n最大メモリ これはどうしようもないので 2 MB でがんばります。\n最大実行時間 ここが一番の問題です。\nCloudFront Functions では最大実行時間が 1ms 未満 ということで、本当に軽い処理の実行しかできなさそうです。とはいえ、 URL の正規化程度であれば実行できそうですが、念のため現状の Lambda@Edge の処理にどれくらいかかっているのか確認しておきます。ちなみに Lambda 関数に割り当てられているメモリは 128 MB です。\n確認方法としては、 Lambda 関数の実行ログから REPORT 内の Duration と Billed Duration の差をみます。\n$ aws logs get-log-events \\ --log-group-name ${LOG_GP_NAME} \\ --log-stream-name \u0026#34;${LOG_ST_NAME}\u0026#34; \\ --query \u0026#34;events[?contains(message, \\`REPORT\\`)].message\u0026#34; \\ --output text REPORT RequestId: 889e0e28-4980-48f3-a134-8327666ccc97\tDuration: 6.44 ms\tBilled Duration: 7 ms\tMemory Size: 128 MB\tMax Memory Used: 63 MB\tInit Duration: 164.61 ms REPORT RequestId: d2e219b5-757a-487f-aa3e-148db76e1292\tDuration: 113.00 ms\tBilled Duration: 113 ms\tMemory Size: 128 MB\tMax Memory Used: 64 MB REPORT RequestId: 9005ebdf-0783-4577-b45a-426db365a5a4\tDuration: 128.46 ms\tBilled Duration: 129 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: e0facf39-87a9-4fb6-a2a5-98example2dc6\tDuration: 1.35 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: f12d9777-142d-4d07-8aa9-6eexample2ebb\tDuration: 5.51 ms\tBilled Duration: 6 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: ad884280-69a2-4784-9883-6aexample6a0c\tDuration: 1.36 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: 93c493a1-f07d-405f-8702-14examplee3d3\tDuration: 1.33 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: 280d7b33-9cee-4aa4-89f7-afexamplebf8e\tDuration: 1.30 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: 26d94ea2-90fe-470b-8140-6bexample9432\tDuration: 1.16 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: 2691e2c7-14e2-45f4-bd48-02examplee3de\tDuration: 1.15 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB REPORT RequestId: e42686f5-5d52-4fdf-906a-40example24b2\tDuration: 1.10 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 65 MB シェルスクリプト力が無いので出力結果をスプレッドシートに貼り付けて確認してみると、最大で 0.9 ms くらい、平均では 0.7334 ms ということがわかりました。実行時間的には問題なさそうですが、ログからも分かる通り毎回 65 MB くらいメモリを使ってのこの結果です。 CloudFront Functions の 2 MB でいけるかどうかは、 Functions のテストを実施したタイミングでわかります。\nAWS CLI でやってみる AWS CLI では、下記の 8 個のサブコマンドが追加されています。 なお、この記事を書いている時点 (2021/05/04) では v1 の最新バージョン 1.19.64 でのみ対応しており、 v2 の最新バージョン 2.2.1 ではまだ使えません。\nv1 では 1.19.64 以降で、 v2 では 2.2.2 以降で利用可能になっています。\ncreate-function delete-function describe-function get-function list-functions publish-function test-function update-function やることとしては、 CloudFront Functions 用のスクリプトを作って、 Function を作って、テストして、公開して、ディストリビューションと紐付ける、です。\nCloudFront Functions 用のスクリプトを作成 CloudFront Functions の Function の実装方法については、下記 CloudFront のドキュメントを参考にします。\nWriting function code (CloudFront Functions programming model) - Amazon CloudFront 書き方としては、 event を受け取る関数 handler を実装します。\nfunction handler(event) { var request = event.request; var uri = request.uri; /** いろいろやる */ // リダイレクトさせたい場合 if (hoge) { var response = { statusCode: 302, statusDescription: \u0026#39;Found\u0026#39;, headers: { \u0026#34;location\u0026#34;: { \u0026#34;value\u0026#34;: redirectUrl } } } return response; } return request; } 今回は Viewer Request をトリガーに実行するので、 return する先はオリジン (S3) となります。 ただし、リクエストに location ヘッダーを付与することで、オリジンに到達する前にリダイレクトされることになります。ここは Lambda@Edge と同じです。\n今回やりたいのは下記のとおりです。\n/ 無しは / ありに 301 リダイレクト /index..html 付きの場合は /index..html 無しに 301 リダイレクト オリジンへは /index..html を補完してリクエスト / ありが正規の URL としたいため、 301 (恒久的な) リダイレクトとしています。\nfunction handler(event) { var host = \u0026#39;https://michimani.net\u0026#39;; var request = event.request; var requestUri = request.uri; // トップページへのリクエストに対しては何もしない if (requestUri == \u0026#39;\u0026#39; || requestUri == \u0026#39;/\u0026#39;) { return request; } // `/` 無し または `index.html` 付きは `/` にリダイレクトする if (requestUri.match(/((page|post|posts|tags|categories|archives|about)(\\/.*[^\\/])?|\\/index\\.html)$/)) { var redirectUrl = host + requestUri.replace(\u0026#39;/index.html\u0026#39;, \u0026#39;\u0026#39;) + \u0026#39;/\u0026#39;; var response = { statusCode: 301, statusDescription: \u0026#39;Found\u0026#39;, headers: { location: { value: redirectUrl } } } // Viewer に対してレスポンスを返す return response; } // オリジンに対しては `/index.html` を補完してリクエストする var actualUri = requestUri.replace(/\\/$/, \u0026#39;\\/index.html\u0026#39;); request.uri = actualUri; // オリジンへリクエスト return request; } これを cf2-redirect.js として保存しておきます。\nOptimizing request URI for CloudFront with CloudFront Functions (CF2) that has HUGO site that deployed to a S3 bucket as origin. | GitHub gist Function を作成 Function の作成には cloudfront reate-function コマンドを使います。\n$ aws cloudfront create-function help ... SYNOPSIS create-function --name \u0026lt;value\u0026gt; --function-config \u0026lt;value\u0026gt; --function-code \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] --function-config では Comment と Runtime を指定しますが、現時点で Runtime に指定できるのは cloudfront-js-1.0 のみです。\n先ほど作成したスクリプトをもとに、 Function を作成します。\n$ aws cloudfront create-function \\ --name cf2-redirect \\ --function-config Comment=\u0026#34;Redirect Function\u0026#34;,Runtime=cloudfront-js-1.0 \\ --function-code file://cf2-redirect.js { \u0026#34;Location\u0026#34;: \u0026#34;https://cloudfront.amazonaws.com/2020-05-31/function/arn:aws:cloudfront::XXXXXXXXXXXX:function/cf2-redirect\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;ETVEXAMPLE\u0026#34;, \u0026#34;FunctionSummary\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;cf2-redirect\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;UNPUBLISHED\u0026#34;, \u0026#34;FunctionConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Redirect Function\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;cloudfront-js-1.0\u0026#34; }, \u0026#34;FunctionMetadata\u0026#34;: { \u0026#34;FunctionARN\u0026#34;: \u0026#34;arn:aws:cloudfront::XXXXXXXXXXXX:function/cf2-redirect\u0026#34;, \u0026#34;Stage\u0026#34;: \u0026#34;DEVELOPMENT\u0026#34;, \u0026#34;CreatedTime\u0026#34;: \u0026#34;2021-05-04T14:26:41.142Z\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;2021-05-04T14:26:41.142Z\u0026#34; } } } ステータスが UNPUBLISHED 、 ステージは DEVELOPMENT になっています。\ndescribe-function で確認してみます。\n$ aws cloudfront describe-function \\ --name cf2-redirect { \u0026#34;ETag\u0026#34;: \u0026#34;ETVEXAMPLE\u0026#34;, \u0026#34;FunctionSummary\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;cf2-redirect\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;UNPUBLISHED\u0026#34;, \u0026#34;FunctionConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Redirect Function\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;cloudfront-js-1.0\u0026#34; }, \u0026#34;FunctionMetadata\u0026#34;: { \u0026#34;FunctionARN\u0026#34;: \u0026#34;arn:aws:cloudfront::XXXXXXXXXXXX:function/cf2-redirect\u0026#34;, \u0026#34;Stage\u0026#34;: \u0026#34;DEVELOPMENT\u0026#34;, \u0026#34;CreatedTime\u0026#34;: \u0026#34;2021-05-04T14:26:41.142Z\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;2021-05-04T14:26:41.185Z\u0026#34; } } } ちなみに get-function では、指定した Function のコードを ouput として取得できます。\nFunction をテスト 作成した Function をテストしてみます。テストするには cloudfront test-function コマンドを使います。\n$ aws cloudfront test-function help ... SYNOPSIS test-function --name \u0026lt;value\u0026gt; --if-match \u0026lt;value\u0026gt; [--stage \u0026lt;value\u0026gt;] --event-object \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] --if-match には、対象の Function の ETag の値を指定します。(describe-function で確認)\n--event-object については、下記 CloudFront のドキュメントを参考にして、次のような JSON オブジェクトを作成して event-object.json として保存しておきます。\n{ \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;context\u0026#34;: { \u0026#34;distributionDomainName\u0026#34;: \u0026#34;test.cloudfront.net\u0026#34;, \u0026#34;distributionId\u0026#34;: \u0026#34;TESTDISTID\u0026#34;, \u0026#34;eventType\u0026#34;: \u0026#34;viewer-request\u0026#34;, \u0026#34;requestId\u0026#34;: \u0026#34;TEST-REQUEST-ID\u0026#34; }, \u0026#34;viewer\u0026#34;: { \u0026#34;ip\u0026#34;: \u0026#34;111.111.111.111\u0026#34; }, \u0026#34;request\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;/categories/aws\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;test.example.com\u0026#34; }, \u0026#34;user-agent\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;test user agent0\u0026#34; } } } } CloudFront Functions event structure - Amazon CloudFront 上記のイベントでは https://michimani.net/category/aws へのリクエストをテストするので、 / ありへのリダイレクトレスポンスが取得できれば OK です。\n$ CFF_NAME=\u0026#34;cf2-redirect\u0026#34; $ aws cloudfront test-function \\ --name \u0026#34;${CFF_NAME}\u0026#34; \\ --if-match $( \\ aws cloudfront describe-function \\ --name \u0026#34;${CFF_NAME}\u0026#34; \\ --query \u0026#34;ETag\u0026#34; \\ --output text) \\ --event-object file://event-object.json { \u0026#34;TestResult\u0026#34;: { \u0026#34;FunctionSummary\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;cf2-redirect\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;UNPUBLISHED\u0026#34;, \u0026#34;FunctionConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Redirect Function\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;cloudfront-js-1.0\u0026#34; }, \u0026#34;FunctionMetadata\u0026#34;: { \u0026#34;FunctionARN\u0026#34;: \u0026#34;arn:aws:cloudfront::XXXXXXXXXXXX:function/cf2-redirect\u0026#34;, \u0026#34;Stage\u0026#34;: \u0026#34;DEVELOPMENT\u0026#34;, \u0026#34;CreatedTime\u0026#34;: \u0026#34;2021-05-04T14:26:41.142Z\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;2021-05-04T15:28:21.410Z\u0026#34; } }, \u0026#34;ComputeUtilization\u0026#34;: \u0026#34;35\u0026#34;, \u0026#34;FunctionExecutionLogs\u0026#34;: [], \u0026#34;FunctionErrorMessage\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;FunctionOutput\u0026#34;: \u0026#34;{\\\u0026#34;response\\\u0026#34;:{\\\u0026#34;headers\\\u0026#34;:{\\\u0026#34;location\\\u0026#34;:{\\\u0026#34;value\\\u0026#34;:\\\u0026#34;https://michimani.net/categories/aws/\\\u0026#34;}},\\\u0026#34;statusDescription\\\u0026#34;:\\\u0026#34;Found\\\u0026#34;,\\\u0026#34;cookies\\\u0026#34;:{},\\\u0026#34;statusCode\\\u0026#34;:301}}\u0026#34; } } FunctionOutput を見てみると、 301 リダイレクト用のレスポンスが返っているのがわかります。また、 ComputeUtilization の値が 35 ということで、これは実行時間が 1ms 未満であることを示しています。\nIt also shows the compute utilization, which is a number between 0 and 100 that indicates the amount of time that the function took to run as a percentage of the maximum allowed time. For example, a compute utilization of 35 means that the function completed in 35% of the maximum allowed time.\nTesting functions - Amazon CloudFront Function を Publish Function のテストができたら、 cloudfront publish-function コマンドで publish します。\n$ aws cloudfront publish-function help ... SYNOPSIS publish-function --name \u0026lt;value\u0026gt; --if-match \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] 必要なのは Function の名前と ETag なので、下記のように実行します。\n$ aws cloudfront publish-function \\ --name \u0026#34;${CFF_NAME}\u0026#34; \\ --if-match $( \\ aws cloudfront describe-function \\ --name \u0026#34;${CFF_NAME}\u0026#34; \\ --query \u0026#34;ETag\u0026#34; \\ --output text) { \u0026#34;FunctionSummary\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;cf2-redirect\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;UNASSOCIATED\u0026#34;, \u0026#34;FunctionConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Redirect Function\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;cloudfront-js-1.0\u0026#34; }, \u0026#34;FunctionMetadata\u0026#34;: { \u0026#34;FunctionARN\u0026#34;: \u0026#34;arn:aws:cloudfront::XXXXXXXXXXXX:function/cf2-redirect\u0026#34;, \u0026#34;Stage\u0026#34;: \u0026#34;LIVE\u0026#34;, \u0026#34;CreatedTime\u0026#34;: \u0026#34;2021-05-04T16:05:34.967Z\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;2021-05-04T16:05:34.967Z\u0026#34; } } } Distribution との紐付け Distribution との紐付けは CloudFront Functions 関連の API ではなく CloudFront の update-distribution で行います。 マネジメントコンソール上では CloudFront Functions の画面の Associate タブから実行できます。\nupdate-distribution を行うには DistributionConfig が必要になるので、既存の DistributionConfig を get-distribution で取得します。\n$ CF_DIST_ID=\u0026#34;YOURDISTID\u0026#34; $ aws cloudfront get-distribution \\ --id ${CF_DIST_ID} \\ | jq \u0026#34;.Distribution.DistributionConfig\u0026#34; \\ \u0026gt; distribution-config.json そして、 DistributionConfig の LambdaFunctionAssociations を削除して、代わりに FunctionAssociations の部分を下記のように記述します。\n{ \u0026#34;FunctionAssociations\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;FunctionARN\u0026#34;: \u0026#34;\u0026lt;CloudFront Function の ARN\u0026gt;\u0026#34;, \u0026#34;EventType\u0026#34;: \u0026#34;viewer-request\u0026#34; } ] } } Function の ARN は下記コマンドで取得します。\n$ aws cloudfront describe-function \\ --name \u0026#34;${CFF_NAME}\u0026#34; \\ --query \u0026#34;FunctionSummary.FunctionMetadata.FunctionARN\u0026#34; \\ --output text 更新前後の差分は下記のようになります。\n--- distribution-config.json\t2021-05-05 00:45:35.000000000 +0900 +++ distribution-config-update.json\t2021-05-05 00:59:53.000000000 +0900 @@ -59,18 +59,18 @@ \u0026#34;SmoothStreaming\u0026#34;: false, \u0026#34;Compress\u0026#34;: false, \u0026#34;LambdaFunctionAssociations\u0026#34;: { + \u0026#34;Quantity\u0026#34;: 0 + }, + \u0026#34;FunctionAssociations\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { - \u0026#34;LambdaFunctionARN\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:XXXXXXXXXXXX:function:CFRedirectIndexDocument:29\u0026#34;, + \u0026#34;FunctionARN\u0026#34;: \u0026#34;arn:aws:cloudfront::XXXXXXXXXXXX:function/cf2-redirect\u0026#34;, \u0026#34;EventType\u0026#34;: \u0026#34;viewer-request\u0026#34;, \u0026#34;IncludeBody\u0026#34;: false } ] }, - \u0026#34;FunctionAssociations\u0026#34;: { - \u0026#34;Quantity\u0026#34;: 0 - }, \u0026#34;FieldLevelEncryptionId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: true, 更新用の JSON ができたので、 cloudfront update-distribution コマンドで更新します。\n$ aws cloudfront update-distribution \\ --id ${CF_DIST_ID} \\ --if-match $( \\ aws cloudfront get-distribution \\ --id ${CF_DIST_ID} \\ --query \u0026#34;ETag\u0026#34; \\ --output text) \\ --distribution-config file://distribution-config-update.json まとめ CloudFront Functions を AWS CLI で触ってみるついでに、ブログの URL 正規化処理を Lambda@Edge から移行した話でした。\nCloudFront Functions は Lambda@Edge と比較して、よりクライアントに近いところで動くため、レスポンスの高速化が期待できます。また、料金についても安くなり、今回の移行により毎月 Lambda@Edge で 0.1 USD くらいかかっていたのが、おそらく無料枠の範囲内に収まりそうです。\n2,000,000 CloudFront Function Invocations\nInvocation pricing is $0.10 per 1 million invocations ($0.0000001 per request).\nCDN Pricing | Free Tier Eligible, Pay-as-you-go | Amazon CloudFront Lambda@Edge と比較して実行時間やメモリ、パッケージサイズの制限は厳しいですが、今回のようなパスルーティング程度の処理であればメリットしか無いので、すぐにでも移行するのが良いかなと思いました。\nViewer トリガーの場合。 Origin トリガーの場合は 50 MB。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nViewer トリガーの場合。 Origin トリガーの場合は 10 GB。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nViewer トリガーの場合。 Origin トリガーの場合は 30 秒。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-normalize-url-using-cloudfront-functions/",
    "title": "CloudFront Functions を AWS CLI で触る ― ついでにブログの URL 正規化を Lambda@Edge から移行した"
  },
  {
    "contents": "AWS CLI v2 には 対話形式でリソースを操作できる wizard コマンドが用意されています。今回はその wizard コマンドを使って DynamoDB のテーブルを作成してみます。\n目次 wizard コマンド DynamoDB のテーブルを作成してみる Intro Capacity Encryption Preview Done 作成されているか確認 まとめ wizard コマンド AWS CLI v2 では、 GA 当初 (v2.0.0) から wizard コマンドが利用可能で、一部のリソースに対する操作を対話的に実行できます。この記事を書いている時点 (2021/04/30) 時点で wizard コマンドに対応しているのは下記の操作で、 configure を除いては aws \u0026lt;サービス\u0026gt; wizard \u0026lt;wizard 用サブコマンド\u0026gt; という形で実行します。\nAWS CLI の config 設定 aws configure 実行時に自動で対話モードになる DynamoDB のテーブル作成 aws dynamodb wizard new-table EventBridge のルール作成 aws events wizard new-rule IAM のロール作成 aws iam wizard new-role Lambda の関数作成 aws lambda wizard new-function DynamoDB のテーブルを作成してみる 今回は、 DynamoDB の wizard コマンドで対話形式でテーブルを作成してみます。\naws dynamodb wizard new-table 上記コマンドを実行すると、次のようなプロンプトが起動します。\nIntro Capacity Encryption Preview Done の 5 ステップあることがわかります。\nIntro Intro では下記項目を入力または選択していきます。\nテーブル名 プライマリーキー名 プライマリーキーのタイプ String / Number / Binary から選択 ソートキーを追加するかどうか Yes / No で選択 (ソートキーを追加する場合) ソートキー名 ソートキーのタイプ String / Number / Binary から選択 Capacity Capacity では下記項目を入力または選択していきます。\nキャパシティーモード Provisioned / On-demand から選択 (Provisioned を選択した場合) Read のキャパシティーユニット Write のキャパシティーユニット Encryption Encryption では下記項目を入力または選択していきます。\nテーブルの暗号化設定 DEFAULT / KMS - AWS managed CMK / KMS - Customer managed CMK から選択 (KMS - Customer managed CMK を選択した場合) カスタマーマネージドな CMK のパス Preview Preview では、作成されるテーブルの内容を AWS CLI コマンドまたは CloudFormation テンプレートとしてプレビューすることができます。\nDone Done では、これまでの設定内容で実行するかどうかを選択します。\nYes を選択すると、 Preview で選択した内容が出力されます。\nAWS CLI command を選択していた場合 Wizard successfully created DynamoDB Table: Table name: user Primary partition key: id (S) Primary sort key: age (N) Steps to create table is equivalent to running the following sample AWS CLI commands: aws dynamodb create-table \\ --table-name \u0026#39;user\u0026#39; \\ --attribute-definitions \\ \u0026#39;AttributeName=id,AttributeType=S\u0026#39; \\ \u0026#39;AttributeName=age,AttributeType=N\u0026#39; \\ --key-schema \\ \u0026#39;AttributeName=id,KeyType=HASH\u0026#39; \\ \u0026#39;AttributeName=age,KeyType=RANGE\u0026#39; \\ --provisioned-throughput \\ \u0026#39;ReadCapacityUnits=1,WriteCapacityUnits=1\u0026#39; CloudFormation template を選択していた場合 Wizard successfully created DynamoDB Table: Table name: user Primary partition key: id (S) Primary sort key: age (N) Steps to create table is equivalent to deploying the following sample AWS CloudFormation template: Resources: user: Type: \u0026#34;AWS::DynamoDB::Table\u0026#34; Properties: TableName: user AttributeDefinitions: - AttributeName: id AttributeType: S - AttributeName: age AttributeType: N KeySchema: - AttributeName: id KeyType: HASH - AttributeName: age KeyType: RANGE ProvisionedThroughput: ReadCapacityUnits: 1 WriteCapacityUnits: 1 作成されているか確認 作成されたテーブルを確認してみます。\n$ aws dynamodb describe-table \\ --table-name user { \u0026#34;Table\u0026#34;: { \u0026#34;AttributeDefinitions\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;N\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], \u0026#34;TableName\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;age\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;TableStatus\u0026#34;: \u0026#34;ACTIVE\u0026#34;, \u0026#34;CreationDateTime\u0026#34;: \u0026#34;2021-05-01T01:05:31.656000+09:00\u0026#34;, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 }, \u0026#34;TableSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;TableArn\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-northeast-1:XXXXXXXXXXXX:table/user\u0026#34;, \u0026#34;TableId\u0026#34;: \u0026#34;3327e6ef-b638-4318-80b5-8722cc840ee6\u0026#34; } } まとめ AWS CLI の wizard コマンドで DynamoDB のテーブルを作成してみた話でした。\nwizard が使えるサービスは限られているものの、対話形式で必要なパラメータを埋めていくのはやはり直感的でわかりやすいです。CLI とマネジメントコンソールでの操作の良いところを合わせたような感じで、今後対応するサービスが増えていくと良いなと思いました。\nあと、実際にテーブルを作成する一つ前の段階で AWS CLI のコマンドと CloudFormation のテンプレートを確認することができるので、作りたいリソースを作成するためにどのようなコマンドやテンプレートが必要になるのかを確認する手段としても良さそうだなと思いました。\nちなみに、対応しているサービスについては、 AWS CLI の下記ディレクトリを見ると確認できます。\naws-cli/awscli/customizations/wizard/wizards at v2 · aws/aws-cli AWS CLI のドキュメントにも上記ディレクトリを見てねと書かれていました。\nTo contribute or view the full list of available AWS CLI wizards, see the AWS CLI wizards folder on GitHub.\nUsing the AWS CLI wizards - AWS Command Line Interface ",
    "permalink": "https://michimani.net/post/aws-create-dynamodb-table-using-wizard-command/",
    "title": "AWS CLI の wizard コマンドを使って対話形式で DynamoDB のテーブルを作成する"
  },
  {
    "contents": "普段はパッチバージョンが更新されていくことが多い AWS CLI ですが、今朝のアップデートでは v2 のマイナーバージョンが上がって 2.1.39 から 2.2.0 になりました。 CHANGELOG をもとに、どこが変わったのか見てみます。\n目次 v2.2.0 の変更点 変更点の詳細 ヘルプで確認できるレベルの変更点 ソースコードレベルの変更 CRT を使って S3 転送を実行してみる default で実行 crt で実行 そもそも AWS Common Runtime (CRT) とは まとめ v2.2.0 の変更点 AWS CLI では、マイナーバージョンが更新されるときは CHANGELOG に feature: として内容が記載されます。\n今回の 2.2.0 では下記の記述がありました。\nfeature:s3: Add experimental support for performing S3 transfers using the AWS Common Runtime (CRT). It provides a C-based S3 transfer client that can improve transfer throughput for s3 commands.\naws-cli/CHANGELOG.rst at 2.2.0 · aws/aws-cli 直訳すると\nAWS Common Runtime（CRT）を使用してS3転送を実行する実験的なサポートを追加しました。これは、CベースのS3転送クライアントを提供し、s3コマンドの転送スループットを向上させることができます。\n\u0026hellip;なるほど。\n変更点の詳細 CHANGELOG およびコミットをもとに、変更点の詳細を確認してみます。\nヘルプで確認できるレベルの変更点 s3 コマンドについて、ヘルプで確認できるレベルの変更点は確認できませんでした。 (利用可能なサブコマンド、ヘルプの内容は同じ)\nソースコードレベルの変更 ソースコードに関しては、下記のコミットにて差分を確認できます。\nMerge pull request #6105 from kyleknap/crt-s3 · aws/aws-cli@2e05300 CHANGELOG で記載のあった転送に関する変更が加えられており、 transfer_client として crt を指定できるようになっているようです。\ns3 コマンドのサブコマンドのうち、 S3 転送を利用するサブコマンドは下記のとおりです。(= S3TransferCommand)\ncp mv rm sync これは awscli/customizations/s3/subcommands.py#L708 あたりで確認できます。\naws-cli/subcommands.py at v2 · aws/aws-cli S3TransferCommand では、 config ファイル (= ~/.aws/config) から取得した S3 に関する設定の中から、 preferred_transfer_client で指定した値を transfer_client として使うようになっています。今回のアップデートでは、config ファイルで transfer_client を指定できるようになったという内容だということがわかりました。\nCRT を使って S3 転送を実行してみる 変更点の内容がわかったので、実際に CRT を使用して S3 転送を実行してみます。今回は s3 cp コマンドでその違いを確認してみます。\nコマンドの実行詳細については --debug オプションを付けることで確認できます。\ndefault で実行 まずは、新たに指定可能となった preferred_transfer_client を使わずに s3 cp を実行してみます。\n$ aws s3 cp sample.txt s3://aws-cli-2-2-0-test-XXXXXXXXXXXX/ --debug デバッグの出力は割と量があるので transfer で grep した結果だけ貼ります。\n2021-04-28 23:27:32,931 - MainThread - botocore.hooks - DEBUG - Event load-cli-arg.custom.cp.force-glacier-transfer: calling handler \u0026lt;awscli.paramfile.URIArgumentHandler object at 0x7f823ca186a0\u0026gt; 2021-04-28 23:27:33,015 - MainThread - s3transfer.utils - DEBUG - Acquiring 0 2021-04-28 23:27:33,015 - ThreadPoolExecutor-1_0 - s3transfer.tasks - DEBUG - UploadSubmissionTask(transfer_id=0, {\u0026#39;transfer_future\u0026#39;: \u0026lt;s3transfer.futures.TransferFuture object at 0x7f823d24e220\u0026gt;}) about to wait for the following futures [] 2021-04-28 23:27:33,015 - ThreadPoolExecutor-1_0 - s3transfer.tasks - DEBUG - UploadSubmissionTask(transfer_id=0, {\u0026#39;transfer_future\u0026#39;: \u0026lt;s3transfer.futures.TransferFuture object at 0x7f823d24e220\u0026gt;}) done waiting for dependent futures 2021-04-28 23:27:33,016 - ThreadPoolExecutor-1_0 - s3transfer.tasks - DEBUG - Executing task UploadSubmissionTask(transfer_id=0, {\u0026#39;transfer_future\u0026#39;: \u0026lt;s3transfer.futures.TransferFuture object at 0x7f823d24e220\u0026gt;}) with kwargs {\u0026#39;client\u0026#39;: \u0026lt;botocore.client.S3 object at 0x7f823d1fb6d0\u0026gt;, \u0026#39;config\u0026#39;: \u0026lt;s3transfer.manager.TransferConfig object at 0x7f823d2394c0\u0026gt;, \u0026#39;osutil\u0026#39;: \u0026lt;s3transfer.utils.OSUtils object at 0x7f823d239550\u0026gt;, \u0026#39;request_executor\u0026#39;: \u0026lt;s3transfer.futures.BoundedExecutor object at 0x7f823d239790\u0026gt;, \u0026#39;transfer_future\u0026#39;: \u0026lt;s3transfer.futures.TransferFuture object at 0x7f823d24e220\u0026gt;} 2021-04-28 23:27:33,022 - ThreadPoolExecutor-1_0 - s3transfer.futures - DEBUG - Submitting task PutObjectTask(transfer_id=0, {\u0026#39;bucket\u0026#39;: \u0026#39;aws-cli-2-2-0-test-XXXXXXXXXXXX\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;sample.txt\u0026#39;, \u0026#39;extra_args\u0026#39;: {\u0026#39;ContentType\u0026#39;: \u0026#39;text/plain\u0026#39;}}) to executor \u0026lt;s3transfer.futures.BoundedExecutor object at 0x7f823d239790\u0026gt; for transfer request: 0. 2021-04-28 23:27:33,022 - ThreadPoolExecutor-1_0 - s3transfer.utils - DEBUG - Acquiring 0 2021-04-28 23:27:33,022 - ThreadPoolExecutor-0_0 - s3transfer.tasks - DEBUG - PutObjectTask(transfer_id=0, {\u0026#39;bucket\u0026#39;: \u0026#39;aws-cli-2-2-0-test-XXXXXXXXXXXX\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;sample.txt\u0026#39;, \u0026#39;extra_args\u0026#39;: {\u0026#39;ContentType\u0026#39;: \u0026#39;text/plain\u0026#39;}}) about to wait for the following futures [] 2021-04-28 23:27:33,022 - ThreadPoolExecutor-0_0 - s3transfer.tasks - DEBUG - PutObjectTask(transfer_id=0, {\u0026#39;bucket\u0026#39;: \u0026#39;aws-cli-2-2-0-test-XXXXXXXXXXXX\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;sample.txt\u0026#39;, \u0026#39;extra_args\u0026#39;: {\u0026#39;ContentType\u0026#39;: \u0026#39;text/plain\u0026#39;}}) done waiting for dependent futures 2021-04-28 23:27:33,023 - ThreadPoolExecutor-1_0 - s3transfer.utils - DEBUG - Releasing acquire 0/None 2021-04-28 23:27:33,023 - ThreadPoolExecutor-0_0 - s3transfer.tasks - DEBUG - Executing task PutObjectTask(transfer_id=0, {\u0026#39;bucket\u0026#39;: \u0026#39;aws-cli-2-2-0-test-XXXXXXXXXXXX\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;sample.txt\u0026#39;, \u0026#39;extra_args\u0026#39;: {\u0026#39;ContentType\u0026#39;: \u0026#39;text/plain\u0026#39;}}) with kwargs {\u0026#39;client\u0026#39;: \u0026lt;botocore.client.S3 object at 0x7f823d1fb6d0\u0026gt;, \u0026#39;fileobj\u0026#39;: \u0026lt;s3transfer.utils.ReadFileChunk object at 0x7f823d24ea30\u0026gt;, \u0026#39;bucket\u0026#39;: \u0026#39;aws-cli-2-2-0-test-XXXXXXXXXXXX\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;sample.txt\u0026#39;, \u0026#39;extra_args\u0026#39;: {\u0026#39;ContentType\u0026#39;: \u0026#39;text/plain\u0026#39;}} 2021-04-28 23:27:33,024 - ThreadPoolExecutor-0_0 - botocore.endpoint - DEBUG - Making request for OperationModel(name=PutObject) with params: {\u0026#39;url_path\u0026#39;: \u0026#39;/aws-cli-2-2-0-test-XXXXXXXXXXXX/sample.txt\u0026#39;, \u0026#39;query_string\u0026#39;: {}, \u0026#39;method\u0026#39;: \u0026#39;PUT\u0026#39;, \u0026#39;headers\u0026#39;: {\u0026#39;Content-Type\u0026#39;: \u0026#39;text/plain\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;aws-cli/2.2.0 Python/3.8.8 Darwin/20.4.0 exe/x86_64 prompt/off command/s3.cp\u0026#39;, \u0026#39;Content-MD5\u0026#39;: \u0026#39;1B2M2Y8AsgTpgAmY7PhCfg==\u0026#39;, \u0026#39;Expect\u0026#39;: \u0026#39;100-continue\u0026#39;}, \u0026#39;body\u0026#39;: \u0026lt;s3transfer.utils.ReadFileChunk object at 0x7f823d24ea30\u0026gt;, \u0026#39;url\u0026#39;: \u0026#39;https://s3.ap-northeast-1.amazonaws.com/aws-cli-2-2-0-test-XXXXXXXXXXXX/sample.txt\u0026#39;, \u0026#39;context\u0026#39;: {\u0026#39;client_region\u0026#39;: \u0026#39;ap-northeast-1\u0026#39;, \u0026#39;client_config\u0026#39;: \u0026lt;botocore.config.Config object at 0x7f823d1fb7c0\u0026gt;, \u0026#39;has_streaming_input\u0026#39;: True, \u0026#39;auth_type\u0026#39;: None, \u0026#39;signing\u0026#39;: {\u0026#39;bucket\u0026#39;: \u0026#39;aws-cli-2-2-0-test-XXXXXXXXXXXX\u0026#39;}}} 2021-04-28 23:27:33,025 - ThreadPoolExecutor-0_0 - botocore.hooks - DEBUG - Event request-created.s3.PutObject: calling handler \u0026lt;function signal_not_transferring at 0x7f823c455040\u0026gt; 2021-04-28 23:27:33,027 - ThreadPoolExecutor-0_0 - botocore.hooks - DEBUG - Event request-created.s3.PutObject: calling handler \u0026lt;function signal_transferring at 0x7f823c0360d0\u0026gt; 2021-04-28 23:27:33,180 - ThreadPoolExecutor-0_0 - s3transfer.utils - DEBUG - Releasing acquire 0/None CRT の文字もなく、普通に実行されてる感があります。\ncrt で実行 続いて、 preferred_transfer_client に crt を指定して実行してみます。 config ファイルでは、下記のように記述します。\n[default] region=ap-northeast-1 output=json s3 = preferred_transfer_client=crt これで実行して、先ほどと同じくデバッグの出力を transfer で grep した結果を貼ります。\n2021-04-28 23:33:09,262 - MainThread - botocore.hooks - DEBUG - Event load-cli-arg.custom.cp.force-glacier-transfer: calling handler \u0026lt;awscli.paramfile.URIArgumentHandler object at 0x7fb2aba1a6a0\u0026gt; 2021-04-28 23:33:09,360 - MainThread - awscli.customizations.s3.subscribers - DEBUG - Not providing transfer size. Future: \u0026lt;s3transfer.crt.CRTTransferFuture object at 0x7fb2ac2a3730\u0026gt; does not offerthe capability to notify the size of a transfer 2021-04-28 23:33:09,365 - MainThread - botocore.hooks - DEBUG - Event request-created.s3.PutObject: calling handler \u0026lt;bound method BotocoreCRTRequestSerializer._capture_http_request of \u0026lt;s3transfer.crt.BotocoreCRTRequestSerializer object at 0x7fb2ac23b4c0\u0026gt;\u0026gt; 2021-04-28 23:33:09,366 - MainThread - botocore.hooks - DEBUG - Event before-send.s3.PutObject: calling handler \u0026lt;bound method BotocoreCRTRequestSerializer._make_fake_http_response of \u0026lt;s3transfer.crt.BotocoreCRTRequestSerializer object at 0x7fb2ac23b4c0\u0026gt;\u0026gt; 2021-04-28 23:33:09,367 - MainThread - botocore.hooks - DEBUG - Event after-call.s3.PutObject: calling handler \u0026lt;bound method BotocoreCRTRequestSerializer._change_response_to_serialized_http_request of \u0026lt;s3transfer.crt.BotocoreCRTRequestSerializer object at 0x7fb2ac23b4c0\u0026gt;\u0026gt; CRT が使われている気配があります。\nということで、 AWS Common Runtime（CRT）を使用してS3転送を実行する実験的なサポート が追加されていることがわかり、実際に実行することができました。\nそもそも AWS Common Runtime (CRT) とは 前項までの内容が AWS CLI v2.2.0 での変更点ということみたいですが、そもそも CRT って何？ となったので調べてみました。\nAWS Common Runtime (CRT) は、 C 言語で書かれたライブラリで、 Go を除く各言語の SDK のベースライブラリとして採用されています。ライブラリ自体は 9 個の独立したパッケージで構成されており、内容 (役割) は下記の通りです。\nawslabs/aws-c-common: 基本的なデータ構造 awslabs/aws-c-io: TCP, UDP, SSL/TLS awslabs/aws-c-mqtt: MQTT awslabs/aws-c-http: HTTP クライアント awslabs/aws-c-cal: 暗号化 awslabs/aws-c-auth: AWS のクライアントサイド認証 awslabs/aws-c-compression: 圧縮アルゴリズム awslabs/aws-c-event-stream: イベントメッセージ処理 awslabs/aws-checksums: CRC32c/CRC32 による整合性チェック 詳細については下記のドキュメントを参照。\nAWS Common Runtime (CRT) libraries - AWS SDKs and Tools AWS CLI は Python で実装されているので SDK for Python (boto) を使っており、その中で aws-crt-python を使っています。\ns3transfer/crt.py at develop · boto/s3transfer awslabs/aws-crt-python: Python bindings for the AWS Common Runtime CHANGELOG にも書かれていましたが、 CRT を使用して S3 転送を行うことで、スループットの向上が期待できるということです。\n\u0026hellip; このあたりは深追いするとキリがなさそうなので、ざっくりとこの辺までにしておきます。\nまとめ AWS CLI v2 のマイナーバージョンが上がったので、変更点について調べてみた話でした。ついでに AWS Common Runtime (CRT) についても少しだけ調べてみました。\nv1 は新機能への対応が早いですが、 v2 はこういった実験的なサポートが追加されていくという点で v1 と差別化してる感じなんでしょうか。\nとりあえず、変更点の内容が把握できたのでよしとします。\n",
    "permalink": "https://michimani.net/post/aws-changes-in-aws-cli-v2.2.0/",
    "title": "AWS CLI v2.2.0 の変更点について調べてみた"
  },
  {
    "contents": "AWS CLI には任意のエイリアスコマンドを設定することができるということを最近知ったので、試してみます。\n目次 AWS CLI のエイリアス 設定方法 エイリアスコマンドで引数を受け取りたい場合 いろいろ設定してみた AWS アカウント ID を取得する 実行ユーザー (ロール) の ARN を取得する ランタイムを指定して Lambda 関数一覧を取得する ECR にログインする まとめ AWS CLI のエイリアス AWS CLI には、よく使うコマンドや長いコマンドに対してあらかじめエイリアスを設定することができます。\nこの機能自体は、 v1 であれば 1.11.24 以降、 v2 であれば 2.0.0 以降で利用可能ということで、だいぶ前から使える機能だったようです。\nfeature:alias: Add ability to alias commands in the CLI aws-cli/CHANGELOG.rst at develop | 1.11.24 · aws/aws-cli 設定方法 設定方法は至って簡単で、 ~/.aws/cli ディレクトリを作成し、そこに alias という名前のファイルを置くだけです。\nalias ファイルでは、下記のようにエイリアスを記述します。\n[toplevel] whoami = sts get-caller-identity 上記の設定では、下記のコマンドが等価となります。\naws sts get-caller-identity aws whoami エイリアスコマンドで引数を受け取りたい場合 設定したエイリアスコマンドで引数を受け取って使いたい場合は、下記のように設定します。\ncreate-bucket = !f() { aws s3api create-bucket \\ --bucket \u0026#34;${1}\u0026#34; \\ --create-bucket-configuration \u0026#34;LocationConstraint=${2}\u0026#34; \\ --region \u0026#34;${2}\u0026#34; }; f 上記の例では第一引数にバケット名、第二引数に対象リージョンを指定することで新たに S3 バケットを作成します。\n$ aws create-bucket mybucket ap-northeast-1 { \u0026#34;Location\u0026#34;: \u0026#34;http://mybucket.s3.amazonaws.com/\u0026#34; } いろいろ設定してみた 設定方法がわかったので、普段よく使うコマンドにエイリアスを設定してみました。\nAWS アカウント ID を取得する id = sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text 実行ユーザー (ロール) の ARN を取得する myarn = sts get-caller-identity \\ --query \u0026#39;Arn\u0026#39; \\ --output text ランタイムを指定して Lambda 関数一覧を取得する # 1st parameter: \u0026lt;runtime\u0026gt; lambda-fn-runtime = !f() { aws lambda list-functions \\ --query \u0026#34;Functions[?contains(to_string(Runtime),\\`${1}\\`)]\u0026#34; }; f ECR にログインする # 1st parameter: \u0026lt;region\u0026gt; ecr-login = !f() { aws ecr get-login-password \\ --region \u0026#34;${1}\u0026#34; \\ | docker login \\ --username AWS \\ --password-stdin $(aws id).dkr.ecr.${1}.amazonaws.com }; f まとめ AWS CLI のエイリアスを試してみた話でした。\nAWS Lab のリポジトリには様々なサービスに対するエイリアスコマンドが記述された alias が公開されていますが、最終更新が 3 年前とかなので自分で作っていくのが良さそうです。\nawslabs/awscli-aliases: Repository for AWS CLI aliases. 今回のブログで紹介したエイリアスは下記のリポジトリにおいています。\nmichimani/awscli-aliases: Collection of my alias commands for the AWS CLI. ",
    "permalink": "https://michimani.net/post/aws-how-to-use-aws-cli-alias/",
    "title": "AWS CLI のエイリアスコマンドを使ってみる"
  },
  {
    "contents": "CloudTrail に記録された証跡から IAM ポリシーを生成する機能が AWS IAM Access Analyzer に追加されました。今回はその機能を AWS CLI から使ってみます。\n目次 新機能の概要 AWS CLI から使ってみる 対応するコマンド サブコマンドの詳細 やってみる 対象となる CloudTrail の証跡の ARN IAM ポリシー生成用の IAM ロールを作成する cloud-trail-details オプション用の JSON を作成 start-generating-policy を実行 生成された IAM ポリシーを確認する まとめ 新機能の概要 AWS IAM Access Analyzer の機能として、CLoudTrail に記録された証跡から IAM ポリシーを生成するという機能が発表されました。これはつまり、実際のリクエストに基づいて最低限必要な権限を付与したポリシーを生成できる1という機能になります。\nIAM Access Analyzer makes it easier to implement least privilege permissions by generating IAM policies based on access activity IAM Access Analyzer makes it easier to implement least privilege permissions by generating IAM policies based on access activity | AWS Security Blog AWS CLI から使ってみる マネジメントコンソールからの操作については、上記の AWS ブログやクラスメソッドさんのブログで既に書かれているので、ここでは AWS CLI からその機能を使ってみます。\n[UPDATE] IAMユーザー/IAMロールの実際の操作履歴からIAMポリシーが生成可能になりました ! | DevelopersIO 対応するコマンド 今回の機能を使うには、 accessanalyzer コマンドの start-policy-generation サブコマンドを使用します。\n例のごとく、この記事を書いている時点では v1 の最新バージョンである 1.19.47 でのみ対応しており、 v2 の最新バージョンである 2.1.35 では使えません。\nサブコマンドの詳細 accessanalyzer start-policy-generation のヘルプでオプションを確認します。\n$ aws accessanalyzer start-policy-generation help ... SYNOPSIS start-policy-generation [--client-token \u0026lt;value\u0026gt;] [--cloud-trail-details \u0026lt;value\u0026gt;] --policy-generation-details \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] ... ふーん、なるほど。ということで --generate-cli-skeleton で詳細を確認してみます。\n$ aws accessanalyzer start-policy-generation --generate-cli-skeleton { \u0026#34;clientToken\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cloudTrailDetails\u0026#34;: { \u0026#34;accessRole\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;1970-01-01T00:00:00\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;1970-01-01T00:00:00\u0026#34;, \u0026#34;trails\u0026#34;: [ { \u0026#34;allRegions\u0026#34;: true, \u0026#34;cloudTrailArn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;\u0026#34; ] } ] }, \u0026#34;policyGenerationDetails\u0026#34;: { \u0026#34;principalArn\u0026#34;: \u0026#34;\u0026#34; } } 割とシンプルですね。\nちなみに、今回の機能に関連して新たに追加されたサブコマンドは、 start-policy-generation を含めて下記の 4 つです。\ncancel-policy-generation get-generated-policy list-policy-generations start-policy-generation IAM ポリシーの生成には少し時間がかかるようなので、生成をキャンセルしたり、生成中の一覧を取得したりできるコマンドが追加されています。\nやってみる では、早速 start-policy-generation で IAM ポリシーを生成してみます。\n必須のオプションは --policy-generation-details のみなので、まずはこれのみ指定して実行してみます。\n--policy-generation-details で指定するのは principalArn です。この ARN とは、生成する IAM ポリシーをアタッチする予定の IAM ユーザー または IAM ロールの ARN になります。今回は AWS CLI を実行するユーザー (ロール) の ARN を指定することにしますが、その ARN を取得するためには sts get-caller-identity を使用します。\n$ PRINCIPAL_ARN=$( \\ aws sts get-caller-identity \\ --query \u0026#34;Arn\u0026#34; \\ --output text ) \u0026amp;\u0026amp; echo \u0026#34;${PRINCIPAL_ARN}\u0026#34; arn:aws:iam::111111111111:user/hogehoge この値を使って、次のように実行してみます。アウトプットとして jobID が出力されるので、変数で受けます。\n$ JOB_ID=$( \\ aws accessanalyzer start-policy-generation \\ --policy-generation-details principalArn=${PRINCIPAL_ARN} \\ --query \u0026#34;jobId\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${JOB_ID}\u0026#34; An error occurred (ValidationException) when calling the StartPolicyGeneration operation: Missing cloudTrailDetails そんな気はしていましたが、 --cloud-trail-details オプションで CloudTrail の条件も指定する必要がありました。\nCloudTrail の条件は下記形式の JSON で指定します。\n{ \u0026#34;accessRole\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;1970-01-01T00:00:00\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;1970-01-01T00:00:00\u0026#34;, \u0026#34;trails\u0026#34;: [ { \u0026#34;allRegions\u0026#34;: true, \u0026#34;cloudTrailArn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;\u0026#34; ] } ] } accessRole : IAM Access Analyzer によって IAM ポリシーを生成するための IAM ロールの ARN です。 endTime : CloudTrail で解析する期間の終了日時です。省略した場合は現在日時がセットされます。 startTime : CloudTrail で解析する期間の開始日時です。 trails : 対象とする CloudTrail の証跡のリストです。 allRegions : すべてのリージョンを対象にするかどうかです。 cloudTrailArn : 解析する対象となる CLoudTrail の証跡の ARN です。 regions : 対象とするリージョンのリストです。 それぞれ必要なものを準備していきます。\n対象となる CloudTrail の証跡の ARN 今回は一つの証跡を対象として、リージョンも東京 (ap-northeast-1) に絞って生成することにします。なので、先に対象の証跡の ARN を取得しておきます。\n$ TRAIL_ARN=$( \\ aws cloudtrail list-trails \\ --query \u0026#34;Trails[0].TrailARN\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${TRAIL_ARN}\u0026#34; arn:aws:cloudtrail:ap-northeast-1:111111111111:trail/hogehoge-trail IAM ポリシー生成用の IAM ロールを作成する accessRole に指定する IAM ポリシー生成用の IAM ロールは新たに作成する必要があるので、作成します。(マネジメントコンソールから実行する場合、初回は勝手に作ってくれます)\nその際に、 CloudTrail の証跡が保存されている S3 バケット名が必要になるので、あらかじめ取得しておきます。\n$ TRAIL_BUCKET_NAME=$( \\ aws cloudtrail get-trail \\ --name \u0026#34;${TRAIL_ARN}\u0026#34; \\ --query \u0026#34;Trail.S3BucketName\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${TRAIL_BUCKET_NAME}\u0026#34; hogehoge-trail-bucket-000000 S3 バケット名を用いて、下記のような IAM ポリシードキュメントを作成します。\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; access-role-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;cloudtrail:GetTrail\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:GenerateServiceLastAccessedDetails\u0026#34;, \u0026#34;iam:GetServiceLastAccessedDetails\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${TRAIL_BUCKET_NAME}\u0026#34;, \u0026#34;arn:aws:s3:::${TRAIL_BUCKET_NAME}/*\u0026#34; ] } ] } EOF ※対象となる CloudTrail の証跡が KMS 暗号化されている場合は、下記の権限も必要になります。\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:kms:\u0026lt;region\u0026gt;:\u0026lt;account-id\u0026gt;:key/\u0026lt;key\u0026gt;\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:ViaService\u0026#34;: \u0026#34;s3.*.amazonaws.com\u0026#34; } } } 生成した JSON をもとに IAM ポリシーを作成します。\n$ ACCESS_ROLE_POLICY_ARN=$( \\ aws iam create-policy \\ --policy-name \u0026#34;iam-access-analizer-generate-policy-role-policy\u0026#34; \\ --path \u0026#34;/sample/\u0026#34; \\ --policy-document file://access-role-policy.json \\ --query \u0026#34;Policy.Arn\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${ACCESS_ROLE_POLICY_ARN}\u0026#34; arn:aws:iam::111111111111:policy/sample/iam-access-analizer-generate-policy-role-policy 続いて、この IAM ポリシーをアタッチする IAM ロールを作成します。その際、 IAM Access Analyzer に対する信頼ポリシーが必要になるので、下記のような信頼ポリシードキュメント (JSON) を作成しておきます。\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; assume-role-policy-doc.json { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;access-analyzer.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF IAM ロールを作成します。\n$ ACCESS_ROLE_ARN=$( \\ aws iam create-role \\ --role-name iam-access-analizer-generate-policy-role \\ --assume-role-policy-document file://assume-role-policy-doc.json \\ --query \u0026#34;Role.Arn\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${ACCESS_ROLE_ARN}\u0026#34; arn:aws:iam::111111111111:role/iam-access-analizer-generate-policy-role 作成した IAM ロールに、先ほど作成した IAM ポリシーをアタッチします。\n$ aws iam attach-role-policy \\ --role-name iam-access-analizer-generate-policy-role \\ --policy-arn \u0026#34;${ACCESS_ROLE_POLICY_ARN}\u0026#34; cloud-trail-details オプション用の JSON を作成 ここまでで、 --cloud-trail-details オプションで指定するために必要な情報が揃いました。実際に start-policy-generation 実行時に指定する際には JSON ファイルとして指定するので、これまでの情報を JSON ファイルとして生成します。\nなお、証跡の解析期間は 2021年に入ってから現時点まで とします。\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; cloud-trail-details.json { \u0026#34;accessRole\u0026#34;: \u0026#34;${ACCESS_ROLE_ARN}\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2021-01-01T00:00:00\u0026#34;, \u0026#34;trails\u0026#34;: [ { \u0026#34;allRegions\u0026#34;: false, \u0026#34;cloudTrailArn\u0026#34;: \u0026#34;${TRAIL_ARN}\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ] } ] } EOF start-generating-policy を実行 諸々準備が整ったのであらためて start-generating-policy を実行します。\n$ JOB_ID=$( \\ aws accessanalyzer start-policy-generation \\ --policy-generation-details principalArn=${PRINCIPAL_ARN} \\ --cloud-trail-details file://cloud-trail-details.json \\ --query \u0026#34;jobId\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${JOB_ID}\u0026#34; An error occurred (ServiceQuotaExceededException) when calling the StartPolicyGeneration operation: Policy Generation limit exceeded: policy generation CloudTrail time range はい、失敗しました。\n原因としては、 証跡の解析に指定できる期間は最大で 90 日 だからですね。ドキュメントを読みましょう。\nSet up for policy template generation – You specify a time period of up to 90 days for IAM Access Analyzer to analyze your historical AWS CloudTrail events.\nGenerate policies based on access activity - AWS Identity and Access Management ということなので、 2021/02 〜 2021/03 の 2ヶ月間に絞って解析してみます。\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; cloud-trail-details.json { \u0026#34;accessRole\u0026#34;: \u0026#34;${ACCESS_ROLE_ARN}\u0026#34;, \u0026#34;endTime\u0026#34;: \u0026#34;2021-04-01T00:00:00\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2021-02-01T00:00:00\u0026#34;, \u0026#34;trails\u0026#34;: [ { \u0026#34;allRegions\u0026#34;: false, \u0026#34;cloudTrailArn\u0026#34;: \u0026#34;${TRAIL_ARN}\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ] } ] } EOF $ JOB_ID=$( \\ aws accessanalyzer start-policy-generation \\ --policy-generation-details principalArn=${PRINCIPAL_ARN} \\ --cloud-trail-details file://cloud-trail-details.json \\ --query \u0026#34;jobId\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo \u0026#34;${JOB_ID}\u0026#34; f4fc04f6-d4b7-4155-a722-321a19885855 無事に実行されました。\nget-generated-policy サブコマンドで実行状況を確認してみます。\n$ aws accessanalyzer get-generated-policy \\ --job-id \u0026#34;${JOB_ID}\u0026#34; { \u0026#34;generatedPolicyResult\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;cloudTrailProperties\u0026#34;: { \u0026#34;endTime\u0026#34;: \u0026#34;2021-04-01T00:00:00Z\u0026#34;, \u0026#34;startTime\u0026#34;: \u0026#34;2021-02-01T00:00:00Z\u0026#34;, \u0026#34;trailProperties\u0026#34;: [ { \u0026#34;allRegions\u0026#34;: false, \u0026#34;cloudTrailArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:ap-northeast-1:111111111111:trail/hogehoge-trail\u0026#34;, \u0026#34;regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ] } ] }, \u0026#34;principalArn\u0026#34;: \u0026#34;arn:aws:iam::111111111111:user/hogehoge\u0026#34; } }, \u0026#34;jobDetails\u0026#34;: { \u0026#34;jobId\u0026#34;: \u0026#34;f4fc04f6-d4b7-4155-a722-321a19885855\u0026#34;, \u0026#34;startedOn\u0026#34;: \u0026#34;2021-04-08T12:27:54Z\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;IN_PROGRESS\u0026#34; } } \u0026quot;status\u0026quot;: \u0026quot;IN_PROGRESS\u0026quot; になってますね。\nステータスだけ確認するには下記コマンドを実行します。\n$ aws accessanalyzer get-generated-policy \\ --job-id \u0026#34;${JOB_ID}\u0026#34; \\ --query \u0026#34;jobDetails.status\u0026#34; \\ --output text SUCCEEDED 生成された IAM ポリシーを確認する ポリシー生成ジョブのステータスが SUCCEEDED になったら、 get-generated-policy サブコマンドの出力結果に生成された IAM ポリシーのドキュメントが含まれています。注意したいのは、生成された IAM ポリシーの情報 (ARN など) ではなく、あくまでも JSON 形式のポリシードキュメントということです。なので、出力結果を JSON ファイルに出力します。\n$ aws accessanalyzer get-generated-policy \\ --job-id \u0026#34;${JOB_ID}\u0026#34; \\ --query \u0026#34;generatedPolicyResult.generatedPolicies[0].policy\u0026#34; \\ --output text \\ | jq . \u0026gt; generated-policy.json \\ \u0026amp;\u0026amp; cat generated-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;SupportedServiceSid0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DescribeAccountAttributes\u0026#34;, \u0026#34;ec2:DescribeAddresses\u0026#34;, \u0026#34;ec2:DescribeAvailabilityZones\u0026#34;, \u0026#34;ec2:DescribeCarrierGateways\u0026#34;, \u0026#34;ec2:DescribeCustomerGateways\u0026#34;, \u0026#34;ec2:DescribeDhcpOptions\u0026#34;, \u0026#34;ec2:DescribeEgressOnlyInternetGateways\u0026#34;, \u0026#34;ec2:DescribeFlowLogs\u0026#34;, \u0026#34;ec2:DescribeHosts\u0026#34;, \u0026#34;ec2:DescribeInstanceStatus\u0026#34;, \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:DescribeInternetGateways\u0026#34;, \u0026#34;ec2:DescribeKeyPairs\u0026#34;, \u0026#34;ec2:DescribeLaunchTemplates\u0026#34;, \u0026#34;ec2:DescribeManagedPrefixLists\u0026#34;, \u0026#34;ec2:DescribeNatGateways\u0026#34;, \u0026#34;ec2:DescribeNetworkAcls\u0026#34;, \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, \u0026#34;ec2:DescribePlacementGroups\u0026#34;, \u0026#34;ec2:DescribeRegions\u0026#34;, \u0026#34;ec2:DescribeRouteTables\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeSnapshots\u0026#34;, \u0026#34;ec2:DescribeStaleSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;ec2:DescribeVolumeAttribute\u0026#34;, \u0026#34;ec2:DescribeVolumeStatus\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeVolumesModifications\u0026#34;, \u0026#34;ec2:DescribeVpcAttribute\u0026#34;, \u0026#34;ec2:DescribeVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DescribeVpcEndpoints\u0026#34;, \u0026#34;ec2:DescribeVpcPeeringConnections\u0026#34;, \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeVpnConnections\u0026#34;, \u0026#34;ec2:DescribeVpnGateways\u0026#34;, \u0026#34;ec2:ModifySubnetAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ecs:CreateCluster\u0026#34;, \u0026#34;ecs:DescribeClusters\u0026#34;, \u0026#34;ecs:DescribeServices\u0026#34;, \u0026#34;ecs:DescribeTaskDefinition\u0026#34;, \u0026#34;ecs:DescribeTasks\u0026#34;, \u0026#34;ecs:ListAccountSettings\u0026#34;, \u0026#34;ecs:ListClusters\u0026#34;, \u0026#34;ecs:ListContainerInstances\u0026#34;, \u0026#34;ecs:ListServices\u0026#34;, \u0026#34;ecs:ListTagsForResource\u0026#34;, \u0026#34;ecs:ListTaskDefinitionFamilies\u0026#34;, \u0026#34;ecs:ListTaskDefinitions\u0026#34;, \u0026#34;ecs:ListTasks\u0026#34;, \u0026#34;ecs:PutClusterCapacityProviders\u0026#34;, \u0026#34;ecs:RegisterTaskDefinition\u0026#34;, \u0026#34;ecs:RunTask\u0026#34;, \u0026#34;ecs:StopTask\u0026#34;, \u0026#34;ecs:TagResource\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancers\u0026#34;, \u0026#34;kms:CreateKey\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:GetKeyPolicy\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;kms:ListKeys\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34;, \u0026#34;ram:GetResourceShareAssociations\u0026#34;, \u0026#34;resource-groups:ListGroups\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:GetBucketWebsite\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;ssm:DeleteParameter\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;ssm:GetParametersByPath\u0026#34;, \u0026#34;ssm:PutParameter\u0026#34;, \u0026#34;sts:GetCallerIdentity\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } ちょっとした感動を覚えますね。\nちなみに、マネジメントコンソールから操作する場合は生成結果からこのドキュメントを確認し、エディタで編集し、 IAM ポリシーの生成、 IAM ロールにアタッチするところまで実行することができます。\nAWS CLI でそこまでやろうとすると、上記の JSON を各種エディタで編集し (必要であれば) 、 iam crate-policy で IAM ポリシーを生成し、 iam attach-policy で IAM ロールにアタッチする必要があります。この部分に関しては新しい要素がないのでここでは省略します。\nまとめ CloudTrail に記録された証跡から IAM ポリシーを生成する機能が AWS IAM Access Analyzer に追加されたので AWS CLI からその機能を使ってみた話でした。\nこのアップデートを見たときは IAM ポリシーまで生成してくれるものだと思っていたのですが、実際には IAM ポリシードキュメントの生成まででした。マネジメントコンソールで操作する場合は、実際に IAM ポリシーの生成から IAM ロールへのアタッチまで連続して操作できますが、 AWS CLI ではそれぞれ必要なコマンドを実行して操作する必要があります。\n必要な権限を吟味した IAM ポリシーの生成はとても大変な作業なので、実際の操作の傾向に基づいた IAM ポリシードキュメントを生成してくれるのはありがたいですね。ただし、この解析結果による権限で十分かというとそうではないので ― 例えば 90 日以上のスパンで実行するような定期作業への権限が欲しい場合 おそらくこの解析では拾えないので、生成されたドキュメントを適宜編集して利用する必要がありそうです。とはいえ、ベースとなるポリシードキュメントを生成してくれるのはいいですね。\n生成されたポリシードキュメントに変更を加えずに使用するような想定であれば、 AWS CLI のコマンドでシェルスクリプト作って良い感じに効率化できそうです。\n実際に生成されるのは JSON 形式のポリシードキュメントで、 IAM ポリシー自体が生成されるわけではない\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-get-started-generating-iam-policies-based-on-actual-activity/",
    "title": "CloudTrail の証跡から IAM ポリシーを作成する IAM Access Analyzer の新機能を AWS CLI で試す"
  },
  {
    "contents": "先日公開された ECS Exec の Interactive モードで実行したコマンドのログを CloudWatch Logs および S3 に出力してみます。\n目次 手順 やってみる ECS タスクの作成・起動 ECS Exec でコマンド実行 CloudWatch Logs で ECS Exec のログを確認 まとめ 手順 手順については下記の ECS のドキュメントを参考にしています。\nUsing Amazon ECS Exec for debugging - Amazon Elastic Container Service やってみる 今回は、適当な Web サーバーを ECS で実行して、そのタスクに対して ECS Exec でコマンドを実行し、そのログを確認してみます。\nECS タスクの作成・起動 まずは、 ECS Exec を実行する対象となる ECS タスクを作成・起動します。手順については以前に書いた下記の記事とほぼ同じなので、重複する部分は折りたたんでます。(VPC、サブネットまわりについては前回のものを使います)\nECS タスクを作って実行して CloudWatch でログを確認するまでを AWS CLI だけでやってみた - michimani.net なお、記事執筆時点では ECS Exec が AWS CLI v2 に対応していないため、 AWS CLI v1 1.19.30 を使います。\n$ aws --version aws-cli/1.19.30 Python/3.8.5 Darwin/20.3.0 botocore/1.20.30 追記 v2 に関しても 2.1.31 で対応していました。\naws-cli/CHANGELOG.rst at 2.1.31 · aws/aws-cli 0. 必要な環境変数の設定 まずは必要な環境変数を設定します。 $ APP_NAME=\u0026#34;ecs-exec-test\u0026#34; $ AWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text) 1. ECR リポジトリ作成 ECR にリポジトリを作成します。 $ ECR_REPO_URI=$( \\ aws ecr create-repository \\ --repository-name ${APP_NAME} \\ --region ap-northeast-1 \\ --query \u0026#34;repository.repositoryUri\u0026#34; \\ --output text) 2. Dockerfile を作成 ECS タスクとして実行するアプリケーションを、下記の Dockerfile で定義します。\nFROM ubuntu:20.10 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install nginx -y COPY index.html /var/www/html EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] index.html は下記の内容です。\nHello ECS Exec! 3. ビルドして ECR に Push ビルドして ECR に Push します。 # ビルド $ docker build -t ${APP_NAME} . # タグ付け $ docker tag ${APP_NAME}:latest ${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com/${APP_NAME}:latest # ECR にログイン $ aws ecr get-login-password \\ --region ap-northeast-1 \\ | docker login \\ --username AWS \\ --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com # push $ docker push ${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com/${APP_NAME}:latest 4. クラスター作成 クラスター作成時に、 ECS Exec で実行したログを出力する CloudWatch Log のロググループ、及び S3 バケットを指定します。\n$ aws ecs create-cluster \\ --cluster-name ${APP_NAME}-cluster \\ --configuration executeCommandConfiguration=\u0026#34;{ \\ logging=OVERRIDE, \\ logConfiguration={ \\ cloudWatchLogGroupName=/dev/${APP_NAME}, \\ s3BucketName=${APP_NAME}-${AWS_ACCOUNT_ID}, \\ s3KeyPrefix=test \\ } \\ }\u0026#34; 5. タスク実行ロール作成 タスク実行ロール作成します。 # AssumeRole Policy Document 作成 $ cat \u0026lt;\u0026lt;EOF \u0026gt; assume-role-policy-doc-for-task-exec-role.json { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF # IAM Role 作成 $ TASK_EXEC_ROLE_ARN=$( \\ aws iam create-role \\ --role-name ${APP_NAME}-task-exec-role \\ --assume-role-policy-document file://assume-role-policy-doc-for-task-exec-role.json \\ --query \u0026#34;Role.Arn\u0026#34; \\ --output text) # アタッチする IAM Policy の ARN 取得 $ POLICY_ARN=$(\\ aws iam list-policies \\ --path-prefix \u0026#34;/service-role/\u0026#34; \\ --max-items 1000 \\ --query \u0026#34;Policies[?PolicyName == \\`AmazonECSTaskExecutionRolePolicy\\`].Arn\u0026#34; \\ --output text) # Role に Policy をアタッチ $ aws iam attach-role-policy \\ --role-name ${APP_NAME}-task-exec-role \\ --policy-arn ${POLICY_ARN} 6. タスクロール作成 タスクロールを作成します。\n今回作成する ECS タスクでは、 ECS Exec を実行/そのログを出力するために CloudWatch Logs、 S3、 SSM への権限が必要になります。\n# AssumeRole Policy Document 作成 $ cat \u0026lt;\u0026lt;EOF \u0026gt; assume-role-policy-doc-for-task-role.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF # IAM Role 作成 $ TASK_ROLE_ARN=$( \\ aws iam create-role \\ --role-name ${APP_NAME}-task-role \\ --assume-role-policy-document file://assume-role-policy-doc-for-task-role.json \\ --query \u0026#34;Role.Arn\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_ROLE_ARN}\u0026#34; # アタッチする IAM Policy を作成 $ cat \u0026lt;\u0026lt;EOF \u0026gt; policy-for-task-role.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetEncryptionConfiguration\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${APP_NAME}-${AWS_ACCOUNT_ID}\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${APP_NAME}-${AWS_ACCOUNT_ID}/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-northeast-1:${AWS_ACCOUNT_ID}:log-group:/dev/${APP_NAME}:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssmmessages:CreateControlChannel\u0026#34;, \u0026#34;ssmmessages:CreateDataChannel\u0026#34;, \u0026#34;ssmmessages:OpenControlChannel\u0026#34;, \u0026#34;ssmmessages:OpenDataChannel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOF # タスクロール用のポリシー作成 $ TASK_ROLE_POLICY_ARN=$( \\ aws iam create-policy \\ --policy-name \u0026#34;${APP_NAME}-task-role-policy\u0026#34; \\ --path \u0026#34;/sample/\u0026#34; \\ --policy-document file://policy-for-task-role.json \\ --query \u0026#34;Policy.Arn\u0026#34; \\ --output text) # タスクロールを作成 $ aws iam attach-role-policy \\ --role-name ${APP_NAME}-task-role \\ --policy-arn ${TASK_ROLE_POLICY_ARN} 7. タスク定義作成 タスク定義を作成します。 $ cat \u0026lt;\u0026lt;EOF \u0026gt; task-definition.json { \u0026#34;family\u0026#34;: \u0026#34;${APP_NAME}\u0026#34;, \u0026#34;networkMode\u0026#34;: \u0026#34;awsvpc\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;${APP_NAME}-app\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;${ECR_REPO_URI}\u0026#34;, \u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;awslogs\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;awslogs-group\u0026#34;: \u0026#34;/dev/${APP_NAME}-task\u0026#34;, \u0026#34;awslogs-region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;awslogs-stream-prefix\u0026#34;: \u0026#34;dev\u0026#34; } } } ], \u0026#34;requiresCompatibilities\u0026#34;: [ \u0026#34;FARGATE\u0026#34; ], \u0026#34;taskRoleArn\u0026#34;: \u0026#34;${TASK_ROLE_ARN}\u0026#34;, \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${TASK_EXEC_ROLE_ARN}\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;512\u0026#34; } EOF # タスク定義を作成 $ TASK_DEF_ARN=$( \\ aws ecs register-task-definition \\ --cli-input-json file://task-definition.json \\ --query \u0026#34;taskDefinition.taskDefinitionArn\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_DEF_ARN}\u0026#34; 8. アプリケーションログ用の ロググループ作成 アプリケーションログ用の ロググループ作成します。 $ aws logs create-log-group \\ --log-group-name /dev/${APP_NAME}-task 9. ExecuteCommand ログ用のロググループ、 S3 バケット作成 ECS Exec で実行したコマンドのログを出力する CloudWatch Logs のロググループ、 S3 バケット作成します。\n$ aws s3 mb \u0026#34;s3://${APP_NAME}-${AWS_ACCOUNT_ID}\u0026#34; $ aws logs create-log-group \\ --log-group-name \u0026#34;/dev/${APP_NAME}\u0026#34; 10. タスク実行 タスクを実行します。このとき --enable-execute-command オプションを付けて ecs run-task を実行します。\n$ TASK_ARN=$(\\ aws ecs run-task \\ --cluster ${APP_NAME}-cluster \\ --task-definition \u0026#34;${TASK_DEF_ARN}\u0026#34; \\ --network-configuration \u0026#34;awsvpcConfiguration={subnets=[${SUBNET_ID}],assignPublicIp=ENABLED}\u0026#34; \\ --launch-type FARGATE \\ --enable-execute-command \\ --query \u0026#34;tasks[0].taskArn\u0026#34; \\ --output text) 実行状況確認 $ aws ecs describe-tasks \\ --tasks ${TASK_ARN} \\ --cluster ${APP_NAME}-cluster \\ --query \u0026#34;tasks[0].lastStatus\u0026#34; \\ --output text 11. レスポンス確認 タスクのステータスが RUNNING になったら、自動的に割り当てられた Public IP に対してリクエストして、レスポンスを確認します。\n割り当てられた パブリック IP を取得 ecs describe-tasks で実行中タスクの詳細を取得します。その中に含まれる networkInterfaceId をもとにして、 ec2 describe-network-interfaces を使って ENI の詳細を取得し、その中に含まれる PublicIp を取得します。\nENDPOINT=$(\\ aws ec2 describe-network-interfaces \\ --network-interface-ids $(\\ aws ecs describe-tasks \\ --tasks ${TASK_ARN} \\ --cluster ${APP_NAME}-cluster \\ --query \u0026#34;tasks[0].attachments[0].details[?name==\u0026#39;networkInterfaceId\u0026#39;].value\u0026#34; \\ --output text) \\ --query \u0026#34;NetworkInterfaces[0].PrivateIpAddresses[0].Association.PublicIp\u0026#34; \\ --output text) レスポンス確認 $ http ${ENDPOINT} HTTP/1.1 200 OK Accept-Ranges: bytes Connection: keep-alive Content-Length: 14 Content-Type: text/html Date: Sun, 21 Mar 2021 06:28:56 GMT ETag: \u0026#34;6056e3af-e\u0026#34; Last-Modified: Sun, 21 Mar 2021 06:02:09 GMT Server: nginx/1.18.0 (Ubuntu) Hello ECS Exec! http コマンドについては下記の記事をご覧ください。\nCLI で http リクエストするなら HTTPie が便利 - michimani.net ECS Exec でコマンド実行 タスクが起動したので、 ECS Exec でコンテナに対してコマンドを実行してみます。\n$ aws ecs execute-command \\ --cluster \u0026#34;${APP_NAME}-cluster\u0026#34; \\ --task \u0026#34;${TASK_ARN}\u0026#34; \\ --command \u0026#34;/bin/bash\u0026#34; \\ --interactive The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. Starting session with SessionId: ecs-execute-command-0e5f16ff1ee160a21 root@68e6de2197e642e7b106a5d205e05641-2282341209:/# cd /var/www/html/ root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# ls index.html root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# echo \u0026#39;Hello AWS !!!\u0026#39; \u0026gt;| index.html root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# cat index.html Hello AWS !!! root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# exit exit Exiting session with sessionId: ecs-execute-command-0e5f16ff1ee160a21. 今回は、 index.html の中身を Hello AWS !!! に書き換えています。\nその結果として、レスポンスが変わってます。\n$ http ${ENDPOINT} HTTP/1.1 200 OK Accept-Ranges: bytes Connection: keep-alive Content-Length: 14 Content-Type: text/html Date: Sun, 21 Mar 2021 06:28:56 GMT ETag: \u0026#34;6056e3af-e\u0026#34; Last-Modified: Sun, 21 Mar 2021 06:13:59 GMT Server: nginx/1.18.0 (Ubuntu) Hello AWS !!! CloudWatch Logs で ECS Exec のログを確認 上記で実行したコマンドのログを CloudWatch Logs で確認してみます。直近のログストリームを取得し、そのログストリーム内のログイベントを取得しています。\n# 最新のログストリーム名を取得 $ LOG_ST_NAME=$(\\ aws logs describe-log-streams \\ --log-group-name /dev/${APP_NAME} \\ --query \u0026#39;max_by(logStreams[], \u0026amp;lastEventTimestamp).logStreamName\u0026#39; \\ --output text) # ログストリームを指定してログイベントを取得 $ aws logs get-log-events \\ --log-group-name /dev/${APP_NAME} \\ --log-stream-name \u0026#34;${LOG_ST_NAME}\u0026#34; \\ --limit 5 \\ --query \u0026#39;events[].[join(``, [ to_string(timestamp) ,`: `,message])]\u0026#39; \\ --output text 1616307128716: Script started on 2021-03-21 06:12:07+00:00 [\u0026lt;not executed on terminal\u0026gt;] root@68e6de2197e642e7b106a5d205e05641-2282341209:/# cd /var/www/html/ root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# ls index.html root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# echo \u0026#39;Hello AWS !!!\u0026#39; \u0026gt;| index.html root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# cat index.html Hello AWS !!! root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# exit exit Script done on 2021-03-21 06:12:07+00:00 [COMMAND_EXIT_CODE=\u0026#34;0\u0026#34;] 実行したコマンド、及び出力された内容がログに記録されています。\nなお、マネジメントコンソールで確認すると下記のような状態で確認できます。\nちなみに S3 バケットに出力されたログは\u0026hellip; S3 バケットに出力されたログは、 CloudWatch Logs に出力されている内容と同じです。\nScript started on 2021-03-21 06:12:07+00:00 [\u0026lt;not executed on terminal\u0026gt;] \u001b]0;root@68e6de2197e642e7b106a5d205e05641-2282341209: /\u0007root@68e6de2197e642e7b106a5d205e05641-2282341209:/# \u001b[K\u001b]0;root@68e6de2197e642e7b106a5d205e05641-2282341209: /\u0007root@68e6de2197e642e7b106a5d205e05641-2282341209:/# cd /var/www/html/ \u001b]0;root@68e6de2197e642e7b106a5d205e05641-2282341209: /var/www/html\u0007root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# ls index.html \u001b]0;root@68e6de2197e642e7b106a5d205e05641-2282341209: /var/www/html\u0007root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# echo \u0026#39;\u0026#39;\u0008H\u0026#39;\u0008e\u0026#39;\u0008l\u0026#39;\u0008l\u0026#39;\u0008o\u0026#39;\u0008 \u0026#39;\u0008A\u0026#39;\u0008W\u0026#39;\u0008S\u0026#39;\u0008 \u0026#39;\u0008!\u0026#39;\u0008!\u0026#39;\u0008!\u0026#39;\u0008\u001b[C \u0026gt;~ \u001b[A\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[K \u001b[K\u001b[A\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C| index.html \u001b]0;root@68e6de2197e642e7b106a5d205e05641-2282341209: /var/www/html\u0007root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# cat index.html Hello AWS !!! \u001b]0;root@68e6de2197e642e7b106a5d205e05641-2282341209: /var/www/html\u0007root@68e6de2197e642e7b106a5d205e05641-2282341209:/var/www/html# exit exit Script done on 2021-03-21 06:12:07+00:00 [COMMAND_EXIT_CODE=\u0026#34;0\u0026#34;] まとめ ECS Exec の Interactive モードで実行したログを CloudWatch Logs および S3 バケットに出力して内容を確認してみた話でした。\n前回の記事では、単発のコマンドは CloudTrail に記録されることは書きましたが、今回は Interactive に実行したコマンドについても記録できることを確認できました。実行したコマンドだけでなく実行時に出力された内容もログに記録されるので、監査目的だけでなくトラブルシュートの作業ログとしても、あとから確認できるのは良さそうです。\nこの流れ (?) で CloudShell での操作もログに記録できるようになるといいのになーと思ってます。\n",
    "permalink": "https://michimani.net/post/aws-logging-ecs-execute-commands-as-interactive/",
    "title": "ECS Exec の Interactive モードで実行したコマンドのログを CloudWatch Logs および S3 に出力する"
  },
  {
    "contents": "Amazon ECS で稼働中のコンテナに対して SSH 接続せずにコマンド実行できる機能 ECS Exec が発表されました。コールされる API は ExecuteCommand です。今回は AWS Copilot と AWS CLI を使って、稼働中のコンテナに対してコマンドを実行してみます。\n目次 前提 ExecuteCommand API AWS Copilot で実行する copilot svc exec AWS CLI で実行する ecs execute-command non-interactive は現時点でサポートされていない CloudTrail で証跡の確認 まとめ 追記 前提 Amazon ECS で稼働しているコンテナが存在することを前提とします。今回は、下記の記事内で構築した ECS タスクに対して実行することにします。\nAWS Copilot で生成されるタスクロールに任意のポリシーをアタッチする - michimani.net ExecuteCommand API 今回発表された API ExecuteCommand は、 ECS で稼働中のコンテナに対して、 SSH 接続せずにコマンドを実行することができる API です。対象のデータプレーンは EC2 および Fargate で、 Fargate のプラットフォームバージョンは 1.4.0 以上である必要があります。また、稼働中のコンテナには バージョン 1.50.2 以上の Container Agent Version がインストールされている必要があります。\n詳しくは AWS の What\u0026rsquo;s New とブログを参照してください。\nAmazon ECS now allows you to execute commands in a container running on Amazon EC2 or AWS Fargate NEW – Using Amazon ECS Exec to access your containers on AWS Fargate and Amazon EC2 | Containers AWS Copilot で実行する まずは AWS Copilot で実行してみます。\nAWS Copilot は、今回の API 発表に合わせて公開された最新バージョン 1.4.0 が必要なのでアップデートしておきます。なお、私の環境 - macOS Big Sur でアップデートしようとしたらエラーになったので、その場合は Command Line Tools を再インストールしたりする必要があります。\nmacOS Big Sur で Copilot をアップデートしようとしたらエラーになった - michimani.net Homebrew でインストールしている場合は下記のコマンドでアップデートします。\n$ brew upgrade copilot-cli $ copilot version version: v1.4.0, built for darwin copilot svc exec AWS Copilot を使ってコマンドを実行する場合、 copilot svc exec コマンドを実行します。\n$ copilot svc exec すると、 Session Manager プラグイン がインストールされていないと言われたので、インストールします。\nLooks like the Session Manager plugin is not installed yet. Would you like to install the plugin to execute into the container? [? for help] (y/N) Session Manager プラグインがインストールされてコンテナに対してコマンドを実行するところで、今度は次のようなエラーが出ました。\nFound only one deployed service list-s3-buckets-app in environment test Execute `/bin/sh` in container list-s3-buckets-app in task e1c5b556450545e5a99895ced3143dc2. ✘ Failed to execute command /bin/sh. Is `exec: true` set in your manifest? ✘ execute command /bin/sh in container list-s3-buckets-app: execute command: InvalidParameterException: The execute command failed because execute command was not enabled when the task was run or the execute command agent isn’t running. Wait and try again or run a new task with execute command enabled and try again. 対象のコンテナでは、環境変数 enableExecuteCommand に true がセットされている必要があります。 AWS Copilot で管理しているサービスの場合、 manifext.yml にて exec: true を記述します。\nこれに関しては下記の Copilot のドキュメントに書かれています。\nLoad Balanced Web Service - exec | AWS Copilot CLI あらためて実行すると、あたかも SSH したかのようにターミナル上でコンテナ内のシェルが起動します。\n$ copilot svc exec Found only one deployed service list-s3-buckets-app in environment test Execute `/bin/sh` in container list-s3-buckets-app in task xxxxxxxxxxxxxxxxxxxx05c661910a95. Starting session with SessionId: ecs-execute-command-08dc04c5e87685c1d /go # あとは普通にコマンド実行するだけです。\n/go # go version go version go1.15.5 linux/amd64 /go # env HOSTNAME=ip-10-0-0-179.ap-northeast-1.compute.internal COPILOT_SERVICE_NAME=list-s3-buckets-app SHLVL=1 HOME=/root AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/v2/credentials/7ce3249f-c697-0000-0000-xxxxxxxxxxxx AWS_EXECUTION_ENV=AWS_ECS_FARGATE AWS_DEFAULT_REGION=ap-northeast-1 COPILOT_ENVIRONMENT_NAME=test ECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/xxxxxxxxxxxxxxxxxxxx05c661910a95-3314152222 TERM=xterm-256color ECS_CONTAINER_METADATA_URI=http://169.254.170.2/v3/xxxxxxxxxxxxxxxxxxxx05c661910a95-3314152222 PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin COPILOT_APPLICATION_NAME=hello-ecs COPILOT_SERVICE_DISCOVERY_ENDPOINT=hello-ecs.local LANG=C.UTF-8 COPILOT_LB_DNS=hello-Publi-SGUJQBQMXO00-1567060405.ap-northeast-1.elb.amazonaws.com GOPATH= AWS_REGION=ap-northeast-1 PWD=/go GOLANG_VERSION=1.15.5 /go # exit Exiting session with sessionId: ecs-execute-command-08dc04c5e87685c1d. AWS CLI で実行する 続いて AWS CLI で、ECS で稼働中のコンテナに対してコマンドを実行してみます。\nAWS CLI では、この記事執筆時点 (2021/03/16) で v1 の 1.19.28 のみ対応しています。\n$ aws --version aws-cli/1.19.28 Python/3.8.5 Darwin/20.3.0 botocore/1.20.28 この時点で v2 の最新は 2.1.30 ですが、まだ対応していませんでした。多分、いつもの流れなら数日後に対応されるでしょう。\necs execute-command AWS CLI では ecs- execute-command コマンドを使います。\n$ aws ecs execute-command help ... SYNOPSIS execute-command [--cluster \u0026lt;value\u0026gt;] [--container \u0026lt;value\u0026gt;] --command \u0026lt;value\u0026gt; --interactive | --non-interactive --task \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] ... 主なパラメータは下記のとおりです。\n--cluster : 対象のコンテナが稼働している ECS クラスター名 --container : 対象のコンテナ名 (タスク内で複数のコンテナが稼働している場合に必要) --command : 実行するコマンド --task : 対象のコンテナが稼働しているタスクの ARN または ID --interactive | --non-interactive : コマンドをどのように実行するか クラスター名とタスク ID が必要になるので、先に取得しておきます。\n$ CLUSTER_NAME=$( \\ aws ecs describe-clusters \\ --clusters \u0026#34;$(\\ aws ecs list-clusters \\ --query \u0026#34;clusterArns[?contains(@, \\`hello-ecs\\`)] | [0]\u0026#34; \\ --output text )\u0026#34; \\ --query \u0026#34;clusters[0].clusterName\u0026#34; \\ --output text) $ TASK_ARN=$(\\ aws ecs list-tasks \\ --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ --query \u0026#34;taskArns[0]\u0026#34; \\ --output text) 実行します。まずは --interactive で実行してみます。\n$ aws ecs execute-command \\ --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ --task \u0026#34;${TASK_ARN}\u0026#34; \\ --command \u0026#34;go version\u0026#34; \\ --interactive The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. Starting session with SessionId: ecs-execute-command-05539dfac9c3bd94d go version go1.15.5 linux/amd64 Exiting session with sessionId: ecs-execute-command-05539dfac9c3bd94d. セッションマネージャーが起動し、コマンドの実行結果が出力され、セッションが終了しました。\n「interactive とは\u0026hellip;」と一瞬思いましたが、実行したコマンドが go version なので対話もクソもなかったです。\nということで、 /bin/sh を実行してみます。\n$ aws ecs execute-command \\ --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ --task \u0026#34;${TASK_ARN}\u0026#34; \\ --command \u0026#34;/bin/sh\u0026#34; \\ --interactive The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. Starting session with SessionId: ecs-execute-command-027f2f4e471434482 /go # Copilot のときと同様に、シェル起動して あたかも SSH したかのように見えます。\nあとは普通にコマンドを実行して、logout または exit で終了します。\n/go # go version go version go1.15.5 linux/amd64 /go # env HOSTNAME=ip-10-0-0-179.ap-northeast-1.compute.internal COPILOT_SERVICE_NAME=list-s3-buckets-app SHLVL=1 HOME=/root AWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/v2/credentials/7ce3249f-c697-0000-0000-xxxxxxxxxxxx AWS_EXECUTION_ENV=AWS_ECS_FARGATE AWS_DEFAULT_REGION=ap-northeast-1 COPILOT_ENVIRONMENT_NAME=test ECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/xxxxxxxxxxxxxxxxxxxx05c661910a95-3314152222 TERM=xterm-256color ECS_CONTAINER_METADATA_URI=http://169.254.170.2/v3/xxxxxxxxxxxxxxxxxxxx05c661910a95-3314152222 PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin COPILOT_APPLICATION_NAME=hello-ecs COPILOT_SERVICE_DISCOVERY_ENDPOINT=hello-ecs.local LANG=C.UTF-8 COPILOT_LB_DNS=hello-Publi-SGUJQBQMXO00-1567060405.ap-northeast-1.elb.amazonaws.com GOPATH= AWS_REGION=ap-northeast-1 PWD=/go GOLANG_VERSION=1.15.5 /go # exit Exiting session with sessionId: ecs-execute-command-027f2f4e471434482. non-interactive は現時点でサポートされていない ちなみに --non-interactive は現時点ではサポートされていないようです。\n$ aws ecs execute-command \\ --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ --task \u0026#34;${TASK_ARN}\u0026#34; \\ --command \u0026#34;go version\u0026#34; \\ --non-interactive The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. An error occurred (InvalidParameterException) when calling the ExecuteCommand operation: Interactive is the only mode supported currently. CloudTrail で証跡の確認 ExecuteCommand API の実行に関しては、 CloudTrail で証跡を確認することができます。\nCopilot で実行した場合 { \u0026#34;eventVersion\u0026#34;: \u0026#34;1.08\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;AssumedRole\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXBBZHM:1615903571022057000\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:sts::000000000000:assumed-role/hello-ecs-test-EnvManagerRole/1615903571022057000\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;000000000000\u0026#34;, \u0026#34;accessKeyId\u0026#34;: \u0026#34;ASIA2FQKQ3TURQ676CHH\u0026#34;, \u0026#34;sessionContext\u0026#34;: { \u0026#34;sessionIssuer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Role\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXBBZHM\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::000000000000:role/hello-ecs-test-EnvManagerRole\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;000000000000\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;hello-ecs-test-EnvManagerRole\u0026#34; }, \u0026#34;webIdFederationData\u0026#34;: {}, \u0026#34;attributes\u0026#34;: { \u0026#34;mfaAuthenticated\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;2021-03-16T14:06:11Z\u0026#34; } } }, \u0026#34;eventTime\u0026#34;: \u0026#34;2021-03-16T14:06:12Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;ecs.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;ExecuteCommand\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;153.225.234.16\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;aws-ecs-cli-v2/v1.4.0 (darwin) aws-sdk-go/1.37.31 (go1.15.6; darwin; amd64)\u0026#34;, \u0026#34;requestParameters\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;hello-ecs-test-Cluster-XXXXXXXXXXdZ\u0026#34;, \u0026#34;container\u0026#34;: \u0026#34;list-s3-buckets-app\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;/bin/sh\u0026#34;, \u0026#34;interactive\u0026#34;: true, \u0026#34;task\u0026#34;: \u0026#34;xxxxxxxxxxxxxxxxxxxx05c661910a95\u0026#34; }, \u0026#34;responseElements\u0026#34;: { \u0026#34;clusterArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:cluster/hello-ecs-test-Cluster-XXXXXXXXXXdZ\u0026#34;, \u0026#34;containerArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:container/69a7d1af-0000-0000-0000-xxxxxxxxxxxx\u0026#34;, \u0026#34;containerName\u0026#34;: \u0026#34;list-s3-buckets-app\u0026#34;, \u0026#34;interactive\u0026#34;: true, \u0026#34;session\u0026#34;: { \u0026#34;sessionId\u0026#34;: \u0026#34;ecs-execute-command-08dc04c5e87685c1d\u0026#34;, \u0026#34;streamUrl\u0026#34;: \u0026#34;wss://ssmmessages.ap-northeast-1.amazonaws.com/v1/data-channel/ecs-execute-command-08dc04c5e87685c1d?role=publish_subscribe\u0026#34;, \u0026#34;tokenValue\u0026#34;: \u0026#34;HIDDEN_DUE_TO_SECURITY_REASONS\u0026#34; }, \u0026#34;taskArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task/hello-ecs-test-Cluster-XXXXXXXXXXdZ/xxxxxxxxxxxxxxxxxxxx05c661910a95\u0026#34; }, \u0026#34;requestID\u0026#34;: \u0026#34;6b08cb73-b9f3-4394-bda9-5c9d1691bf9c\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;9ce15175-3700-48c6-8e50-081950d14f65\u0026#34;, \u0026#34;readOnly\u0026#34;: false, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, \u0026#34;managementEvent\u0026#34;: true, \u0026#34;eventCategory\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;000000000000\u0026#34; } AWS CLI で実行した場合 { \u0026#34;eventVersion\u0026#34;: \u0026#34;1.08\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;IAMUser\u0026#34;, \u0026#34;principalId\u0026#34;: \u0026#34;AIDAIXKQKK3IJ7GXIEEGE\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:iam::000000000000:user/HOGE\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;000000000000\u0026#34;, \u0026#34;accessKeyId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXL2X\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;HOGE\u0026#34; }, \u0026#34;eventTime\u0026#34;: \u0026#34;2021-03-16T14:55:36Z\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;ecs.amazonaws.com\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;ExecuteCommand\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;153.225.234.16\u0026#34;, \u0026#34;userAgent\u0026#34;: \u0026#34;aws-cli/1.19.28 Python/3.8.5 Darwin/20.3.0 botocore/1.20.28\u0026#34;, \u0026#34;requestParameters\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;hello-ecs-test-Cluster-XXXXXXXXXXdZ\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;/bin/sh\u0026#34;, \u0026#34;interactive\u0026#34;: true, \u0026#34;task\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task/hello-ecs-test-Cluster-XXXXXXXXXXdZ/xxxxxxxxxxxxxxxxxxxx05c661910a95\u0026#34; }, \u0026#34;responseElements\u0026#34;: { \u0026#34;clusterArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:cluster/hello-ecs-test-Cluster-XXXXXXXXXXdZ\u0026#34;, \u0026#34;containerArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:container/69a7d1af-0000-0000-0000-xxxxxxxxxxxx\u0026#34;, \u0026#34;containerName\u0026#34;: \u0026#34;list-s3-buckets-app\u0026#34;, \u0026#34;interactive\u0026#34;: true, \u0026#34;session\u0026#34;: { \u0026#34;sessionId\u0026#34;: \u0026#34;ecs-execute-command-027f2f4e471434482\u0026#34;, \u0026#34;streamUrl\u0026#34;: \u0026#34;wss://ssmmessages.ap-northeast-1.amazonaws.com/v1/data-channel/ecs-execute-command-027f2f4e471434482?role=publish_subscribe\u0026#34;, \u0026#34;tokenValue\u0026#34;: \u0026#34;HIDDEN_DUE_TO_SECURITY_REASONS\u0026#34; }, \u0026#34;taskArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task/hello-ecs-test-Cluster-XXXXXXXXXXdZ/xxxxxxxxxxxxxxxxxxxx05c661910a95\u0026#34; }, \u0026#34;requestID\u0026#34;: \u0026#34;87faac46-6ae1-4887-8218-599e0db3ba85\u0026#34;, \u0026#34;eventID\u0026#34;: \u0026#34;ae940740-7a41-4355-a0a0-714d608ece7c\u0026#34;, \u0026#34;readOnly\u0026#34;: false, \u0026#34;eventType\u0026#34;: \u0026#34;AwsApiCall\u0026#34;, \u0026#34;managementEvent\u0026#34;: true, \u0026#34;eventCategory\u0026#34;: \u0026#34;Management\u0026#34;, \u0026#34;recipientAccountId\u0026#34;: \u0026#34;000000000000\u0026#34; } 実行したコマンドも証跡には含まれていますが、これはあくまでも ExecuteCommand API コール時に指定したコマンドです。今回試したように /bin/sh を実行して interactive モードになったあとに実行したコマンドについては CloudTrail で確認することができません。ここは監査的に悩ましい点かと思います。\nまとめ ECS で稼働中のコンテナに対して SSH せずにコマンド実行できる API が公開されたので AWS Copilot と AWS CLI それぞれを使ってコマンド実行してみた話でした。\nコマンドを実行してその結果が出力されるだけなのかなと想像してましたが、あたかもコンテナに SSH したかのようにコマンドを実行できたので、感動しました。\nこれで稼働中コンテナの調査はやりやすくなった反面、簡単にコマンド実行できるようになったので、必要なときだけ IAM Role で許可するなどして制限しておいたほうが良いのかなと思いました。というのも、 ExecuteCommand API コール時のコマンドは CloudTrail で確認できますが、シェルを起動して対話的に実行したコマンドは追えないからです。\nとは言え、この操作を実現するために SSH 用のインスタンスを保持していた場合は、それらが不要になりますし、諸々の調査はかなり捗るようになりそうです。\n追記 あとでドキュメントを確認したところ、クラスター作成時に ExecuteCommand API で対話的に実行したコマンドに関する設定をすることで、 CloudWatch Logs または S3 にそのログが出せそうでした。\nUsing Amazon ECS Exec for debugging - Amazon Elastic Container Service これは次回試してみます。 書きました。\nECS Exec の Interactive モードで実行したコマンドのログを CloudWatch Logs および S3 に出力する - michimani.net ",
    "permalink": "https://michimani.net/post/aws-about-exuecution-command-in-container-running-on-ecs-fargate/",
    "title": "AWS Copilot と AWS CLI を使って ECS Exec を試す"
  },
  {
    "contents": "macOS Big Sur で AWS Copilot をアップデートしようとしたらエラーになったので、その時の対応をメモしておきます。結論としては CLT の再インストールでした。\n起こったこと AWS Copilot 1.1.0 を、最新の 1.4.0 にアップデートしようとしたら、エラーになりました。\nCopilot 1.1.0 は macOS Catalina (10.x) のときにインストールしており、アップデートしようとした時点で macOS は Big Sur (11.2.3) にアップデートしていました。\n$ brew upgrade copilot-cli Updating Homebrew... ... ... ==\u0026gt; Upgrading 1 outdated package: aws/tap/copilot-cli 1.1.0 -\u0026gt; 1.4.0 ==\u0026gt; Upgrading aws/tap/copilot-cli 1.1.0 -\u0026gt; 1.4.0 ==\u0026gt; Downloading https://github.com/aws/copilot-cli/releases/download/v1.4.0/copilot_1.4.0_macOS_amd64.tar.gz ==\u0026gt; Downloading from https://github-releases.githubusercontent.com/202018792/01197900-8595-11eb-81da-2085a3edc283?X-Amz-Algorithm=AWS4- ######################################################################## 100.0% Error: Your Command Line Tools (CLT) does not support macOS 11. It is either outdated or was modified. Please update your Command Line Tools (CLT) or delete it if no updates are available. Update them from Software Update in System Preferences or run: softwareupdate --all --install --force If that doesn\u0026#39;t show you any updates, run: sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install Alternatively, manually download them from: https://developer.apple.com/download/more/. Error: An exception occurred within a child process: SystemExit: exit 対応 エラーメッセージに記載されているとおりに対応していきます。\nソフトウェアアップデートの確認 まずはソフトウェアアップデートの確認です。\n$ softwareupdate --all --install --force Software Update Tool Finding available software No updates are available. アップデートはありませんでした。\nCLT (Command Line Tools) を再インストール ソフトウェアアップデートがなかったので、 CLT を再インストールします。\nインストール済みの CLT を削除。\n$ sudo rm -rf /Library/Developer/CommandLineTools Password: # パスワード入力 CLT を再インストール。\n$ sudo xcode-select --install xcode-select: note: install requested for command line developer tools ダイアログが出るので、利用規約に同意してインストールします。\ncopilot-cli をアップデート 再度 brew upgrade を実行して copilot-cli をアップデートします。\n$ brew upgrade copilot-cli ==\u0026gt; Upgrading 1 outdated package: aws/tap/copilot-cli 1.1.0 -\u0026gt; 1.4.0 ==\u0026gt; Upgrading aws/tap/copilot-cli 1.1.0 -\u0026gt; 1.4.0 ==\u0026gt; Downloading https://github.com/aws/copilot-cli/releases/download/v1.4.0/copilot_1.4.0_macOS_amd64.tar.gz Already downloaded: /Users/yito/Library/Caches/Homebrew/downloads/dd28217387b8d74caa7947d0dfedf18c129289ae2ecdc1a4f485d825765911c3--copilot_1.4.0_macOS_amd64.tar.gz ==\u0026gt; Caveats zsh completions have been installed to: /usr/local/share/zsh/site-functions ==\u0026gt; Summary 🍺 /usr/local/Cellar/copilot-cli/1.4.0: 5 files, 44.7MB, built in 7 seconds Removing: /usr/local/Cellar/copilot-cli/1.1.0... (5 files, 42.4MB) バージョン確認。\n$ copilot version version: v1.4.0, built for darwin ",
    "permalink": "https://michimani.net/post/aws-error-occuerd-in-upgrading-copilot-at-macos-bigsur/",
    "title": "macOS Big Sur で Copilot をアップデートしようとしたらエラーになった"
  },
  {
    "contents": "Go の練習のために AWS SDK for Go V2 を使って ACM を簡単に扱えるパッケージ的なものを作っています。その際、ユニットテストをするために SDK をモックする必要があったので、その方法について書きます。\n目次 はじめに goacm AWS SDK for Go V2 のモック化とユニットテスト ユニットテストを見据えた実装 サービスの API のモック化 モックを利用したテスト ひとこと はじめに AWS SDK for Go V2 では、 V1 のときにあった ***iface 系のパッケージが含まれていないので、 SDK 部分をモックしたい場合は自分で API のインターフェイスを作成する必要があります。この記事では、公式のドキュメントをもとに SDK のモックを作ってユニットテストを実行するところまでの方法について書いています。\nAWS SDK for Go のモック化について悩んでいる方の参考になれば良いのですが、逆に書き方おかしいところ等あればコメントいただけると嬉しいです。\ngoacm 今回題材にするのは goacm というパッケージで、 ACM (AWS Certificate Manager) での操作をラップしたものになってます。プログラムから ACM を操作することってあんまりないと思うんですが、 ACM でできることって限られているので迷子にならないかなと思って選びました。あと、個人的に ACM 好きなので。\nmichimani/goacm: goacm is a simple package for using AWS Certificate Manager from applications implimented golang. SDK のモック化とテストについてはこのあと書いていきますが、多分ソースコードを見たほうがわかりやすいので↑の GitHub のほうを見てください。\nAWS SDK for Go V2 のモック化とユニットテスト AWS SDK for Go V1 には ***iface というパッケージ (ACM であれば acmiface) が含まれていて、それをもとに mockgen でモックを簡単に生成することができていました。 V2 でも GA 前のバージョン (ACM であれば v0.24.0 ) までは存在していましたが、それ以降削除されています。\nなので、モック化したい API のインターフェイスを作る必要があります。また、内部で SDK を使う関数では、その引数としてサービスのクライアントを受け取るようにすることでテストがしやすくなります。\nなお、参考にするのは下記の AWS SDK for Go V2 のドキュメントです。\nUnit Testing with the AWS SDK for Go V2 | AWS SDK for Go V2 ユニットテストを見据えた実装 まず、ユニットテストをしやすいような実装についてです。独自で関数を作成する際に、引数にサービスのクライアントを受け取るようにします。例えば、 ARN を指定して Certificate を取得する GetCertificate という関数を作る場合、次のようにします。\n// GetCertificate returns the details of the certificate. func GetCertificate(ctx context.Context, api ACMDescribeCertificateAPI, arn string) (Certificate, error) { in := acm.DescribeCertificateInput{ CertificateArn: aws.String(arn), } out, err := api.DescribeCertificate(ctx, \u0026amp;in) if err != nil { return Certificate{}, err } vMethod := \u0026#34;\u0026#34; recordSet := RecordSet{} if out.Certificate.DomainValidationOptions != nil { vMethod = string(out.Certificate.DomainValidationOptions[0].ValidationMethod) if vMethod == string(types.ValidationMethodDns) { recordSet.HostedDomainName = aws.ToString(out.Certificate.DomainValidationOptions[0].ValidationDomain) recordSet.Name = aws.ToString(out.Certificate.DomainValidationOptions[0].ResourceRecord.Name) recordSet.Value = aws.ToString(out.Certificate.DomainValidationOptions[0].ResourceRecord.Value) recordSet.Type = string(out.Certificate.DomainValidationOptions[0].ResourceRecord.Type) } } return Certificate{ Arn: arn, DomainName: aws.ToString(out.Certificate.DomainName), Status: string(out.Certificate.Status), Type: string(out.Certificate.Type), FailureReason: string(out.Certificate.FailureReason), ValidationMethod: vMethod, ValidationRecordSet: recordSet, }, nil } 引数として Certificate の ARN だけを受け取るのではなく、 ACM のクライアント (API) (と Context) を受け取るようにしています。\n諸々の構造体は下記のように定義しています。\n// ACMDescribeCertificateAPI is an interface that defines the set of ACM API operations required by the DescribeCertificate function. type ACMDescribeCertificateAPI interface { DescribeCertificate(ctx context.Context, params *acm.DescribeCertificateInput, optFns ...func(*acm.Options)) (*acm.DescribeCertificateOutput, error) } // RecordSet is a structure that reopresents a record set for Route 53. type RecordSet struct { HostedDomainName string Name string Value string Type string TTL int64 } // Certificate is a structure that represents a Certificate. type Certificate struct { Arn string Region string DomainName string Type string Status string FailureReason string ValidationMethod string ValidationRecordSet RecordSet } この関数の場合、内部で ACM の DescribeCertificate() を呼んでいるので、その部分をモック化する必要があります。また、モック化したあとにテストしやすいように、 GetCertificate() では、メソッド DescribeCertificate() を持つインターフェイス ACMDescribeCertificateAPI を受け取るようにしています。\n次の項で DescribeCertificate() を持つ ACMDescribeCertificateAPI をモック化していきます。\nサービスの API のモック化 まず、モック用の構造体、関数、及びメソッドを次のように定義します。\n// MockACMAPI is a struct that represents an ACM client. type MockACMAPI struct { DescribeCertificateAPI MockACMDescribeCertificateAPI } // MockACMDescribeCertificateAPI is a type that represents a function that mock ACM\u0026#39;s DescribeCertificate. type MockACMDescribeCertificateAPI func(ctx context.Context, params *acm.DescribeCertificateInput, optFns ...func(*acm.Options)) (*acm.DescribeCertificateOutput, error) // DescribeCertificate returns a function that mock original of ACM DescribeCertificate. func (m MockACMAPI) DescribeCertificate(ctx context.Context, params *acm.DescribeCertificateInput, optFns ...func(*acm.Options)) (*acm.DescribeCertificateOutput, error) { return m.DescribeCertificateAPI(ctx, params, optFns...) } そして、モックの中身を生成する関数を次のように定義します。\n// MockACMParams is a structure with the elements needed to generate a mock. type MockACMParams struct { Certificate Certificate } // GenerateMockACMAPI return MockACMAPI. func GenerateMockACMAPI(mockParams []MockACMParams) MockACMAPI { return MockACMAPI{ DescribeCertificateAPI: GenerateMockACMDescribeCertificateAPI(mockParams), } } // GenerateMockACMDescribeCertificateAPI returns MockACMDescribeCertificateAPI. func GenerateMockACMDescribeCertificateAPI(mockParams []MockACMParams) MockACMDescribeCertificateAPI { return MockACMDescribeCertificateAPI(func(ctx context.Context, params *acm.DescribeCertificateInput, optFns ...func(*acm.Options)) (*acm.DescribeCertificateOutput, error) { if params.CertificateArn == nil { return nil, errors.New(\u0026#34;expect Certificate ARN to not be nil\u0026#34;) } var availableCertificates map[string]*acm.DescribeCertificateOutput = map[string]*acm.DescribeCertificateOutput{} for _, mp := range mockParams { if mp.Certificate.Arn == \u0026#34;\u0026#34; { continue } dv := types.DomainValidation{ ValidationMethod: types.ValidationMethod(mp.Certificate.ValidationMethod), } if mp.Certificate.ValidationMethod == string(types.ValidationMethodDns) { dv.DomainName = aws.String(mp.Certificate.DomainName) dv.ValidationDomain = aws.String(mp.Certificate.ValidationRecordSet.HostedDomainName) dv.ResourceRecord = \u0026amp;types.ResourceRecord{ Name: aws.String(mp.Certificate.ValidationRecordSet.Name), Value: aws.String(mp.Certificate.ValidationRecordSet.Value), Type: types.RecordType(mp.Certificate.ValidationRecordSet.Type), } } availableCertificates[mp.Certificate.Arn] = \u0026amp;acm.DescribeCertificateOutput{ Certificate: \u0026amp;types.CertificateDetail{ CertificateArn: aws.String(mp.Certificate.Arn), DomainName: aws.String(mp.Certificate.DomainName), Status: types.CertificateStatus(mp.Certificate.Status), Type: types.CertificateType(mp.Certificate.Type), FailureReason: types.FailureReason(mp.Certificate.FailureReason), DomainValidationOptions: []types.DomainValidation{dv}, }, } } dco := availableCertificates[*params.CertificateArn] if dco == nil { return nil, fmt.Errorf(\u0026#34;certificate arn not found arn: %s\u0026#34;, *params.CertificateArn) } return dco, nil }) } この GenerateMockACMDescribeCertificateAPI() が何をしているかというと、モック生成用のパラメーター MockParams のスライスを受け取って、その分の Certificate を availableCertificates map[string]*acm.DescribeCertificateOutput として保持するような DescribeCertificate のモック MockACMDescribeCertificateAPI を返しています。 (伝わってほしい)\nこのモックに対して acm.DescribeCertificateInput で ARN を指定すると、 availableCertificates のキーに存在する ARN であればそれに紐づく acm.DescribeCertificateOutput を返してくれます。\nGenerateMockACMAPI() では、 ACM の他の API (ListCertificates とか DeleteCertificate) のモックを追加した際に、それらをすべて持つような ACM クライアントのモックを作るためです。ここか goacm のコード 見てもらうとわかりやすいかと思います。\nモックを利用したテスト モックができたので、それを使ったテストを下記のように実装します。\nfunc TestGetCertificate(t *testing.T) { ap := []MockACMParams{ { Certificate: Certificate{ Arn: \u0026#34;arn:aws:acm:ap-northeast-1:000000000000:certificate/this-is-a-sample-arn\u0026#34;, DomainName: \u0026#34;test.example.com\u0026#34;, Status: string(types.CertificateStatusIssued), Type: string(types.CertificateTypeAmazonIssued), }, }, } cases := []struct { name string acmClient func(t *testing.T) MockACMAPI arn string wantErr bool expect Certificate }{ { name: \u0026#34;normal\u0026#34;, acmClient: func(t *testing.T) MockACMAPI { return GenerateMockACMAPI(ap) }, arn: \u0026#34;arn:aws:acm:ap-northeast-1:000000000000:certificate/this-is-a-sample-arn\u0026#34;, wantErr: false, expect: Certificate{ Arn: \u0026#34;arn:aws:acm:ap-northeast-1:000000000000:certificate/this-is-a-sample-arn\u0026#34;, DomainName: \u0026#34;test.example.com\u0026#34;, Status: string(types.CertificateStatusIssued), Type: string(types.CertificateTypeAmazonIssued), FailureReason: \u0026#34;\u0026#34;, }, }, { name: \u0026#34;notFound\u0026#34;, acmClient: func(t *testing.T) MockACMAPI { return GenerateMockACMAPI(ap) }, arn: \u0026#34;arn:aws:acm:ap-northeast-1:000000000000:certificate/not-found-arn\u0026#34;, wantErr: true, expect: Certificate{}, }, } for _, tt := range cases { t.Run(tt.name, func(t *testing.T) { ctx := context.TODO() c, err := GetCertificate(ctx, tt.acmClient(t), tt.arn) if tt.wantErr { assert.Error(t, err) return } assert.NoError(t, err) assert.Equal(t, tt.expect, c) }) } } テストケースとしては、 指定した ARN で Certificate が取得できること と、 指定した ARN に該当する Certificate が存在しない場合にエラーになること です。\nモック用の API を生成するためのパラメーターとして下記を定義して、先ほど実装した GenerateMockACMDescribeCertificateAPI() に渡しています。\nap := []MockACMParams{ { Certificate: Certificate{ Arn: \u0026#34;arn:aws:acm:ap-northeast-1:000000000000:certificate/this-is-a-sample-arn\u0026#34;, DomainName: \u0026#34;test.example.com\u0026#34;, Status: string(types.CertificateStatusIssued), Type: string(types.CertificateTypeAmazonIssued), }, }, } 生成したモック API を使って自作の GetCertificate() を実行して、各テストケースを実行しています。\nひとこと Go の練習のために AWS SDK for Go V2 を使って作ったパッケージのユニットテストをするために SDK をモックする必要があったので、その方法について書きました。\nSDK for Go V2 のドキュメントを参考にしたものの、こういう作り方で良いのか？みたいなのは自信がないので、とりあえずやったことを書いてみました。この記事がどれだけの人の目にふれるかわかりませんが、 AWS SDK for Go V2 のモック化、ユニットテストについて何か反応があると嬉しいなーと思ってます。\n",
    "permalink": "https://michimani.net/post/aws-mock-for-aws-sdk-go-v2/",
    "title": "AWS SDK for Go V2 をモックしてユニットテストをする"
  },
  {
    "contents": "ECS と Fargate を使ってバッチ処理とか実行するのは楽だなという気持ちになっているのですが、気になるのはタスクを作成してから実際にアプリケーションが実行されるまでの時間です。今回はその時間を計測してみます。最近リリースされた Go 1.16 と AWS SDK for Go V2 を使って計測用のコードを書きました。\n目次 はじめに タスクで実行するコンテナイメージ 起動時間を計測するコード go:embed ECS タスクの実行 TasksStoppedWaiter でタスクが停止するまで待機 実行してみる まとめ はじめに Fargate for ECS でタスクを実行し、実際に Fargate でコンテナが起動するまでの時間を計測します。\nタスクの実行時には、タスクの情報として CreatedAt が記録され、コンテナ起動時には StartedAt が記録されます。なので、この差分を計測するのが今回の目的です。\nAWS CLI を使う場合、タスクの実行には ecs run-task コマンドを、タスクの情報を取得するには ecs-describe-tasks コマンドをそれぞれ実行します。ただ、コンテナ起動までの時間をコマンドで計測するのはちょっとめんどくさそうなので、今回はタスクの実行から時間の計測までを行うスクリプトを AWS SDK for Go V2 を使って実装してみます。\nまた、最近リリースされた Go 1.16 の機能の中から go:embed も使ってみます。\nなお、今回の計測にあたっては下記の記事を参考にさせていただきました🙇🏻‍♂️\nAWS Fargate のイメージサイズ毎の起動時間の違い - Qiita タスクで実行するコンテナイメージ 今回は起動すればいいだけなので、下記のような Dockerfile でイメージを作成します。\nFROM busybox:latest ARG PAD=0 RUN dd if=/dev/urandom of=/padding bs=1M count=$PAD ビルド時に PAD に値を指定して、任意のサイズのイメージを作成します。今回は 256MB のイメージを作成します。\n$ docker build -t fargate-speed-test . --build-arg PAD=256 # イメージサイズの確認 (\u0026#34;だいたい\u0026#34; 256 MB) $ docker images fargate-speed-test REPOSITORY TAG IMAGE ID CREATED SIZE fargate-speed-test latest fc479f1d67e2 4 hours ago 270MB これを ECR に push しておきます。このあたりの詳しいコマンド操作については前回の記事を参照してください。\nECS タスクを作って実行して CloudWatch でログを確認するまでを AWS CLI だけでやってみた - michimani.net 起動時間を計測するコード タスクを実行してコンテナ起動までの時間を計測するコードを書きます。\npackage main import ( \u0026#34;context\u0026#34; _ \u0026#34;embed\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/ecs\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/ecs/types\u0026#34; ) type AppConfig struct { Cluster string `json:\u0026#34;cluster\u0026#34;` TaskDefinitionArn string `json:\u0026#34;taskDefinitionArn\u0026#34;` SubnetID string `json:\u0026#34;subnetId\u0026#34;` Region string `json:\u0026#34;region\u0026#34;` } //go:embed config.json var configJson []byte // runECSTask runs ECS task func runECSTask() error { var appConfig AppConfig if jerr := json.Unmarshal(configJson, \u0026amp;appConfig); jerr != nil { return jerr } cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(appConfig.Region)) if err != nil { return err } client := ecs.NewFromConfig(cfg) runTaskIn := \u0026amp;ecs.RunTaskInput{ TaskDefinition: aws.String(appConfig.TaskDefinitionArn), Cluster: aws.String(appConfig.Cluster), NetworkConfiguration: \u0026amp;types.NetworkConfiguration{ AwsvpcConfiguration: \u0026amp;types.AwsVpcConfiguration{ Subnets: []string{ appConfig.SubnetID, }, AssignPublicIp: types.AssignPublicIpEnabled, }, }, LaunchType: types.LaunchTypeFargate, } runOut, rerr := client.RunTask(context.TODO(), runTaskIn) if rerr != nil { return rerr } taskArn := aws.ToString(runOut.Tasks[0].TaskArn) fmt.Println(\u0026#34;Task Created.\u0026#34;) fmt.Println(\u0026#34;\\tTaskARN: \u0026#34;, taskArn) fmt.Printf(\u0026#34;\\taws ecs describe-tasks --cluster %s --tasks %s\\n\\n\u0026#34;, appConfig.Cluster, taskArn) waiter := ecs.NewTasksStoppedWaiter(client) waitParams := \u0026amp;ecs.DescribeTasksInput{ Tasks: []string{taskArn}, Cluster: aws.String(appConfig.Cluster), } maxWaitTime := 5 * time.Minute if werr := waiter.Wait(context.TODO(), waitParams, maxWaitTime); werr != nil { return werr } describeIn := \u0026amp;ecs.DescribeTasksInput{ Tasks: []string{taskArn}, Cluster: aws.String(appConfig.Cluster), } stopOut, serr := client.DescribeTasks(context.TODO(), describeIn) if serr != nil { return serr } createdAt := *stopOut.Tasks[0].CreatedAt startedAt := *stopOut.Tasks[0].StartedAt stoppedAt := *stopOut.Tasks[0].StoppedAt takenTime := startedAt.Sub(createdAt) fmt.Println(\u0026#34;Task Stopped.\u0026#34;) fmt.Println(\u0026#34;\\tTaskARN: \u0026#34;, aws.ToString(stopOut.Tasks[0].TaskArn)) fmt.Println(\u0026#34;\\tCreatedAt: \u0026#34;, createdAt) fmt.Println(\u0026#34;\\tStartedAt: \u0026#34;, startedAt) fmt.Println(\u0026#34;\\tStoppedAt: \u0026#34;, stoppedAt) fmt.Printf(\u0026#34;\\tTakenTimeToStart: %s\u0026#34;, takenTime) return nil } func main() { if err := runECSTask(); err != nil { fmt.Println(err.Error()) } } gist - This is a script using the AWS SDK for Go V2 and Go 1.16. Measure the time it takes to launch AWS Fargate for ECS. 中身を簡単に解説しておきます。\ngo:embed import ( _ \u0026#34;embed\u0026#34; ) //go:embed config.json var configJson []byte この部分で、 Go 1.16 で実装された go:embed を使っています。\n何をしているかというと、上の記述ではこの main.go と同じディレクトリにある config.json を byte のスライスとして configJson に代入しています。そしてその configJson を AppConfig の構造体に変換しています。\ngo:embed を使って埋め込まれるファイルはバイナリにも含まれるため、ウェブアプリケーションであれば画像や CSS などを go:embed で埋め込むことで諸々を一つのバイナリに含めることができます。 go:embed および Go 1.16 のその他のリリース内容については下記の記事で詳しく解説されています。\nGo 1.16連載が始まります | フューチャー技術ブログ config.json では、 ECS タスクの実行に最低限必要な情報を保持しています。\n{ \u0026#34;cluster\u0026#34;: \u0026#34;fargate-speed-test-cluster\u0026#34;, \u0026#34;taskDefinitionArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task-definition/fargate-speed-test:5\u0026#34;, \u0026#34;subnetId\u0026#34;: \u0026#34;subnet-06ee4b00000000000\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34; } ECS タスクの実行 下記の部分で ECS タスクを実行しています。\ncfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(appConfig.Region)) if err != nil { return err } client := ecs.NewFromConfig(cfg) runTaskIn := \u0026amp;ecs.RunTaskInput{ TaskDefinition: aws.String(appConfig.TaskDefinitionArn), Cluster: aws.String(appConfig.Cluster), NetworkConfiguration: \u0026amp;types.NetworkConfiguration{ AwsvpcConfiguration: \u0026amp;types.AwsVpcConfiguration{ Subnets: []string{ appConfig.SubnetID, }, AssignPublicIp: types.AssignPublicIpEnabled, }, }, LaunchType: types.LaunchTypeFargate, } runOut, rerr := client.RunTask(context.TODO(), runTaskIn) if rerr != nil { return rerr } ecs - Client.RunTask · pkg.go.dev TasksStoppedWaiter でタスクが停止するまで待機 下記の部分で、実行したタスクが停止するまで待機しています。\nwaiter := ecs.NewTasksStoppedWaiter(client) waitParams := \u0026amp;ecs.DescribeTasksInput{ Tasks: []string{taskArn}, Cluster: aws.String(appConfig.Cluster), } maxWaitTime := 5 * time.Minute if werr := waiter.Wait(context.TODO(), waitParams, maxWaitTime); werr != nil { return werr } ecs - NewTasksStoppedWaiter · pkg.go.dev これにより、タスクが終了した状態のタスクの状態を取得でき、タスク実行からコンテナが起動するまでの時間を計測することができます。\n実行してみる $ go run main.go Task Created. TaskARN: arn:aws:ecs:ap-northeast-1:000000000000:task/fargate-speed-test-cluster/fc2a03112a0000000000xxxxxxxxxxxx aws ecs describe-tasks --cluster fargate-speed-test-cluster --tasks arn:aws:ecs:ap-northeast-1:000000000000:task/fargate-speed-test-cluster/fc2a03112a0000000000xxxxxxxxxxxx Task Stopped. TaskARN: arn:aws:ecs:ap-northeast-1:000000000000:task/fargate-speed-test-cluster/fc2a03112a0000000000xxxxxxxxxxxx CreatedAt: 2021-02-19 15:33:20.308 +0000 UTC StartedAt: 2021-02-19 15:33:58.187 +0000 UTC StoppedAt: 2021-02-19 15:34:41.937 +0000 UTC TakenTimeToStart: 37.879s 37.879s かかったことがわかりました。\nまとめ AWS SDK for Go V2 を使って Farget for ECS でタスクを実行して、実際にコンテナが起動するまでの時間を計測してみた話でした。ついでに、 Go 1.16 で盛り込まれた go:embed も使ってみました。\n今後やりたいこととしては、 ECS_IMAGE_PULL_BEHAVIOR を有効にしたときに起動までの時間がどれくらい短くなるのかを計測したいと思っています。\nECS (EC2) だと、コンテナイメージをキャッシュするための設定として ECS_IMAGE_PULL_BEHAVIOR が使えるのですが、この記事を書いている時点では Fargate に対応していません。\nAmazon ECS on EC2でキャッシュされたコンテナイメージを使用するには? | DevelopersIO なので、次はこの設定が使えるようになったら改めて計測して、どれだけ早くなるのか試してみたいと思います。\n",
    "permalink": "https://michimani.net/post/aws-how-long-does-fargate-take-to-start/",
    "title": "AWS SDK for Go V2 を使って Fargate for ECS でタスクを実行してコンテナが起動するまでの時間を計測してみた"
  },
  {
    "contents": "前に AWS Copilot を使って ECS タスクを実行する環境 (VPC とか サブネットとか) をサクッと作ったのですが、今回はなんとなく AWS CLI で全部作ってみました。気軽に始めた割には結構なボリュームになったので、備忘録として残しておきます。\nAWS Copilot を使ってスケジュールされた ECS タスクをデプロイする - michimani.net 目次 はじめに 作成するリソース 作っていく ECR VPC ECS CloudWatch 実行する 確認する ECS タスクの実行状況 CloudWatch Logs のイベント まとめ はじめに 今回は、 AWS アカウント内の S3 バケットの数を出力する ECS タスク を作成して実行します。\n実行するまでに必要な諸々のリソースを作成して、実際に実行し、CloudWatch Logs に出力されたログを確認するところまでを AWS CLI でやってみます。必要になる諸々の値は変数に設定しながら進めているので、すべて同一のセッションで続けて実行することを想定しています。\nAWS CLI のバージョンは、実行時点で最新だった 2.1.26 を使います。\n$ aws --version aws-cli/2.1.26 Python/3.7.4 Darwin/19.6.0 exe/x86_64 prompt/off 作成するリソース 作成するリソースは下記の通りです。\nECR リポジトリ VPC サブネット インターネットゲートウェイ ルートテーブル ECS クラスター タスク定義 IAM ロール CloudWatch ロググループ 作っていく では、前項で挙げたリソースを順に作っていきます。今回作成するタスク (アプリケーション) の名前は esc-task-test とします。\n$ APP_NAME=\u0026#34;esc-task-test\u0026#34; また、 AWS アカウント ID も必要になるので、先に変数に設定しておきます。\n$ AWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${AWS_ACCOUNT_ID}\u0026#34; また、コマンド内で漏れているかもしれませんが、グローバルなリソースを除いてすべて東京 ap-northeast-1 リージョンに作成します。\nECR まずは ECR のリポジトリです。\n$ ECR_REPO_URI=$( \\ aws ecr create-repository \\ --repository-name ${APP_NAME} \\ --region ap-northeast-1 \\ --query \u0026#34;repository.repositoryUri\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${ECR_REPO_URI}\u0026#34; リポジトリが作成できたら、 docker イメージを作成して push しておきます。今回は AWS アカウント内の S3 バケットの数を標準出力に出力するだけのタスクなので、 Amazon Linux をベースイメージとして、中で AWS CLI をインストールし、 s3api list-buckets コマンドを実行するようなイメージを作成します。 Dockerfile は下記の通りです。\nFROM amazonlinux:2.0.20210126.0-with-sources RUN yum install -y unzip less \\ \u0026amp;\u0026amp; curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; \\ \u0026amp;\u0026amp; unzip awscliv2.zip \\ \u0026amp;\u0026amp; ./aws/install \\ \u0026amp;\u0026amp; aws --version CMD BUCKET_CNT=$(\\ aws s3api list-buckets --query \u0026#34;Buckets[] | length(@)\u0026#34; --output text) \\ \u0026amp;\u0026amp; echo \u0026#34;There are ${BUCKET_CNT} S3 buckets\u0026#34; ビルドして、作成した ECR リポジトリに push します。\n# ビルド $ docker build -t ${APP_NAME} . # タグ付け $ docker tag ${APP_NAME}:latest ${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com/${APP_NAME}:latest # ECR にログイン $ aws ecr get-login-password \\ --region ap-northeast-1 \\ | docker login \\ --username AWS \\ --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com # push $ docker push ${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com/${APP_NAME}:latest VPC 続いて、 VPC などのネットワーク関連リソースを作成します。今回は ECR からイメージを取得して実行するので、 ECS タスクを実行するサブネットからは外部ネットワークに接続できる必要があります。実際のアプリケーションでは、プライベートサブネットで実行して NAT ゲートウェイ経由で接続する方法が一般的かと思いますが、今回はパブリックサブネットで実行することにします。(NAT ゲートウェイ高いので\u0026hellip;)\nなので、諸々のリソースの作成順序としては、下記の通りです。\nVPC 作成 インターネットゲートウェイ (IGW) 作成、VPC にアタッチ カスタムルートテーブル作成、IGW へのルートを作成 サブネットを作成 ルートテーブルとサブネットを関連付け Public IP の自動付与設定 では順番にやっていきます。\n1. VPC 作成 $ VPC_ID=$( \\ aws ec2 create-vpc \\ --cidr-block 10.0.0.0/16 \\ --query \u0026#34;Vpc.VpcId\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo \u0026#34;${VPC_ID}\u0026#34; 2. インターネットゲートウェイ (IGW) 作成、VPC にアタッチ # IGW を作成 $ IGW_ID=$(\\ aws ec2 create-internet-gateway \\ --query \u0026#34;InternetGateway.InternetGatewayId\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo \u0026#34;${IGW_ID}\u0026#34; # VPC にアタッチ $ aws ec2 attach-internet-gateway \\ --vpc-id \u0026#34;${VPC_ID}\u0026#34; \\ --internet-gateway-id \u0026#34;${IGW_ID}\u0026#34; 3. カスタムルートテーブル作成、IGW へのルートを作成 # カスタムルートテーブルを作成 $ ROUTE_TABLE_ID=$( \\ aws ec2 create-route-table \\ --vpc-id \u0026#34;${VPC_ID}\u0026#34; \\ --query \u0026#34;RouteTable.RouteTableId\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo \u0026#34;${ROUTE_TABLE_ID}\u0026#34; # IGW へのルートの作成 $ aws ec2 create-route \\ --route-table-id \u0026#34;${ROUTE_TABLE_ID}\u0026#34; \\ --destination-cidr-block 0.0.0.0/0 \\ --gateway-id \u0026#34;${IGW_ID}\u0026#34; 4. サブネットを作成 $ SUBNET_ID=$( \\ aws ec2 create-subnet \\ --cidr-block 10.0.0.0/24 \\ --vpc-id \u0026#34;${VPC_ID}\u0026#34; \\ --query \u0026#34;Subnet.SubnetId\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo \u0026#34;${SUBNET_ID}\u0026#34; 5. ルートテーブルとサブネットを関連付け $ aws ec2 associate-route-table \\ --subnet-id \u0026#34;${SUBNET_ID}\u0026#34; \\ --route-table-id \u0026#34;${ROUTE_TABLE_ID}\u0026#34; 6. Public IP の自動付与設定 $ aws ec2 modify-subnet-attribute \\ --subnet-id \u0026#34;${SUBNET_ID}\u0026#34; \\ --map-public-ip-on-launch どうでもいいんですが、 VPC 関連のリソースを扱う際にも AWS CLI のコマンドは ec2 なんですよね。最近はもう VPC は EC2 だけのものではない気もするのですが、過去の経緯からもこうなっているんでしょうか。\nECS 続いては ECR のリソースです。作成するのは下記のリソースです。\nクラスター タスク定義 クラスター $ aws ecs create-cluster \\ --cluster-name ${APP_NAME}-cluster \\ --region ap-northeast-1 タスク定義 タスク定義を作成する際に、 タスク実行ロール と タスクロール が必要になるので、それぞれ作成します。\nタスク実行ロール ロールにアタッチするポリシーは、予め用意されている AmazonECSTaskExecutionRolePolicy を使います。\n# AssumeRole Policy Document 作成 $ cat \u0026lt;\u0026lt;EOF \u0026gt; assume-role-policy-doc-for-task-exec-role.json { \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF # IAM Role 作成 $ TASK_EXEC_ROLE_ARN=$( \\ aws iam create-role \\ --role-name ${APP_NAME}-task-exec-role \\ --assume-role-policy-document file://assume-role-policy-doc-for-task-exec-role.json \\ --query \u0026#34;Role.Arn\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_EXEC_ROLE_ARN}\u0026#34; # アタッチする IAM Policy の ARN 取得 $ POLICY_ARN=$(\\ aws iam list-policies \\ --path-prefix \u0026#34;/service-role/\u0026#34; \\ --max-items 1000 \\ --query \u0026#34;Policies[?PolicyName == \\`AmazonECSTaskExecutionRolePolicy\\`].Arn\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo \u0026#34;${POLICY_ARN=$}\u0026#34; # Role に Policy をアタッチ $ aws iam attach-role-policy \\ --role-name ${APP_NAME}-task-exec-role \\ --policy-arn ${POLICY_ARN} タスクロール 今回のタスクでは S3 バケットのリストを取得しているので、 S3 に対する読み取り権限があれば十分です。なので、ロールにアタッチするポリシーは、予め用意されている AmazonS3ReadOnlyAccess を使用します。\n# AssumeRole Policy Document 作成 $ cat \u0026lt;\u0026lt;EOF \u0026gt; assume-role-policy-doc-for-task-role.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF # IAM Role 作成 $ TASK_ROLE_ARN=$( \\ aws iam create-role \\ --role-name ${APP_NAME}-task-role \\ --assume-role-policy-document file://assume-role-policy-doc-for-task-role.json \\ --query \u0026#34;Role.Arn\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_ROLE_ARN}\u0026#34; # アタッチする IAM Policy の ARN 取得 $ TASK_ROLE_POLICY_ARN=$(\\ aws iam list-policies \\ --max-items 1000 \\ --query \u0026#34;Policies[?PolicyName == \\`AmazonS3ReadOnlyAccess\\`].Arn\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_ROLE_POLICY_ARN}\u0026#34; # Role に Policy をアタッチ $ aws iam attach-role-policy \\ --role-name ${APP_NAME}-task-role \\ --policy-arn ${TASK_ROLE_POLICY_ARN} タスク定義 必要なロールが揃ったので、タスク定義を作成します。タスク定義の JSON は下記のコマンドでテンプレートを確認して、今回は必要最低限の項目のみ指定します。\n# タスク定義用の JSON フォーマット確認 $ aws ecs register-task-definition \\ --generate-cli-skeleton # 必要最低限の内容で task-definition.json を作成 $ cat \u0026lt;\u0026lt;EOF \u0026gt; task-definition.json { \u0026#34;family\u0026#34;: \u0026#34;${APP_NAME}\u0026#34;, \u0026#34;networkMode\u0026#34;: \u0026#34;awsvpc\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;${APP_NAME}-app\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;${ECR_REPO_URI}\u0026#34;, \u0026#34;logConfiguration\u0026#34;: { \u0026#34;logDriver\u0026#34;: \u0026#34;awslogs\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;awslogs-group\u0026#34;: \u0026#34;/dev/${APP_NAME}-task\u0026#34;, \u0026#34;awslogs-region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;awslogs-stream-prefix\u0026#34;: \u0026#34;dev\u0026#34; } } } ], \u0026#34;requiresCompatibilities\u0026#34;: [ \u0026#34;FARGATE\u0026#34; ], \u0026#34;taskRoleArn\u0026#34;: \u0026#34;${TASK_ROLE_ARN}\u0026#34;, \u0026#34;executionRoleArn\u0026#34;: \u0026#34;${TASK_EXEC_ROLE_ARN}\u0026#34;, \u0026#34;cpu\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;512\u0026#34; } EOF # タスク定義を作成 $ TASK_DEF_ARN=$( \\ aws ecs register-task-definition \\ --cli-input-json file://task-definition.json \\ --query \u0026#34;taskDefinition.taskDefinitionArn\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_DEF_ARN}\u0026#34; CloudWatch 最後に、 CloudWatch Logs のロググループを作成しておきます。\n$ aws logs create-log-group \\ --log-group-name /dev/${APP_NAME}-task 実行する 諸々準備が整ったので、タスクを実行します。\n$ TASK_ARN=$(\\ aws ecs run-task \\ --cluster ${APP_NAME}-cluster \\ --task-definition \u0026#34;${TASK_DEF_ARN}\u0026#34; \\ --network-configuration \u0026#34;awsvpcConfiguration={subnets=[${SUBNET_ID}],assignPublicIp=ENABLED}\u0026#34; \\ --launch-type FARGATE \\ --query \u0026#34;tasks[0].taskArn\u0026#34; \\ --output text )\\ \u0026amp;\u0026amp; echo ${TASK_ARN} 確認する 実行状況を確認しつつ、実行が終わったら CloudWatch Logs でログを確認します。\nECS タスクの実行状況 # タスクの実行状況確認 $ aws ecs describe-tasks \\ --tasks ${TASK_ARN} \\ --cluster ${APP_NAME}-cluster \\ --query \u0026#34;tasks[0].lastStatus\u0026#34; \\ --output text STOPPED になっていればタスクの実行は終わっています。\nCloudWatch Logs にイベントが出力されていないとか、その他結果がおかしい場合には --query オプションを外して確認します。\nCloudWatch Logs のイベント # 最新のログストリーム名を取得 $ LOG_ST_NAME=$(\\ aws logs describe-log-streams \\ --log-group-name /dev/${APP_NAME}-task \\ --query \u0026#39;max_by(logStreams[], \u0026amp;lastEventTimestamp).logStreamName\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; echo \u0026#34;${LOG_ST_NAME}\u0026#34; # ログイベント取得 $ aws logs get-log-events \\ --log-group-name /dev/${APP_NAME}-task \\ --log-stream-name \u0026#34;${LOG_ST_NAME}\u0026#34; \\ --limit 5 \\ --query \u0026#39;events[].[join(``, [ to_string(timestamp) ,`: `,message])]\u0026#39; \\ --output text 1613656782996: There are 80 S3 buckets のような出力がされれば OK です。\nまとめ ECS タスクを作って、実行して CloudWatch でログを確認するまでを AWS CLI だけでやってみた話でした。\n楽さでいうと Copilot のほうが圧倒的です。が、ネットワーク周りのリソースとか、その他 ECS でタスク実行するときに必要なリソースとその関係について理解するには、やっぱり AWS CLI は最適でした。\nECS まわりは最近になってちゃんと触ってるんですが、とりあえず動くものを Copilot でサクッと作って、その後で CLI で一つずつ作っていくっていう方法が個人的にはかなりしっくりきてます。ECS よくわからんっていう方は同じような順序で触ってみると理解しやすいかもしれません。\n",
    "permalink": "https://michimani.net/post/aws-run-ecs-task-using-only-aws-cli/",
    "title": "ECS タスクを作って実行して CloudWatch でログを確認するまでを AWS CLI だけでやってみた"
  },
  {
    "contents": "Google スプレッドシート上で作成した表を Markdown 形式のテーブルに変換するアドオンを作成して、無事に Google Workspace Marketplace で公開されました🎉\n公開されるまで色々大変だったので、忘れないうちに何をやったのか書いておきます。ところどころスクリーンショットを貼って説明していますが、いつ UI が変更されるかわからないので、もし参考にされる方はそのあたりご理解ください。\n目次 概要 経緯 やったこと一覧 やったこと詳細 1. GCP にプロジェクトを作成 2. アドオンを作成 3. プロジェクトにデプロイ 4. OAuth 同意画面の承認を受ける 5. Google Workspace Marketplace に申請する Google Workspace Marketplace で公開された 概要 Google スプレッドシート用のアドオンを自作して Google Workspace Marketplace に公開しました。公開までにいくつかやることがあったので、何をやったのか書き記しておきます。\nちなみに、公開したのは GenerateMarkdownTable というアドオンです。その名前の通り、 Google スプレッドシート上で作成した表を Markdown 形式のテーブル表記に変換するアドオンです。\n以前に似たような事ができるアドオンがあったのですが、利用できなくなっていたため自作しました。もしよかったらインストールして使ってみてください。\nGenerateMarkdownTable - Google Workspace Marketplace ソースコードは GitHub に置いているので、バグとかあれば issue 立ててもらえると頑張って直します。要望とかも issue でもらえると対応するかもしれません。\nmichimani/gen-md-table: A Google Spreadsheet add-on that converts selected tables to Markdown table formats. 経緯 以前使っていたアドオンが使えなくなっていたことと、同じように Twitter で嘆かれているツイートを見て、とりあえず雑に作ってみるか、と思ったのが経緯です。\nこのアドオン使えなくなっちゃったのかぁぁぁぁhttps://t.co/0QoV9GdfRH\n\u0026mdash; 深澤俊 (@shun_quartet) January 27, 2021 やったこと一覧 まずは Google スプレッドシート用のアドオンを作成して公開するまでにやったことを書き出してみます。\nGCP にプロジェクトを作成 アドオンを作成 プロジェクトにデプロイ OAuth 同意画面の承認を受ける Google Workspace Marketplace に申請する 意外と少ないと思われるかもしれないですが、特に 4 と 5 で準備するものが色々あるので大変です。\nやったこと詳細 多分長くなりますが、上で挙げた内容の詳細を書いていきます。\n1. GCP にプロジェクトを作成 まずは GCP でプロジェクトを作成します。\nGCP のコンソールってどこに何があるのか未だによくわからないのですが、ダッシュボードのヘッダーにあるプロジェクト選択画面から新しいプロジェクトを作成します。既にプロジェクトがある場合はそれを使っても良いと思いますが、今回は新たにプロジェクトを作成しました。\n2. アドオンを作成 続いてアドオンを作成します。アドオンは Google Apps Script で実装します。\n今回は Google スプレッドシートのアドオンなので、スプレッドシートを新規作成して、スクリプトエディタからコードを書いていきます。コードの詳細については GitHub を見てもらうとして、アドオンとして実装する場合には下記のような構成で実装します。\nfunction onInstall(e) { onOpen(e); } function onOpen(e) { SpreadsheetApp.getUi().createAddonMenu() .addItem(\u0026#39;do something\u0026#39;, \u0026#39;doSomething\u0026#39;) .addToUi(); } function doSomething() { // do something } addItem() の第一引数には アドオンで表示されるメニュー を、第二引数には メニュー選択時に実行される関数名 を指定します。複数指定することでアドオンとして実行できるメニューを増やすことができます。アドオン自体の名前は、 App Script のプロジェクト名になります。\n実装が完了したら、スプレッドシートをリロードすることでアドオンメニューから使用できるようになります。つまり、特定のスプレッドシートでだけ使用できるアドオンであれば、特に公開する必要もなく使えるというわけです。あまりそんな用途でアドオン作らないと思いますが\u0026hellip;。\n3. プロジェクトにデプロイ アドオンが実装できたら、 1 で作成したプロジェクトにデプロイします。\nスクリプトエディタの左にある プロジェクトの設定 (歯車マーク) を押して、プロジェクトを変更\n先ほど GCP で作成したプロジェクトの番号 (11桁の数字) を入力します。\nスクリプトエディタの右上にある デプロイ ボタンを押して 新しいデプロイ を選択\n開いたダイアログの左にある歯車から アドオン を選択\n任意の説明文を入れて デプロイ ボタンを押します\n4. OAuth 同意画面の承認を受ける このステップと次のステップがなかなか重めでした。\nOAuth 同意画面の承認 とは、作成したアドオンがユーザーのデータに正しい方法でアクセスしているかどうか、どのような方法でアクセスしているかを示して、承認してもらう というステップです。具体的には、 GCP のプロジェクトのメニュー OAuth 同意画面 から申請を行います。\nここからアプリ登録をして承認を受けるのですが、入力が必要な情報は下記の通りです。(結構あります)\nアプリ名 申請をするアプリの名前です。\nここで入力した名前は Marketplace で表示される名前ではなく、ユーザーが OAuth 認証する際の画面に表示される名前です。\nユーザーサポートメール OAuth 同意画面に表示される、ユーザーサポート用のメールアドレスです。\nGoogle アカウントの Gmail アドレスで良さそうです。\nアプリのロゴ OAuth 同意画面に表示されるアプリのロゴです。\n後に必要になる Marketplace 掲載用のロゴと合わせると良さそうです。\nアプリケーションのホームページ その名の通り、アプリケーションのホームページです。\nここで入力する URL のドメインは、 Google Search Console で承認済みになっている必要があります。今回はこのブログのドメインを使って、 /projects 以下にアプリ用のページを作成したので追加での操作は不要でしたが、まずひとつ高めのハードルかなと思います。\n今回は下記ページをホームページとして入力しました。\nhttps://michimani.net/projects/gen-markdown-table/about/ ホームページについては英語で記載されている必要があります。\nアプリケーションプライバシーポリシー へのリンク プライバシーポリシーへのリンクです。\nドメインの制限としてはホームページと同様です。プライバシーポリシーについても英語で記載されている必要があり、内容は Google API Services User Data Policy の内容を満たしている必要があります。\n今回は下記ページをプライバシーポリシーへのリンクとして入力しました。\nhttps://michimani.net/projects/gen-markdown-table/privacy-policy/ アプリケーション利用規約 へのリンク 利用規約へのリンクです。\nドメイン制限については同上で、こちらも英語での記述が必要です。\n今回は下記ページを利用規約へのリンクとして入力しました。\nhttps://michimani.net/projects/gen-markdown-table/terms-of-service/ デベロッパーの連絡先情報 (メールアドレス) Google アカウントの Gmail アドレスで良さそうです。\nスコープ アプリケーションがユーザーに対して許可を求めるスコープを入力します。\n今回作成するドメインでどんなスコープが必要になるかは、スクリプトエディタの概要ページで確認することができます。\n今回であれば下記のスコープが必要になります。\nhttps://www.googleapis.com/auth/spreadsheets https://www.googleapis.com/auth/script.container.ui これらは 機密性の高い API スコープ にあたるため、どのような目的でこのスコープが必要になるのかを入力します。ここは日本語でも大丈夫でした。\nスコープの使用方法を示したデモ動画 機密性の高いスコープに許可を求める場合、どのようにそのスコープを使用するかというデモ動画が必要になります。デモ動画は YouTube にアップロードして限定公開として、 URL をフォームに入力します。\nデモ動画の内容としては、実際に アドオンとして使用して OAuth 同意画面を表示し、申請されているスコープと整合性があるかを示します。また、実際にアドオンでどんな操作をした際に該当のスコープが必要になっているかを示します。デモ動画は自分で喋る必要はないですが、説明を加えるのであれば英語でテロップを入れたりすると良いと思います。\n公開されていないアドオンを アドオンとして使用 するためには、スクリプトエディタの実行メニューから アドオンとしてテスト を選択します。ここでハマったのが、このメニュー、スクリプトエディタの旧 UI でしか出ないんですよね。ということで、新 UI で使っている場合には右端にある 以前のエディタを使用 から旧 UI に戻して実行します。\nあとは、実際の操作画面を録画して iMovie とかで良い感じに編集するだけです。これも結構なハードルです。\nポイントとしては、アドオンとして実行した際に表示される OAuth 同意画面の URL に含まれている client_id が、今申請しようとしているプロジェクトの client_id つまり プロジェクト番号 と一致している必要があり、それを動画内で示す必要があります。方法としては、 OAuth 同意画面のアドレスバーを拡大表示するか、またはコピーしてエディタに貼り付けて表示させるか、が良さそうです。\nその他、省略可能な項目が出てきますが、文字通り省略して大丈夫でした。\n上記を入力して申請をすると数日経ってから Google さんからレビューがメールで届きます。メールのタイトルは OAuth Verification Request for Project みたいな感じです。\n申請した内容に対してこちらで対応が必要な場合は、対応方法を教えてくれます。ここまでの手順ではこのメールで何度かやり取りをして修正した内容を反映しているので、全部できていればスムーズにいくと思います。ただし、必ず一発で通る保証はありません。\nメールはもちろん英語なので、 Google 翻訳や DeepL 翻訳を駆使して頑張ってやりとりしましょう。\n5. Google Workspace Marketplace に申請する OAuth 同意画面の承認が通っていなくても並行して申請することは可能です。ただし、 Google Workspace Marketplace での審査項目に OAuth 同意画面の承認がなされていること という類のものがあるので、審査を通過するためには OAuth 同意画面の承認は必須です。\nMarketplace への申請を行うためには、まずは GCP のプロジェクトにて Google Workspace Marketplace SDK を有効にする必要があります。\nGCP ダッシュボードのヘッダーにある検索フォームから 「Google Workspace Marketplace」で検索すると出てくるので、選択して有効にします。\nそして、 Google Workspace Marketplace SDK の管理画面から、 アプリの構成 を選択し、下記内容を入力していきます。\nアプリの統合 スプレッドシートのアドオン のみにチェックを入れます。\nスプレッドシート アドオンのプロジェクト スクリプト ID には、スクリプトエディタのプロジェクト設定ページで確認できる スクリプト ID を入力します。\nスプレッドシート アドオン スクリプトのバージョン には、スクリプトエディタのデプロイボタンから デプロイを管理 で確認できるバージョン番号を入力します。最新のバージョン番号を入力します。\nOAuth スコープ OAuth 同意画面の申請時に入力したものと同じスコープを入力します。\nデフォルトでメールとプロファイルに対するスコープは入力されているので、それはそのままにしておきます。\nDeveloper Link 下記の情報を入力します。それぞれ、 Marketplace で表示される内容です。\nアプリケーション ウェブサイトの URL アドオンのホームページ URL です。 デベロッパー名 開発者として表示される名前です。 michimani にしています。 デベロッパー ウェブサイトの URL 開発者のホームページです。このブログの URL にしています。 デベロッパーのメールアドレス Google アカウントの Gmail アドレスにしています。 以上、 アプリの構成 に関しては内容を保存するだけです。\n続いて、 ストアの掲載情報 でアドオンの申請を進めていきます。\nアプリの詳細 少なくとも1つの言語で、下記内容を入力します。\nアプリケーション名 アドオンの名前。ストアだけでなく、スプレッドシートのアドオンメニューにも表示されます。 簡単な説明 アドオンの概要 詳細な説明 操作方法など カテゴリは、適当なものを選択します。\nグラフィックアセット アイコンやバナー、スクリーンショットが必要になります。オプションですが、YouTube にアップしたプロモーション動画を指定することもできます。OAuth 認証のデモ動画と違い、ここで指定した動画は公開設定にしておきます。\nアイコン 32x32 必須 48x48 96x96 128x128 必須 バナー 440x280 必須 スクリーンショット サイズの指定は無いが 4 枚必要 サポートリンク 利用規約やプライバシーポリシーなどへのリンクを入力します。8 箇所ほど入力項目がありますが、必須なのは下記のリンクのみです。\n利用規約の URL プライバシーポリシーの URL サポートの URL また、 インストール後のヒント として、簡単な説明文を入力します。これは、Marketplace のアドオンのページで、説明欄の一番上に太字で表示されます。\n以上の内容を入力したら、内容を保存して申請します。\n申請から数日経つと、 Google さんからメールが届きます。メールには Google ドキュメントが共有されており、その中で各項目の指摘内容が記載されています。指摘事項を修正したら、ドキュメント内のコメントで「直しましたよ〜」とコメントする感じです。\nGoogle Workspace Marketplace で公開された アドオンの公開なんて初めての経験だったので、何度も Google の人とメールをやりとりして、なんとか無事に公開されました。最初の申請から公開まで、ちょうど 2 週間かかりました。途中、英語でのやりとりに心が折れかけましたが、めげずに頑張ってよかったです。\n時差の関係で深夜や早朝にレビュー結果が来て、日中対応してもそれに返答が来るのがまた深夜や早朝ということである意味リズム感はあったものの、どうしてもレスポンスまでの時間が空いてしまうのがもどかしかったです。\n無事に公開されると GenerateMarkdownTable - Editor Add-on - Launched というタイトルでメールが届き、 Google さんとのやりとりはおしまいです。本文には We wanted to thank you for your patience and for working on our comments and suggestions. We’ve completed our review and your item has been published. と書かれていて、こちらこそ Thank you って感じでした。\nとりあえず公開までできてホッとしているのと、もしよかったら使っていただきたいな、という気持ちです。\nGenerateMarkdownTable - Google Workspace Marketplace ",
    "permalink": "https://michimani.net/post/development-how-to-publish-google-spreadsheet-add-on/",
    "title": "Google スプレッドシートを Markdown 形式のテーブルに変換するアドオンを作って公開しました"
  },
  {
    "contents": "先日 Amazon CloudFront の新しい料金プラン Security Savings Bundle というものが発表されました。個人利用の範囲でもどれくらいのコスト削減ができるのか試算してみました。\n目次 Security Savings Bundle とは Security Savings Bundle に関する Q\u0026amp;amp;A 実際にどれくらい削減できるのか 個人利用での毎月の料金 CloudFront のダッシュボードから試算 まとめ Security Savings Bundle とは Today, we are announcing the Amazon CloudFront Security Savings Bundle, a flexible self-service pricing plan that helps you save up to 30% on your CloudFront bill in exchange for a monthly spend commitment for a 1-year term. The savings bundle also includes free AWS WAF (Web Application Firewall) usage up to 10% of your committed amount. Any additional Standard CloudFront or WAF charges not covered by the CloudFront Security Savings Bundle still apply.\nIntroducing Amazon CloudFront Security Savings Bundle 詳細については上記の公式アナウンスに書いてありますが、ざっとまとめるとこんな感じです。\n毎月支払う料金 (commitment) を予め決めて支払う commitment に応じて CloudFront および AWS WAF で使用できるクレジットが付与される CloudFront 用のクレジットは commitment の 約 1.4285 倍 (= 30% オフ) AWS WAF 用のクレジットは commitment の 10% commitment としてカバーできる料金には Lambda@Edge にかかるコストも含まれる 上記のアナウンスページで説明されている commitment を 70 USD として場合の例 を用いて具体的な数値を出すと、下記のような計算になります。\nCloudFront の料金支払いに使用できるクレジットが 100 USD 付与されます。(つまり 30% コスト削減) AWS WAF で使用できるクレジットが 7 USD 付与されます AWS WAF の 7 USD とは、約 11,666,666 リクエスト分のコスト イメージとしては、 EC2 や EDS のリザーブドインスタンスのようなイメージです。\nSecurity Savings Bundle に関する Q\u0026amp;A Security Savings Bundle に関する Q\u0026amp;A に関しては下記の公式ページで色々書かれていますが、そこから抜粋していくつか内容を見てみます。\nCloudFront security savings bundle - Amazon CloudFront Q. commitment には CloudFront のすべての料金に適用されるのか？ データ転送料金、リクエスト料金、Lambda@Edge料金など、すべてのCloudFront利用料金に適用されます。\nQ. 特定の期間中にクレジットを使い切らなかった場合どうなるか？ 使い切らなかった commitment は失効し、次の期間に繰り越すことはできません。\nQ. クレジットを超過した場合はどうなる？ 超過した分は通常の料金設定で請求されます。\nQ. CloudFront の利用量がクレジットを超過した際に通知を受け取ることはできる？ AWS Budgets で閾値を指定することで、 メールや Amazon SNS を通じて通知を受け取ることができます。\n実際にどれくらい削減できるのか 最大で 30% とのことですが、実際にどれくらい削減できるのか試算してみます。個人で利用している AWS アカウントでは普段から大したコストが発生していないのですが、それでも効果があるのでしょうか。\n個人利用での毎月の料金 現在、個人利用の範囲では CloudFront の料金として毎月約 1.7 USD 程度です。単純に計算すると、年間 20.4 USD なので、約 2,142 円 ということになります。\nこれがどれくらい削減できるのか CloudFront のダッシュボードで試算してみます。\nCloudFront のダッシュボードから試算 マネジメントコンソールで CloudFront のダッシュボードにアクセスすると、左のメニューに Savings Bundle のメニューが追加されています。ここの Overview から試算、およびクレジットの購入ができます。\n試算方法としては、過去のコストからレコメンドしてくれる Historical usage と、カリキュレーターによる Calculator があります。ただし、過去に十分な支払履歴がないと、 Historical usage による試算は利用できません。\nCalculator による試算では、 CloudFront の各リージョン (地域) に対する転送量 (GB) と、オブジェクトの平均サイズを入力することで試算ができます。ただし、ここでは Lambda@Edge の使用量を入力することはできません。\nどちらの試算方法でも、最後は自分で commitment を入力する画面になり、その画面でも下記の情報を確認することができます。\ncommitment に応じたクレジット CloudFront および WAF に割り当てられるクレジット 年間で削減できるコスト レコメンドでは 1.16 USD と表示されていましたが、今回は 1.2 USD の commitment で設定してみます。すると、 CF 用のクレジットが 1.71 USD 付与されることがわかります。また、それに加えて AWS WAF で使用できるクレジットが 0.12 USD 付与されることもわかります。\nコンソール上で表示されている年間削減コストは WAF 用の分も含まれているので 7.61 USD と表示されていますが、 CloudFront だけに限ると 6.12 USD の削減となります。\nこの内容で良ければ Next を押して確認画面へ進みます。\n問題なければ Purchase を押して commitment を購入します。\n購入直後は Status が Planned になっていますが、しばらくすると Active に変わります。この内容は Savings Bundle のメニューの Inventory から確認できます。\nまとめ Amazon CloudFront の新しい料金プラン Security Savings Bundle でどれくらいコスト削減できるのか試算してみた話でした。個人利用の範囲でも、ほぼ 30 % コスト削減できることがわかりました。\n大量にリクエストがあったり、キャッシュ短めでオリジンへのリクエストが多くなったりすると CloudFront のコストは高くなってくるので、 CloudFront を使っている方は一度試算してみるとよさそうです。また、 WAF のために CloudFront を使っているというパターンもあると思うので、そういう方には WAF のコストに充てられるクレジットもついてくるのは嬉しいのではないでしょうか。\n",
    "permalink": "https://michimani.net/post/aws-get-started-cloudfront-saving-bundle/",
    "title": "CloudFront の新料金プラン Security Savings Bundle でどれくらいコスト削減できるのか試算してみた"
  },
  {
    "contents": "Go でちょっとした CLI ツールを作ったので、 GitHub Actions と GoReleaser を使って brew コマンドでインストールできるようにしてみました。その時の手順をまとめておきます。\n目次 概要 やったこと 1. homebrew-genlink リポジトリを作成 2. genlink リポジトリを作成 3. brew コマンドでインストール まとめ 参考にした記事 概要 GitHub Actions と GoReleaser を使って Go で書いた CLI ツールを brew コマンドでインストールできるようにします。\n実際にこの方法で公開しているツールがあるので、これを公開するときにとった手順を書いていきます。\nmichimani/genlink - GitHub ちなみにこれは、 URL を渡すといろんな形式 (Markdown/HTML/QR コード) でリンクを生成してくれるツールです。\nやったこと やったことは下記の通りです。\nhomebrew-genlink リポジトリを作成 genlink リポジトリを作成 CLI ツールの実装 CREDITS ファイルを作成 .github/workflows/release.yml と .goreleaser.yml を作成 タグ付けして Push brew コマンドでインストール それぞれ詳細を書いていきます。\n1. homebrew-genlink リポジトリを作成 まずは GitHub で homebrew-genlink というリポジトリを作ります。\nこれは、作成したツールを brew tap を使用して配布するために必要なリポジトリとなります。\nbrew コマンドでインストールできるツールは、基本的には Homebrew 公式の Homebrew/homebrew-core に追加されている必要があります。ただし、公式リポジトリへ追加してもらうとなると、 PR を作ってレビューしてもらってマージしてもらう流れになり、公開までに時間がかかります。また、バージョンアップのたびにこのフローを踏む必要があります。\nbrew tap を使うと、公式以外のリポジトリにあるツールを brew コマンドでインストールできるようになります。その際、対象となるリポジトリの命名規則が「homebrew-{hoge}」となっています。\nこのリポジトリはあくまでも brew tap で認識させるためのリポジトリで、このリポジトリでソースコードを管理するわけではありません。\nbrew/Taps.md at master · Homebrew/brew 2. genlink リポジトリを作成 実際にソースコードを管理するリポジトリ genlink を作成します。リポジトリを作成する際、 LICENSE を一緒に作っておきます。(これは任意です)\n2-1. CLI ツールの実装 ツールを実装します。\nGo で CLI ツールを作成する際、標準ライブラリの flag パッケージを使用することでコマンド実行時のパラメータをいい感じに扱う事ができます。 main.go はこんな感じになってます。\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) var ( targetUrl *string = flag.String(\u0026#34;u\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Type of link to output\u0026#34;) genType *string = flag.String(\u0026#34;t\u0026#34;, \u0026#34;md\u0026#34;, \u0026#34;Type of link to output\u0026#34;) outDir *string = flag.String(\u0026#34;o\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Directory to output QR code\u0026#34;) version string revision string ) func usage() { format := ` _ _ _ __ _ ___ _ __ | (_)_ __ | | __ / _\u0026#39; |/ _ \\ \u0026#39;_ \\| | | \u0026#39;_ \\| |/ / | (_| | __/ | | | | | | | | \u0026lt; \\__, |\\___|_| |_|_|_|_| |_|_|\\_\\ |___/ Version: %s-%s Usage: genlink [flags] [values] Flags: -u (required) URL -t Type of link to output md: Markdown (default) html: HTML a tag html-bl: HTML a tag with \u0026#39;target=\u0026#34;_blank\u0026#34;\u0026#39; qr: QR code image -o Absolute path to directory to output QR code Use this flag in combination with \u0026#39;-t qr\u0026#39; Default is current directory Author: michimani \u0026lt;michimani210@gmail.com\u0026gt; ` fmt.Fprintln(os.Stderr, fmt.Sprintf(format, version, revision)) } func main() { flag.Usage = usage flag.Parse() os.Exit(run()) } func run() int { res, err := Generate(*targetUrl, *genType, *outDir) if err != nil { fmt.Println(err.Error()) return 1 } fmt.Println(res) return 0 } flag.Usage に代入した関数は、 -h または --help オプションを付けて実行した際に実行されるので、コマンドの使い方を出力するようにしています。\nversion string revision string 宣言だけして初期化していないこの変数については後で説明します。\nちなみに 「genlink」アスキーアートは figlet コマンドで生成しました。 macOS であれば brew install figlet でインストールできます。\n2-2. CREDITS ファイルを作成 Go で作成した (Go に限らずですが) ツールを配布する際、そのツール内で使用したパッケージやライブラリのライセンス情報をバイナリに同梱する必要があります。Go の場合、サードパーティのパッケージだけでなく標準パッケージについてもライセンス情報を含める必要があります。これについてはこのあたりで議論されています。\nStandard library licensing question · Issue #19893 · golang/go ただ、使用しているパッケージのライセンス情報をすべて確認するのは面倒です。そこで便利なのが、 gocredits というツールです。\nSongmu/gocredits: creates CREDITS file from LICENSE files of dependencies brew または go get でインストールし、プロジェクトのルートディレクトリで gocredits . \u0026gt; CREDITS を実行するだけで依存パッケージのライセンス情報が CREDITS ファイルに出力されます。\n2-3. .github/workflows/release.yml と .goreleaser.yml を作成 .github/workflows/release.yml は GitHub Actions を起動するためのファイルです。これについては GoReleaser のドキュメント内にサンプルがあり、ほぼそのまま使用することができます。\nGitHub Actions - GoReleaser 今回はこのサンプルを下記のように変更しています。\nname: release on: push: tags: - \u0026#34;v[0-9]+.[0-9]+.[0-9]+\u0026#34; jobs: goreleaser: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 with: fetch-depth: 1 - name: Setup Go uses: actions/setup-go@v2 with: go-version: 1.15 - name: Run GoReleaser uses: goreleaser/goreleaser-action@v2 with: version: latest args: release --rm-dist env: GITHUB_TOKEN: ${{ secrets.ACCESS_TOKEN }} サンプルからの主な変更点を書いておきます。\nまずは、GitHub Actions の起動条件を、tag が Push されたときにしています。\non: push: tags: - \u0026#34;v[0-9]+.[0-9]+.[0-9]+\u0026#34; 続いて、 GITHUB_TOKEN に設定する値の名前を secrets.ACCESS_TOKEN に変更しています。これは GITHUB_TOKEN のまま実行した際に Actions で下記のようなエラーになったからです。\n⨯ release failed after 45.53s error=homebrew tap formula: failed to publish artifacts: PUT https://api.github.com/repos/michimani/homebrew-genlink/contents/Formula/genlink.rb: 403 Resource not accessible by integration [] Error: The process \u0026#39;/opt/hostedtoolcache/goreleaser-action/0.155.0/x64/goreleaser\u0026#39; failed with exit code 1 この Actions では、後に説明する .goreleaser.yml で作成した Homebrew 用の formula ファイルを、最初に作成した homebre-genlink リポジトリに Push します。その際に権限がないと怒られています。\nこの対処法として、 GitHub の Settings \u0026gt; Developer settings \u0026gt; Personal access tokens から新たに token を作成し、 genlink リポジトリの Settings \u0026gt; Secrets から名前をつけて追加します。このときの名前として GITHUB_TOKEN は使用できないため ACCESS_TOKEN として追加しています。必要 scope は、とりあえず repo にチェックが入っていれば OK です。\n.goreleaser.yml は、ビルドのオプションや formula の配置場所などを指定しています。内容は下記の通り。\nproject_name: genlink env: - GO111MODULE=on before: hooks: - go mod tidy builds: - main: . binary: genlink ldflags: -s -w -X main.version={{.Version}} -X main.revision={{.ShortCommit}} -X main.date={{.Date}} archives: - replacements: darwin: darwin linux: linux windows: windows amd64: x86_64 files: - LICENSE - CREDITS release: prerelease: auto brews: - tap: owner: michimani name: homebrew-genlink folder: Formula homepage: \u0026#39;https://github.com/michimani/genlink\u0026#39; description: \u0026#39;Generates the URL link in various formats\u0026#39; license: \u0026#34;MIT\u0026#34; 内容について少し解説。\nbuilds: - main: . binary: genlink ldflags: -s -w -X main.version={{.Version}} -X main.revision={{.ShortCommit}} -X main.date={{.Date}} この部分で Go のバイナリをビルドしていますが、その際に ldflags を指定して main パッケージの変数 version 、 revision 、 date に値を埋め込んでいます。ツールの実装の部分で触れましたが、宣言だけしていた変数はここで値を埋め込むために宣言していたわけです。\nこうすることで、ツール内でバージョンやリビジョン情報を表示する際にもソースコードの変更は必要なく、ビルド時に値を埋め込むことができます。\nbrews: - tap: owner: michimani name: homebrew-genlink folder: Formula homepage: \u0026#39;https://github.com/michimani/genlink\u0026#39; description: \u0026#39;Generates the URL link in various formats\u0026#39; この部分で、作成した formula を配置するリポジトリとディレクトリを指定しています。実際に配置されるのは .rb ファイルで、中身は下記のような内容です。\n# typed: false # frozen_string_literal: true # This file was generated by GoReleaser. DO NOT EDIT. class Genlink \u0026lt; Formula desc \u0026#34;Generates the URL link in various formats\u0026#34; homepage \u0026#34;https://github.com/michimani/genlink\u0026#34; version \u0026#34;0.1.3\u0026#34; license \u0026#34;MIT\u0026#34; bottle :unneeded if OS.mac? url \u0026#34;https://github.com/michimani/genlink/releases/download/v0.1.3/genlink_0.1.3_darwin_x86_64.tar.gz\u0026#34; sha256 \u0026#34;9a4e7ec00e526565578f8b39a5f1d7253bb91b26277d6064b57ff3241d767b75\u0026#34; end if OS.linux? \u0026amp;\u0026amp; Hardware::CPU.intel? url \u0026#34;https://github.com/michimani/genlink/releases/download/v0.1.3/genlink_0.1.3_linux_x86_64.tar.gz\u0026#34; sha256 \u0026#34;c7ed041d314c16906711bc536ab82a8b170d9c83b7f057bfcfbe1926d9c75d7b\u0026#34; end if OS.linux? \u0026amp;\u0026amp; Hardware::CPU.arm? \u0026amp;\u0026amp; Hardware::CPU.is_64_bit? url \u0026#34;https://github.com/michimani/genlink/releases/download/v0.1.3/genlink_0.1.3_linux_arm64.tar.gz\u0026#34; sha256 \u0026#34;6afa8842542b8ee55833b5af5b9b3b7542b102274c1fe9bfd31408451555dd1f\u0026#34; end def install bin.install \u0026#34;genlink\u0026#34; end end .goreleaser.yml について、その他のオプションについては公式ドキュメントまたは goreleaser のリポジトリを参照してください。\nHomebrew - GoReleaser goreleaser/.goreleaser.yml at master · goreleaser/goreleaser 2-4. タグ付けして Push あとは、ローカルから Push する際に tag をつけて、 tag も一緒に Push します。\n$ git tag v0.1.0 $ git push origin main:main --tag 念のため Actions が起動して正常終了することを確かめておきます。\n3. brew コマンドでインストール Actions が正常に動けば、あとは brew コマンドでインストールするだけです。Homebrew の公式リポジトリに追加されているツールであれば brew install \u0026lt;ツール名\u0026gt; でインストールできますが、 brew tap を使用した配布の場合は下記のようなコマンドでインストールします。\n$ brew install michimani/genlink/genlink ... ... 🍺 /usr/local/Cellar/genlink/0.1.3: 4 files, 5.7MB, built in 3 seconds $ genlink -h _ _ _ __ _ ___ _ __ | (_)_ __ | | __ / _\u0026#39; |/ _ \\ \u0026#39;_ \\| | | \u0026#39;_ \\| |/ / | (_| | __/ | | | | | | | | \u0026lt; \\__, |\\___|_| |_|_|_|_| |_|_|\\_\\ |___/ Version: 0.1.3-53fc163 Usage: genlink [flags] [values] Flags: -u (required) URL -t Type of link to output md: Markdown (default) html: HTML a tag html-bl: HTML a tag with \u0026#39;target=\u0026#34;_blank\u0026#34;\u0026#39; qr: QR code image -o Absolute path to directory to output QR code Use this flag in combination with \u0026#39;-t qr\u0026#39; Default is current directory Author: michimani \u0026lt;michimani210@gmail.com\u0026gt; ツール名を指定する部分が異なるだけで、アップデートやアンインストールについても同様です。\n# アップデート $ brew upgrade michimani/genlink/genlink # アンインストール $ brew uninstall michimani/genlink/genlink まとめ Go でちょっとした CLI ツールを作ったので、 GitHub Actions と GoReleaser を使って brew コマンドでインストールできるようにしてみた話でした。\n配置する YAML ファイルについてはほぼコピペで使えるので、結構簡単に公開できます。\n参考にさせていただいたブログに\nみなさんも是非こんなものがあったらいいなという CLI ツールを同様の手順で作成、公開して、 GitHub Star 5000兆個を目指してください。\nと書かれていたのでこのツールで Star 5000兆個目指したいと思います。\nmichimani/genlink - GitHub 参考にした記事 下記の記事を参考にさせていただきました。ありがとうございます。\ngoreleaserでGitHub Actionsから簡単にオレオレCLIをbrew installできるようにする - Qiita GoReleaser\u0026#43;GithubActionsを使って、releaseファイルのアップロードとhomebrew対応を自動で行う - 年中アイス git switch からはじめる CLI ツール作成 ",
    "permalink": "https://michimani.net/post/development-publish-cli-tool-to-homebrew/",
    "title": "Go で書いた CLI ツールを GitHub Actions と GoReleaser を使って brew コマンドでインストールできるようにした"
  },
  {
    "contents": " What\u0026rsquo;s this? This is a Google Spreadsheet add-on that converts selected tables to Markdown table formats.\nFeatures Converts the selected area on Spreadsheet to Markdown table format Supports multiple selections Usage Install this add-on from Google Workspace Marketplace.\nGenerateMarkdownTable - Google Workspace Marketplace Select one or more ranges in your spreadsheet.\nUse this add-on from add-ons menu.\nThe generated Markdown table format is displayed in a dialog.\nSupports multiple selections.\nSource Code michimani/gen-md-table: A Google Spreadsheet add-on that converts selected tables to Markdown table formats. If you have any comments or bug reports, please create an issue in the above repository.\nPrivacy Policy \u0026amp; Terms of Service Please read here.\nPrivacy Policy - GenerateMarkdownTable Terms of Service - GenerateMarkdownTable Logo ",
    "permalink": "https://michimani.net/projects/gen-markdown-table/about/",
    "title": "About GenerateMarkdownTable"
  },
  {
    "contents": " \u0026lt; About GenerateMarkdownTable We treat your privacy with respect and it is secured and will never be sold, shared or rented to third parties.\nTypes and methods of data to access This add-on only accesses your spreadsheet with read-only permissions. Specifically, it accesses the name of the spreadsheet, the sheet name, the selected range, and the values ​​contained in the cell.\nHow To Handle Your Data This add-on accesses the data (the values in each cell) on your spreadsheet. The data retrieved from your spreadsheet will be used only within the add-on, and will not be sent or stored externally without your approval. The retrieved data will be used to convert the data into Markdown table format, and the converted values will be displayed as a dialog on the spreadsheet. This is the main function of this add-on.\nInformation We Collect In operating our add-on, we may collect and process the following data about you:\nDetails of your visits to our website and the resources that you access, including, but not limited to, traffic data, location data, weblogs and other communication data. Information that you provide by filling in forms on our website, such as when you registered for information or make a purchase. Information provided to us when you communicate with us for any reason. ",
    "permalink": "https://michimani.net/projects/gen-markdown-table/privacy-policy/",
    "title": "Privacy Policy - GenerateMarkdownTable"
  },
  {
    "contents": " \u0026lt; About GenerateMarkdownTable Please read these Terms of Use (the \u0026ldquo;Terms\u0026rdquo;) carefully before using GenerateMarkdownTable (the \u0026ldquo;Add-on\u0026rdquo;).\nYour use of the Add-on is conditional upon your acceptance of and compliance with these Terms of Use. These Terms of Use apply to all users of the Add-on.\nBy using this Add-on, you agree to be bound by these Terms. If you do not agree to any part of the Terms, you may not use this Add-on.\ndisclaimer/warranty your use of this add-on is at your own risk. This Add-on is provided on an \u0026ldquo;AS IS\u0026rdquo; and \u0026ldquo;AS AVAILABLE\u0026rdquo; basis.\nNo warranty of any kind is given or implied with respect to the use of this add-on. If you have any problems with this Add-on, or if you have any suggestions for improving this Add-on, please contact me in the manner described below.\nChanges I reserve the right, at my sole discretion, to modify or replace these Terms of Use at any time. If I make any changes to these Terms of Use, I will notify you via the Website or Marketplace.\nContact us about GenerateMarkdownTable If you have any suggestions, requests, questions, or remarks, please feel free to contact me via email or GitHub issue.\nemail: michimani210@gmail.com GitHub: michimani/gen-md-table ",
    "permalink": "https://michimani.net/projects/gen-markdown-table/terms-of-service/",
    "title": "Terms of Service - GenerateMarkdownTable"
  },
  {
    "contents": "AWS Copilot で ECS で実行されるアプリケーションを管理する際、タスクロールもよしなに作ってくれます。今回は、そのタスクロールに任意の IAM ポリシーをアタッチしてみます。\n目次 概要 アプリケーションの実装 Copilot でアプリのセットアップ manifest.yml を作成 任意の IAM ポリシーを追加 デプロイ 確認 まとめ 概要 AWS Copilot で生成されるタスクロールに、任意の IAM ポリシーをアタッチします。\n前回のブログを書いた際に AWS の中の人からリプをもらったので、実際にやってみます。\nThank you 🙏 for the awesome blog post! You can also modify the task role using an Addons template https://t.co/Gvnq8YPw23\n\u0026mdash; Efe Karakus (@efekarakus) January 23, 2021 サンプルのアプリケーションとしては、 AWS アカウント内の S3 バケット名リストを返すような API を Go で実装してみます。その際に、最近 GA になった SDK for Go v2 も使ってみます。\nアプリケーションの実装 まずは、 S3 バケット名のリストを返すような API を実装します。フレームワークとして echo を使って、 AWS SDK は v2 を使います。\nlabstack/echo: High performance, minimalist Go web framework package main import ( \u0026#34;context\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; \u0026#34;github.com/labstack/echo/v4\u0026#34; ) type HelloResponse struct { Message string `json:\u0026#34;message\u0026#34;` } type BucketsResponse struct { Buckets []string `json:\u0026#34;buckets\u0026#34;` } func main() { e := echo.New() e.GET(\u0026#34;/\u0026#34;, hello) e.GET(\u0026#34;/buckets\u0026#34;, buckets) e.Logger.Fatal(e.Start(\u0026#34;:1323\u0026#34;)) } func hello(c echo.Context) error { return c.JSON(http.StatusOK, HelloResponse{ Message: \u0026#34;Hello ECS!\u0026#34;, }) } func buckets(c echo.Context) error { region := os.Getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;) if region == \u0026#34;\u0026#34; { c.Echo().Logger.Error(\u0026#34;Environment variable AWS_DEFAULT_REGION is undefined.\u0026#34;) return c.JSON(http.StatusInternalServerError, nil) } cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(region)) if err != nil { c.Echo().Logger.Error(\u0026#34;Failed to laod AWS deault config.\u0026#34;, err.Error()) return c.JSON(http.StatusInternalServerError, nil) } s3client := s3.NewFromConfig(cfg) out, err := s3client.ListBuckets(context.TODO(), \u0026amp;s3.ListBucketsInput{}) if err != nil { c.Echo().Logger.Error(\u0026#34;Failed to list buckets.\u0026#34;, err.Error()) return c.JSON(http.StatusInternalServerError, nil) } var buckets []string for _, b := range out.Buckets { buckets = append(buckets, aws.ToString(b.Name)) } return c.JSON(http.StatusOK, BucketsResponse{ Buckets: buckets, }) } エンドポイントは 2 つ用意しています。\nGET / : {\u0026quot;message\u0026quot;:\u0026quot;Hello ECS!\u0026quot;} GET /buckets : {\u0026quot;buckets\u0026quot;:[\u0026quot;bucket1\u0026quot;,...,\u0026quot;bucket999\u0026quot;]} SDK for Go v2 の話を少し書いておくと、v2 では各クライアントの生成部分とメソッド実行時の引数 (第1引数に context を受け取るようになった) が変わっています。その他、パッケージの構成やメソッドについても細かい変更があるようです。\n例えば *string を string に変換する際、v1 では aws.StringValue() を使っていましたが、 v2 では aws.ToString() を使うようになっています。\nsdk · pkg.go.dev Copilot にデプロイする際の Dockerfile は下記内容で準備します。\nFROM golang:1.15.5-alpine3.12 as build ENV GOPATH= ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main ENTRYPOINT [ \u0026#34;/main\u0026#34; ] Copilot でアプリのセットアップ manifest.yml を作成 copilot init コマンドで manifest.yml を作成します。前回と違って今回はワークロードに Load Balanced Web Service を選択します。また、 Port にはコンテナ側で待ち受けるポート番号を指定します。今回であれば 1323 で echo のサーバーを起動しているので 1323 を指定します。\n$ copilot init ... Use existing application: No Application name: list-s3-buckets Workload type: Load Balanced Web Service Service name: list-s3-buckets-app Dockerfile: ./Dockerfile no EXPOSE statements in Dockerfile ./Dockerfile Port: 1323 ... この時点で下記のようなディレクトリ構成になっています。\n. ├── Dockerfile ├── copilot │ └── list-s3-buckets-app │ └── manifest.yml ├── go.mod ├── go.sum └── main.go 任意の IAM ポリシーを追加 今回のアプリケーションでは、 S3 のバケット一覧を取得する権限が必要になります。 IAM のアクション名でいうと s3:ListAllMyBuckets です。このアクションを許可する IAM ポリシーを作成して、 Copilot によって生成されるタスクロールにアタッチします。\nそのために、 copilot/list-s3-buckets-app 配下に addons ディレクトリを作成し、その中に CloudFormation のテンプレートファイルを作成します。今回であれば s3-list-bucket-policy.yml です。\n. ├── Dockerfile ├── README.md ├── copilot │ └── list-s3-buckets-app │ ├── addons │ │ └── s3-list-bucket-policy.yml │ └── manifest.yml ├── go.mod ├── go.sum └── main.go テンプレートファイルの内容は下記の通りです。\nParameters: App: Type: String Description: Your application\u0026#39;s name. Env: Type: String Description: The environment name for the service. Name: Type: String Description: The name of the service. Resources: ListBucketPolicy: Type: AWS::IAM::ManagedPolicy Properties: PolicyDocument: Version: 2012-10-17 Statement: - Sid: ListBucketActions Effect: Allow Action: - s3:ListAllMyBuckets Resource: \u0026#34;*\u0026#34; Outputs: ListBucketPolicyArn: Description: \u0026#34;The ARN of the ManagedPolicy to attatch to the task role.\u0026#34; Value: !Ref ListBucketPolicy CFn テンプレートを作成するうえで注意したいのは下記の 2 点です\nParameters として App , Env , Name を定義する 定義したリソースの ARN を Outputs で出力する その他、追加のリソースの定義については下記のドキュメントを参照してください。\nAdditional AWS Resources - AWS Copilot CLI デプロイ test 環境を作ってデプロイします。\n$ copilot env init --name test --profile default --default-config $ copilot deploy --env test 前回の Job の場合と違って、 Service のほうがデプロイに時間がかかる印象です。(10分弱)\n確認 デプロイされたアプリケーションの URL はデプロイ時にも出力されますが、再度確認するためには copilot svc show コマンドを実行します。\n$ copilot svc show Only found one service, defaulting to: list-s3-buckets-app About Application list-s3-buckets Name list-s3-buckets-app Type Load Balanced Web Service Configurations Environment Tasks CPU (vCPU) Memory (MiB) Port ----------- ----- ---------- ------------ ---- test 1 0.25 512 1323 Routes Environment URL ----------- --- test http://list-Publi-1JMT8L0BFYS47-390312034.ap-northeast-1.elb.amazonaws.com Service Discovery Environment Namespace ----------- --------- test list-s3-buckets-app.list-s3-buckets.local:1323 Variables Name Container Environment Value ---- --------- ----------- ----- COPILOT_APPLICATION_NAME list-s3-buckets-app test list-s3-buckets COPILOT_ENVIRONMENT_NAME \u0026#34; \u0026#34; test COPILOT_LB_DNS \u0026#34; \u0026#34; list-Publi-1JMT8L0BFYS47-390312034.ap-northeast-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT \u0026#34; \u0026#34; list-s3-buckets.local COPILOT_SERVICE_NAME \u0026#34; \u0026#34; list-s3-buckets-app httpie で確認してみます。\n$ http http://list-Publi-1JMT8L0BFYS47-390312034.ap-northeast-1.elb.amazonaws.com HTTP/1.1 200 OK Connection: keep-alive Content-Length: 25 Content-Type: application/json; charset=UTF-8 Date: Tue, 26 Jan 2021 14:21:40 GMT { \u0026#34;message\u0026#34;: \u0026#34;Hello ECS!\u0026#34; } $ http http://list-Publi-1JMT8L0BFYS47-390312034.ap-northeast-1.elb.amazonaws.com/buckets HTTP/1.1 200 OK Connection: keep-alive Content-Type: application/json; charset=UTF-8 Date: Tue, 26 Jan 2021 14:28:33 GMT Transfer-Encoding: chunked { \u0026#34;buckets\u0026#34;: [ \u0026#34;bucket_00001\u0026#34;, ... \u0026#34;bucket_99999\u0026#34; ] } CLI で http リクエストするなら HTTPie が便利 - michimani.net まとめ AWS Copilot で ECS で実行されるアプリケーションをデプロイする際に、作成されるタスクロールに任意の IAM ポリシーをアタッチしてみた話でした。\nこれどうするんかなーと思っていたところで解決案をもらったわけですが、めちゃくちゃ簡単に任意のポリシーをアタッチできました。まあ、ドキュメントにしっかりと書いてあったんですが。もし同じ内容でお困りの方がいたら参考にしてみてください。\n今回のコードは GitHub に置いています。\nhello-ecs/list-s3-buckets at master · michimani/hello-ecs ",
    "permalink": "https://michimani.net/post/aws-inject-custom-policy-to-ecs-task-role-using-copilot/",
    "title": "AWS Copilot で生成されるタスクロールに任意のポリシーをアタッチする"
  },
  {
    "contents": "前回はスケジュール実行されるタスクを ECS にデプロイしました。今回は、そのタスクを AWS CLI を使って手動で実行してみます。\n目次 ECS タスクの作成 実行に必要な情報を取得 タスク定義の ARN ネットーワーク情報 実行 コンテナを上書きして実行 まとめ ECS タスクの作成 これに関しては、前回の記事で AWS Copilot を使ってサクッと作ったものがあるのでそれを使います。\nAWS Copilot を使ってを使ってスケジュールされた ECS タスクをデプロイする - michimani.net 実行に必要な情報を取得 AWS CLI で ECS タスクを実行するには ecs run-task コマンドを使います。事前にわかっている情報として、下記の値は AWS CLI やマネコンからあらかじめ取得しておきます。\nクラスター名 ファミリー名 サブネットの ID セキュリティグループの ID ecs run-task のヘルプを見てもらうとオプションの数が割と多いとがわかります。\n$ aws ecs run-task help ... SYNOPSIS run-task [--capacity-provider-strategy \u0026lt;value\u0026gt;] [--cluster \u0026lt;value\u0026gt;] [--count \u0026lt;value\u0026gt;] [--enable-ecs-managed-tags | --no-enable-ecs-managed-tags] [--group \u0026lt;value\u0026gt;] [--launch-type \u0026lt;value\u0026gt;] [--network-configuration \u0026lt;value\u0026gt;] [--overrides \u0026lt;value\u0026gt;] [--placement-constraints \u0026lt;value\u0026gt;] [--placement-strategy \u0026lt;value\u0026gt;] [--platform-version \u0026lt;value\u0026gt;] [--propagate-tags \u0026lt;value\u0026gt;] [--reference-id \u0026lt;value\u0026gt;] [--started-by \u0026lt;value\u0026gt;] [--tags \u0026lt;value\u0026gt;] --task-definition \u0026lt;value\u0026gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] 今回は必要最低限のオプションで実行することを目的とします。なので、指定するオプションは下記のとおりです。\n--cluster : 対象のクラスターの名前 --task-definition : タスク定義の ARN --network-configuration : ネットワーク情報 (サブネット、セキュリティグループ) --launch-type : 起動タイプ --overrides : コンテナの上書き 事前にわかっている情報は変数に設定し、それ以外に必要な値を以降で取得していきます。\nCLUSTER_NAME=\u0026#34;hello-ecs-test-Cluster-XXXXXXXXXXXX\u0026#34; SUBNET_ID=\u0026#34;subnet-00a78dcxxxxxxxxxx\u0026#34; SG_ID=\u0026#34;sg-0600d2cxxxxxxxxxx\u0026#34; FAMILY_NAME=\u0026#34;hello-ecs-test-hello-ecs-job\u0026#34; タスク定義の ARN タスク定義は複数存在する場合があるので、最新リビジョンの ARN を取得します。\nその前に、 Family Name を取得します。\n$ TASK_DEF_ARN=$(aws ecs list-task-definitions \\ --family-prefix \u0026#34;${FAMILY_NAME}\u0026#34; \\ --query \u0026#34;reverse(taskDefinitionArns)[0]\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo \u0026#34;${TASK_DEF_ARN}\u0026#34; arn:aws:ecs:ap-northeast-1:000000000000:task-definition/hello-ecs-test-hello-ecs-job:4 ネットーワーク情報 事前にわかっているサブネットおよびセキュリティグループの ID をもと、 --network-configuration で指定する文字列を生成しておきます。\nNETWORK_CONFIG=\u0026#34;awsvpcConfiguration={subnets=[${SUBNET_ID}],securityGroups=[${SG_ID}],assignPublicIp=ENABLED}\u0026#34; 実行 必要な情報が準備できたので、実行します。\n$ aws ecs run-task \\ --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ --task-definition \u0026#34;${TASK_DEF_ARN}\u0026#34; \\ --network-configuration \u0026#34;${NETWORK_CONFIG}\u0026#34; \\ --launch-type FARGATE { \u0026#34;tasks\u0026#34;: [ { \u0026#34;attachments\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;6fd9a03c-2d49-41bf-afca-a883cce4f42f\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ElasticNetworkInterface\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;PRECREATED\u0026#34;, \u0026#34;details\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;subnetId\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;subnet-00a78dcxxxxxxxxxx\u0026#34; } ] } ], \u0026#34;availabilityZone\u0026#34;: \u0026#34;ap-northeast-1a\u0026#34;, \u0026#34;clusterArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:cluster/hello-ecs-test-Cluster-XXXXXXXXXXXX\u0026#34;, \u0026#34;containers\u0026#34;: [ { \u0026#34;containerArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:container/96bf19a1-2e9a-4922-af8b-9a9336ec3498\u0026#34;, \u0026#34;taskArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task/hello-ecs-test-Cluster-XXXXXXXXXXXX/d17f05dc06ed4cedxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;hello-ecs-job\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;000000000000.dkr.ecr.ap-northeast-1.amazonaws.com/hello-ecs/hello-ecs-job:783bf7b\u0026#34;, \u0026#34;lastStatus\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;networkInterfaces\u0026#34;: [], \u0026#34;cpu\u0026#34;: \u0026#34;0\u0026#34; } ], \u0026#34;cpu\u0026#34;: \u0026#34;256\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2021-01-23T14:47:41.760000+09:00\u0026#34;, \u0026#34;desiredStatus\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;group\u0026#34;: \u0026#34;family:hello-ecs-test-hello-ecs-job\u0026#34;, \u0026#34;lastStatus\u0026#34;: \u0026#34;PROVISIONING\u0026#34;, \u0026#34;launchType\u0026#34;: \u0026#34;FARGATE\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;512\u0026#34;, \u0026#34;overrides\u0026#34;: { \u0026#34;containerOverrides\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;hello-ecs-job\u0026#34; } ], \u0026#34;inferenceAcceleratorOverrides\u0026#34;: [] }, \u0026#34;platformVersion\u0026#34;: \u0026#34;1.3.0\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;taskArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task/hello-ecs-test-Cluster-XXXXXXXXXXXX/d17f05dc06ed4cedxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;taskDefinitionArn\u0026#34;: \u0026#34;arn:aws:ecs:ap-northeast-1:000000000000:task-definition/hello-ecs-test-hello-ecs-job:4\u0026#34;, \u0026#34;version\u0026#34;: 1 } ], \u0026#34;failures\u0026#34;: [] } 出力として、作成された実行タスクの情報が出力されます。 しばらくすると Slack に 「Hello ECS!!!」 というメッセージが届きます。\nコンテナを上書きして実行 今回実行しているアプリケーションは、実行時のコマンドラインオプション -m で Slack にポストするメッセージを指定することができます。このように、コンテナ実行時のコマンドなどを上書きしたい場合は --overrides オプションで JSON 形式の値を指定します。指定するのはファイルでもよいので、下記のコマンドで override.json を作成します。\n$ ONTAINER_NAME=$(aws ecs describe-task-definition \\ --task-definition \u0026#34;${TASK_DEF_ARN}\u0026#34; \\ --query \u0026#34;taskDefinition.containerDefinitions[0].name\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; cat \u0026lt;\u0026lt;EOF \u0026gt; overrides.json { \u0026#34;containerOverrides\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;${ONTAINER_NAME}\u0026#34;, \u0026#34;command\u0026#34;: [\u0026#34;-m\u0026#34;, \u0026#34;Hello Amazon DynamoDB !!!\u0026#34;] } ] } EOF 内容を確認しておきます。 (エラーが出なければ OK です)\n$ python3 -m json.tool ./overrides.json { \u0026#34;containerOverrides\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;hello-ecs-job\u0026#34;, \u0026#34;command\u0026#34;: [ \u0026#34;-m\u0026#34;, \u0026#34;Hello Amazon DynamoDB !!!\u0026#34; ] } ] } 作成した overrides.json を --overrides で指定して実行してみます。\n$ aws ecs run-task \\ --cluster \u0026#34;${CLUSTER_NAME}\u0026#34; \\ --task-definition \u0026#34;${TASK_DEF_ARN}\u0026#34; \\ --network-configuration \u0026#34;${NETWORK_CONFIG}\u0026#34; \\ --launch-type FARGATE \\ --overrides file://overrides.json まとめ AWS CLI で ECS のタスクを手動で実行してみた話でした。\n今回の手順で可変な部分は、おそらくコンテナ上書き時のコマンドくらいなので、何度も手動実行するような機会があるなら今回の手順をスクリプト化しておくと楽かもしれません。\n",
    "permalink": "https://michimani.net/post/aws-run-ecs-task-using-aws-cli/",
    "title": "AWS CLI を使って Amazon ECS のタスクを手動で実行する"
  },
  {
    "contents": "ECS でスケジュールされたタスクを AWS CLI で手動実行したい、でもそもそもそのタスクを用意するのが大変そう。ということで、昨年夏頃に発表されて秋には GA となった AWS Copilot を使ってスケジュールされたタスクを作成してみます。CLI での実行は次回。\n目次 AWS Copilot とは やること やってみる 1. スケジュール実行されるタスク (アプリケーション) の実装 2. AWS Copilot のインストール 3. AWS Copilot を使ってデプロイ まとめ ※追記 AWS Copilot とは GitHub の README には次のように説明されています。\nThe AWS Copilot CLI is a tool for developers to build, release and operate production ready containerized applications on Amazon ECS and AWS Fargate.\naws/copilot-cli コンテナ化されたアプリケーションを Amazon ECS で構築・リリース・運用するための CLI です、と。ECS を使ったアプリケーションの構築には、 VPC 作ったりタスク定義作ったりと、正直簡単ではないなという印象です。 AWS Copilot を使うとその辺の設定をよしなにやってくれます。\n既に公式ドキュメントやいろんな方のブログで触れられているので、詳細な使い方については下記の参考記事を参照してください。\nServices - AWS Copilot CLI AWS Copilot のご紹介 | Amazon Web Services ブログ Amazon ECSの新たなデプロイツールとなるAWS CopilotがGAに！ECS環境の構築が便利になるぞ！！ | Developers.IO AWS Copilot JobsでECSタスクの定期実行を行う やること 今回やることは次のとおりです。\nスケジュール実行されるタスク (アプリケーション) の実装 AWS Copilot のインストール AWS Copilot を使ってデプロイ やってみる では順番にやっていきます。\n1. スケジュール実行されるタスク (アプリケーション) の実装 まずはスケジュールされたタスクとして実行するアプリケーションの実装です。今回は、Slack のとあるチャンネルに 「Hello ECS!!!」 とポストするだけのアプリを Go で実装します。\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/url\u0026#34; \u0026#34;os\u0026#34; ) const ( iconEmoji string = \u0026#34;:mega:\u0026#34; userName string = \u0026#34;Hello ECS Job\u0026#34; channel string = \u0026#34;dev\u0026#34; ) type payload struct { Text string `json:\u0026#34;text\u0026#34;` IconEmoji string `json:\u0026#34;icon_emoji\u0026#34;` UserName string `json:\u0026#34;username\u0026#34;` Channel string `json:\u0026#34;channel\u0026#34;` } func getWebhookURL() (string, error) { webhookURL := os.Getenv(\u0026#34;SLACK_WEBHOOK_URL\u0026#34;) if webhookURL == \u0026#34;\u0026#34; { return \u0026#34;\u0026#34;, errors.New(\u0026#34;The environment value SLACK_WEBHOOK_URL is required.\u0026#34;) } return webhookURL, nil } func postToSlack(message, webhookURL string) error { p, err := json.Marshal(payload{ Text: message, IconEmoji: iconEmoji, UserName: userName, Channel: channel, }) if err != nil { return err } resp, err := http.PostForm(webhookURL, url.Values{\u0026#34;payload\u0026#34;: {string(p)}}) if err != nil { return err } defer resp.Body.Close() return nil } func main() { m := flag.String(\u0026#34;m\u0026#34;, \u0026#34;Hello ECS!!!\u0026#34;, \u0026#34;Message posted to Slack\u0026#34;) flag.Parse() webhookUrl, err := getWebhookURL() if err != nil { fmt.Println(\u0026#34;Failed to get Slack Webhook URL.\u0026#34;, err.Error()) return } if err := postToSlack(*m, webhookUrl); err != nil { fmt.Println(\u0026#34;Failed to post message to Slack.\u0026#34;, err.Error()) } } Slack の Webhook URL は環境変数 SLACK_WEBHOOK_URL から取得します。。 ポストするメッセージはデフォルトで 「Hello ECS!!!」 ですが、実行時に -m オプションで任意のメッセージを指定できるようにしています。\nCopilot でデプロイする際には Dockerfile が必要になるので、下記の内容で作成しておきます。\nFROM golang:1.15.5-alpine3.12 as build ADD . . RUN go build -o /main FROM golang:1.15.5-alpine3.12 COPY --from=build /main /main ENTRYPOINT [ \u0026#34;/main\u0026#34; ] 2. AWS Copilot のインストール 続いて AWS Copilot のインストールです。 macOS であれば Homebrew を使ってインストールできます。\n$ brew install aws/tap/copilot-cli $ copilot version version: v1.1.0, built for darwin 3. AWS Copilot を使ってデプロイ デプロイするには manifest.yml が必要になるので、まずはそれを作成し、その後デプロイします。\nmanifest.yml を作成 copilot init コマンドで manifest.yml を作成します。\n$ copilot init Welcome to the Copilot CLI! We\u0026#39;re going to walk you through some questions to help you get set up with an application on ECS. An application is a collection of containerized services that operate together. Application name: hello-ecs Workload type: Scheduled Job Job name: hello-ecs-job Dockerfile: ./Dockerfile Custom Schedule: */5 * * * * Your job will run at the following times: Every 5 minutes Would you like to use this schedule? Yes Ok great, we\u0026#39;ll set up a Scheduled Job named hello-ecs-job in application hello-ecs running on the schedule */5 * * * *. ✔ Created the infrastructure to manage services and jobs under application hello-ecs. ✔ Wrote the manifest for job hello-ecs-job at copilot/hello-ecs-job/manifest.yml Your manifest contains configurations like your container size and job schedule (*/5 * * * *). ✔ Created ECR repositories for job hello-ecs-job. All right, you\u0026#39;re all set for local development. Deploy: No No problem, you can deploy your service later: - Run `copilot env init --name test --profile default --app hello-ecs` to create your staging environment. - Update your manifest copilot/hello-ecs-job/manifest.yml to change the defaults. - Run `copilot job deploy --name hello-ecs-job --env test` to deploy your job to a test environment. この時点で下記のようなディレクトリ構成になっています。\n$ . ├── Dockerfile ├── copilot │ └── hello-ecs-job │ └── manifest.yml └── main.go 今回、アプリケーション内では Slack の Webhook URL を環境変数から取得するようにしています。タスク実行時に環境変数を設定するため、 Webhook URL の値を SSM パラメータストアに登録しておきます。\n$ aws ssm put-parameter \\ --name /test/slack-webhook \\ --value \u0026#34;https://hooks.slack.com/services/XXXXXXXXXXX\u0026#34; \\ --type String \\ --tags Key=copilot-environment,Value=test Key=copilot-application,Value=hello-ecs ここでタグをしているのは、後のデプロイによって生成されるタスク実行ロールが下記のようなポリシーを持つからです。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;ssm:ResourceTag/copilot-environment\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;ssm:ResourceTag/copilot-application\u0026#34;: \u0026#34;hello-ecs\u0026#34; } }, \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:GetParameters\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ssm:ap-northeast-1:000000000000:parameter/*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;secretsmanager:ResourceTag/copilot-application\u0026#34;: \u0026#34;hello-ecs\u0026#34;, \u0026#34;secretsmanager:ResourceTag/copilot-environment\u0026#34;: \u0026#34;test\u0026#34; } }, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:GetSecretValue\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:secretsmanager:ap-northeast-1:000000000000:secret:*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:kms:ap-northeast-1:000000000000:key/*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } SSM パラメータストア、及び SecretManager から値を取得して環境変数に設定する場合、それらのリソースには copilot-application と copilot-environment をタグ名として、それぞれ値を設定しておく必要があります。\nそして、 copilot/hello-ecs-job/manifest.yml を下記のように修正します。\n- #secrets: # Pass secrets from AWS Systems Manager (SSM) Parameter Store. - # GITHUB_TOKEN: GITHUB_TOKEN # The key is the name of the environment variable, the value is the name of the SSM parameter. + secrets: # Pass secrets from AWS Systems Manager (SSM) Parameter Store. + SLACK_WEBHOOK_URL: \u0026#34;/test/slack-webhook\u0026#34; # The key is the name of the environment variable, the value is the name of the SSM parameter. これについては Copilot のドキュメントにも書かれています。\nSecrets - AWS Copilot CLI デプロイ 準備が整ったので、 test 環境にデプロイしてみます。\n$ copilot job deploy --name hello-ecs-job --env test ✘ get environment test configuration: couldn\u0026#39;t find environment test in the application hello-ecs 環境が無いらしい。ということで作ります。\n$ copilot env init --name test --profile default --default-config ✔ Proposing infrastructure changes for the hello-ecs-test environment. - Creating the infrastructure for the hello-ecs-test environment. [rollback complete] [37.1s]0s] The following resource(s) failed to create: [InternetGateway, Cluster, VPC]. Rollback requested by user. - An IAM Role for AWS CloudFormation to manage resources [not started] - An ECS cluster to group your services. [delete complete] [2.1s] Resource creation cancelled - An IAM Role to describe resources in your environment [not started] - A security group to allow your containers to talk to each other [not started] - An Internet Gateway to connect to the public internet [delete complete] [14.4s] Resource creation cancelled - Private subnet 1 for resources with no internet access [not started] - Private subnet 2 for resources with no internet access [not started] - Public subnet 1 for resources that can access the internet [not started] - Public subnet 2 for resources that can access the internet [not started] - A Virtual Private Cloud to control networking of your AWS resources [delete complete] [2.1s] The maximum number of VPCs has been reached. (Service: AmazonEC2; Stat us Code: 400; Error Code: VpcLimitExceeded; Request ID: 9571d86e-a7e6- 496b-ae4f-5a4797c1c7a0; Proxy: null) ✘ stack hello-ecs-test did not complete successfully and exited with status ROLLBACK_COMPLETE The maximum number of VPCs has been reached.\nVPC の数が上限に達したらしいです\u0026hellip;なので、不要な VPC を削除して再度実行します。\n$ copilot env init --name test --profile default --default-config ... ... ✔ Created environment test in region ap-northeast-1 under application hello-ecs. test 環境が作成されたので、あらためてデプロイします。\n$ copilot job deploy --name hello-ecs-job --env test ... ... ✔ Deployed hello-ecs-job. このコマンドでは\nDockerfile を元にイメージをビルド\n-\u0026gt; ビルドしたイメージにタグ付け\n-\u0026gt; ECR に Push\n-\u0026gt; ECS のタスク定義を作成\nが実行されます。\n暫く待つと、スケジュールされたタイミングでタスクが実行されて Slack に通知が来ます。\nまとめ AWS Copilot を使ってスケジュールされた ECS タスクをデプロイしてみた話でした。 VPC 関連リソースの作成、タスク定義の作成、 ECR リポジトリの作成など、 ECS + Fargate でタスクを実行する際に必要なリソースをほぼ意識せずに簡単に作成できるのは良いなと思いました。一方で、カスタマイズした設定、例えばタスクロールにカスタマイズした Role を使いたいとかは実現が難しそうです。タスクロールのカスタマイズについては、調べている中で下記のツイートを見つけたので、参考にしたいと思います。\n「スタンドアロンのタスクを起動できる Role」を例のタグ付きで別に作っておいて、そのロールを「Copilot が ECS タスク用に作ったロールから Assume できるようにしてあげる」と、Copilot タスクから別の ECS タスクを呼び出すのいけました！(ちょっとめんどくさい) pic.twitter.com/2TI1Z2JL6g\n\u0026mdash; ポジティブな Tori (@toricls) October 23, 2020 今回のコードは GitHub に置いてます。\nmichimani/hello-ecs: A sample that uses AWS Copilot to deploy a scheduled task to Amazon ECS. The sample app posts a message to Slack. 次回は、今回作成したスケジュールされたタスクを、 AWS CLI を使って任意のタイミングで実行してみます。\n※追記 このブログが AWS でコンテナ関連の開発をしている方に 見つかって 見ていただいて、下記のリプを頂きました。\nThank you 🙏 for the awesome blog post! You can also modify the task role using an Addons template https://t.co/Gvnq8YPw23\n\u0026mdash; Efe Karakus (@efekarakus) January 23, 2021 どうやら copilot/{service-name}/addons ディレクトリに CFn テンプレートを置くことで対応できそうです。\nAdditional AWS Resources - AWS Copilot CLI まさか中の人からリプが飛んでくるとは思ってなかったのですが、有益な情報をいただけて嬉しいです。ありがとございます！\n",
    "permalink": "https://michimani.net/post/aws-deploy-ecs-scheduled-job-using-copilot/",
    "title": "AWS Copilot を使ってスケジュールされた ECS タスクをデプロイする"
  },
  {
    "contents": "昨年の re:Invent で、Lambda のデプロイパッケージとしてコンテナイメージがサポートされるようになったと発表がありました。今回は、 Go で実装した Lambda 関数をコンテナイメージとしてデプロイしてみます。\n目次 概要 やってみる 1. Go で実装 2. Dockerfile 作成・ビルド 3. ローカルで実行 4. ECR Repository を作成・Push 5. ECR のイメージから Lambda 関数を作成 まとめ 概要 S3 バケット名リストを取得して返却する Lambda 関数を Go で実装し、コンテナイメージにして ECR に Push、そのイメージを Lambda にデプロイします。 サンプルコードは GitHub に置いているので、こっちを見てもらえばだいたい分かると思います。\nmichimani/go-lambda-sample: This is a sample that implements the AWS Lambda function in Go language and deploys it as a container image. やってみる 手順としては下記の通りです。\nGo で実装 Dockerfile 作成・ビルド ローカルで実行 ECR Repository を作成・Push ECR のイメージから Lambda 関数を作成 今回はマネジメントコンソールを使わずに AWS CLI で諸々操作していきます。\n手順については下記の公式ドキュメントを参考にしています。\nDeploy Go Lambda functions with container images - AWS Lambda 1. Go で実装 まずは Go で Lmabda 関数の処理を実装します。SDK は v1 です。\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; runtime \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/s3\u0026#34; ) type Response struct { Message string `json:\u0026#34;message\u0026#34;` BucketList []string `json:\u0026#34;bucket_list\u0026#34;` } func listBuckets(c *s3.S3) ([]string, error) { out, err := c.ListBuckets(\u0026amp;s3.ListBucketsInput{}) if err != nil { return nil, err } var list []string for _, b := range out.Buckets { list = append(list, aws.StringValue(b.Name)) } return list, nil } func handleRequest() (Response, error) { if os.Getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;) == \u0026#34;\u0026#34; { return Response{}, errors.New(\u0026#34;\u0026#39;AWS_DEFAULT_REGION\u0026#39; is required.\u0026#34;) } region := os.Getenv(\u0026#34;AWS_DEFAULT_REGION\u0026#34;) s3sess := session.Must(session.NewSession(\u0026amp;aws.Config{ Region: aws.String(region), })) s3client := s3.New(s3sess) buckets, err := listBuckets(s3client) if err != nil { return Response{Message: fmt.Sprintf(\u0026#34;An error occurred: %s\u0026#34;, err.Error())}, nil } return Response{ Message: \u0026#34;Success!\u0026#34;, BucketList: buckets, }, nil } func main() { runtime.Start(handleRequest) } Go で Lambda 関数を実装する場合、 handleRequest() を main() 内で runtime.Start() に渡す形にします。1\nhandleRequest() では引数としてコンテキストとリクエストを受け取りますが、今回は特に何か受け取ることはないので引数なしにしてます。\n2. Dockerfile 作成・ビルド Dockerfile は下記のような内容で作成します。\nFROM public.ecr.aws/lambda/provided:al2 as build RUN yum install -y golang RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main FROM public.ecr.aws/lambda/provided:al2 COPY --from=build /main /main COPY entry.sh / RUN chmod 755 /entry.sh ENTRYPOINT [ \u0026#34;/entry.sh\u0026#34; ] ENTRYPOINT として指定している entry.sh は下記です。\n#!/bin/sh if [ -z \u0026#34;${AWS_LAMBDA_RUNTIME_API}\u0026#34; ]; then exec /usr/local/bin/aws-lambda-rie \u0026#34;$@\u0026#34; else exec \u0026#34;$@\u0026#34; fi 環境変数 AWS_LAMBDA_RUNTIME_API が設定されていない場合、つまりローカル実行時には RIE (Runtime Interface Emulator) を使って main を実行します。\n上記の Dockerfile でビルドします。\n$ docker build -t go-lambda-sample . ちなみに、 provided:al2 ではなく alpine をベースイメージとする場合は、下記の内容になります。\nFROM alpine as build RUN apk add go git RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main FROM alpine COPY --from=build /main /main ADD https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie /usr/bin/aws-lambda-rie RUN chmod 755 /usr/bin/aws-lambda-rie COPY entry.sh / RUN chmod 755 /entry.sh ENTRYPOINT [ \u0026#34;/entry.sh\u0026#34; ] 3. ローカルで実行 下記のコマンドでローカル実行します。\n$ docker run \\ --rm \\ -p 9000:8080 \\ -e AWS_DEFAULT_REGION=\u0026#34;ap-northeast-1\u0026#34; \\ -e AWS_ACCESS_KEY_ID=\u0026#34;************\u0026#34; \\ -e AWS_SECRET_ACCESS_KEY=\u0026#34;************\u0026#34; \\ go-lambda-sample-ed-1:latest /main 今回のサンプルコード内では Amazon S3 にアクセスしているので、必要な Role を持った Credential が必要になります。なので、環境変数 AWS_ACCESS_KEY_ID と AWS_SECRET_ACCESS_KEY に値を設定して実行しています。また、 AWS_DEFAULT_REGION も必要なのでこれも設定してます。\nLambda で実行する場合、これらは不要です。権限については Lambda にアタッチする IAM Role で、 デフォルトリージョンについては Lambda 関数を作成したリージョンが設定されます。\n上記コマンドを実行し、ターミナルの別セッションで invoke 用のエンドポイントにアクセスすると、実装した Lambda 関数を実行できます。\n$ curl -XPOST \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; -d \u0026#39;{}\u0026#39; -o out.json ## レスポンス確認 $ cat out.json | jq . { \u0026#34;message\u0026#34;: \u0026#34;Success!\u0026#34;, \u0026#34;bucket_list\u0026#34;: [ ... ] } また、 docker run したターミナルでは標準出力に Lambda 実行時のログが出力されます。\nSTART RequestId: d0ac5bfb-10f6-4941-b7b4-dbd653eb9f2a Version: $LATEST END RequestId: d0ac5bfb-10f6-4941-b7b4-dbd653eb9f2a REPORT RequestId: d0ac5bfb-10f6-4941-b7b4-dbd653eb9f2a Init Duration: 0.38 ms Duration: 515.25 ms Billed Duration: 600 ms Memory Size: 3008 MB Max Memory Used: 3008 MB START RequestId: 8267a9ea-d234-4c04-a730-70049eefbdd8 Version: $LATEST END RequestId: 8267a9ea-d234-4c04-a730-70049eefbdd8 REPORT RequestId: 8267a9ea-d234-4c04-a730-70049eefbdd8 Duration: 283.04 ms Billed Duration: 300 msMemory Size: 3008 MB Max Memory Used: 3008 MB ちなみに、ベースイメージに alpine を使用して RIE をイメージに含めなかった場合は、ローカルマシンに RIE をインストールして docker run 時にエントリーポイントとしてローカルの RIE を指定することでも実行可能です。各自で RIE をインストールする必要はありますが、イメージサイズを小さくしたい場合はこの方法が良さそうです。\nRIE を含めない場合の Dockerfile は下記のような内容で。\nFROM alpine as build RUN apk add go git RUN go env -w GOPROXY=direct ADD go.mod go.sum ./ RUN go mod download ADD . . RUN go build -o /main FROM alpine COPY --from=build /main /main ENTRYPOINT [ \u0026#34;/main\u0026#34; ] ## aws-lambda-rie のインストールとセットアップ $ mkdir -p ~/.aws-lambda-rie \\ \u0026amp;\u0026amp; curl -Lo ~/.aws-lambda-rie/aws-lambda-rie \\ https://github.com/aws/aws-lambda-runtime-interface-emulator/releases/latest/download/aws-lambda-rie \\ \u0026amp;\u0026amp; chmod +x ~/.aws-lambda-rie/aws-lambda-rie ## ローカル実行 $ docker run \\ --rm \\ --entrypoint /aws-lambda/aws-lambda-rie \\ -v ~/.aws-lambda-rie:/aws-lambda \\ -p 9000:8080 \\ -e AWS_DEFAULT_REGION=\u0026#34;ap-northeast-1\u0026#34; \\ -e AWS_ACCESS_KEY_ID=\u0026#34;************\u0026#34; \\ -e AWS_SECRET_ACCESS_KEY=\u0026#34;************\u0026#34; \\ go-lambda-sample:latest /main 4. ECR Repository を作成・Push まずは ECR のリポジトリを作成します。\n$ aws ecr create-repository \\ --repository-name go-lambda-sample \\ --region ap-northeast-1 続いて、ビルドしたイメージにタグを追加して、 ECR にログイン、 Push します。\n$ AWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text ) \\ ## タグ追加 $ docker tag go-lambda-sample:latest \u0026#34;${AWS_ACCOUNT_ID}\u0026#34;.dkr.ecr.ap-northeast-1.amazonaws.com/go-lambda-sample:latest ## ECR にログイン $ aws ecr get-login-password --region ap-northeast-1 \\ | docker login \\ --username AWS \\ --password-stdin \u0026#34;${AWS_ACCOUNT_ID}\u0026#34;.dkr.ecr.ap-northeast-1.amazonaws.com ## Push $ docker push \u0026#34;${AWS_ACCOUNT_ID}\u0026#34;.dkr.ecr.ap-northeast-1.amazonaws.com/go-lambda-sample:latest 5. ECR のイメージから Lambda 関数を作成 $ IMAGE_URI=\u0026#34;${AWS_ACCOUNT_ID}.dkr.ecr.ap-northeast-1.amazonaws.com/go-lambda-sample:latest\u0026#34; $ aws lambda create-function \\ --function-name \u0026#34;go-lambda-sample\u0026#34; \\ --package-type \u0026#34;Image\u0026#34; \\ --code \u0026#34;ImageUri=${IMAGE_URI}\u0026#34; \\ --timeout 30 \\ --role \u0026#34;\u0026lt;iam-role-arn\u0026gt;\u0026#34; \\ --region ap-northeast-1 \u0026lt;iam-role-arn\u0026gt; にはあらかじめ作成した Role の ARN を指定します。今回のサンプルであれば CloudWatchLogs に加えて S3 への Read 権限があれば十分です。\nlambda create-function コマンドには --runtime オプションがありますが、コンテナイメージから作成する場合は指定は不要です。指定した場合、下記のエラーになります。\nAn error occurred (InvalidParameterValueException) when calling the CreateFunction operation: The Runtime parameter is not supported for functions created with container images. まとめ Go で実装した Lambda 関数をコンテナイメージとしてデプロイしてみた話でした。\n関数をコンテナイメージで用意できて、 RIE を使ってローカルで Lambda 関数として実行できるが良さげです。今回はただローカルで実行しただけなので、他のサービス (DynamoDB とか) と合わせたローカルでのテストとかはやり方考えたいと思います。\n\u0026ldquo;github.com/aws/aws-lambda-go/lambda\u0026rdquo; にエイリアスを付けているのは、 \u0026ldquo;github.com/aws/aws-sdk-go/service/lambda\u0026rdquo; が存在するからです。まあ、今回は使ってないのでエイリアス付けなくてもいいんですが\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-deploy-lambda-function-as-container-image/",
    "title": "Go で実装した Lambda 関数をコンテナイメージとしてデプロイする"
  },
  {
    "contents": "昨年に引き続き、今年も自分のインプットとアウトプット事情について振り返ってみます。\n2020 年の目標にしていたこと 2019 年の 12 月にはこんな記事を書いていました。\nその最後で\nAWS 認定に関しては Professional レベル 2 種類に合格できるよう頑張ります。\n少し出遅れた感じはありますが、機械学習に関する技術についても勉強していきたいと思っています。\nもっとサーバレスやりたい。\nと書いてます。\nこれらの目標 (と言っても明確に目標だと宣言はしていないのですが) が達成できたのかどうか、振り返ってみます。\nインプット事情 2020 年は、年始直後からコロナの影響でオフラインの勉強会やイベントがなくなり、代わりにオンラインで開催されるようになりました。また、仕事も 3 月くらいからは在宅勤務になりました。その結果、時間や場所の制約を受けることがほぼなくなり、去年より多くのイベントに参加することができました。\nオンラインイベントへの参加 主に参加していたのは下記のようなイベントです。\nJAWS-UG 初心者支部 JAWS-UG の初心者支部です。 AWS の初心者、イベント参加の初心者がとても参加しやすいイベントです。今年になって初めて参加したんですが、結果的に初心者支部のイベントで登壇デビューもすることになりました。AWS 関連のイベントに参加してみたい！という方はぜひこの初心者支部から参加してみてはいかがでしょうか。\nJAWS-UG初心者支部 - connpass JAWS-UG CLI専門支部 JAWS-UG の CLI 専門支部です。ほぼ隔週で AWS の何かしらのサービスを AWS CLI で操作するハンズオンを開催していただいており、可能な限り参加していました。その結果、この一年で AWS CLI のことがだいぶ好きになった気がします。\nJAWS-UG CLI専門支部 - connpass JAWS-UG朝会 通勤がなくなり、またイベントの開催場所もオンラインになったおかげで、始業前のイベントに参加することもできるようになりました。 JAWS-UG 朝会は、文字通り朝に開催されるイベントです。毎回ラジオ体操から始まるイベントで、始業前に頭も体もリフレッシュできます。個人的に朝は強いので、月一回ですが、この時間帯に開催していただけるのはとてもありがたかったです。\nJAWS-UG朝会 - connpass 新時代のサーバーレス AWSJ の西谷さん ( @keisuke69 ) が個人で開催されているイベントで、サーバーレス関連で西谷さんが興味を持っている領域や、 AWS のサーバーレス関連のアップデートに関するお話を聞くことができます。一口にサーバーレスと言っても、 Lambda や API Gateway の話だけではなく、最近は JAMStack とか SSR とか、個人的にあまり触れる機会のないフロントエンド寄りの話も聞くことができるのが嬉しいです。また、毎回質問にも丁寧に答えていただけるのもありがたいです。\n新時代のサーバーレス - connpass 西谷さんはこれ以外にも キャンプ好きエンジニアMeetup というイベントも主催されていて、キャンパー() としてはこちらのイベントも気になるところです。一度参加しましたが、キャンプ x エンジニアリング の話がどれも興味深かったです。\nキャンプ好きエンジニアMeetup - connpass Serverless Community (JP) ServerlessConf Tokyo および Serverless Meetups in Japan の実行組織でもある Serverless Community (JP) のイベントです。特定のパブリッククラウドに閉じた話ではなく、サーバーレスという領域全体の話を聞くことができるイベントです。 AWS に特化した回があったり、 GCP に特化した回があったり、 Algoria だけの回があったりと、サーバーレス領域のさまざまな情報を得ることができるのが嬉しいです。\nServerless - connpass 他にも、 Forkwell さんが主催している Infra Study Meetup や Front-End Study といったイベントも盛り上がっていました。(盛り上がっています)\nまた、オンラインで 24 時間開催された JAWS SONIC 2020 も印象的でした。\nさらに、クラスメソッドさんが毎年開催されている技術イベント Developers.IO も今年はオンラインでの開催でした。オフラインでの開催時にはエンジニアさんの登壇力の高さに圧倒されていましたが、オンライン開催になった今年は動画作成能力の高さにも圧倒されました。\nオフラインイベントとの違い 今年はほぼオンラインでのイベントにしか参加していませんが、個人的に感じるオンライン開催であることのメリットデメリットを書いておきます。\nメリット 時間と場所に制約がない 動画か後で公開される場合が多い 途中参加でも後追いで見れる 参加できなくなってもあとから見れる イベント参加後の復習にすぐ着手できる 会場から家までの移動時間がないので、帰って疲れて寝てしまうということがない デメリット 会場の温度感がわからない 情報が欠落する場合がある 音声や映像が乱れる場合がある とは言っても、やっぱりメリットのほうが大きいかなと思ってます。オンラインで開催していただいている主催者の方々には感謝です。\nアウトプット事情 アウトプットとしては、引き続きこのブログと、あと今年はイベントでの登壇デビューも果たしました。LT ですが。この 2 つについて振り返ります。\n去年より記事数が増えた 一昨年は 48 本、去年は 54 本と記事数は増えていましたが、今年は更に増えて 90 本 書いていました。(この記事含まず)\n100 本っていう数字は途中から意識しましたが、結局到達できませんでした。ちなみに、後述するイベント登壇で一緒になったクラスメソッドの Batchi さん ( @batchicchi ) は 100 本書かれていました。しかも、毎回すごい濃い内容の記事なので、凄いとしか言えません。ご自身でも振り返りされています。\n某ブログの会社に JOIN して 354 日でブログ 100 本書いてみた | Developers.IO 自分の話に戻ると、今年はイベント参加が多く、その参加レポートを書いたことで記事数が伸びたというのも要因の一つです。インプット事情で書いたイベントについては、下記の通り参加レポート書いてきました。\nJAWS-UG 初心者支部 JAWS-UG 初心者支部#31 「監視編 サーバーのモニタリングの基本を学ぼう」 JAWS-UG 初心者支部＆千葉支部#26 新人さん歓迎！ハンズオン\u0026amp;amp;LT(オンライン) に参加してきました JAWS-UG 初心者支部#24 サーバレスハンズオン勉強会の宿題をやってみた JAWS-UG 初心者支部#22 Fin-JAWSコラボ＆ミニハンズオン会 に行ってきました JAWS-UG CLI専門支部 #173R S3基礎 (レプリケーション) #172R S3基礎 (オブジェクト) #171R S3基礎 通知 (Lambdaの自動実行) #170R S3基礎 ライフサイクル #169R S3基礎 バージョニング #168R S3基礎 Webサイト\u0026amp;amp;ログ #167R EventBridge入門 Serverless Community (JP) Serverless Meetup Japan Virtual #5 の参加メモ Serverless Meetup Japan Virtual #3 の参加メモ Serverless Meetup Japan Virtual #2 の参加メモ Developers.IO Developers.IO 2020 CONNECT というイベントについて JAWS SONIC 2020 Amplify Console を使って Hugo で作ったサイトをデプロイする Moto で CDK プロジェクト内の Lambda 関数をテストする イベントの参加レポートを書くことで、自分の復習になるのはもちろん、主催者や登壇された方に感想が伝えられるのと、継続的にレポートを書いていると同じイベント参加者の方にも認知していただけるという効果があります。(最後のは CLI 専門支部での話ですが)\nまた、 JAWS SONIC 2020 に関するアウトプットに関しては、イベント Tシャツ獲得にも繋がりました。\nなんと！追加で選んでいただき JAWS SONIC 2020 のTシャツをゲットしました！ありがとうございます！\nめっちゃ派手🦈🌈#jawssonic2020 #jawsug pic.twitter.com/RiygVh9KH5\n\u0026mdash; よっしー Lv.859 | michimani (@michimani210) September 20, 2020 相変わらずブログの内容としては個人のメモレベルを抜け出せていないですが、書いた記事に対するリアクションも去年より増えた気がするので引き続き継続して書いていけたらなと思ってます。\nあ、このブログ以外に Qiita でも一つ記事を書きました。会社の Advent Calendar の一記事として書いたのですが、 CLI 専門支部で得た知見を元に作ったハンズオン資料みたいになってます。結構時間かけて書いた割には反応が薄かった (公開日が土曜日だったからだと思いたい) ので、もしこの記事を読んでいる方はこの気持を救うために LGTM してもらえると喜びます。\nイベントで登壇した もう一つのアウトプットとして、今年はイベントでの登壇デビューを果たしました。といってもオンラインイベントの LT 枠ですが、参加者 100 人くらいのイベントで 10 分も話せたのは大きな進歩かと思います。\n登壇に関するポエムについては下記の記事で書いてます。\nJAWS-UG 初心者支部＆千葉支部#26 新人さん歓迎！ハンズオン\u0026amp;amp;LT(オンライン) に参加してきました 要約すると、とにかく楽しかったなという感想です。イベントのハッシュタグでツイートを掘ると、少しですが感想も頂いていて、それを見るのが楽しかったです。また機会があればやりたいと思いつつ年末になってしまったので、来年は朝会とか CLI 専門支部とかで何か話すのを目標にします。\nまとめ 2020 年の自分のインプット・アウトプット事情について振り返ってみました。\nここで、もう一度 2019 年の最後に書いていた目標を確認してみます。\nAWS 認定に関しては Professional レベル 2 種類に合格できるよう頑張ります。\n少し出遅れた感じはありますが、機械学習に関する技術についても勉強していきたいと思っています。\nもっとサーバレスやりたい。\n結果として、上の 2 つについては全く達成できずでした。サーバーレスに関しては、関連するイベントに参加して情報はキャッチアップできているかなと思います。が、手を動かす量は全然足りてないです。\nとは言え、ブログの継続とかイベントでの登壇とか、アウトプットに関しては去年より成長した部分があったので、そこは良かったと思います。\n今年は 11 月に転職して環境が大きく変わったので、来年はもっとアプリケーション寄り、というかアーキテクチャとかの内容、あとはプロジェクトの進め方とかそのへんの知見を深めたいなと思います。あとは、 Go 言語をメインで触ることになったのでもっと Go の勉強します。\nということで色々あった 2020 年ももう終わりです。来年も頑張りましょう。\n",
    "permalink": "https://michimani.net/post/other-retrospect-in-2020/",
    "title": "2020 年の自分のインプット・アウトプット事情について振り返ってみる"
  },
  {
    "contents": "AWS SDK for Go の RC 版が公開されたので、CloudWatch Logs と DynamoDB について、以前に v1 で操作したときとどのように変わったのか確認してみます。\n※ 2020/12/26 時点での情報です\n※ RC 版ということなので正式版になるまでに変更が加えられる可能性があります\n目次 概要 使ってみる CloudWatch Logs DynamoDB まとめ v1 で触ってみた記事はこちら。\n概要 AWS SDK for Go v2 の RC 版が公開されました。\nAWS SDK for Go version 2 (v2) – Release Candidate | AWS Developer Blog Developer Preview が公開されたのは 2017/12/21 みたいなので、約 3 年の間に熟された結果の RC 版です。\nAWS SDK for Go 2.0 Developer Preview | AWS Developer Blog 使ってみる AWS Developer Blog に従って準備していきます。\n$ mkdir go-sdk-v2-rc-demo $ cd go-sdk-v2-rc-demo $ go mod init go-sdk-v2-rc-demo 今回は CloudWatch Logs と DynamoDB をさわるので、それぞれのディレクトリも作っておきます。\n$ mkdir -p cwlogs dynamodb $ tree . ├── cwlogs ├── dynamodb └── go.mod 2 directories, 1 file AWS SDK for Go v2 のドキュメントはこちらです。\nsdk · pkg.go.dev v2 を利用する場合、 import 文は下記のようになります。\nimport ( \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/cloudwatchlogs/types\u0026#34; ) 今回は CloudWatch Logs と DynamoDB について実装しますが、ここではその一部分のみについて書いています。その他の実装部分についてはこのリポジトリを見てください。\nCloudWatch Logs 前回と同じように、ロググループの作成からストリームの作成、ログイベントの Put までやってみます。\nまずクライアントを生成するところから違ってます。\nconfig, err := config.LoadDefaultConfig(context.TODO()) if err != nil { log.Fatalln(err) } client := cloudwatchlogs.NewFromConfig(config) あとは、各サービスの関数実行時に、第一引数に context.Context を渡すようになってます。例えば、ロググループを作成する部分だと下記のような感じ。\nin := cloudwatchlogs.CreateLogGroupInput{ LogGroupName: aws.String(name), } _, cerr := client.CreateLogGroup(context.TODO(), \u0026amp;in) if cerr != nil { return cerr } DynamoDB テーブルの作成、項目の追加、普通のスキャン、特定の属性のみ取得するスキャンを試します。\nv1 からの違いとして、定数に当たるものは別のパッケージとして分割されています。例えば、テーブルを作成するときの実装は下記のようになります。\nfunc createTable(client *dynamodb.Client, name string) error { describeIn := dynamodb.DescribeTableInput{ TableName: aws.String(name), } _, err := client.DescribeTable(context.TODO(), \u0026amp;describeIn) if err == nil { // table exists return nil } in := dynamodb.CreateTableInput{ TableName: aws.String(name), KeySchema: []types.KeySchemaElement{ { AttributeName: aws.String(\u0026#34;Name\u0026#34;), KeyType: types.KeyTypeHash, }, { AttributeName: aws.String(\u0026#34;CreatedAt\u0026#34;), KeyType: types.KeyTypeRange, }, }, AttributeDefinitions: []types.AttributeDefinition{ { AttributeName: aws.String(\u0026#34;Name\u0026#34;), AttributeType: types.ScalarAttributeTypeS, }, { AttributeName: aws.String(\u0026#34;CreatedAt\u0026#34;), AttributeType: types.ScalarAttributeTypeN, }, }, BillingMode: types.BillingModePayPerRequest, } if _, err := client.CreateTable(context.TODO(), \u0026amp;in); err != nil { return err } return nil } dynamodb.CreateTableInput の KeySchema フィールドは、 v1 では []*KeySchemaElement だったものが v2 では []types.KeySchemaElement という感じで別パッケージ types 1 の型を使うようになっています。\nまた、 KeySchema 内で指定する各属性の KeyType も types パッケージの KeyTypeHash などを使用するようになっています。(CloudWatch Logs でもその様になっている部分がありました)\nあとは、 Go の型と DynamoDB の Attribute との変換については、 attributevalue.MarshalMap を使って下記のように書けます。\ntype SmapleItem struct { Name string `json:\u0026#34;name\u0026#34;` CreatedAt int64 `json:\u0026#34;created_at\u0026#34;` Message1 string `json:\u0026#34;message1\u0026#34;` Message2 string `json:\u0026#34;message2\u0026#34;` Message3 string `json:\u0026#34;message3\u0026#34;` } item := SmapleItem{ Name: fmt.Sprintf(\u0026#34;Sample Item %d\u0026#34;, n), CreatedAt: time.Now().UnixNano(), Message1: fmt.Sprintf(\u0026#34;This is a sample message %d-1\u0026#34;, n), Message2: fmt.Sprintf(\u0026#34;This is a sample message %d-2\u0026#34;, n), Message3: fmt.Sprintf(\u0026#34;This is a sample message %d-3\u0026#34;, n), } if err := putItem(client, tableName, item); err != nil { log.Fatal(err) } func putItem(client *dynamodb.Client, tableName string, item SmapleItem) error { av, err := attributevalue.MarshalMap(item) if err != nil { return err } in := dynamodb.PutItemInput{ TableName: aws.String(tableName), Item: av, } _, perr := client.PutItem(context.TODO(), \u0026amp;in) if perr != nil { return err } fmt.Println(item.Name) return nil } めっちゃ便利！と思ったんですが、これ v1 でもあったみたいですね。やりながら気づきました。\nまとめ AWS SDK for Go v2 の RC 版が公開されたので触ってみた話でした。\n今回はざっくりサンプル実装してみた感じなので、 v1 とどの辺が変わったかはもう少し時間を書けて見てみたいと思います。\n※ 2020/12/26 時点での情報です\n※ RC 版ということなので正式版になるまでに変更が加えられる可能性があります\nmichimani/aws-sdk-for-go-v2-sample: This repository is a collection of sample implementations using the AWS SDK for Go v2. types · pkg.go.dev \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-use-aws-sdk-for-go-v2/",
    "title": "AWS SDK for Go v2 の RC 版が公開されたのでとりあえず触ってみる"
  },
  {
    "contents": "シェルスクリプトをキレイにするためのツールとして shellcheck と shfmt というものの存在を教えてもらったので、先日作った ACM の Certificate 発行スクリプトに対してそれらを実行し、キレイにしていきたいと思います。\n目次 概要 shellcheck shfmt shellcheck で文法チェック インストール 実行 shfmt でフォーマット インストール 実行 まとめ 概要 shellcheck と shfmt というツールを使って、シェルスクリプトをキレイにします。ここで言う キレイに とは、文法上の問題をなくしたり、想定外の実行結果にならないようにしたり、見た目を整えたりすることを指します。\n今回使う shellcheck と sh というツールについても簡単に紹介しておきます。\nshellcheck shellcheck (koalaman/shellcheck) は、対象のシェルスクリプトに対して文法上の問題や想定外な実行結果が起きないように、問題点を指摘してくれるツールです。指摘してくれるポイントとしては、 GitHub の README に次のように書かれています。\nTo point out and clarify typical beginner\u0026rsquo;s syntax issues that cause a shell to give cryptic error messages. To point out and clarify typical intermediate level semantic problems that cause a shell to behave strangely and counter-intuitively. To point out subtle caveats, corner cases and pitfalls that may cause an advanced user\u0026rsquo;s otherwise working script to fail under future circumstances. ざっくり翻訳すると\nわかりにくいエラーメッセージを出すような初歩的な記述の問題点 想定と異なる動作をするような、中級レベルの問題点 将来的に動作しなくなるような、またあまり起こらないような微妙なケース、落とし穴などの上級レベルの問題点 について指摘してくれます。\nクライアントにインストールして使用することもできますし、 Web 上ではシェルスクリプトを貼り付けるだけでチェックしてくれます。\nShellCheck – shell script analysis tool koalaman/shellcheck: ShellCheck, a static analysis tool for shell scripts shfmt shfmt は mvdan/sh リポジトリに含まれるツールで、対象のシェルスクリプトをフォーマットしてくれる Go 製のツールです。\nmvdan/sh: A shell parser, formatter, and interpreter with bash support; includes shfmt shellcheck で文法チェック では、早速 shellcheck で文法チェックしてみます。\n今回はローカルマシン (macOS) にインストールして実行してみます。\nインストール brew install でインストールできます。\n$ brew install shellcheck . . . 🍺 /usr/local/Cellar/shellcheck/0.7.1: 7 files, 9.0MB $ shellcheck -V ShellCheck - shell script analysis tool version: 0.7.1 license: GNU General Public License, version 3 website: https://www.shellcheck.net 実行 以前作成した ACM の Certificate を発行するスクリプト を issue-acm-certificate.sh という名前で保存しておいて、そのファイルに対して shellcheck で文法チェックをかけます。\n$ shellcheck issue-acm-certificate.sh In issue-acm-certificate.sh line 5: if [ $# != 3 ] || [ $1 = \u0026#34;\u0026#34; ] || [ $2 = \u0026#34;\u0026#34; ] || [ $3 = \u0026#34;\u0026#34; ]; then ^-- SC2086: Double quote to prevent globbing and word splitting. ^-- SC2086: Double quote to prevent globbing and word splitting. ^-- SC2086: Double quote to prevent globbing and word splitting. Did you mean: if [ $# != 3 ] || [ \u0026#34;$1\u0026#34; = \u0026#34;\u0026#34; ] || [ \u0026#34;$2\u0026#34; = \u0026#34;\u0026#34; ] || [ \u0026#34;$3\u0026#34; = \u0026#34;\u0026#34; ]; then In issue-acm-certificate.sh line 24: --domain-name ${TARGET_DOMAIN} \\ ^--------------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --domain-name \u0026#34;${TARGET_DOMAIN}\u0026#34; \\ In issue-acm-certificate.sh line 26: --region ${REGION} \\ ^-------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --region \u0026#34;${REGION}\u0026#34; \\ In issue-acm-certificate.sh line 36: --certificate-arn ${CERT_ARN} \\ ^---------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --certificate-arn \u0026#34;${CERT_ARN}\u0026#34; \\ In issue-acm-certificate.sh line 38: --region ${REGION} \\ ^-------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --region \u0026#34;${REGION}\u0026#34; \\ In issue-acm-certificate.sh line 44: --certificate-arn ${CERT_ARN} \\ ^---------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --certificate-arn \u0026#34;${CERT_ARN}\u0026#34; \\ In issue-acm-certificate.sh line 46: --region ${REGION} \\ ^-------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --region \u0026#34;${REGION}\u0026#34; \\ In issue-acm-certificate.sh line 56: if [ $VALIDATION_RECORD_NAME == $NONE ] || [ $VALIDATION_RECORD_VALUE == $NONE ] || [ $HOSTED_DOMAIN == $NONE ]; then ^---------------------^ SC2086: Double quote to prevent globbing and word splitting. ^----------------------^ SC2086: Double quote to prevent globbing and word splitting. ^------------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: if [ \u0026#34;$VALIDATION_RECORD_NAME\u0026#34; == $NONE ] || [ \u0026#34;$VALIDATION_RECORD_VALUE\u0026#34; == $NONE ] || [ \u0026#34;$HOSTED_DOMAIN\u0026#34; == $NONE ]; then In issue-acm-certificate.sh line 63: --hosted-zone-id ${HOSTED_ZONE_ID} \\ ^---------------^ SC2086: Double quote to prevent globbing and word splitting. Did you mean: --hosted-zone-id \u0026#34;${HOSTED_ZONE_ID}\u0026#34; \\ In issue-acm-certificate.sh line 82: if [ $? == 0 ]; then ^-- SC2181: Check exit code directly with e.g. \u0026#39;if mycmd;\u0026#39;, not indirectly with $?. For more information: https://www.shellcheck.net/wiki/SC2086 -- Double quote to prevent globbing ... https://www.shellcheck.net/wiki/SC2181 -- Check exit code directly with e.g... \u0026hellip;いっぱい出ましたね。と言ってもほぼ同じ内容に関する指摘のようです。\nSC2086: Double quote to prevent globbing and word splitting. まずこの指摘は、 glob や文字列の分割を防ぐためにダブルクオーテーションで囲みましょう というものです。例えば、下記のスクリプトは S3 バケットに Cache-Control 付きのオブジェクトを Put するスクリプトです。\nCACHE_CONTROL=\u0026#34;public, max-age=1209600\u0026#34; aws s3api put-object \\ --bucket sample-bucket \\ --key sample.txt \\ --body ./sample.txt \\ --cache-control $CACHE_CONTROL このシェルスクリプト (s3-put-object.sh) を実行すると、下記のようにエラーとなります。\n$ sh s3-put-object.sh usage: aws [options] \u0026lt;command\u0026gt; \u0026lt;subcommand\u0026gt; [\u0026lt;subcommand\u0026gt; ...] [parameters] To see help text, you can run: aws help aws \u0026lt;command\u0026gt; help aws \u0026lt;command\u0026gt; \u0026lt;subcommand\u0026gt; help Unknown options: max-age=1209600 これは、変数 CACHE_CONTROL がそのまま展開されて、 AWS CLI の s3api put-object コマンドのオプション内で分割された文字列が指定されたことによるエラーです。このようなことにならないためにも、変数をダブルクオーテーションで囲みましょうという指摘です。\nshellcheck には各指摘に関する詳細ページが用意されているので、そちらで内容及び修正方法を確認することができます。\nSC2086 · koalaman/shellcheck Wiki SC2181: Check exit code directly with e.g. \u0026lsquo;if mycmd;\u0026rsquo;, not indirectly with $?. これは、スクリプトの終了コードは $? を使わずに直接チェックしましょうという指摘です。詳細ページでは $? = 0 で判定するのが冗長だと書かれています。 この指摘に対する対応としては、今回はコマンドを関数にして、その関数の実行を判定することにしてみます。例えば、 Certificate をリクエストする部分を次のように書き換えます。\nrequest_certificate () { # request certificate echo \u0026#34;Request certificate for \u0026#39;${TARGET_DOMAIN}\u0026#39; to ACM.\u0026#34; CERT_ARN=$( \\ aws acm request-certificate \\ --domain-name \u0026#34;${TARGET_DOMAIN}\u0026#34; \\ --validation-method DNS \\ --region \u0026#34;${REGION}\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; sleep 5 \\ \u0026amp;\u0026amp; echo -e \u0026#34;\\t CERT_ARN = ${CERT_ARN}\u0026#34; } if ! request_certificate; then echo \u0026#34;Failed to request certificate.\u0026#34; exit 1 fi SC2181 · koalaman/shellcheck Wiki shfmt でフォーマット 続いて、 shfmt を使ってシェルスクリプトをフォーマットしてみます。\nインストール README に従って、下記のコマンドでインストールします。\n$ GO111MODULE=on go get mvdan.cc/sh/v3/cmd/shfmt $ shfmt -version v3.2.1 実行 色々オプションがありますが、今回は フォーマットを行ったファイルのファイル名を出力する -l と、フォーマット結果を標準出力ではなくファイルに書き出す -w を指定しています。それ以外のオプションについては shfmt -h とかで実行すれば確認できます。\n$ shfmt -l -w issue-acm-certificate.sh issue-acm-certificate.sh 確認してみると、インデントとか諸々調整されててキレイにフォーマットされていました。\nまとめ shellcheck と shfmt というツールを使って、シェルスクリプトをキレイにした話でした。\n最終的には下記のようなシェルスクリプトになったので、過去との差分については Revisions を参照してください。\n",
    "permalink": "https://michimani.net/post/development-check-and-format-shell-script/",
    "title": "shellcheck と shfmt を使ってシェルスクリプトをキレイにしてみる"
  },
  {
    "contents": "はてなブログで書いていたブログを Hugo で作り直したいと思い、 Go の勉強も兼ねて簡単な移行ツールを作ってみました。\n目次 概要 経緯 AtomPub を利用して記事を参照する 参照用エンドポイントにアクセスして XML を取得 Go で XML をパースする 完成したもの 特徴 使い方 まとめ 概要 はてなブログに投稿していた記事を Hugo 用の Markdown ファイルとしてエクスポートするツールを作りました。\n経緯 このブログとは別に、趣味のバイクに関する記事をはてなブログで書いています。(最後の更新は一年以上前ですが\u0026hellip;)\nはてなブログに移行する前は WordPress で書いていたため、独自ドメインをそのまま使うためにはてなブログ Pro を契約して使っています。そのはてなブログ Pro の更新が来年の 1月末ということで、それまでに移行したいなと思っていたのが経緯です。\nAtomPub を利用して記事を参照する はてなブログには AtomPub を利用して記事の参照、投稿、編集、削除を行うことができます。\nAtomPub を利用するには OAuth 認証、WSSE認証、Basic認証 のいずれかの方法で認証を行う必要がありますが、今回は Basic 認証で利用してみます。 Basic 認証の ID は はてなID で、 パスワードは API キー を使用します。この API キー については、はてなブログ管理画面の 設定 \u0026gt; 詳細設定 の AtomPub APIキー で確認することができます。\nAtomPub の詳しい仕様については下記ページに書かれているので、ここでは記事の参照についてのみ触れます。\nはてなブログAtomPub - Hatena Developer Center 参照用エンドポイントにアクセスして XML を取得 記事一覧を参照するには、下記のエンドポイントにアクセスします。\nhttps://blog.hatena.ne.jp/{はてなID}/{ブログID}/atom/entry/ このエンドポイントについては、はてなブログ管理画面の 設定 \u0026gt; 詳細設定 にある AtomPub ルートエンドポイント で確認できます。\nレスポンスは ContentType: application/atom+xml で返却されます。\n今回は Basic 認証を用いてアクセスするので、 Go では次のようにして XML を取得します。(import 文は省略してます)\nfunc main() { xmlData, err := getXML(next) if err != nil { fmt.Println(err) break } } func getXML(url string) (string, error) { req, err := http.NewRequest(\u0026#34;GET\u0026#34;, url, nil) if err != nil { return \u0026#34;\u0026#34;, err } var basicAuthZ string = generateBasicAuthZ() req.Header.Set(\u0026#34;Authorization\u0026#34;, \u0026#34;Basic \u0026#34;+basicAuthZ) client := new(http.Client) res, err := client.Do(req) if err != nil { return \u0026#34;\u0026#34;, err } if res.StatusCode != 200 { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;Failed to get xml. [%s]\u0026#34;, res.Status) } body, _ := ioutil.ReadAll(res.Body) defer res.Body.Close() return string(body), nil } func generateBasicAuthZ() string { raw := hatenaId + \u0026#34;:\u0026#34; + hatenaAPIKey bytes := []byte(raw) return base64.StdEncoding.EncodeToString(bytes) } 取得できる XML の内容は下記のようになっています。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;feed xmlns=\u0026#34;http://www.w3.org/2005/Atom\u0026#34; xmlns:app=\u0026#34;http://www.w3.org/2007/app\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;first\u0026#34; href=\u0026#34;{参照用のルートエンドポイント}\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;next\u0026#34; href=\u0026#34;{次のページの参照用エンドポイント}\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;{ブログタイトル}\u0026lt;/title\u0026gt; \u0026lt;subtitle\u0026gt;{ブログのサブタイトル}\u0026lt;/subtitle\u0026gt; \u0026lt;link rel=\u0026#34;alternate\u0026#34; href=\u0026#34;{ブログの URL}\u0026#34;/\u0026gt; \u0026lt;updated\u0026gt;2019-06-14T17:41:59+09:00\u0026lt;/updated\u0026gt; \u0026lt;author\u0026gt; \u0026lt;name\u0026gt;{はてな ID}\u0026lt;/name\u0026gt; \u0026lt;/author\u0026gt; \u0026lt;generator uri=\u0026#34;https://blog.hatena.ne.jp/\u0026#34; version=\u0026#34;4d0fe3f3cd3000000000020000000000\u0026#34;\u0026gt;Hatena::Blog\u0026lt;/generator\u0026gt; \u0026lt;id\u0026gt;hatenablog://blog/00000000000000000000\u0026lt;/id\u0026gt; \u0026lt;entry\u0026gt; ... \u0026lt;/entry\u0026gt; \u0026lt;entry\u0026gt; ... \u0026lt;/entry\u0026gt; \u0026lt;/feed\u0026gt; また、 \u0026lt;entry\u0026gt; の中は下記のようになっています。\n\u0026lt;entry\u0026gt; \u0026lt;id\u0026gt;tag:blog.hatena.ne.jp,2013:blog-{はてな ID}-00000000000000000000-00000000000000000\u0026lt;/id\u0026gt; \u0026lt;link rel=\u0026#34;edit\u0026#34; href=\u0026#34;{個別記事を参照するエンドポイント}\u0026#34;/\u0026gt; \u0026lt;link rel=\u0026#34;alternate\u0026#34; type=\u0026#34;text/html\u0026#34; href=\u0026#34;{記事の URL}\u0026#34;/\u0026gt; \u0026lt;author\u0026gt;\u0026lt;name\u0026gt;{はてな ID}\u0026lt;/name\u0026gt;\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;{記事タイトル}\u0026lt;/title\u0026gt; \u0026lt;updated\u0026gt;2020-12-22T00:03:46+09:00\u0026lt;/updated\u0026gt; \u0026lt;published\u0026gt;2020-12-21T21:00:58+09:00\u0026lt;/published\u0026gt; \u0026lt;app:edited\u0026gt;2020-12-22T00:03:46+09:00\u0026lt;/app:edited\u0026gt; \u0026lt;summary type=\u0026#34;text\u0026#34;\u0026gt;{記事の概要}\u0026lt;/summary\u0026gt; \u0026lt;content type=\u0026#34;text/x-markdown\u0026#34;\u0026gt; ... \u0026lt;/content\u0026gt; \u0026lt;hatena:formatted-content type=\u0026#34;text/html\u0026#34; xmlns:hatena=\u0026#34;http://www.hatena.ne.jp/info/xmlns#\u0026#34;\u0026gt; ... \u0026lt;/hatena:formatted-content\u0026gt; \u0026lt;category term=\u0026#34;{カテゴリ名1}\u0026#34; /\u0026gt; \u0026lt;category term=\u0026#34;{カテゴリ名2}\u0026#34; /\u0026gt; \u0026lt;app:control\u0026gt; \u0026lt;app:draft\u0026gt;yes\u0026lt;/app:draft\u0026gt; \u0026lt;/app:control\u0026gt; \u0026lt;/entry\u0026gt; これを Go の構造体に落とし込みます。\nGo で XML をパースする 今回は XML をパースするために、 encoding/xml パッケージを使用します。\nxml · pkg.go.dev パースする前に、 XML の構造に合わせて次のような構造体を定義します。\ntype Atom struct { Title string `xml:\u0026#34;title\u0026#34;` SubTitle string `xml:\u0026#34;subtitle\u0026#34;` Links []Link `xml:\u0026#34;link\u0026#34;` Entries []Entry `xml:\u0026#34;entry\u0026#34;` } type Entry struct { Links []Link `xml:\u0026#34;link\u0026#34;` Author string `xml:\u0026#34;author\u0026gt;name\u0026#34;` Title string `xml:\u0026#34;title\u0026#34;` Published string `xml:\u0026#34;updated\u0026#34;` Content string `xml:\u0026#34;content\u0026#34;` Summary string `xml:\u0026#34;summary\u0026#34;` Draft string `xml:\u0026#34;control\u0026gt;draft\u0026#34;` Categories []Category `xml:\u0026#34;category\u0026#34;` } type Category struct { Term string `xml:\u0026#34;term,attr\u0026#34;` } type Link struct { Rel string `xml:\u0026#34;rel,attr\u0026#34;` Href string `xml:\u0026#34;href,attr\u0026#34;` } xml:\u0026quot;title\u0026quot; のようにタグを指定することで、 XML 内の要素と関連付けることができます。また、 xml:\u0026quot;term,attr\u0026quot; のようにタグを指定することで、 XML の各要素の属性と関連付けることができます。\n実際にパースする部分は下記のとおりです。\nfunc main() { // ... atom := Atom{} xerr := xml.Unmarshal([]byte(xmlData), \u0026amp;atom) if xerr != nil { fmt.Println(xerr.Error()) break } fmt.Println(atom.Title) fmt.Println(atom.SubTitle) fmt.Println(\u0026#34;-----\u0026#34;) for _, entry := range atom.Entries { fmt.Println(entry.Title) } } 上記のような実装で、 XML の情報を構造体に落とし込むことができました。\nみちのえきまにあ SC59 CBR1000RRで道の駅巡りをしているまったりツーリングライダーのブログ ----- test 泊まりのツーリングで初めて車体トラブルが発生した話 SSTR 2019 に参加してきました！ . . . あとは、これらのデータをゴニョゴニョやって、下記のような Hugo 用の Markdown ファイルを生成します。\n--- title: \u0026#34;%s\u0026#34; date: %s draft: %s author: [\u0026#34;%s\u0026#34;] categories: [%s] archives: [\u0026#34;%s\u0026#34;, \u0026#34;%s\u0026#34;] description: \u0026#34;%s\u0026#34; url: \u0026#34;/%s\u0026#34; --- %s 完成したもの 特徴 README にも書いていますが、特徴を簡単に書いておきます。\nはてなブログのすべての記法 (Markdown/はてな記法/見たまま) に対応しています はてな記法で書かれた記事の下記の要素は、それぞれ HTML に変換されます リンク文字列 はてなブログカード Twitter 埋め込み はてなフォトライフの画像埋め込み 下書きの記事は draft: true として生成されます Permalink は url: \u0026quot;/entry/...\u0026quot; として生成されます 特に、各記事の URL を Permalink として生成しておくことで、 URL をそのまま引き継ぐことができるようになってます。\nただし注意点としては、Markdown ファイルとして生成するものの、中身は純粋な Markdown ではなく HTML も混じった状態で生成されます。そのため、 Hugo の設定ファイル config.toml に下記の記述を追加し、記事内で HTML 及び JavaScript のコードを生で記述しても解釈されるようにする必要があります。\n[markup] [markup.goldmark] [markup.goldmark.renderer] unsafe = true また、上記のはてな記法以外のタグ (Amazon の商品リンクなど) は変換されないので、手で直す必要があります。(今後対応するかもしれません)\n使い方 使い方は簡単で、上のリポジトリを clone してもらって docker build して docker run するだけです。\n詳しくは README を読んでください。\nhtn2hugo/README.md at master · michimani/htn2hugo まとめ はてなブログで書いた記事を Hugo に移行してみた話でした。\nGo の勉強も兼ねて作ってみたツールなので、書き方おかしいとかコード汚いとか全然動かないとかあれば issue、PR いただけると嬉しいです。\n",
    "permalink": "https://michimani.net/post/development-transition-hatena-blog-to-hugo/",
    "title": "はてなブログの記事を Hugo に移行するツールを作ってみた"
  },
  {
    "contents": "先日 re:Invent で AWS の新サービス AWS CloudShell が発表されました。マネジメントコンソール上で AWS CLI を利用できるサービスです。今回は、 CloudShell 上で AWS CLI v1 を使えるようにしてみます。小ネタです。\n目次 概要 やってみる デフォルトのバージョン確認 AWS CLI v1 用の環境を作成 AWS CLI v1 をインストール まとめ おまけ 概要 AWS CloudShell 上で AWS CLI v1 を使えるようにします。デフォルトでインストールされているのは AWS CLI v2 です。\nAWS CloudShell 自体のサービス概要についてはクラメソさんの記事でわかりやすくまとめられています。いつも最速のブログ化ありがとうございます！\n待望の新サービス AWS CloudShell がリリースされました！ #reinvent | Developers.IO やってみる では、やっていきます。\nデフォルトのバージョン確認 まずは CloudShell のデフォルトの AWS CLI バージョンを確認してみます。\n[cloudshell-user@ip-10-0-40-46 ~]$ aws --version aws-cli/2.0.58 Python/3.7.3 Linux/4.14.209-160.335.amzn2.x86_64 exec-env/CloudShell exe/x86_64.amzn.2 2.0.58 がインストールされています。なお、この記事を書いている 2020/12/17 時点での AWS CLI v2 の最新バージョンは 2.1.11 なので、少しラグがあるようです。\nAWS CLI v1 用の環境を作成 v1 を使えるようにするために、 Python の venv モジュールを使って仮想環境を作成します。CloudShell ではデフォルトで Python 3 系が使えるようになっているので、インストールは不要です。\n[cloudshell-user@ip-10-0-40-46 ~]$ python3 -V Python 3.7.9 今回は aws-cli-v1 という環境を作ります。\n[cloudshell-user@ip-10-0-40-46 ~]$ python3 -m venv ~/aws-cli-v1 作成した環境に入ります。環境に入ると、プロンプトに (aws-cli-v1) が表示されます。\n[cloudshell-user@ip-10-0-40-46 ~]$ source ~/aws-cli-v1/bin/activate (aws-cli-v1) [cloudshell-user@ip-10-0-40-46 ~]$ AWS CLI v1 をインストール 最後に AWS CLI v1 を pip でインストールします。\n(aws-cli-v1) [cloudshell-user@ip-10-0-40-46 ~]$ pip install awscli --upgrade ... Successfully installed PyYAML-5.3.1 awscli-1.18.198 botocore-1.19.38 colorama-0.4.3 docutils-0.15.2 jmespath-0.10.0 pyasn1-0.4.8 python-dateutil-2.8.1 rsa-4.5 s3transfer-0.3.3 six-1.15.0 urllib3-1.26.2 バージョンを確認します。\n(aws-cli-v1) [cloudshell-user@ip-10-0-40-46 ~]$ aws --version aws-cli/1.18.198 Python/3.7.9 Linux/4.14.209-160.335.amzn2.x86_64 exec-env/CloudShell botocore/1.19.38 v2 を利用したい場合は deactivate で環境から抜けます。\n(aws-cli-v1) [cloudshell-user@ip-10-0-40-46 ~]$ deactivate [cloudshell-user@ip-10-0-40-46 ~]$ aws --version aws-cli/2.0.58 Python/3.7.3 Linux/4.14.209-160.335.amzn2.x86_64 exec-env/CloudShell exe/x86_64.amzn.2 まとめ AWS CloudShell 上で AWS CLI v1 を使えるようにした話でした。ま、他の環境で v1/v2 を切り替える方法と同じですね。\nおまけ デフォルトでインストールされている AWS CLI v2 を最新バージョンにするには下記のコマンドを実行します。\n[cloudshell-user@ip-10-0-40-46 ~]$ curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; \\ \u0026amp;\u0026amp; unzip awscliv2.zip \\ \u0026amp;\u0026amp; sudo ./aws/install --update \\ \u0026amp;\u0026amp; rm -rf awscliv2.zip 途中で\nreplace aws/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: と聞かれた場合は A で進みます。\n実行が終わったらバージョンを確認します。\n[cloudshell-user@ip-10-0-40-46 ~]$ aws --version aws-cli/2.1.11 Python/3.7.3 Linux/4.14.209-160.335.amzn2.x86_64 exec-env/CloudShell exe/x86_64.amzn.2 prompt/off ",
    "permalink": "https://michimani.net/post/aws-use-aws-cli-v1-at-cloudshell/",
    "title": "AWS CloudShell で AWS CLI v1 を使う"
  },
  {
    "contents": "こんにちは。 弁護士ドットコム株式会社にてバックエンドエンジニアとして 電子契約サービス クラウドサイン の開発をしている @michimani です。今回は、個人的に最近よく触っている AWS CLI について書きます。\nはじめに この記事は 弁護士ドットコム Advent Calendar 2020 の 12 日目の記事です。\n昨日の記事は @happylifetaka さんの 「 VueUseについて調べてみました 」 でした。ぜひこちらの記事もご覧ください。\n私の記事のメインは AWS CLI を使ったハンズオン資料になっているので、休日や年末年始のお休みの際にぽちぽちやっていただけたら幸いです。\n目次 はじめに 目次 やること 長い前置き Hugo とは なぜ AWS CLI？ マネジメントコンソールでぽちぽち それ以外 AWS CLI やってみる 前提事項 作成するリソース ※手順の留意点 1. ACM で Certificate (SSL 証明書) を発行 1.1 証明書発行をリクエスト 1.2 ドメイン認証 2. CloudFront で CloudFrontOriginAccessIdentity を作成 3. Hugo で生成された静的ファイルを配置する S3 Bucket を作成 3.1 S3 Bucket を作成 3.2 バケットポリシーをアタッチ 4. CloudFront で Distribution を作成 5. Route 53 に RecordSet を作成 6. 設定したドメインでアクセスできるか確認 Hugo で生成したサイトをデプロイしてみる この構成を成長させる デプロイの自動化 URL の正規化 キャッシュの最適化 リソースの削除 1. S3 Bucket を削除 2. Route 53 の RecordSet を削除 3. CloudFront の Distribution を削除 4. CloudFront の CloudFrontOriginAccessIdentity を削除 5. ACM の Certificate とドメイン認証用の RecordSet を削除 まとめ やること 今回は、AWS CLI のみを使って Hugo のホスティング環境を構築してみます。構築していく過程で様々なサービスを AWS CLI を通して操作していきます。また、サブタイトルに 「はじめよう、技術ブログ」 と付けている通り、ホスティング環境を作ったらそのまま Hugo でブログを公開してアウトプットしていきませんか？というお誘いです。\n年末のこの時期になると、「来年こそは自分もアウトプットしないと\u0026hellip;」 と思う方が少なからずいらっしゃると思います。この記事を見て、環境を構築してみて、 Hugo でブログを始めてみるのはいかがでしょうか。(というのを Qiita で書いている)\nまた、AWS CLI は AWS で各サービス・リソースを扱う上でとても基本的な部分に触れることができるツールなので、既に AWS を使ってサービス運用しているという方はもちろん、これから AWS を使って何かやろうとしている方にもぜひ使っていただきたいツールです。個人的にも最近は AWS CLI で何かやる機会が増えてきたので、今回取り上げています。\nこのあと前置きがちょっと長くなっているので、すぐに環境構築に入りたい！という方は ここ まで飛んでください。\n長い前置き 実際に AWS CLI で Hugo のホスティング環境を構築するまでに、前置きとして Hugo や AWS CLI について少しだけ書いておきます。また、この記事では AWS CLI との比較として様々なサービスやツールについて触れていますが、それらを悪者にしようという意図はまったくありませんし、 AWS CLI が至高！という感じでもありません。\nHugo とは Hugo とは Go で実装されているオープンソースの静的サイトジェネレーターです。特徴としては、とにかくビルドがめちゃくちゃ速いです。\nビルドするサイトに適用するテーマの種類も豊富で、一般的なブログ、個人のポートフォリオ、コーポレートサイトなど、様々なタイプの Web サイトを構築することができます。\nThe world’s fastest framework for building websites | Hugo なぜ AWS CLI？ Hugo に限らず、 AWS 上で何かしらの環境構築をするには、 AWS CLI 以外に下記のような方法があります。\nマネジメントコンソールでぽちぽち AWS CloudFormation を使って構成管理 AWS CDK, AWS Chalice などの AWS 製ツール Terraform, Serverless Framework などの 3rd パーティ製ツール 上記については、 マネジメントコンソールでぽちぽち と それ以外 に大別できます。なので、その 2 つと AWS CLI との比較を簡単に書いておきます。\nマネジメントコンソールでぽちぽち AWS を触り始めると、まずはマネジメントコンソールにアクセスして色んなリソースを触ってみると思います。世に出ている手順書やブログ記事も、マネジメントコンソールのキャプチャを用いて説明されているものも多いです。\n初めて AWS に触れる際にはマネジメントコンソールを見ながら直感的に操作していくのはとてもわかりやすいと思いますが、 「あれ？このキャプチャ、実際の画面と違うんやけど\u0026hellip;」 「そんなボタン無いやん\u0026hellip;」 「どうすればええんや\u0026hellip;」 という感じで キャプチャと実際の画面の差異に戸惑ったことはないでしょうか？ というのも、 AWS のマネジメントコンソールの UI は日々変更が加わっていて、昨日なかった項目が今日追加されていることもあれば、逆に昨日あったボタンが今日なくなっているなんてこともあります。マネジメントコンソールでの操作はわかりやすい分、UI の変更に対応するのが大変という面があります。\nそれ以外 AWS CloudFormation (以下、CFn) や AWS CDK、 Terraform などを使った構成管理は、リソースのあるべき状態をコードで管理することができます。もちろん、リソースを操作する UI が変わっても影響はありません。その他、 Infrastructure as Code を実践することで得られるメリットについてはもう既に色んな場所で書かれていることなので詳しくは触れません。\nこれらのツールやサービスを利用するにあたって障壁となるのは、初期の学習コストです。 CFn で管理する場合はテンプレートの書き方を知る必要があります。 AWS CDK は各言語 (TypeScript が多いと思います) でリソースを定義できると言え、 CDK での書き方や、リソースによっては CFn での書き方を調べる必要が出てきます。Terraform も、 .tf ファイルの書き方を知る必要があります。 IaC するためにずっと同じツールやサービスを使い続けるのであれば問題ないですが、実際はプロジェクトやチーム、個人の趣味で色んなツールを使うことになると思うので、そのたびに学習コストが発生するのは辛いです。また、 3rd パーティのツールを使う場合は、そのツール自体のバージョンアップにも追従していく必要があります。特に AWS CDK に関しては GA となってからもアップデートスピードは非常に速く、破壊的変更が加えられることもあります。リソースの定義や管理がしやすい反面、実際のリソースに加えてツールのメンテナンスもしてくのは辛いポイントです。\nAWS CLI じゃあ AWS CLI は上記のような辛さは無いのか！と言われると、決してそんなことはありません。コマンドを毎回調べてリソースを一つずつ作成していくのは辛いと感じるときもあります。ただ、そんな辛さ以上に AWS CLI を触ることによって得られるメリットは大きいと思います。具体的には、次のような特徴・メリットがあります。\nサブコマンドが各サービスの API と (ほぼ) 1 対 1 で対応している aws \u0026lt;サービスを指定するコマンド\u0026gt; \u0026lt;API を指定するサブコマンド\u0026gt; の形 各リソースを操作するのにどんな API を実行すればよいかがわかる コマンドで確認できるヘルプが充実している 新サービス・新機能への追従が早い API と (ほぼ) 対になっているので、新しいサービスや機能も AWS CLI ならすぐ試すことができる v1 と v2 があるが、 v1 のほうが先に対応することが多い 長年大きな変化がないシェルの技術が元になっている シェル芸 (やりすぎ注意) 特に一つ目の特徴として上げている サブコマンドが各サービスの API と (ほぼ) 1 対 1 で対応している に関しては、 AWS を理解するためには凄く良い点です。 API は頻繁に変わるものではないので、過去に覚えた API (コマンド) をほぼずっと使っていくことができます。また、 API を理解することで CFn 等でリソースを定義する際に、実際に作成されるリソースのイメージもつきやすくなります。\nということで、今後の AWS との付き合いをより良いものにするために AWS CLI は凄く良いツールですよ、という話です。AWS CLI の良さについては色んな方がアウトプットされていますので、そちらもご覧ください。\n長く使えるAWSスキルを効率良く身に付けよう /20200912-jaws-sonic-awscli - Speaker Deck JAWS-UG CLI 専門支部を運営している波田野さんの JAWS SONIC 2020 での発表資料です 初心者だからこそ触りたい、AWS CLI ~ \u0026amp;ldquo;躓きやすい\u0026amp;quot;を無くしたい ~ #devio2020 - YouTube クラスメソッド北川さんの Developers.IO 2020 CONNECT での動画です また、個人的には最近 JAWS-UG CLI 専門支部のイベントに参加することが多いです。内容としては、毎回各サービスのとある機能を AWS CLI で操作するというハンズオンになっています。今年に入ってからはオンラインで開催されているので、 AWS CLI に興味のある方はチェックしてみてはいかがでしょうか。\nJAWS-UG CLI専門支部 - connpass やってみる 前置きがだいぶ長くなりましたが、ここからは実際に AWS CLI のみを使って Hugo のホスティング環境を構築していきましょう。いくつか前提事項があるので、もしその中で準備が済んでいないものがあればご用意ください。\n前提事項 AWS CLI の実行環境は macOS を想定 jq コマンドが使用可能であること macOS 上で AWS CLI のセットアップが完了していること v1/v2 どちらでも結構ですが、今回は v2 を使います (執筆時点の最新バージョンは 2.1.6 。公開時には上がっていると思います) macOS での AWS CLI バージョン 2 のインストール、更新、アンインストール - AWS Command Line Interface git コマンドが使用可能であること Hugo がインストールされていること macOS であれば brew install hugo でインストールできます Install Hugo | Hugo 利用可能なドメインが Amazon Route 53 で管理されていること HostedZone に登録されていること 作成するリソース 今回 Hugo のホスティング環境として下図のような構成を構築していきます。\n作成するリソースは下記のとおりです。(以降の手順内でもリソースを指す文脈では下記のリソース名を用いるようにしますが、ゆれがあったらよしなに読み解いてください\u0026hellip;)\nAWS Certificate Manager Certificate Amazon Route 53 RecordSet Amazon CloudFront Distribution CloudFrontOriginAccessIdentity Amazon S3 Bucket BucketPolicy 各リソース (構成要素) の作成手順は下記の通りなので、この順番で進めていきます。\nACM で Certificate (SSL 証明書) を発行 CloudFront で CloudFrontOriginAccessIdentity を作成 Hugo で生成された静的ファイルを配置する S3 Bucket を作成 CloudFront で Distribution を作成 Route 53 に RecordSet を作成 設定したドメインでアクセスできるか確認 ※手順の留意点 手順内のコマンドについては十分確認していますが、そのまま本番環境で運用できるようなものではないので個人の利用範囲に留めておいてください (記載ミスなどあれば編集リクエストください) 手順内で特に記述がない場合、リソースの作成先リージョンは 東京 (ap-northeast-1) とします 各コードブロックは単一のコマンドになっています お手元で実行する際は各コードブロックのコマンドを全てコピーして実行してください (Qiita のコードブロックって右上にコピーボタンついてたんですね。便利) 全てターミナルの同一セッションで実行してください (コマンド実行時に使用するパラメーターを変数に設定して利用するため) 手順の書き方については JASW-UG CLI 専門支部 のハンズオン資料を参考にさせていただいてます！ 1. ACM で Certificate (SSL 証明書) を発行 ACM で Certificate を発行するにはリクエストしたあとにドメイン承認する必要があるので、それぞれの手順を 1.1 と 1.2 に分けています。\n1.1 証明書発行をリクエスト まず最初に、 Hugo で作成したサイトのドメインに対する SSL 証明書を ACM (Amazon Certificate Manager) で発行します。今回は \u0026lt;your-domain\u0026gt; というドメインが Route 53 で管理されているとして、そのサブドメインの hugo.\u0026lt;your-domain\u0026gt; というサブドメインをサイトのドメインにしてみます。\nコマンド上で扱いやすいように、変数に代入しておきます。(以降の手順でも適宜 変数に代入してコマンドで使用する形をとります)\nHOSTED_DOMAIN=\u0026#34;\u0026lt;your-domain\u0026gt;\u0026#34; HUGO_DOMAIN=\u0026#34;hugo.${HOSTED_DOMAIN}\u0026#34; Certificate の発行には acm request-certificate コマンドを使用します。また、ここで発行される Certificate は後に CloudFront の Distribution にアタッチするので、 バージニア北部 (us-east-1) リージョンで作成する必要があります。\n次の手順であるドメイン認証の方法としては、メールによる認証と Route 53 に認証用の RecordSet を追加する方法がありますが、今回は後者で認証します。これを --validation-method オプションで指定します。\n実行結果として CertificateArn が出力されるので、以降の手順で使用するために変数に設定します。\nSSL_ARN=$( \\ aws acm request-certificate \\ --domain-name ${HUGO_DOMAIN} \\ --validation-method DNS \\ --region us-east-1 \\ --output text) \\ \u0026amp;\u0026amp; echo ${SSL_ARN} リクエストが完了すると ARN が出力されます。(************ はアカウント ID です)\narn:aws:acm:us-east-1:************:certificate/fd93963c-91da-4cda-abcd-1234xx9999xx もし下記のようなエラーが出た場合は、 Certificate のリクエスト上限に達しているので、上限緩和を申請するか、既存の Certificate を削除してください。\nAn error occurred (LimitExceededException) when calling the RequestCertificate operation (reached max retries: 2): Cannot request more certificates in this account. Contact Customer Service for details.\n発行ステータスを確認するには acm describe-certificate コマンドを使って、下記のように確認します。\naws acm describe-certificate \\ --certificate-arn ${SSL_ARN} \\ --query \u0026#34;Certificate.Status\u0026#34; \\ --output text \\ --region us-east-1 現時点ではまだドメイン認証がされていないため、実行結果としては PENDING_VALIDATION と出力されます。\n1.2 ドメイン認証 ドメイン認証に必要なレコードの情報はステータスの確認と同様に acm describe-certificate コマンドで確認できます。確認したあと Route 53 に登録する必要があるので、出力結果を変数に設定します。\nVLIDATION_RECORD_JSON=$( \\ aws acm describe-certificate \\ --certificate-arn ${SSL_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ResourceRecord\u0026#34; \\ --region us-east-1) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_NAME=$(echo ${VLIDATION_RECORD_JSON} | jq -r .\u0026#34;Name\u0026#34;) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_TYPE=$(echo ${VLIDATION_RECORD_JSON} | jq -r .\u0026#34;Type\u0026#34;) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_VALUE=$(echo ${VLIDATION_RECORD_JSON} | jq -r .\u0026#34;Value\u0026#34;) \\ \u0026amp;\u0026amp; echo \u0026#34; Name:\\t${VALIDATION_RECORD_NAME} Type:\\t${VALIDATION_RECORD_TYPE} Value:\\t${VALIDATION_RECORD_VALUE}\u0026#34; 実行結果として、ドメイン認証用のレコード情報が出力されます。\nName:\t_01bca14fa7d9xxxxxxxxxxxxxxxxxxxx.hugo.\u0026lt;your-domain\u0026gt;. Type:\tCNAME Value:\t_10c99aaddfd2xxxxxxxxxxxxxxxxxxxx.wggjkglgrm.acm-validations.aws. このレコード情報を Route 53 に登録するには route53 change-resource-record-set コマンドを使用します。その際に、対象のドメインの HostedZoneId が必要になるので、予め取得して変数に設定しておきます。使用するコマンドは route53 list-hosted-zones コマンドです。\nHOSTED_ZONE_ID=$( \\ aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;${HOSTED_DOMAIN}.\u0026#39;].Id\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo ${HOSTED_ZONE_ID} 実行結果として、対象のドメインの HostedZoneId が出力されます。\n/hostedzone/Z3911234567890 ドメイン認証に必要な情報が揃ったので、認証用のレコードを作成します。\naws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch \\ \u0026#34;{ \\\u0026#34;Changes\\\u0026#34;: [ { \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;CREATE\\\u0026#34;, \\\u0026#34;ResourceRecordSet\\\u0026#34;: { \\\u0026#34;Name\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_NAME}\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_TYPE}\\\u0026#34;, \\\u0026#34;TTL\\\u0026#34;: 300, \\\u0026#34;ResourceRecords\\\u0026#34;: [{\\\u0026#34;Value\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_VALUE}\\\u0026#34;}] } } ] }\u0026#34; 実行結果として、 RecordSet の変更情報が出力されます。\n{ \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C0573087HOZHOD5SURWO\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2020-12-02T14:12:55.563000+00:00\u0026#34; } } 実行時の出力では Status が Pending となっていますが、ほぼリアルタイムに変更が適用されます。\nしばらくしてから Certificate の発行ステータスを確認すると、 ISSUED になっていることが確認できます。\naws acm describe-certificate \\ --certificate-arn ${SSL_ARN} \\ --query \u0026#34;Certificate.Status\u0026#34; \\ --output text \\ --region us-east-1 ISSUED これで hugo.\u0026lt;your-domain\u0026gt; に対する Certificate の発行が完了しました。\n2. CloudFront で CloudFrontOriginAccessIdentity を作成 Amazon CloudFront では Distribution と CloudFrontOriginAccessIdentity というリソースを作成しますが、ここではまず CloudFrontOriginAccessIdentity を作成します。使用するコマンドは aws cloudfront create-cloud-front-origin-access-identity です。実行結果として作成された CloudFrontOriginIdentity の情報が出力されるので、その中から後で使用する Id を変数に設定します。\nCFOAI_ID=$( \\ aws cloudfront create-cloud-front-origin-access-identity \\ --cloud-front-origin-access-identity-config \\ CallerReference=\u0026#34;hugo-contents\u0026#34;,Comment=\u0026#34;OAI for Hugo bucket\u0026#34; \\ --query \u0026#34;CloudFrontOriginAccessIdentity.Id\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo ${CFOAI_ID} 実行結果として、作成された CloudFrontOriginAccessIdentity の Id が出力されます。\nE3NRXXXXXXXXXX 3. Hugo で生成された静的ファイルを配置する S3 Bucket を作成 Hugo で生成された静的ファイルを配置する S3 Bucket を作成します。バケットを作成したあとに、作成したバケットに対してバケットポリシーをアタッチする必要があるので、バケットの作成とポリシーのアタッチ それぞれを手順 3.1 と 3.2 とします。\n3.1 S3 Bucket を作成 作成するバケット名は \u0026lt;サイトのドメイン\u0026gt;-\u0026lt;AWS アカウント ID\u0026gt; という名前にします。アカウント ID を付けているのは、グローバルユニークな名前にするためです。(アカウント名を付けたら必ずユニークになるというわけではありません)\nAWS アカウント ID は sts get-caller-identity コマンドの実行結果に含まれているので、そこから抜き出します。\nAWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text ) \\ \u0026amp;\u0026amp; echo ${AWS_ACCOUNT_ID} 実行結果\n************ あとは、ドメイン名と結合したものをバケット名として変数に設定します。\nS3_BUCKET_NAME=\u0026#34;${HUGO_DOMAIN}-${AWS_ACCOUNT_ID}\u0026#34; echo ${S3_BUCKET_NAME} 実行結果\nhugo.\u0026lt;your-domain\u0026gt;-************ S3 Bucket を作成するには s3api create-bucket コマンドを使用します。今回 S3 Bucket は東京 (ap-northeast-1) リージョンに作成するので、 --create-bucket-configuration でリージョンを指定します。\naws s3api create-bucket \\ --bucket ${S3_BUCKET_NAME} \\ --create-bucket-configuration \u0026#34;LocationConstraint=ap-northeast-1\u0026#34; 実行結果\n{ \u0026#34;Location\u0026#34;: \u0026#34;http://hugo.\u0026lt;your-domain\u0026gt;-************.s3.amazonaws.com/\u0026#34; } 3.2 バケットポリシーをアタッチ 3.1 で作成した S3 Bucket に対しては、バケットポリシーを用いて次のようなアクセス制限を設定します。\nパブリックアクセスをすべてブロック CloudFront 経由で読み込み可能 2 つ目の制限を設定するために、先ほど作成した CloudFrontOriginAccessIdentity の Id が必要になります。\nバケットポリシーは JSON ファイルとして作成しておく必要があるので、下記のコマンドで bucket-policy.json を作成します。\ncat \u0026lt;\u0026lt; EOF \u0026gt; bucket-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity ${CFOAI_ID}\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::${S3_BUCKET_NAME}/*\u0026#34;, \u0026#34;arn:aws:s3:::${S3_BUCKET_NAME}\u0026#34; ] } ] } EOF 中身を確認します。\ncat ./bucket-policy.json 実行結果\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity E3NRXXXXXXXXXX\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::hugo.\u0026lt;your-domain\u0026gt;-************/*\u0026#34;, \u0026#34;arn:aws:s3:::hugo.\u0026lt;your-domain\u0026gt;-************\u0026#34; ] } ] } バケットポリシーのアタッチには s3api put-bucket-policy コマンドを使用します。\naws s3api put-bucket-policy \\ --bucket ${S3_BUCKET_NAME} \\ --policy file://bucket-policy.json 出力はなにもないので、 s3api get-bucket-policy コマンドで正しくアタッチされたかどうか確認しておきます。\naws s3api get-bucket-policy \\ --bucket ${S3_BUCKET_NAME} 実行結果\n{ \u0026#34;Policy\u0026#34;: \u0026#34;{\\\u0026#34;Version\\\u0026#34;:\\\u0026#34;2012-10-17\\\u0026#34;,\\\u0026#34;Statement\\\u0026#34;:[{\\\u0026#34;Sid\\\u0026#34;:\\\u0026#34;2\\\u0026#34;,\\\u0026#34;Effect\\\u0026#34;:\\\u0026#34;Allow\\\u0026#34;,\\\u0026#34;Principal\\\u0026#34;:{\\\u0026#34;AWS\\\u0026#34;:\\\u0026#34;arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity E3NRXXXXXXXXXX\\\u0026#34;},\\\u0026#34;Action\\\u0026#34;:[\\\u0026#34;s3:GetObject\\\u0026#34;,\\\u0026#34;s3:ListBucket\\\u0026#34;],\\\u0026#34;Resource\\\u0026#34;:[\\\u0026#34;arn:aws:s3:::hugo.\u0026lt;your-domain\u0026gt;-************/*\\\u0026#34;,\\\u0026#34;arn:aws:s3:::hugo.\u0026lt;your-domain\u0026gt;-************\\\u0026#34;]}]}\u0026#34; } ちょっと見にくいですが、アタッチできていることがわかります。\n4. CloudFront で Distribution を作成 Distribution を作成 と簡単に書いていますが、設定項目が非常に多いです。Distribution が持っている要素としては次のようなものがあります。\nAlias : カスタムドメインの情報 DefaultCacheBehavior : デフォルトの Behavior Origins : オリジンの情報 HttpVersion : 許可する HTTP のバージョン これら以外にも多くの設定項目がありますが、それらはコマンドのオプションで指定するのではなく、あらかじめ JSON ファイル (distribution-config.json) として作成しておきます。すべての項目を設定すると大変なので、省略可能なものは省略しています。\ncat \u0026lt;\u0026lt; EOF \u0026gt; distribution-config.json { \u0026#34;Aliases\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ \u0026#34;${HUGO_DOMAIN}\u0026#34; ] }, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;S3-${S3_BUCKET_NAME}\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;${S3_BUCKET_NAME}.s3.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;origin-access-identity/cloudfront/${CFOAI_ID}\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;S3-${S3_BUCKET_NAME}\u0026#34;, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;redirect-to-https\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 2, \u0026#34;Items\u0026#34;: [ \u0026#34;HEAD\u0026#34;, \u0026#34;GET\u0026#34; ], \u0026#34;CachedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 2, \u0026#34;Items\u0026#34;: [ \u0026#34;HEAD\u0026#34;, \u0026#34;GET\u0026#34; ] } }, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: false, \u0026#34;Cookies\u0026#34;: { \u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;Headers\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0 }, \u0026#34;QueryStringCacheKeys\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0 } }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 60, \u0026#34;MaxTTL\u0026#34;: 300 }, \u0026#34;Enabled\u0026#34;: true, \u0026#34;ViewerCertificate\u0026#34;: { \u0026#34;CloudFrontDefaultCertificate\u0026#34;: false, \u0026#34;ACMCertificateArn\u0026#34;: \u0026#34;${SSL_ARN}\u0026#34;, \u0026#34;SSLSupportMethod\u0026#34;: \u0026#34;sni-only\u0026#34;, \u0026#34;MinimumProtocolVersion\u0026#34;: \u0026#34;TLSv1.1_2016\u0026#34;, \u0026#34;Certificate\u0026#34;: \u0026#34;${SSL_ARN}\u0026#34;, \u0026#34;CertificateSource\u0026#34;: \u0026#34;acm\u0026#34; }, \u0026#34;HttpVersion\u0026#34;: \u0026#34;http2\u0026#34;, \u0026#34;IsIPV6Enabled\u0026#34;: true, \u0026#34;CallerReference\u0026#34;: \u0026#34;hugo-distribution\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;Distribution for Hugo site\u0026#34; } EOF 中身を確認します。\ncat distribution-config.json 実行結果\n{ \u0026#34;Aliases\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ \u0026#34;hugo.\u0026lt;your-domain\u0026gt;\u0026#34; ] }, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;S3-hugo.\u0026lt;your-domain\u0026gt;-************\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;hugo.\u0026lt;your-domain\u0026gt;-************.s3.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;origin-access-identity/cloudfront/E3NRXXXXXXXXXX\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;S3-hugo.\u0026lt;your-domain\u0026gt;-************\u0026#34;, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;redirect-to-https\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 2, \u0026#34;Items\u0026#34;: [ \u0026#34;HEAD\u0026#34;, \u0026#34;GET\u0026#34; ], \u0026#34;CachedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 2, \u0026#34;Items\u0026#34;: [ \u0026#34;HEAD\u0026#34;, \u0026#34;GET\u0026#34; ] } }, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: false, \u0026#34;Cookies\u0026#34;: { \u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;Headers\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0 }, \u0026#34;QueryStringCacheKeys\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0 } }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 60, \u0026#34;MaxTTL\u0026#34;: 300 }, \u0026#34;Enabled\u0026#34;: true, \u0026#34;ViewerCertificate\u0026#34;: { \u0026#34;CloudFrontDefaultCertificate\u0026#34;: false, \u0026#34;ACMCertificateArn\u0026#34;: \u0026#34;arn:aws:acm:us-east-1:************:certificate/fd93963c-91da-4cda-abcd-1234xx9999xx\u0026#34;, \u0026#34;SSLSupportMethod\u0026#34;: \u0026#34;sni-only\u0026#34;, \u0026#34;MinimumProtocolVersion\u0026#34;: \u0026#34;TLSv1.1_2016\u0026#34;, \u0026#34;Certificate\u0026#34;: \u0026#34;arn:aws:acm:us-east-1:************:certificate/fd93963c-91da-4cda-abcd-1234xx9999xx\u0026#34;, \u0026#34;CertificateSource\u0026#34;: \u0026#34;acm\u0026#34; }, \u0026#34;HttpVersion\u0026#34;: \u0026#34;http2\u0026#34;, \u0026#34;IsIPV6Enabled\u0026#34;: true, \u0026#34;CallerReference\u0026#34;: \u0026#34;hugo-distribution\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;Distribution for Hugo site\u0026#34; } Distribution の作成には cloudfront create-distribution コマンドを使用します。\naws cloudfront create-distribution \\ --distribution-config file://distribution-config.json 実行結果として、作成された Distribution のすべての情報が出力されますが、ここでは出力結果の記述は省略します)\ncloudfront list-distributions コマンドを使用して、 Distribution が正しく作成できたことの確認も兼ねて、後に使用する Distribution の DomainName を取得します。\nCF_DOMAIN_NAME=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Aliases.Items[0]==\u0026#39;${HUGO_DOMAIN}\u0026#39;].DomainName\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo ${CF_DOMAIN_NAME} 実行結果\nd1xxxxxxxxxxxx.cloudfront.net 5. Route 53 に RecordSet を作成 Hugo で生成したサイトに当てるドメインを Route 53 の RecordSet として追加します。と言っても、 SSL 証明書発行手順の中でドメイン認証のためのレコードを追加したので、コマンドはそれとほぼ同じです。 今回は RecordSet の Type が A 、つまり Alias レコードになるので、 AliasTarget としてレコードの情報を指定します。\naws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch \\ \u0026#34;{ \\\u0026#34;Changes\\\u0026#34;: [ { \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;CREATE\\\u0026#34;, \\\u0026#34;ResourceRecordSet\\\u0026#34;: { \\\u0026#34;Name\\\u0026#34;: \\\u0026#34;${HUGO_DOMAIN}\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;A\\\u0026#34;, \\\u0026#34;AliasTarget\\\u0026#34;: { \\\u0026#34;HostedZoneId\\\u0026#34;: \\\u0026#34;Z2FDTNDATAQYW2\\\u0026#34;, \\\u0026#34;DNSName\\\u0026#34;: \\\u0026#34;${CF_DOMAIN_NAME}\\\u0026#34;, \\\u0026#34;EvaluateTargetHealth\\\u0026#34;: false } } } ] }\u0026#34; 実行結果\n{ \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C0557598RTB2IBN8A248\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2020-12-03T15:57:20.421000+00:00\u0026#34; } } 6. 設定したドメインでアクセスできるか確認 Hugo のサイトをデプロイする前に、シンプルな HTML ファイルを置いてドメインでアクセスできることを確認してみます。\ncat \u0026lt;\u0026lt; EOF \u0026gt; index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Hello Hugo!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello Hugo!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; EOF S3 Bucket にアップロードします。\naws s3 cp index.html s3://${S3_BUCKET_NAME}/ 実行結果\nupload: ./index.html to s3://hugo.\u0026lt;your-domain\u0026gt;-************/index.html ブラウザ等で設定したドメインにアクセスして確認します。ここでは curl コマンドで確認してみます。\ncurl -i https://${HUGO_DOMAIN} 実行結果\nHTTP/2 200 content-type: text/html content-length: 148 last-modified: Thu, 03 Dec 2020 16:03:06 GMT accept-ranges: bytes server: AmazonS3 date: Thu, 03 Dec 2020 16:06:17 GMT etag: \u0026#34;630ccccc8ccc651b9ccccbf1bf364c8c\u0026#34; x-cache: Hit from cloudfront via: 1.1 a8f6d439d4b35a734e48cf0ced363c2d.cloudfront.net (CloudFront) x-amz-cf-pop: NRT57-C2 x-amz-cf-id: odV08a6Fk2TZXDmyRxarBGbHH-Xv_gMz6_AL1c7Ww1V_Erz3PxJhKA== age: 37 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Hello Hugo!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello Hugo!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 無事にアクセスできました。場合によってはドメインでアクセスできるようになるまで数分から数時間かかる場合もあります。\nHugo で生成したサイトをデプロイしてみる では最後に Hugo でサイトを生成してデプロイしてみます。下記のコマンドで、 Hugo 公式のクイックスタート の通りにサイトを生成してサンプルの投稿を作ってビルドするところまでやっています。(途中ちょっとおまじないを入れてます 1 )\nhugo new site hugo-sample \\ \u0026amp;\u0026amp; cd hugo-sample \\ \u0026amp;\u0026amp; git init \\ \u0026amp;\u0026amp; git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke \\ \u0026amp;\u0026amp; echo \u0026#39;theme = \u0026#34;ananke\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml \\ \u0026amp;\u0026amp; sed -i -r \u0026#34;s/http:\\/\\/example.org/https:\\/\\/${HUGO_DOMAIN}/\u0026#34; config.toml \\ \u0026amp;\u0026amp; sed -i -r \u0026#39;s/draft: true/draft: false\\ url: \u0026#34;\\/{{ .Type }}\\/{{ .Name}}.html\u0026#34;/g\u0026#39; archetypes/default.md \\ \u0026amp;\u0026amp; hugo new posts/my-first-post.md \\ \u0026amp;\u0026amp; hugo 続いて、 S3 Bucket にデプロイします。\naws s3 sync public/ s3://${S3_BUCKET_NAME}/ CloudFront に先ほどアクセスしたキャッシュが残っている可能性があるので、 cloudfront create-invalidation コマンドでキャッシュをクリアします。その際、対象の Distribution の Id が必要になるので、 DomainName を取得したときと同じ方法で変数に設定します。\nCF_DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Aliases.Items[0]==\u0026#39;${HUGO_DOMAIN}\u0026#39;].Id\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo ${CF_DIST_ID} 実行結果\nE3QDXXXXXXXXXX この変数を用いて cloudfront create-invalidation コマンドを実行します。\naws cloudfront create-invalidation \\ --distribution-id ${CF_DIST_ID} \\ --paths \u0026#34;/*\u0026#34; 実行結果として、発行した Invalidation の情報が出力されます。\n{ \u0026#34;Location\u0026#34;: \u0026#34;https://cloudfront.amazonaws.com/2020-05-31/distribution/E3QDXXXXXXXXXX/invalidation/I394QPIOSBJY5Y\u0026#34;, \u0026#34;Invalidation\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;I394QPIOSBJY5Y\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;InProgress\u0026#34;, \u0026#34;CreateTime\u0026#34;: \u0026#34;2020-12-03T16:42:29.295000+00:00\u0026#34;, \u0026#34;InvalidationBatch\u0026#34;: { \u0026#34;Paths\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ \u0026#34;/*\u0026#34; ] }, \u0026#34;CallerReference\u0026#34;: \u0026#34;cli-1607013748-344932\u0026#34; } } } Invalidation のステータスは cloudfront list-invalidations コマンドで確認します。\naws cloudfront list-invalidations \\ --distribution-id ${CF_DIST_ID} 実行結果\n{ \u0026#34;InvalidationList\u0026#34;: { \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;I394QPIOSBJY5Y\u0026#34;, \u0026#34;CreateTime\u0026#34;: \u0026#34;2020-12-03T16:42:29.295000+00:00\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Completed\u0026#34; } ] } } ステータスが Completed になったら、今度はブラウザでアクセスしてみます。\nトップページが見えていますね 🎉\n記事ページも見えていますね 🎉\n以上で Hugo のホスティング環境の構築が完了しました。お疲れさまでした。今回作成したリソースに対して発生するコストとしては下記のとおりです。(ドメインの準備は事前準備に含まれるので、ドメイン自体の料金と Route 53 の HostedZone にかかるコストも除外しています)\nAmazon CloudFront リクエストに応じたコスト Amazon S3 オブジェクトのサイズに応じたコスト 使用しているストレージに対して発生するコスト S3 Bucket にオブジェクトを Put する際に発生するコスト といっても、それぞれ発生するコストは微々たるものです。この構成で発生するコストについては以前に個人ブログの方に書いたので、参考にしてください。\nCloudFront \u0026#43; S3 でブログを運用し始めて 1 ヶ月経つので、かかった費用について振り返ってみた - michimani.net これでアウトプットする場が完成したので、どんどんアウトプットしていきましょう！\nこの構成を成長させる 今回 構築した構成は最低限の内容になっています。ブログとして記事を書く側としても読む側としても、改善していく部分はいくつかあるので、過去に個人ブログでやってきた内容を例として挙げておきます。\nデプロイの自動化 今回の手順では、Hugo で生成した静的ファイルをデプロイする際 s3 sync コマンドでファイルをアップロードして cloudfront create-invalidation でキャッシュをクリアしていましたが、毎回これを実行するのは面倒です。これらを自動化するために、 GitHub への Push をトリガーにして、 AWS CodeBuild を起動してデプロイするという構成が考えられます。\nこれに関しては以前に個人ブログに書いているので、参考にしてみてください。\nHugo のビルドとデプロイに CodePipeline をやめて CodeBuild だけを使うようにした - michimani.net Git リポジトリとして AWS CodeCommit を使う場合は AWS CodePipeline をあわせて使うと楽になります。こちらも参考記事を載せておきます。\nAWS CDK で Hugo のビルド環境とホスティング環境を作ってみた - michimani.net URL の正規化 CloudFront + S3 で静的ファイルを配信する際、インデックスドキュメントの挙動が通常の Web サーバーとは異なります。というのは、 /index.html を省略してくれない場合が存在するということです。 CloudFront のエッジサーバーで動く Lambda@Edge を使えばそのあたりを上手く調整することができます。\nLambda@Edge で静的サイトの URL を正規化する - michimani.net キャッシュの最適化 キャッシュと一言で言っても、サーバー側のキャッシュとクライアント側のキャッシュがあります。それらを適切に設定することで、アクセスしてくる人の体験向上に繋がります。サーバー側のキャッシュについては CloudFront の Distribution で設定します。キャッシュの時間、クエリパラメータやヘッダー情報でキャッシュするかどうかの設定などがあってちょっと複雑なので、公式ドキュメントや以前まとめた記事を参考にしてみてください。\nAmazon CloudFront とは何ですか? - Amazon CloudFront Amazon CloudFront のキャッシュ仕様についてあらためて調べてみた - michimani.net クライアント側のキャッシュについては、 S3 オブジェクトのメタデータとして設定します。そこで設定した値と CloudFront の設定によって挙動が決まるので、上記のページに加えて下記のページも参考になるかと思います。\n[Hugo] 静的サイトのキャッシュ戦略について - michimani.net リソースの削除 このままアウトプットしましょう！とか成長させましょう！とか書いておいてあれなんですが、せっかくハンズオンっぽい内容になっているので、最後に今回作成したリソースを削除する手順も書いておきます。\nリソースを削除する順番は下記のとおりです。基本的にリソースの作成と逆の順序で削除していきます。\nS3 Bucket を削除 Route 53 の RecordSet を削除 CloudFront の Distribution を削除 CloudFront の CloudFrontOriginAccessIdentity を削除 ACM の Certificate とドメイン認証用の RecordSet を削除 1. S3 Bucket を削除 まずは S3 Bucket を削除しますが、削除するには S3 Bucket が空である必要があります。ただし S3 Bucket を空にする というコマンドは存在しないので、既存のコマンドを使ってすべてのオブジェクトを削除します。\nS3 Bucket 内のオブジェクトをすべて削除するには、ローカルで空のディレクトリを作って s3 sync コマンドを実行する方法と、すべてのオブジェクトに対して s3api delete-object コマンドを実行する方法があります。 今回は前者の方法で実行します。\nあらためて必要な情報は変数に定義して進めていきます。\nHOSTED_DOMAIN=\u0026#34;\u0026lt;your-domain\u0026gt;\u0026#34; HUGO_DOMAIN=\u0026#34;hugo.${HOSTED_DOMAIN}\u0026#34; AWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text ) S3_BUCKET_NAME=\u0026#34;${HUGO_DOMAIN}-${AWS_ACCOUNT_ID}\u0026#34; \\ \u0026amp;\u0026amp; echo \u0026#34; HOSTED_DOMAIN\\t= ${HOSTED_DOMAIN} HUGO_DOMAIN\\t= ${HUGO_DOMAIN} AWS_ACCOUNT_ID\\t= ${AWS_ACCOUNT_ID} S3_BUCKET_NAME\\t= ${S3_BUCKET_NAME}\u0026#34; HOSTED_DOMAIN = \u0026lt;your-domain\u0026gt; HUGO_DOMAIN = hugo.\u0026lt;your-domain\u0026gt; AWS_ACCOUNT_ID\t= ************ S3_BUCKET_NAME\t= hugo.\u0026lt;your-domain\u0026gt;-************ s3 sync でオブジェクトをすべて削除するには、ローカルで適当なディレクトリを作成し、そのディレクトリと対象の S3 Bucket を s3 sync コマンドに --delete オプションを付けて同期します。\nmkdir -p empty_dir \\ \u0026amp;\u0026amp; aws s3 sync ./empty_dir/ s3://${S3_BUCKET_NAME}/ --delete \u0026gt;\u0026amp;/dev/null \\ \u0026amp;\u0026amp; aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} 確認用に s3api list-objects-v2 コマンドを実行していますが、空の S3 Bucket になっているため何も出力されません。\nS3 Bucket が空になったら、 s3api delete-bucket コマンドで S3 Bucket を削除します。\naws s3api delete-bucket \\ --bucket ${S3_BUCKET_NAME} 実行結果は何も出力されません。念のため、対象の S3 Bucket が存在しないことを s3api list-buckets コマンドで確認しておきます。\naws s3api list-buckets \\ --query \u0026#34;Buckets[?Name==\u0026#39;${S3_BUCKET_NAME}\u0026#39;]\u0026#34; \\ --output text 何も出力されなければ、 S3 Bucket は削除されています。\nちなみに、 s3api delete-object コマンドで削除する場合は、 s3api list-objects-v2 コマンドと合わせて下記のように実行します。\nfor i in $( aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} \\ --query \u0026#39;Contents[].Key\u0026#39; \\ --output text ); do aws s3api delete-object \\ --bucket ${S3_BUCKET_NAME} \\ --key ${i} done 2. Route 53 の RecordSet を削除 次に、 Route 53 に登録した RecordSet を削除します。ここで削除するのは、サイト用のドメインにあたる RecordSet です。 ACM で SSL 証明書を発行するために作成したドメイン認証用の RecordSet は後ほど削除します。\nRecordSet を削除するには、登録するときと同様に route53 change-resource-record-sets コマンドを使用しますが、その前にドメインの HostedZoneId を取得しておきます。\nHOSTED_ZONE_ID=$( \\ aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;${HOSTED_DOMAIN}.\u0026#39;].Id\u0026#34; \\ --output text) 削除前に、対象の RecordSet の存在を確認するついでに、削除時に必要となる RecordSet の Value (Alias の DNSName) を取得します。\nCF_DOMAIN_NAME=$( \\ aws route53 list-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --query \u0026#34;ResourceRecordSets[?Name==\u0026#39;${HUGO_DOMAIN}.\u0026#39;].AliasTarget.DNSName\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo ${CF_DOMAIN_NAME} 実行結果\nd1xxxxxxxxxxxx.cloudfront.net. これらの値を用いて route53 change-resource-record-sets コマンドで削除します。パラメータや値は作成時とほぼ一緒で、 Canges.Action が CREATE ではなく DELETE になります。\naws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch \\ \u0026#34;{ \\\u0026#34;Changes\\\u0026#34;: [ { \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;DELETE\\\u0026#34;, \\\u0026#34;ResourceRecordSet\\\u0026#34;: { \\\u0026#34;Name\\\u0026#34;: \\\u0026#34;${HUGO_DOMAIN}\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;A\\\u0026#34;, \\\u0026#34;AliasTarget\\\u0026#34;: { \\\u0026#34;HostedZoneId\\\u0026#34;: \\\u0026#34;Z2FDTNDATAQYW2\\\u0026#34;, \\\u0026#34;DNSName\\\u0026#34;: \\\u0026#34;${CF_DOMAIN_NAME}\\\u0026#34;, \\\u0026#34;EvaluateTargetHealth\\\u0026#34;: false } } } ] }\u0026#34; 実行結果\n{ \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C06975891HKJHOX12B72Z\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2020-12-07T13:02:59.240000+00:00\u0026#34; } } route53 list-resource-record-sets コマンドで、対象の RecordSet が削除されていることを確認します。\naws route53 list-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --query \u0026#34;ResourceRecordSets[?Name==\u0026#39;${HUGO_DOMAIN}.\u0026#39;]\u0026#34; 実行結果\n[] これでサイト用の RecordSet が削除できました。\n3. CloudFront の Distribution を削除 続いて CloudFront の Distribution を削除します。削除するには cloudfront delete-distribution コマンドを使用しますが、その前に cloudfront update-distribution コマンドで対象の Distribution の State を Disabled (DistributionConfig.Enabled を false) に変更しておく必要があります。(Enabled のままだとエラーになります 2) これらのコマンドの実行時には対象となる Distribution の Id と ETag が必要になるので、まずはそれらを事前に取得しておきます。\nCF_DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Aliases.Items[0]==\u0026#39;${HUGO_DOMAIN}\u0026#39;].Id\u0026#34; \\ --output text) 取得した Id を元に ETag を取得します。\nCF_DIST_ETAG=$(aws cloudfront get-distribution \\ --id ${CF_DIST_ID} \\ --query \u0026#34;ETag\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo ${CF_DIST_ETAG} 実行結果\nE38H402CF0QSGY また、変更前の Distribution の情報を取得して、 Distribution.DistributionConfig の値を更新時に使用する JSON ファイルとして保存しておきます。その際、 Distribution.DistributionConfig.Enabled の値を false に変更しておきます。\naws cloudfront get-distribution \\ --id ${CF_DIST_ID} \\ | jq \u0026#34;.Distribution.DistributionConfig.Enabled|=false\u0026#34; \\ | jq \u0026#34;.Distribution.DistributionConfig\u0026#34; \\ \u0026gt; distribution-config-update.json 以上、 2 つの値と JSON ファイルを使用して、まずは Distribution の State を変更します。変更時には、作成時と同様に DistributionConfig として指定する JSON ファイルが必要になるので、先ほど作成した JSON ファイルを指定します。実行後は ETag の値が変わるので、出力結果から新しい ETag の値を取得しておきます。\nCF_DIST_ETAG=$(aws cloudfront update-distribution \\ --id ${CF_DIST_ID} \\ --if-match ${CF_DIST_ETAG} \\ --distribution-config file://distribution-config-update.json \\ --query \u0026#34;ETag\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo ${CF_DIST_ETAG} 実行結果\nE18V3ES2M2IU4T これで State を Disabled に変更できたので、 Distribution を削除します。(実行後すぐに削除しようとすると、まだ State の変更が反映されていないためエラーになる場合があります)\naws cloudfront delete-distribution \\ --id ${CF_DIST_ID} \\ --if-match ${CF_DIST_ETAG} 出力は特にありません。\ncloudfront get-distribution コマンドで、対象の Distribution が存在しないことを確認します。\naws cloudfront get-distribution \\ --id ${CF_DIST_ID} 実行結果\nAn error occurred (NoSuchDistribution) when calling the GetDistribution operation: The specified distribution does not exist. 対象の Distribution は存在しないので NoSuchDistribution エラーになりました。\n4. CloudFront の CloudFrontOriginAccessIdentity を削除 続いて、 CloudFront の CloudFrontOriginAccessIdentity を削除します。削除するには cloudfront delete-cloudfront-origin-access-identity コマンドを使用します。その際に、対象の CloudFrontOriginAccessIdentity の Id と ETag が必要になるので、事前に取得します。\nCFOAI_ID=$(aws cloudfront list-cloud-front-origin-access-identities \\ --query \u0026#34;CloudFrontOriginAccessIdentityList.Items[?Comment==\u0026#39;OAI for Hugo bucket\u0026#39;].Id\u0026#34; \\ --output text) 取得した Id を元に ETag を取得します。\nCFOAI_ETAG=$(aws cloudfront get-cloud-front-origin-access-identity \\ --id ${CFOAI_ID} \\ --query \u0026#34;ETag\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo ${CFOAI_ETAG} 実行結果\nEGBZ51UWS66MX この 2 つの値を使って CloudFrontOriginAccessIdentity を削除します。\naws cloudfront delete-cloud-front-origin-access-identity \\ --id ${CFOAI_ID} \\ --if-match ${CFOAI_ETAG} 出力は特にありません。\ncloudfront get-cloud-front-origin-access-identity コマンドで対象の CloudFrontOriginAccessIdentity が存在しないことを確認します。\naws cloudfront get-cloud-front-origin-access-identity \\ --id ${CFOAI_ID} 実行結果\nAn error occurred (NoSuchCloudFrontOriginAccessIdentity) when calling the GetCloudFrontOriginAccessIdentity operation: The specified CloudFront origin access identity does not exist. 対象の CloudFrontOriginAccessIdentity が存在しないので NoSuchCloudFrontOriginAccessIdentity エラーになりました。\n5. ACM の Certificate とドメイン認証用の RecordSet を削除 最後に、 ACM で発行した Certificate (SSL 証明書) と、発行時のドメイン認証のために作成した Route 53 の RecordSet を削除します。 Certificate の削除には Certificate ARN が、 RecordSet の削除には Name と Type および Value がそれぞれ必要になるので、 Certificate の情報からそれらを取得します。\nまずは Certificate ARN を取得します。\nSSL_ARN=$( \\ aws acm request-certificate \\ --domain-name ${HUGO_DOMAIN} \\ --validation-method DNS \\ --region us-east-1 \\ --output text) \\ \u0026amp;\u0026amp; echo ${SSL_ARN} 実行結果\narn:aws:acm:us-east-1:************:certificate/fd93963c-91da-4cda-abcd-1234xx9999xx 続いて RecordSet の Name と Type と Value の情報を Certificate から取得します。\nVLIDATION_RECORD_JSON=$( \\ aws acm describe-certificate \\ --certificate-arn ${SSL_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ResourceRecord\u0026#34; \\ --region us-east-1) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_NAME=$(echo ${VLIDATION_RECORD_JSON} | jq -r .\u0026#34;Name\u0026#34;) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_TYPE=$(echo ${VLIDATION_RECORD_JSON} | jq -r .\u0026#34;Type\u0026#34;) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_VALUE=$(echo ${VLIDATION_RECORD_JSON} | jq -r .\u0026#34;Value\u0026#34;) \\ \u0026amp;\u0026amp; echo \u0026#34; Name:\\t${VALIDATION_RECORD_NAME} Type:\\t${VALIDATION_RECORD_TYPE} Value:\\t${VALIDATION_RECORD_VALUE}\u0026#34; 実行結果\nName: _01bca14fa7d9xxxxxxxxxxxxxxxxxxxx.hugo.\u0026lt;your-domain\u0026gt;. Type:\tCNAME Value: _10c99aaddfd2xxxxxxxxxxxxxxxxxxxx.wggjkglgrm.acm-validations.aws. これらの情報を元に、まずは RecordSet を削除します。使用するコマンドは、 RecordSet 作成時と同じ route53 change-resource-record-sets で、 Action を DELETE にして実行します。\naws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch \\ \u0026#34;{ \\\u0026#34;Changes\\\u0026#34;: [ { \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;DELETE\\\u0026#34;, \\\u0026#34;ResourceRecordSet\\\u0026#34;: { \\\u0026#34;Name\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_NAME}\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_TYPE}\\\u0026#34;, \\\u0026#34;TTL\\\u0026#34;: 300, \\\u0026#34;ResourceRecords\\\u0026#34;: [{\\\u0026#34;Value\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_VALUE}\\\u0026#34;}] } } ] }\u0026#34; 実行結果\n{ \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C0511374XADT3XZRSQ2D\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2020-12-07T15:15:06.439000+00:00\u0026#34; } } route53 list-resource-record-sets コマンドで、対象の RecordSet が削除されていることを確認します。\naws route53 list-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --query \u0026#34;ResourceRecordSets[?Name==\u0026#39;${VALIDATION_RECORD_NAME}.\u0026#39;]\u0026#34; 実行結果\n[] これでドメイン認証用に作成した RecordSet が削除できました。\nさて、これで最後です。 ACM の Certificate を削除します。削除するには acm delete-certificate コマンドを使用します。\naws acm delete-certificate \\ --certificate-arn ${SSL_ARN} \\ --region us-east-1 出力は特にありません。\nacm get-certificate コマンドで対象の Certificate が存在しないことを確認します。\naws acm get-certificate \\ --certificate-arn ${SSL_ARN} 実行結果\nAn error occurred (ResourceNotFoundException) when calling the GetCertificate operation: Could not find certificate arn:aws:acm:us-east-1:************:certificate/fd93963c-91da-4cda-abcd-1234xx9999xx. 対象の Certificate が存在しないので ResourceNotFoundException エラーになりました。\n以上で、今回作成したリソースの削除が完了しました。\nまとめ だいぶ長くなってしまいましたが、 AWS CLI だけで Hugo のホスティング環境を構築してみた話でした。\n冒頭にも書いたように、 AWS CLI は AWS で各サービス、リソースを扱っていく上で、それらに対してより理解を深めるためには最高のツールです。なので、特に AWS を触り始めてまだ間もない方にこそ使ってほしいツールかなと思います。いきなり全てを CLI でやろうと思うと辛いので、 AWS CLI での操作をマネジメントコンソールで都度確認しながらやるスタイルで使ってみてください。\n今回、このようなハンズオンっぽい形で手順を書くのは初めてだったので、何かわかりにくい点などあればコメントなり Twitter なりにフィードバックいただけると幸いです！\n以上、 弁護士ドットコム Advent Calendar 2020 の 12 日目の記事は AWS CLI で Hugo のホスティング環境を作ってみる話でした。\n明日の担当は @enkdsn です。お楽しみに 👋\nCloudFront 経由で S3 にアクセスする際、対象の S3 Bucket の直下にある index.html のみ省略が可能で、それ以降の階層にアクセスするためには明示的に index.html \u0008にアクセスする必要があります。記事のパーマリンクを post-title.html として設定するように archetypes/default.md を編集しています。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAn error occurred (DistributionNotDisabled) when calling the DeleteDistribution operation: The distribution you are trying to delete has not been disabled.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-provision-hugo-on-aws-using-only-aws-cli/",
    "title": "AWS CLI だけで Hugo のホスティング環境を構築してみる ー はじめよう、技術ブログ"
  },
  {
    "contents": "JAWS-UG CLI専門支部 #173R S3基礎 (レプリケーション) に参加したので、そのレポートです。と言っても耳だけ参加だったので、 S3 レプリケーションについての概要まとめとハンズオン + α の内容になってます。(いつもそんな感じか)\nconnpass のイベントページはこちら。\nこれまでの CLI 専門支部参加レポートはこちら。\n#172R S3基礎 (オブジェクト) #171R S3基礎 通知 (Lambdaの自動実行) #170R S3基礎 ライフサイクル #169R S3基礎 バージョニング #168R S3基礎 Webサイト\u0026amp;amp;ログ #167R EventBridge入門 目次 S3 のレプリケーションとは 概要 レプリケーションの利用用途 要件 ハンズオン レプリケーション元とレプリケーション先の S3 バケット作成 レプリケーション設定を追加 オブジェクトを Put して確認 知らんかったぞ、ではなく、ハマったぞそれ ReplicationStatus が Failed なのにレプリケートされている LT まとめ S3 のレプリケーションとは そもそも S3 のレプリケーションの機能について理解が乏しかったので、公式ドキュメントを元に概要をまとめてみます。\n概要 S3 のレプリケーションとは、異なる S3 バケット間でオブジェクトを自動的に非同期でコピーできる機能 メタデータ (作成日時、バージョン IDなど) は保持される レプリケーション用に指定する S3 バケットは 同一リージョンでも別リージョンでも OK 同一リージョン内のレプリケーション : 同一リージョンレプリケーション (SRR - Same Region Replication) 異なるリージョンのバケットへのレプリケーション : クロスリージョンレプリケーション (CRR - Cross Region Replication) 同一アカウントでも別アカウントでも OK 異なるストレージクラスでも OK レプリケーションの設定は、レプリケーション 元 のバケットに追加する レプリケーションの利用用途 全体としての用途・メリット メタデータも保持されるので、単純にレプリカを保持できる 異なるストレージクラスにレプリケーションすることで、レプリカ保持のコストを削減できる 異なるアカウントにレプリケーションすることで、別の所有権で同一のオブジェクトを扱える CRR を利用する場合の用途・メリット コンプライアンス要件への対応 レイテンシー縮小 SRR を利用する場合の用途・メリット 同種のオブジェクト (e.g ログ) の集約 異なるステージ間 (本番/ステージング/開発) とのオブジェクト共有 要件 ソースバケットの所有者は、レプリケーション先のリージョンを有効にしている必要がある レプリケート元と先の両方のバケットでバージョニングを有効にする必要がある その他、詳細な仕様については公式ドキュメントをどうぞ。\nレプリケーション - Amazon Simple Storage Service ハンズオン 今回のハンズオンでは、同一アカウントの同一リージョン内にあるバケット同士でレプリケーションをするという内容でしたが、ここでは別リージョンのバケットへのレプリケーション (CRR) としてやってみます。\nなお、手元で実行する際は macOS 環境で、 AWS CLI のバージョンは 2.1.9 です。\naws --version aws-cli/2.1.9 Python/3.7.4 Darwin/19.6.0 exe/x86_64 prompt/off レプリケーション元とレプリケーション先の S3 バケット作成 レプリケーション元を 東京 ap-northeast-1 に、レプリケーション先を オレゴン us-west-2 に作成します。まずは必要な情報を変数に設定します。\n$ AWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text ) $ S3_SRC_REGION=\u0026#34;ap-northeast-1\u0026#34; $ S3_DST_REGION=\u0026#34;us-west-2\u0026#34; $ S3_SRC_BUCKET_NAME=\u0026#34;handson-cli-s3-replication-replication-source-${AWS_ACCOUNT_ID}\u0026#34; $ S3_DST_BUCKET_NAME=\u0026#34;handson-cli-s3-replication-replication-destination-${AWS_ACCOUNT_ID}\u0026#34; それぞれのバケットを作成し、バージョニングを有効にします。\n$ aws s3api create-bucket \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --create-bucket-configuration \u0026#34;LocationConstraint=${S3_SRC_REGION}\u0026#34; \\ --region ${S3_SRC_REGION} \\ \u0026amp;\u0026amp; aws s3api put-bucket-versioning \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --versioning-configuration Status=Enabled \\ --region ${S3_SRC_REGION} $ aws s3api create-bucket \\ --bucket ${S3_DST_BUCKET_NAME} \\ --create-bucket-configuration \u0026#34;LocationConstraint=${S3_DST_REGION}\u0026#34; \\ --region ${S3_DST_REGION} \\ \u0026amp;\u0026amp; aws s3api put-bucket-versioning \\ --bucket ${S3_DST_BUCKET_NAME} \\ --versioning-configuration Status=Enabled \\ --region ${S3_DST_REGION} それぞれのバケットの存在と、リージョン、バージョニングのステータスを確認します。\n$ aws s3api get-bucket-location \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --output text \\ \u0026amp;\u0026amp; aws s3api get-bucket-versioning \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --query \u0026#34;Status\u0026#34; \\ --output text ap-northeast-1 Enabled $ aws s3api get-bucket-location \\ --bucket ${S3_DST_BUCKET_NAME} \\ --output text \\ \u0026amp;\u0026amp; aws s3api get-bucket-versioning \\ --bucket ${S3_DST_BUCKET_NAME} \\ --query \u0026#34;Status\u0026#34; \\ --output text us-west-2 Enabled ちなみに、 S3 バケットに対するコマンドには下記のようなものがあります。\n$ aws s3api help | grep bucket o create-bucket o delete-bucket o delete-bucket-analytics-configuration o delete-bucket-cors o delete-bucket-encryption o delete-bucket-intelligent-tiering-configuration o delete-bucket-inventory-configuration o delete-bucket-lifecycle o delete-bucket-metrics-configuration o delete-bucket-ownership-controls o delete-bucket-policy o delete-bucket-replication o delete-bucket-tagging o delete-bucket-website o get-bucket-accelerate-configuration o get-bucket-acl o get-bucket-analytics-configuration o get-bucket-cors o get-bucket-encryption o get-bucket-intelligent-tiering-configuration o get-bucket-inventory-configuration o get-bucket-lifecycle-configuration o get-bucket-location o get-bucket-logging o get-bucket-metrics-configuration o get-bucket-notification-configuration o get-bucket-ownership-controls o get-bucket-policy o get-bucket-policy-status o get-bucket-replication o get-bucket-request-payment o get-bucket-tagging o get-bucket-versioning o get-bucket-website o head-bucket o list-bucket-analytics-configurations o list-bucket-intelligent-tiering-configurations o list-bucket-inventory-configurations o list-bucket-metrics-configurations o list-buckets o put-bucket-accelerate-configuration o put-bucket-acl o put-bucket-analytics-configuration o put-bucket-cors o put-bucket-encryption o put-bucket-intelligent-tiering-configuration o put-bucket-inventory-configuration o put-bucket-lifecycle-configuration o put-bucket-logging o put-bucket-metrics-configuration o put-bucket-notification-configuration o put-bucket-ownership-controls o put-bucket-policy o put-bucket-replication o put-bucket-request-payment o put-bucket-tagging o put-bucket-versioning o put-bucket-website 今回のように S3 バケットのリージョンとかバージョニングとかの情報をひとつのコマンドで取得したいと思うのは私だけでしょうか。他のリソースで言う describe-*** コマンドがあってそれで取得できればいいのになと思いました。\nレプリケーション設定を追加 レプリケーション設定を追加するには s3api put-bucket-replication コマンドを使いますが、その際にレプリケーションの設定情報を下記のような JSON で指定します。\n{ \u0026#34;Role\u0026#34;: \u0026#34;\u0026lt;IAM ロールの ARN\u0026gt;\u0026#34;, \u0026#34;Rules\u0026#34;: [ { \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Priority\u0026#34;: 1, \u0026#34;DeleteMarkerReplication\u0026#34;: { \u0026#34;Status\u0026#34;: \u0026#34;Disabled\u0026#34; }, \u0026#34;Filter\u0026#34; : { \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34;}, \u0026#34;Destination\u0026#34;: { \u0026#34;Bucket\u0026#34;: \u0026#34;\u0026lt;レプリケーション先の S3 バケットの ARN\u0026gt;\u0026#34; } } ] } 見てもらうと分かる通り S3 がユーザーに代わってオブジェクトをレプリケートするための IAM ロールが必要になるので、まずはそれを作ります。\nIAM ロールの作成 順番としては、IAM ポリシーを作って、 IAM ロールを作ってアタッチする流れです。作成する IAM ロールには S3 が Assume Role できるように信頼ポリシーを付与します。\nアタッチするポリシーの作成 下記のようなポリシードキュメントを policy.json として作成しておいて、 iam create-policy コマンドで --policy-document file://policy.json で指定して IAM ポリシーを作成します。(コマンド実行については新しいものではないので省略します)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObjectVersion\u0026#34; \u0026#34;s3:GetObjectVersionAcl\u0026#34;, \u0026#34;s3:GetObjectVersionTagging\u0026#34;, ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::handson-cli-s3-replication-replication-source-************/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetReplicationConfiguration\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::handson-cli-s3-replication-replication-source-************\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ReplicateObject\u0026#34;, \u0026#34;s3:ReplicateDelete\u0026#34;, \u0026#34;s3:ReplicateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::handson-cli-s3-replication-replication-destination-************/*\u0026#34; } ] } ハンズオン資料では下記の公式ドキュメントと異なる設定だったのですが、後述するおかしな現象が発生したため公式ドキュメント通りの内容にしました。\nSetting up permissions for replication - Amazon Simple Storage Service IAM ロールを作成、 IAM ポリシーをアタッチ IAM ロール作成にあたって、下記のような信頼ポリシーを trust-policy.json として作成し、 iam create-role コマンドで --assume-role-policy-document file://trust-policy.json で指定して IAM ロールを作成します。(コマンド実行については新しいものではないのでry)\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;s3.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } IAM ロールを作成したら、先ほど作成した IAM ポリシーをアタッチします。コマンドは iam attach-role-policy でアタッチします。(コマンド実行についてはry)\nレプリケーションの設定を追加 レプリケーションの設定情報も、下記のように事前に JSON ファイルとして作成しておきます。\n$ cat \u0026lt;\u0026lt; EOF \u0026gt; ./replication-doc.json { \u0026#34;Role\u0026#34;: \u0026#34;${S3_REPLICA_ROLE_ARN}\u0026#34;, \u0026#34;Rules\u0026#34;: [ { \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Priority\u0026#34;: 1, \u0026#34;DeleteMarkerReplication\u0026#34;: { \u0026#34;Status\u0026#34;: \u0026#34;Disabled\u0026#34; }, \u0026#34;Filter\u0026#34; : { \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34;}, \u0026#34;Destination\u0026#34;: { \u0026#34;Bucket\u0026#34;: \u0026#34;arn:aws:s3:::${S3_DST_BUCKET_NAME}\u0026#34; } } ] } EOF レプリケーションを S3 バケットに追加するには s3api put-bucket-replication コマンドを使用します。\n$ aws s3api put-bucket-replication \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --replication-configuration file://replication-doc.json オブジェクトを Put して確認 適当なファイルを作成して、レプリケーション元の S3 バケットに Put し、レプリケーション先の S3 バケットで確認します。\n$ echo \u0026#34;This is a sample file.\u0026#34; \u0026gt; ./sample.txt メタデータなども含めてレプリケートされるとのことなので、色々付与して Put します。\n$ aws s3api put-object \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --key sample.txt \\ --body ./sample.txt \\ --metadata custom-metadata-1=value1,custom-metadata-2=value2,Custom-Metadata-3=value3 \\ --cache-control \u0026#34;public, max-age=1209600\u0026#34; \\ --content-type \u0026#34;text/plain8\u0026#34; \\ --tagging \u0026#34;TagKey1=TagValue1\u0026#34; レプリケーションは通常 15 分以内に行われるため、それまではレプリケーション元のオブジェクトの ReplicationStatus は PENDING となっています。\n$ aws s3api head-object \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --key sample2.txt { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-12-10T14:26:18+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 26, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6d8b1ff98614bae68f36f5d2986f9e3c\\\u0026#34;\u0026#34;, \u0026#34;VersionId\u0026#34;: \u0026#34;L1CTlhLzrDmtq.Aupo_hzVb5uSBXmprQ\u0026#34;, \u0026#34;CacheControl\u0026#34;: \u0026#34;public, max-age=1209600\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;text/plain8\u0026#34;, \u0026#34;Metadata\u0026#34;: { \u0026#34;custom-metadata-1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;custom-metadata-2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;custom-metadata-3\u0026#34;: \u0026#34;value3\u0026#34; }, \u0026#34;ReplicationStatus\u0026#34;: \u0026#34;PENDING\u0026#34; } それぞれのオブジェクトの情報を s3api head-object コマンドで取得して差分を見てみます。\n$ aws s3api head-object \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --key sample.txt \\ \u0026gt;| src-object.json \\ \u0026amp;\u0026amp; aws s3api head-object \\ --bucket ${S3_DST_BUCKET_NAME} \\ --key sample.txt \\ \u0026gt;| dst-object.json \\ \u0026amp;\u0026amp; diff -u src-object.json dst-object.json --- src-object.json\t2020-12-10 23:51:04.000000000 +0900 +++ dst-object.json\t2020-12-10 23:51:05.000000000 +0900 @@ -11,5 +11,5 @@ \u0026#34;custom-metadata-2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;custom-metadata-3\u0026#34;: \u0026#34;value3\u0026#34; }, - \u0026#34;ReplicationStatus\u0026#34;: \u0026#34;COMPLETED\u0026#34; + \u0026#34;ReplicationStatus\u0026#34;: \u0026#34;REPLICA\u0026#34; } ReplicationStatus 以外は一致しています。レプリケーション元のオブジェクトの ReplicationStatus は COMPLETED で、レプリケーション先のオブジェクトの ReplicationStatus は REPLICA となっています。\n知らんかったぞ、ではなく、ハマったぞそれ この 「知らんかったぞ、それ」 コーナー密かな人気があるとのことなので今回もいくつか個人的に知らなかったことを取り上げたかったのですが、今回はレプリケーションの機能自体ほとんど知らなかったので、代わりにハンズオンでハマったポイントについて書きます。\nReplicationStatus が Failed なのにレプリケートされている ハンズオンのとおりに進めていって最後にレプリケーション元と先のオブジェクトの情報を比較すると、おかしな現象が起きました。\n$ aws s3api head-object \\ --bucket ${S3_SRC_BUCKET_NAME} \\ --key failed-sample.txt \\ \u0026gt;| src-failed-object.json \\ \u0026amp;\u0026amp; aws s3api head-object \\ --bucket ${S3_DST_BUCKET_NAME} \\ --key failed-sample.txt \\ \u0026gt;| dst-failed-object.json \\ \u0026amp;\u0026amp; diff -u src-failed-object.json dst-failed-object.json --- src-failed-object.json\t2020-12-11 00:04:18.000000000 +0900 +++ dst-failed-object.json\t2020-12-11 00:04:19.000000000 +0900 @@ -11,5 +11,5 @@ \u0026#34;custom-metadata-2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;custom-metadata-3\u0026#34;: \u0026#34;value3\u0026#34; }, - \u0026#34;ReplicationStatus\u0026#34;: \u0026#34;FAILED\u0026#34; + \u0026#34;ReplicationStatus\u0026#34;: \u0026#34;REPLICA\u0026#34; } 手順の途中にも書きましたが、ハンズオンの手順書と公式ドキュメントでは IAM ポリシーに下記のような差異がありました。\n--- policy-handson.json\t2020-12-10 23:57:25.000000000 +0900 +++ policy.json\t2020-12-10 23:58:45.000000000 +0900 @@ -4,8 +4,9 @@ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ - \u0026#34;s3:GetObjectVersionForReplication\u0026#34;, - \u0026#34;s3:GetObjectVersionAcl\u0026#34; + \u0026#34;s3:GetObjectVersion\u0026#34;, + \u0026#34;s3:GetObjectVersionAcl\u0026#34;, + \u0026#34;s3:GetObjectVersionTagging\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::handson-cli-s3-replication-replication-source-************/*\u0026#34; @@ -26,8 +27,7 @@ \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ReplicateObject\u0026#34;, \u0026#34;s3:ReplicateDelete\u0026#34;, - \u0026#34;s3:ReplicateTags\u0026#34;, - \u0026#34;s3:GetObjectVersionTagging\u0026#34; + \u0026#34;s3:ReplicateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::handson-cli-s3-replication-replication-destination-************/*\u0026#34; } 違いとしては、レプリケーション元の S3 バケットに s3:GetObjectVersionTagging がなかったという点です。これが原因でなぜこのような状況が発生するのかはわからなかったのですが、ちょっとハマったポイントでした。\nLT 今回は SAYJOY@gringriffin さんによる LT がありました。\n以前に JAWS-UG 千葉支部で開催されたサーバーレスハンズオン (の一部) の内容を AWS CLI で構築した際の振り返りや気付きなどについて話されていました。CLI 専門支部のハンズオン資料を元に資料化をされている途中とのことなので、もしかしたらご自身のブログで公開されるかも？しれません。\nSAYJOYblog (終わってから言うのもあれなんですが) 自分も資料が整えば LT しようかと思っていましたが、結局間に合わずでしたので供養ブログを書きました。\nまとめ JAWS-UG CLI専門支部 #173R S3基礎 (レプリケーション) に参加したので、そのレポートでした。\n今回はリアルタイムでは耳だけ参加で、終わってからハンズオンをやってみました。S3 のレプリケーション機能は今まで使ったことがなかったのですが、思っていた以上に簡単であっさりしているなという印象です。異なるリージョン、異なるアカウント、異なるストレージクラスへのレプリケーションも可能ということで、あらゆるシチュエーションに対応できるのでは、と思いました。\n年内のハンズオンは今回で最後だったようなのですが、来年以降もスケジュールは用意されているということなので引き続き参加していきたいと思います。もともと今年に入ってからオフラインで参加しようとしていたところで今のような状況になってしまいました。その後、すぐにオンラインでの開催していただけるようになったので、そこからはほぼ毎回参加してきました。おかげさまで抵抗なく、むしろ好んで AWS CLI を使うようになりました。開催していただいている波田野さんには感謝です。\nそういえば、自分も AWS CLI を使ったハンズオン資料のようなものを作成中なので、近々公開できるかなと思ってます。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-173r-s3-replication/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #173R S3基礎 (レプリケーション) に参加しました #jawsug_cli"
  },
  {
    "contents": "AWS Certificate Manager で SSL 証明書を発行する手順は何度も繰り返し実行するものではないため、 IaC に載せずに手作業で作成している方は少なくないと思います。手作業とはいえ毎回マネジメントコンソールでポチポチやるのは面倒なので、 AWS CLI を使ってスクリプト化してみました。\n目次 ACM で SSL 証明書を発行するまでの手順 前提 AWS CLI でやってみる 1. ACM で Certificate の発行をリクエストする 2. DNS でドメイン認証する 3. ドメイン認証ステータスを確認 シェルスクリプト化する 実行してみる まとめ ACM で SSL 証明書を発行するまでの手順 ACM で SSL 証明書 (Certificate) を発行するには、下記の手順を踏むことになります。\nACM で Certificate の発行を リクエスト する メールまたは DNS で ドメイン認証 する ドメイン認証にはメールによる認証と DNS による認証があります。 DNS による認証の場合、認証に必要なレコードを DNS に登録します。(DNS として Route 53 を使う場合、マネジメントコンソールからはボタン一つでレコードの登録ができます)\n手順としては多くないですが、今回は AWS CLI を使ってシェルスクリプト化することで、より簡潔に Certificate の発行をやってみたいと思います。\n前提 前提として、対象のドメインは Route 53 で管理されているものとします。また、ドメイン認証は DNS で行います。\nAWS CLI でやってみる 今回操作するサービスは AWS Certificate Manager と Amazon Route 53 です。(ドメイン認証を DNS で行うため)\nなので、 AWS CLI のコマンドとしては acm と route53 を使います。スクリプト化する前に、前項で挙げた手順の 1 と 2 をそれぞれ AWS CLI で実行します。\nAWS CLI のバージョンは、2020/12/09 時点で最新の 2.1.8 を使います。(v1 の最新は 1.18.192)\n$ aws --version aws-cli/2.1.8 Python/3.7.4 Darwin/19.6.0 exe/x86_64 prompt/off 1. ACM で Certificate の発行をリクエストする 使用するコマンドは acm request-certificate です。\n$ aws acm request-certificate help ... SYNOPSIS request-certificate --domain-name \u0026lt;value\u0026gt; [--validation-method \u0026lt;value\u0026gt;] [--subject-alternative-names \u0026lt;value\u0026gt;] [--idempotency-token \u0026lt;value\u0026gt;] [--domain-validation-options \u0026lt;value\u0026gt;] [--options \u0026lt;value\u0026gt;] [--certificate-authority-arn \u0026lt;value\u0026gt;] [--tags \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] ... 必須なのは対象のドメイン名を指定する --domain-name ですが、今回は認証方法を指定する --validation-method オプションで DNS による認証であることを指定します。ここを省略するとメールによる認証となります。\n今回は Route 53 のホストゾーンとして登録されている michimani.net のサブドメインである curry.michimani.net に対する SSL 証明書を発行してみようと思います。\n$ HOSTED_DOMAIN=\u0026#34;michimani.net\u0026#34; $ TARGET_DOMAIN=\u0026#34;curry.${HOSTED_DOMAIN}\u0026#34; $ aws acm request-certificate \\ --domain-name ${TARGET_DOMAIN} \\ --validation-method DNS { \u0026#34;CertificateArn\u0026#34;: \u0026#34;arn:aws:acm:ap-northeast-1:************:certificate/d8009e70-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34; } 今回は特にリージョンを指定せずにプロファイルの default のリージョンに対して実行しましたが、発行した Certificate を CloudFornt で使用する場合は バージニア北部 us-east-1 に対して実行する必要があります。\nリクエストした Certificate の ARN は以降の手順で使用するので、変数に設定しておきます。(上記のコマンド実行時に、出力を変数に設定してもよいです)\n$ CERT_ARN=$( \\ aws acm list-certificates \\ --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;${TARGET_DOMAIN}\u0026#39;].CertificateArn\u0026#34; \\ --output text ) \\ \u0026amp;\u0026amp; echo ${CERT_ARN} arn:aws:acm:ap-northeast-1:************:certificate/d8009e70-XXXX-XXXX-XXXX-XXXXXXXXXXXX 2. DNS でドメイン認証する DNS でドメイン認証するには、認証用のレコード情報が必要になります。マネジメントコンソールで操作する場合はボタン一つで Route 53 に登録することができますが、 AWS CLI だとそうはいきません。認証用のレコード情報は Certificate が持っているので、 acm describe-certificate コマンドで取得します。\n$ aws acm describe-certificate \\ --certificate-arn ${CERT_ARN} { \u0026#34;Certificate\u0026#34;: { \u0026#34;CertificateArn\u0026#34;: \u0026#34;arn:aws:acm:ap-northeast-1:************:certificate/d8009e70-XXXX-XXXX-XXXX-XXXXXXXXXXXX\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;curry.michimani.net\u0026#34;, \u0026#34;SubjectAlternativeNames\u0026#34;: [ \u0026#34;curry.michimani.net\u0026#34; ], \u0026#34;DomainValidationOptions\u0026#34;: [ { \u0026#34;DomainName\u0026#34;: \u0026#34;curry.michimani.net\u0026#34;, \u0026#34;ValidationDomain\u0026#34;: \u0026#34;curry.michimani.net\u0026#34;, \u0026#34;ValidationStatus\u0026#34;: \u0026#34;PENDING_VALIDATION\u0026#34;, \u0026#34;ResourceRecord\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;_6709bba0b171XXXXXXXXXXXXXXXXXXXX.curry.michimani.net.\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;CNAME\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;_2166df0b8981XXXXXXXXXXXXXXXXXXXX.wggjkglgrm.acm-validations.aws.\u0026#34; }, \u0026#34;ValidationMethod\u0026#34;: \u0026#34;DNS\u0026#34; } ], \u0026#34;Subject\u0026#34;: \u0026#34;CN=curry.michimani.net\u0026#34;, \u0026#34;Issuer\u0026#34;: \u0026#34;Amazon\u0026#34;, \u0026#34;CreatedAt\u0026#34;: \u0026#34;2020-12-09T07:04:37+09:00\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING_VALIDATION\u0026#34;, \u0026#34;KeyAlgorithm\u0026#34;: \u0026#34;RSA-2048\u0026#34;, \u0026#34;SignatureAlgorithm\u0026#34;: \u0026#34;SHA256WITHRSA\u0026#34;, \u0026#34;InUseBy\u0026#34;: [], \u0026#34;Type\u0026#34;: \u0026#34;AMAZON_ISSUED\u0026#34;, \u0026#34;KeyUsages\u0026#34;: [], \u0026#34;ExtendedKeyUsages\u0026#34;: [], \u0026#34;RenewalEligibility\u0026#34;: \u0026#34;INELIGIBLE\u0026#34;, \u0026#34;Options\u0026#34;: { \u0026#34;CertificateTransparencyLoggingPreference\u0026#34;: \u0026#34;ENABLED\u0026#34; } } } Certificate.DomainValidationOptions[0].ResourceRecord がドメイン認証用のレコード情報です。 Type は CNAME で固定なので、 Name と Value を変数に設定します。一度に 2 つの値を設定する良い方法が思い浮かばないので、 2 回に分けて設定します。なにか良い方法があれば教えて下さい。\n$ VALIDATION_RECORD_NAME=$( \\ aws acm describe-certificate \\ --certificate-arn ${CERT_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ResourceRecord.Name\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; VALIDATION_RECORD_VALUE=$( \\ aws acm describe-certificate \\ --certificate-arn ${CERT_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ResourceRecord.Value\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo \u0026#34; VALIDATION_RECORD_NAME = ${VALIDATION_RECORD_NAME} VALIDATION_RECORD_VALUE = ${VALIDATION_RECORD_VALUE}\u0026#34; VALIDATION_RECORD_NAME = _6709bba0b171XXXXXXXXXXXXXXXXXXXX.curry.michimani.net. VALIDATION_RECORD_VALUE = _2166df0b8981XXXXXXXXXXXXXXXXXXXX.wggjkglgrm.acm-validations.aws. この情報を Route 53 に登録します。その際、ホストゾーンに登録しているドメインの HostedZoneId が必要になるので、 route53 list-hosted-zones コマンドの出力から変数に設定しておきます。\n$ HOSTED_ZONE_ID=$( \\ aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;${HOSTED_DOMAIN}.\u0026#39;].Id\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo ${HOSTED_ZONE_ID} /hostedzone/Z39XXXXXXXXXX Route 53 の RecordSet の作成には route53 change-resource-record-sets コマンドを使用します。\n$ route53 change-resource-record-sets help ... SYNOPSIS change-resource-record-sets --hosted-zone-id \u0026lt;value\u0026gt; --change-batch \u0026lt;value\u0026gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] ... このコマンドは作成時だけでなく、変更・削除の際にも使用します。どのアクションを実行するかはオプション内の値で指定します。\n今回は RecordSet の作成なので、次のように実行します。\n$ aws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch \\ \u0026#34;{ \\\u0026#34;Changes\\\u0026#34;: [ { \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;CREATE\\\u0026#34;, \\\u0026#34;ResourceRecordSet\\\u0026#34;: { \\\u0026#34;Name\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_NAME}\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;CNAME\\\u0026#34;, \\\u0026#34;TTL\\\u0026#34;: 300, \\\u0026#34;ResourceRecords\\\u0026#34;: [{\\\u0026#34;Value\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_VALUE}\\\u0026#34;}] } } ] }\u0026#34; { \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C0057826YCD9M3MBZEJV\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2020-12-08T22:33:51.455000+00:00\u0026#34; } } 実行結果として、リソースの変更情報が出力されます。実行直後は Status が PENDING になっていますが、体感としてはすぐに INSYNC (変更完了) になります。一応 route53 get-change コマンドを使ってステータスを確認することはできます。\n$ aws route53 get-change \\ --id /change/C0057826YCD9M3MBZEJV { \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C0057826YCD9M3MBZEJV\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;INSYNC\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2020-12-08T22:33:51.455000+00:00\u0026#34; } } 3. ドメイン認証ステータスを確認 最後に、ドメイン認証のステータスを確認します。確認には acm describe-certificate コマンドを使います。先ほどの実行結果でもわかるように Certificate.DomainValidationOptions[0].ValidationStatus にドメイン認証のステータスが含まれているので、その部分だけ出力します。\n$ aws acm describe-certificate \\ --certificate-arn ${CERT_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ValidationStatus\u0026#34; \\ --output text SUCCESS 実行タイミングにもよりますが、認証が成功していれば SUCCESS と出力されます。\nシェルスクリプト化する 上記でやった手順をスクリプト化します。と言っても、ほとんどコピペしてつなげただけです。\n#!/bin/bash set -e if [ $# != 3 ] || [ $1 = \u0026#34;\u0026#34; ] || [ $2 = \u0026#34;\u0026#34; ] || [ $3 = \u0026#34;\u0026#34; ]; then echo -e \u0026#34;Three parameters are required 1st - string: Hosted Domain Name on Route 53 (e.g. example.com) 2nd - string: Domain Name for Certificate (e.g. sub.mexample.com) 3rd - string: Target Region (e.g. us-east-1) example command \\t sh ./issue-certificate.sh example.com sub.example.com\u0026#34; exit fi HOSTED_DOMAIN=$1 TARGET_DOMAIN=$2 REGION=$3 NONE=\u0026#34;None\u0026#34; # request certificate echo \u0026#34;Request certificate for \u0026#39;${TARGET_DOMAIN}\u0026#39; to ACM.\u0026#34; CERT_ARN=$( \\ aws acm request-certificate \\ --domain-name ${TARGET_DOMAIN} \\ --validation-method DNS \\ --region ${REGION} \\ --output text) \\ \u0026amp;\u0026amp; sleep 5 \\ \u0026amp;\u0026amp; echo -e \u0026#34;\\t CERT_ARN = ${CERT_ARN}\u0026#34; # create domain validation record set echo \u0026#34;Create record set to validate domain in Route 53.\u0026#34; VALIDATION_RECORD_NAME=$( \\ aws acm describe-certificate \\ --certificate-arn ${CERT_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ResourceRecord.Name\u0026#34; \\ --region ${REGION} \\ --output text) \\ \u0026amp;\u0026amp; echo -e \u0026#34;\\t VALIDATION_RECORD_NAME = ${VALIDATION_RECORD_NAME}\u0026#34; VALIDATION_RECORD_VALUE=$( \\ aws acm describe-certificate \\ --certificate-arn ${CERT_ARN} \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ResourceRecord.Value\u0026#34; \\ --region ${REGION} \\ --output text) \\ \u0026amp;\u0026amp; echo -e \u0026#34;\\t VALIDATION_RECORD_VALUE = ${VALIDATION_RECORD_VALUE}\u0026#34; HOSTED_ZONE_ID=$( \\ aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;${HOSTED_DOMAIN}.\u0026#39;].Id\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo -e \u0026#34;\\t HOSTED_ZONE_ID = ${HOSTED_ZONE_ID}\u0026#34; if [ $VALIDATION_RECORD_NAME == $NONE ] || [ $VALIDATION_RECORD_VALUE == $NONE ] || [ $HOSTED_DOMAIN == $NONE ]; then echo \u0026#34;Failed to get the parameters required for domain validation.\u0026#34; exit fi CHANGE_ID=$( \\ aws route53 change-resource-record-sets \\ --hosted-zone-id ${HOSTED_ZONE_ID} \\ --change-batch \\ \u0026#34;{ \\\u0026#34;Changes\\\u0026#34;: [ { \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;CREATE\\\u0026#34;, \\\u0026#34;ResourceRecordSet\\\u0026#34;: { \\\u0026#34;Name\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_NAME}\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;CNAME\\\u0026#34;, \\\u0026#34;TTL\\\u0026#34;: 300, \\\u0026#34;ResourceRecords\\\u0026#34;: [{\\\u0026#34;Value\\\u0026#34;: \\\u0026#34;${VALIDATION_RECORD_VALUE}\\\u0026#34;}] } } ] }\u0026#34; \\ --query \u0026#34;ChangeInfo.Id\u0026#34; \\ --output text) \\ \u0026amp;\u0026amp; echo -e \u0026#34;\\t Change ID : ${CHANGE_ID}\\n\u0026#34; if [ $? == 0 ]; then echo -e \u0026#34;\\nFinished to request certificate and create record set to validate domain. Please run command bellow to check validation status. aws acm describe-certificate \\\\ --certificate-arn ${CERT_ARN} \\\\ --query \\\u0026#34;Certificate.DomainValidationOptions[0].ValidationStatus\\\u0026#34; \\\\ --region ${REGION} \\\\ --output text\u0026#34; else echo -e \u0026#34;\\nFailed to issue certificate.\u0026#34; fi 最初の Certificate のリクエストのあとに sleep を入れている理由は、リクエスト直後だと acm describe-certificate コマンドで Certificate の情報を取得できない可能性があるからです。その場合、ドメイン認証用の Name または Value が None となり、 route53 change-resource-record-sets コマンドを実行時に下記のようなエラーとなり失敗してしまいます。\nAn error occurred (InvalidChangeBatch) when calling the ChangeResourceRecordSets operation: [RRSet with DNS name none. is not permitted in zone michimani.net.]\n実行してみる このシェルスクリプトでは、実行時の引数として 3 つの値を指定します。\n第一引数 : ホストゾーンのドメイン名 (例: michimani.net) 第二引数 : Certificate に紐付けるドメイン名 (例: sub.michimani.net) 第三引数 : Certificate を発行するリージョン (例: ap-northeast-1) 上記の例の値で実行してみます。\n$ ./issue-certificate.sh michimani.net sub.michimani.net ap-northeast-1 Request certificate for \u0026#39;sub.michimani.net\u0026#39; to ACM. CERT_ARN = arn:aws:acm:ap-northeast-1:************:certificate/ac2c1f33-96ea-XXXX-XXXX-XXXXXXXXXX Create record set to validate domain in Route 53. VALIDATION_RECORD_NAME = _d82165d09e36XXXXXXXXXXXXXXXXXXXX.sub.michimani.net. VALIDATION_RECORD_VALUE = _e1a9a5b806b2XXXXXXXXXXXXXXXXXXXX.wggjkglgrm.acm-validations.aws. HOSTED_ZONE_ID = /hostedzone/Z39XXXXXXXXXX Change ID : /change/C06129382Z4LWAZ4JDF96 Finished to request certificate and create record set to validate domain. Please run command bellow to check validation status. aws acm describe-certificate \\ --certificate-arn arn:aws:acm:ap-northeast-1:************:certificate/ac2c1f33-96ea-XXXX-XXXX-XXXXXXXXXX \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ValidationStatus\u0026#34; \\ --region ap-northeast-1 \\ --output text 最後に Certificate の認証ステータスを確認するコマンドが出力されるので、それを実行してステータスを確認します。\n$ aws acm describe-certificate \\ --certificate-arn arn:aws:acm:ap-northeast-1:************:certificate/ac2c1f33-96ea-XXXX-XXXX-XXXXXXXXXX \\ --query \u0026#34;Certificate.DomainValidationOptions[0].ValidationStatus\u0026#34; \\ --region ap-northeast-1 \\ --output text SUCCESS まとめ なんとなく手作業で作成している ACM の Certificate を AWS CLI でスクリプト化してサクッと作成できるようにしてみた話でした。\n各プロダクト・サービスごとに一回しかやらない作業とは言え、スクリプト化されているとかなり楽です。AWS CLI を使えば、各 API を組み合わせて一連の操作をスクリプト化することにより、独自のコマンドのようなものが簡単に作成できます。特に、今回作成したような、コードで管理するほどでもないリソースを作成するような場面で AWS CLI は活きるなと思いました。\nシェルスクリプトがうまく動かないとか、こうしたほうがいいとか、ご意見あれば下記の gist にコメントを頂けると幸いです。\nThis is a shell script for issuing SSL certificates with ACM (Amazon Certificate Manager). Gist ",
    "permalink": "https://michimani.net/post/aws-issue-certificate-at-acm-by-shell-script/",
    "title": "ACM での SSL 証明書発行を AWS CLI でスクリプト化する"
  },
  {
    "contents": "AWS が提供しているフルマネージドなコンテナレジストリサービス Amazon ECR がパブリックレジストリとして利用できるようになりました。いわゆる、 Docker Hub のような使い方ができるようになるようです。ということで、実際に ECR のパブリックレジストリでイメージを公開してみようと思います。\n概要 Amazon ECR がパブリックレジストリとして利用できるようになりました。これまでは AWS 内部 (ECS/EKS) から使うことが基本 (というかそれしかできなかった？) でしたが、パブリックなレジストリとして利用できるようになったことで、 Docker Hub のように AWS 内外に関わらずコンテナイメージを利用できるようになります。\n詳しくは既にクラスメソッドのハマコーさんがブログ化されています。毎年 re:Invent 開催期には最速で情報発信しているクラスメソッドさんにはあらためて感謝です。今年は期間が長いのでみなさんのブログ筋が最後まで元気に動くように陰ながら応援しています。\nイメージを公開してみる では、さっそくコンテナイメージを公開してみたいと思います。今回はすべて AWS CLI で操作をしていきます。\nAWS CLI のバージョン確認 現時点 (2020/12/02) で v1 と v2 の最新バージョンは下記のとおりです。\nv1\naws-cli-v1 $ aws --version aws-cli/1.18.187 Python/3.8.5 Darwin/19.6.0 botocore/1.19.27 v2\n$ aws --version aws-cli/2.1.5 Python/3.7.4 Darwin/19.6.0 exe/x86_64 prompt/off ECR のパブリックレジストリに対する操作は ecr-public という新たに追加されたコマンドを使いますが、上記のバージョンで対応しているのは v1 のみです。 v2 ではまだ使えないようなので、注意です。 その後 v2 の最新バージョン 2.1.6 がリリースされ、 ecr-public コマンドが追加されていることを確認しました。v1 だけでなく v2 でも実行可能です。\necr-public コマンドで使用できるサブコマンドは下記の通りです。\n$ aws ecr-public help ... AVAILABLE COMMANDS o batch-check-layer-availability o batch-delete-image o complete-layer-upload o create-repository o delete-repository o delete-repository-policy o describe-image-tags o describe-images o describe-registries o describe-repositories o get-authorization-token o get-login-password o get-registry-catalog-data o get-repository-catalog-data o get-repository-policy o help o initiate-layer-upload o put-image o put-registry-catalog-data o put-repository-catalog-data o set-repository-policy o upload-layer-part リポジトリの作成 まずは ECR にパブリックなリポジトリを作成します。サブコマンドは create-repository です。\n$ aws ecr-public create-repository help ... SYNOPSIS create-repository --repository-name \u0026lt;value\u0026gt; [--catalog-data \u0026lt;value\u0026gt;] [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] リポジトリ名だけ指定すれば良さそうです。\n以前、 Amazon Linux 2 に Python 3.8 をインストールしたイメージを作って Docker Hub に置いているので、同じものを ECR で公開してみます。\nmichimani/amzn2py38 - Docker Hub ということで、リポジトリ名は amzn2py38 とします。\n$ aws ecr-public create-repository \\ --repository-name amzn2py38 Could not connect to the endpoint URL: \u0026#34;https://api.ecr-public.ap-northeast-1.amazonaws.com/\u0026#34; できませんでした。ということでリージョンを us-east-1 にして再実行してみます。\n$ aws ecr-public create-repository \\ --repository-name amzn2py38 \\ --region us-east-1 { \u0026#34;repository\u0026#34;: { \u0026#34;repositoryArn\u0026#34;: \u0026#34;arn:aws:ecr-public::************:repository/amzn2py38\u0026#34;, \u0026#34;registryId\u0026#34;: \u0026#34;************\u0026#34;, \u0026#34;repositoryName\u0026#34;: \u0026#34;amzn2py38\u0026#34;, \u0026#34;repositoryUri\u0026#34;: \u0026#34;public.ecr.aws/e4v1s0v0/amzn2py38\u0026#34;, \u0026#34;createdAt\u0026#34;: 1606861651.668 }, \u0026#34;catalogData\u0026#34;: {} } 作成できました。一応 describe-repositories サブコマンドで確認します。\n$ aws ecr-public describe-repositories \\ --repository-names amzn2py38 \\ --region us-east-1 { \u0026#34;repositories\u0026#34;: [ { \u0026#34;repositoryArn\u0026#34;: \u0026#34;arn:aws:ecr-public::************:repository/amzn2py38\u0026#34;, \u0026#34;registryId\u0026#34;: \u0026#34;************\u0026#34;, \u0026#34;repositoryName\u0026#34;: \u0026#34;amzn2py38\u0026#34;, \u0026#34;repositoryUri\u0026#34;: \u0026#34;public.ecr.aws/e4v1s0v0/amzn2py38\u0026#34;, \u0026#34;createdAt\u0026#34;: 1606861651.668 } ] } 取得できました。\nイメージの Push 作成したリポジトリにイメージを Push します。これまで ECR にイメージを Push してきたときと同じように、 ECR にログイン後、 docker push コマンドでイメージを Push します。従来の方法と異なっているのは、 ECR にログインする際に ecr-public get-login-password コマンドを使うようになっている点です。\nということで、まずは ECR にログインします。\n$ aws ecr-public get-login-password \\ --region us-east-1 \\ | docker login \\ --username AWS --password-stdin public.ecr.aws Login Succeeded イメージをビルドします。\n$ docker build -t amzn2py38 . タグ付けします。\n$ docker tag amzn2py38:latest public.ecr.aws/e4v1s0v0/amzn2py38:latest Push します。\ndocker push public.ecr.aws/e4v1s0v0/amzn2py38:latest Push したイメージを確認してみます。サブコマンドは describe-images です。\n$ aws ecr-public describe-images help ... SYNOPSIS describe-images [--registry-id \u0026lt;value\u0026gt;] --repository-name \u0026lt;value\u0026gt; [--image-ids \u0026lt;value\u0026gt;] [--cli-input-json \u0026lt;value\u0026gt;] [--starting-token \u0026lt;value\u0026gt;] [--page-size \u0026lt;value\u0026gt;] [--max-items \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] リポジトリ名を指定します。\n$ aws ecr-public describe-images \\ --repository-name amzn2py38 \\ --region us-east-1 { \u0026#34;imageDetails\u0026#34;: [ { \u0026#34;registryId\u0026#34;: \u0026#34;************\u0026#34;, \u0026#34;repositoryName\u0026#34;: \u0026#34;amzn2py38\u0026#34;, \u0026#34;imageDigest\u0026#34;: \u0026#34;sha256:8e6d74d747e42aefa22e27b2656b07356da814f3a2e6c17ba0f54310fdea8c54\u0026#34;, \u0026#34;imageTags\u0026#34;: [ \u0026#34;latest\u0026#34; ], \u0026#34;imageSizeInBytes\u0026#34;: 206732282, \u0026#34;imagePushedAt\u0026#34;: 1606863922.0, \u0026#34;imageManifestMediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34; } ] } イメージが Push できてます。\nこちらが公開されたイメージです。\nECR Public Gallery | e4v1s0v0/amzn2py38 まとめ Amazon ECR がパブリックレジストリとして利用できるようになったので、イメージを公開してみた話でした。\nDocker Hub の利用制限 (Pull 回数の制限) に苦しんでいる方は ECR で公開されているパブリックイメージを利用してみるのも良さそうです。ECR で公開されているイメージは ECR Public Gallery で検索できるのですが、既に MySQL や nginx といったイメージも公開されているようなので、前向きに検討できそうです。\nあと、やっぱり AWS CLI の新機能に対する対応はまだまだ v1 のほうが早いので、すぐに試したい！という方は v1 と v2 の両方を共存させておくとよいかもしれません。\n参考 [TIPS] AWS CLIの「v1」と「v2」を共存させて使う方法 | Developers.IO [TIPS] AWS CLIの「v1」と「v2」を共存させて使う方法 (Dockerコンテナ編) | Developers.IO ",
    "permalink": "https://michimani.net/post/aws-push-image-to-ecr-public-registry/",
    "title": "Amazon ECR のパブリックレジストリでイメージを公開してみる"
  },
  {
    "contents": "JAWS-UG CLI 専門支部 #172R S3基礎 (オブジェクト) に参加したので、そのレポートです。\nconnpass のイベントページはこちら。\nこれまでの CLI 専門支部参加レポートはこちら。\n#171R S3基礎 通知 (Lambdaの自動実行) #170R S3基礎 ライフサイクル #169R S3基礎 バージョニング #168R S3基礎 Webサイト\u0026amp;amp;ログ #167R EventBridge入門 目次 S3 の全体像、オブジェクトの操作 2020/11/26 時点での AWS CLI のバージョン ハンズオン 2.2 S3 オブジェクトの新規アップロード 2.4 S3 バケットのオブジェクト一覧取得 2.7 S3 オブジェクトの更新 知らんかったぞ、それ S3 バケットのロケーション (リージョン) だけ取得する head-object でストレージクラスの確認 delete-object と delete-objects まとめ S3 の全体像、オブジェクトの操作 ※基本的には前回までと同内容\nOutposts 用の s3outposts コマンドが出てきた\n$ aws s3outposts help ... NAME s3outposts - DESCRIPTION Amazon S3 on Outposts provides access to S3 on Outposts operations. AVAILABLE COMMANDS o create-endpoint o delete-endpoint o help o list-endpoints その他、 S3 としては下記のコマンドがある\ns3 : ハイレベルコマンド s3api : 基本の API s3control : アクセスポイント、バッチオペレーション、 Storage Lens などの操作 ファイルを置くだけではなく、他の人・リソースに共有するための仕組みも多数\nストレージの分類\nブロックストレージ 固定長 OS が管理している (論理的に可変長にしている) ファイルストレージ NAS など オブジェクトストレージ S3 オブジェクトの名前スキーマ\nバージョニング無効 : S3://\u0026lt;バケット名\u0026gt;/\u0026lt;キー\u0026gt; バージョニング有効 : S3://\u0026lt;バケット名\u0026gt;/\u0026lt;キー\u0026gt;\u0026lt;バージョンID\u0026gt; 2020/11/26 時点での AWS CLI のバージョン これまでにもレポートを書いてきましたが、その当時の AWS CLI バージョンを書いておこうと思います。今回のハンズオンがあった 2020/11/26 時点での v1 および v2 それぞれの最新バージョンは下記のとおりでした。\nv1\naws-cli-v1 $ aws --version aws-cli/1.18.185 Python/3.8.5 Darwin/19.6.0 botocore/1.19.25 v2\n$ aws --version aws-cli/2.1.4 Python/3.7.4 Darwin/19.6.0 exe/x86_64 今回は特に最新の機能を使うわけではないので、 v2 を使用していきます。ここ数日は re:Invent 前のアップデートが怒涛の勢いで出てきていますが、新しい API に関しては (後に v2 でも使えるものの) v1 でのみ対応している場合があるので、最新の機能を CLI で試す場合には v1 を使ったほうがよさそうです。\n参考 PartiQL が DynamoDB に対応したので AWS CLI を使って DynamoDB テーブルに SQL 文を実行してみる Amazon S3 Storage Lens を AWS CLI で触ってみる ハンズオン 今回のハンズオンでは、新たに作成した S3 バケットに対してオブジェクトを追加し、そのオブジェクトに対してコピーや属性の更新などを実行した後、削除するところまでを CLI で操作しました。\n今回も、ハンズオンの詳細な手順についてはイベントページの資料におまかせするとして、手順の中で気になったところを抜粋して確認してみます。\n2.2 S3 オブジェクトの新規アップロード オブジェクトのアップロードには s3api put-object コマンドを使います。オプションはたくさんありますが、必須なのは --bucket と --key のみです。\n$ aws s3api put-object help ... SYNOPSIS put-object [--acl \u0026lt;value\u0026gt;] [--body \u0026lt;value\u0026gt;] --bucket \u0026lt;value\u0026gt; [--cache-control \u0026lt;value\u0026gt;] [--content-disposition \u0026lt;value\u0026gt;] [--content-encoding \u0026lt;value\u0026gt;] [--content-language \u0026lt;value\u0026gt;] [--content-length \u0026lt;value\u0026gt;] [--content-md5 \u0026lt;value\u0026gt;] [--content-type \u0026lt;value\u0026gt;] [--expires \u0026lt;value\u0026gt;] [--grant-full-control \u0026lt;value\u0026gt;] [--grant-read \u0026lt;value\u0026gt;] [--grant-read-acp \u0026lt;value\u0026gt;] [--grant-write-acp \u0026lt;value\u0026gt;] --key \u0026lt;value\u0026gt; [--metadata \u0026lt;value\u0026gt;] [--server-side-encryption \u0026lt;value\u0026gt;] [--storage-class \u0026lt;value\u0026gt;] [--website-redirect-location \u0026lt;value\u0026gt;] [--sse-customer-algorithm \u0026lt;value\u0026gt;] [--sse-customer-key \u0026lt;value\u0026gt;] [--sse-customer-key-md5 \u0026lt;value\u0026gt;] [--ssekms-key-id \u0026lt;value\u0026gt;] [--ssekms-encryption-context \u0026lt;value\u0026gt;] [--request-payer \u0026lt;value\u0026gt;] [--tagging \u0026lt;value\u0026gt;] [--object-lock-mode \u0026lt;value\u0026gt;] [--object-lock-retain-until-date \u0026lt;value\u0026gt;] [--object-lock-legal-hold-status \u0026lt;value\u0026gt;] [--expected-bucket-owner \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] 各オプションの詳細については下記の公式ドキュメントを参照してください。\nput-object — AWS CLI 2.1.4 Command Reference ここでは新たにオブジェクトをアップロードするので、 --body オプションでローカルにあるファイルを指定します。\n$ cat alpha.txt This is A text. $ aws s3api put-object \\ --bucket ${S3_BUCKET_NAME} \\ --key alpha/message_a.txt \\ --body ./alpha.txt { \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;75721060f35a9f9f6bdaf9985394e9a8\\\u0026#34;\u0026#34; } 同様に\n$ cat beta.txt\nThis is B text.\n$ aws s3api put-object \u0026ndash;bucket ${S3_BUCKET_NAME} \u0026ndash;key beta/message_b.txt \u0026ndash;body ./beta.txt\n{ \u0026ldquo;ETag\u0026rdquo;: \u0026ldquo;\u0026quot;424e9ac2402771285730c91435c5a23d\u0026quot;\u0026rdquo; }\n正しくアップロードできたか確認します。ここでは簡潔に s3 ls コマンドで確認します。\n$ aws s3 ls s3://${S3_BUCKET_NAME}/alpha/ 2020-11-26 22:49:09 16 message_a.txt $ aws s3 ls s3://${S3_BUCKET_NAME}/beta/ 2020-11-26 22:49:17 17 message_b.txt 2.4 S3 バケットのオブジェクト一覧取得 S3 バケット内のオブジェクト一覧を取得するコマンドとしては list-objects と list-objects-v2 が存在しますが、ドキュメントでは Warning として次のように書かれています。\nThis API has been revised. We recommend that you use the newer version, ListObjectsV2 , when developing applications. For backward compatibility, Amazon S3 continues to support ListObjects .\nlist-objects — AWS CLI 2.1.4 Command Reference ということで、 list-objects-v2 を使うようにしましょう。\n$ aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} { \u0026#34;Contents\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;alpha/message_a.txt\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T13:49:09+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;75721060f35a9f9f6bdaf9985394e9a8\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 16, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;beta/message_b.txt\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T13:49:17+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;424e9ac2402771285730c91435c5a23d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 17, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; } ] } 2.7 S3 オブジェクトの更新 S3 オブジェクトの更新には、新規アップロードの時と同じ put-object コマンドを使います。先程も書いたように、必須のオプションは --bucket と --key です。オブジェクト自体を更新しない場合は --body の指定は不要で、更新したい属性に対応するオプションを指定すれば OK です。例えば、オブジェクトにメタデータを追加する場合は --metadata オプションを使って下記のように実行します。\n$ aws s3api put-object \\ --bucket ${S3_BUCKET_NAME} \\ --key alpha/message_a.txt \\ --metadata secretLevel=1,Category=\u0026#34;sample text\u0026#34; { \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34; } 確認してみます。\n$ aws s3api head-object \\ --bucket ${S3_BUCKET_NAME} \\ --key alpha/message_a.txt { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T13:54:44+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 0, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;sample text\u0026#34;, \u0026#34;secretlevel\u0026#34;: \u0026#34;1\u0026#34; } } ここで注意したいのが、メタデータのキーは大文字/小文字が区別されないことです。 put-object コマンドでは secretLevel と Category を指定しましたが、実際には category と secretlevel が設定されています。 ただし、 SDK for Go でメタデータにアクセスする際、レスポンスのキー名はキャメルケースになるのが個人的にはハマりポイントかなと思います。\nメタデータ以外の例として、ストレージクラスを変更してみます。\n$ aws s3api put-object \\ --bucket ${S3_BUCKET_NAME} \\ --key beta/message_b.txt \\ --storage-class ONEZONE_IA { \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34; } 確認しておきます。\n$ aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} { \u0026#34;Contents\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;alpha/message_a.txt\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T13:58:42+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 0, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;beta/message_b.txt\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T14:08:24+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 0, \u0026#34;StorageClass\u0026#34;: \u0026#34;ONEZONE_IA\u0026#34; } ] } 知らんかったぞ、それ 今回に限らずハンズオンでは新しく知ることがたくさんあるので、この 知らんかったぞ、それ 項目では新しく知ったことを自分なりに深堀りしてみたいと思います。(恒例化したい)\nというわけで、前回と前々回はこのコーナーがなかったのですが、今回はいくつか取り上げてみます。\nS3 バケットのロケーション (リージョン) だけ取得する かなりの小ネタっぽい内容ですが、対象のバケットのリージョンだけを取得するコマンド (API) が存在していることを初めて知りました。\n$ aws s3api get-bucket-location \\ --bucket ${S3_BUCKET_NAME} { \u0026#34;LocationConstraint\u0026#34;: \u0026#34;ap-northeast-1\u0026#34; } head-object でストレージクラスの確認 オブジェクトのストレージクラスを確認するには list-object-v2 コマンドを使うしか無いと思っていたのですが、 head-object コマンドでも取得できるようでした。ただし条件があります。まずは beta/message_b.txt の情報を取得してみます。\n$ aws s3api head-object \\ --bucket ${S3_BUCKET_NAME} \\ --key beta/message_b.txt { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T14:08:24+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 0, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: {}, \u0026#34;StorageClass\u0026#34;: \u0026#34;ONEZONE_IA\u0026#34; } 取得できてますね。では alpha/message_a.txt も取得してみます。\n$ aws s3api head-object \\ --bucket ${S3_BUCKET_NAME} \\ --key alpha/message_a.txt { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-26T13:58:42+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 0, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;d41d8cd98f00b204e9800998ecf8427e\\\u0026#34;\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;sample text\u0026#34;, \u0026#34;secretlevel\u0026#34;: \u0026#34;1\u0026#34; } } はい。 StorageClass の属性がありません。\nこれはどういうことかというと、どうやらストレージクラスが S3 標準 (STANDARD) の場合は head-object のレスポンス内に情報が含まれないようです。\n個人的には、 head-object で取得できないと思っていたので嬉しい誤算でしたが、基本的に扱うのは標準クラスのオブジェクトなので、やっぱりちょっと残念だなという思いです。\ndelete-object と delete-objects S3 オブジェクトを削除するコマンドとして delete-object と delete-objects が存在しています。コマンド名だけ見ると delete-objects のほうは複数のオブジェクトをまとめて削除できそうなので便利かなと思うんですが、実際は対象のオブジェクトをすべて列挙する必要があるため逆に使いづらいようです。\n$ aws s3api delete-objects help ... OPTIONS ... --delete (structure) Container for the request. Objects -\u0026gt; (list) The objects to delete. (structure) Object Identifier is unique value to identify objects. Key -\u0026gt; (string) Key name of the object to delete. VersionId -\u0026gt; (string) VersionId for the specific version of the object to delete. Quiet -\u0026gt; (boolean) Element to enable quiet mode for the request. When you add this element, you must set its value to true. Shorthand Syntax: Objects=[{Key=string,VersionId=string},{Key=string,VersionId=string}],Quiet=boolean JSON Syntax: { \u0026#34;Objects\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;VersionId\u0026#34;: \u0026#34;string\u0026#34; } ... ], \u0026#34;Quiet\u0026#34;: true|false } 削除対象は --delete オプションで指定するのですが、 Shorthand Syntax で指定するにしても\nObjects=[{Key=string,VersionId=string},{Key=string,VersionId=string}],Quiet=boolean のような形で指定する必要があり、この文字列を生成するのはなかなか面倒です。\nであれば、削除対象のオブジェクトを list-objects-v2 コマンドで取得して、それらに対して delete-object を使ったほうが楽そうです。実際には次のような形で実行します。\n$ for i in $( aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} \\ --query \u0026#39;Contents[].Key\u0026#39; \\ --output text ); do aws s3api delete-object \\ --bucket ${S3_BUCKET_NAME} \\ --key ${i} done これで list-objects-v2 で取得できるオブジェクトをすべて削除することができます。\nまとめ JAWS-UG CLI 専門支部 #172R S3基礎 (オブジェクト) に参加したので、そのレポートでした。\n今回はオブジェクトに対する操作を、ハイレベルコマンドの aws s3 ではなく API と対になっている aws s3api で実行しました。 s3 コマンドでは cp や ls といったサブコマンドで操作できるので、単にオブジェクトの移動や削除を行う際には非常に簡単です。一方 s3api コマンドでは、 API と対になったサブコマンドを使用して操作するため、 JSON 形式で生の情報を取得することができます。そのため、実行結果から必要な部分を --query で抜き出したり、他のコマンドに渡したりしやすくなります。\nそれ以前に、やはり API ベースで何が行われているのかがわかるのが s3api コマンドの良いところだと思います。\n次回のレプリケーションで S3 については一旦区切りかなと思いますが、だいぶ S3 に関する知識がついてきたような気がします。そんな中で感じるのが、やっぱり S3 は Simple じゃないってことですね。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-172r-s3-object/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #172R S3基礎 (オブジェクト) に参加しました #jawsug_cli"
  },
  {
    "contents": "re:Invent 前の怒涛のアップデートが続く中で、 PartiQL が Amazon DynamoDB に対応したというアップデートが発表されました。これによって、 DynamoDB のテーブルに対して SQL 文を実行することができます。今回は、 AWS CLI 経由で DynamoDB のテーブルに対して SQL 文を実行してみます。\n目次 概要 PartiQL とは AWS CLI で試す v2 は未対応 execute-statement コマンド SELECT 文 INSERT 文 まとめ 概要 PartiQL が Amazon DynamoDB に対応し、DynamoDB のテーブルに対して SQL 文を実行できるようになりました。\nPartiQL - A SQL-Compatible Query Language for Amazon DynamoDB - Amazon DynamoDB PartiQL とは PartiQL とは、 AWS がオープンソースとして公開している SQL 互換の問い合わせ言語です。CSV や JSON などのフォーマットで保存されているデータに対して、 SQL 文で操作を行うことができます。\nAnnouncing PartiQL: One query language for all your data | AWS Open Source Blog AWS CLI で試す DynamoDB テーブルに対する SQL の実行は、マネジメントコンソール、 AWS SDK、 AWS CLI から行うことができます。今回は AWS CLI から SELECT 文を INSERT 文の実行を試してみます。\nv2 は未対応 2020/11/24 時点で AWS CLI の v1/v2 それぞれの最新バージョンは下記のとおりです。\nv1 aws-cli-v1 $ aws --version aws-cli/1.18.184 Python/3.8.5 Darwin/19.6.0 botocore/1.19.24 v2 $ aws --version aws-cli/2.1.3 Python/3.7.4 Darwin/19.6.0 exe/x86_64 DynamoDB テーブルに対して SQL 文を実行するときに使うコマンドは dynamodb execute-statement なのですが、 上記の時点で対応しているのは v1 のみです 今日 (2020/11/25) にリリースされた 2.1.4 で v2 でも利用できるようになっています。\nなので、今回は 1.18.184 で実行していきます。\nexecute-statement コマンド SQL の実行に使用する execute-statement コマンドは、次のようなパラメータを受け付けます。\n$ aws dynamodb execute-statement help ... SYNOPSIS execute-statement --statement \u0026lt;value\u0026gt; [--parameters \u0026lt;value\u0026gt;] [--consistent-read | --no-consistent-read] [--next-token \u0026lt;value\u0026gt;] [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] --statement オプションで SQL 文を指定する形になります。\nSELECT 文 まずは SELECT 文を実行してみます。\n$ aws dynamodb execute-statement \\ --statement \u0026#39;SELECT * FROM Thread;\u0026#39; { \u0026#34;Items\u0026#34;: [ { \u0026#34;LastPostedDateTime\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2015-09-22T19:58:22.514Z\u0026#34; }, \u0026#34;Replies\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CloudFront thread 1 message\u0026#34; }, \u0026#34;LastPostedBy\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34; }, \u0026#34;Answered\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon CloudFront\u0026#34; }, \u0026#34;Tags\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;index\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;primarykey\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;table\u0026#34; } ] }, \u0026#34;Views\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CloudFront Thread 1\u0026#34; } }, { \u0026#34;LastPostedDateTime\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2015-09-29T19:58:22.514Z\u0026#34; }, \u0026#34;Replies\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;S3 thread 1 message\u0026#34; }, \u0026#34;LastPostedBy\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34; }, \u0026#34;Answered\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Views\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon S3\u0026#34; }, \u0026#34;Tags\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;largeobjects\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;multipart upload\u0026#34; } ] }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;S3 Thread 1\u0026#34; } }, { \u0026#34;LastPostedDateTime\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2015-09-22T19:58:22.514Z\u0026#34; }, \u0026#34;Replies\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB thread 1 message\u0026#34; }, \u0026#34;LastPostedBy\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34; }, \u0026#34;Answered\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Views\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34; }, \u0026#34;Tags\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;index\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;primarykey\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;table\u0026#34; } ] }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 1\u0026#34; } }, { \u0026#34;LastPostedDateTime\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2015-09-15T19:58:22.514Z\u0026#34; }, \u0026#34;Replies\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB thread 2 message\u0026#34; }, \u0026#34;LastPostedBy\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34; }, \u0026#34;Answered\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34; }, \u0026#34;Views\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Tags\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;items\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;attributes\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;throughput\u0026#34; } ] }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34; } } ] } INSERT 文 続いて、 INSERT 文で項目を追加してみます。\n$ aws dynamodb execute-statement \\ --statement \u0026#34;SELECT * FROM Thread WHERE ForumName=\u0026#39;DynamoDB\u0026#39; AND Subject=\u0026#39;PartiQL for DynamoDB\u0026#39;;\u0026#34; { \u0026#34;Items\u0026#34;: [] } 正しく追加できたか確認してみます。\n$ aws dynamodb execute-statement \\ --statement \u0026#34;SELECT * FROM Thread WHERE ForumName=\u0026#39;Amazon DynamoDB\u0026#39; AND Subject=\u0026#39;PartiQL for DynamoDB\u0026#39;;\u0026#34; { \u0026#34;Items\u0026#34;: [ { \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;This is a sample SQL.\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34; }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PartiQL for DynamoDB\u0026#34; } } ] } まとめ PartiQL が DynamoDB に対応したので AWS CLI を使って DynamoDB テーブルに SQL 文を実行してみた話でした。\n正直、 AWS CLI から SQL 文を実行することはあまりないかと思います。むしろ、各 SDK から実行できるようになったことで、これまで取得し辛かった内容などは SQL 文を使うことで柔軟に取得できるようになるかもしれません。\n",
    "permalink": "https://michimani.net/post/aws-use-partiql-for-dynamodb-by-aws-cli/",
    "title": "PartiQL が DynamoDB に対応したので AWS CLI を使って DynamoDB テーブルに SQL 文を実行してみる"
  },
  {
    "contents": "先日、 Amazon S3 Storage Lens というサービス (機能) がリリースされました。これは S3 のオブジェクトストレージ使用状況などを可視化できるサービスです。今回はその Amazon S3 Storage Lens を AWS CLI を使って触ってみます。\n目次 Amazon S3 Storage Lens とは AWS CLI で触ってみる Storage Lens に関連するコマンド 既存の情報を取得してみる StorageLensConfiguration の内容 ダッシュボードを作成する まとめ Amazon S3 Storage Lens とは Amazon S3 Storage Lens について、公式ドキュメントでは次のように書かれています。\nAmazon S3 Storage Lens provides a single view of object storage usage and activity across your Amazon S3 storage. With drill-down options to generate insights at the organization, account, bucket, object, or even prefix level. S3 Storage Lens analyzes storage metrics to deliver contextual recommendations to help optimize storage costs and apply best practices on data protection.\nUsing Amazon S3 Storage Lens - Amazon Simple Storage Service 内容を翻訳して要約すると、 Amazon S3 のストレージ使用状況、アクティビティに関する様々な情報をダッシュボードで表示できるサービスです。 AWS Organizations にも対応しており、 Organizations に所属している AWS アカウントの情報も表示することができます。\n詳細についてはクラメソさんのブログで既に紹介されています。\nまだ日本語のドキュメントは用意されていないようですが、英語では下記のドキュメントを参照してください。\nAssessing your storage activity and usage with Amazon S3 Storage Lens - Amazon Simple Storage Service AWS CLI で触ってみる では、早速 AWS CLI を使って Amazon S3 Storage Lens を操作してみます。AWS CLI の v1 と v2 それぞれの最新バージョンは、2020/11/21 時点で 1.18.183 と 2.1.3 となっています。\n(aws-cli-v1) $ aws --version aws-cli/1.18.183 Python/3.8.5 Darwin/19.6.0 botocore/1.19.23 $ aws --version aws-cli/2.1.3 Python/3.7.4 Darwin/19.6.0 exe/x86_64 以降のコマンドは 2.1.3 で実行した結果です。\nStorage Lens に関連するコマンド Amazon S3 Storage Lens に関する AWS CLI のコマンドは s3control に含まれています。\n$ aws s3control help | grep storage-lens o delete-storage-lens-configuration o delete-storage-lens-configuration-tagging o get-storage-lens-configuration o get-storage-lens-configuration-tagging o list-storage-lens-configurations o put-storage-lens-configuration o put-storage-lens-configuration-tagging 既存の情報を取得してみる Amazon S3 Storage Lens ではダッシュボードを作成して S3 に関する情報を可視化するのですが、デフォルトでバージニア北部 (us-east-1) に一つダッシュボードが作成されています。なので、まずはその情報を取得してみます。\nlist-storage-lens-configurations まずは list-storage-lens-configurations です。\n$ aws s3control list-storage-lens-configurations help ... SYNOPSIS list-storage-lens-configurations --account-id \u0026lt;value\u0026gt; [--next-token \u0026lt;value\u0026gt;] [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] --account-id のみが必須のパラメータになっています。実際に実行してみますが、 アカウント ID についても CLI で取得しておきます。\n$ AWS_ACCOUNT_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text \\ ) $ aws s3control list-storage-lens-configurations \\ --account-id ${AWS_ACCOUNT_ID} \\ --region us-east-1 { \u0026#34;StorageLensConfigurationList\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;default-account-dashboard\u0026#34;, \u0026#34;StorageLensArn\u0026#34;: \u0026#34;arn:aws:s3:us-east-1:************:storage-lens/default-account-dashboard\u0026#34;, \u0026#34;HomeRegion\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;IsEnabled\u0026#34;: true } ] } get-storage-lens-configuration ダッシュボードの詳細情報を取得するには get-storage-lens-configuration コマンドを使います。\n$ aws s3control get-storage-lens-configuration help ... SYNOPSIS get-storage-lens-configuration --config-id \u0026lt;value\u0026gt; --account-id \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] --config-id と --account-id が必須パラメータになっています。 --config-id には先ほど取得した StorageLensConfigurationList[0].Id の値を指定します。\n$ aws s3control get-storage-lens-configuration \\ --account-id ${AWS_ACCOUNT_ID} \\ --config-id default-account-dashboard \\ --region us-east-1 { \u0026#34;StorageLensConfiguration\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;default-account-dashboard\u0026#34;, \u0026#34;AccountLevel\u0026#34;: { \u0026#34;BucketLevel\u0026#34;: {} }, \u0026#34;IsEnabled\u0026#34;: true, \u0026#34;StorageLensArn\u0026#34;: \u0026#34;arn:aws:s3:us-east-1:************:storage-lens/default-account-dashboard\u0026#34; } } デフォルトのダッシュボードということもあってあまり情報がありません。\nget-storage-lens-configuration-tagging ダッシュボードに対して付与されたタグ情報を取得するには get-storage-lens-configuration-tagging コマンドを使います。\n$ aws s3control get-storage-lens-configuration-tagging help ... SYNOPSIS get-storage-lens-configuration-tagging --config-id \u0026lt;value\u0026gt; --account-id \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] get-storage-lens-configuration と同じく --config-id と --account-id が必須パラメータになっているので、それぞれ先ほどと同じ値を指定して実行してみます。\n$ aws s3control get-storage-lens-configuration-tagging \\ --account-id ${AWS_ACCOUNT_ID} \\ --config-id default-account-dashboard \\ --region us-east-1 { \u0026#34;Tags\u0026#34;: [] } タグがついてないので空の情報です。\nデフォルトのダッシュボードでは StorageLensConfiguration の情報が乏しかったので、実際にはにどんな情報が含まれるのか調べてみます。\nStorageLensConfiguration の内容 デフォルトのダッシュボードでは内容が乏しかったですが、実際にはどんな情報 (設定項目) が含まれるのか、 put-storage-lens-configuration の --generate-cli-skeleton で確認してみます。\n$ aws s3control put-storage-lens-configuration \\ --generate-cli-skeleton { \u0026#34;ConfigId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;AccountId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;StorageLensConfiguration\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;AccountLevel\u0026#34;: { \u0026#34;ActivityMetrics\u0026#34;: { \u0026#34;IsEnabled\u0026#34;: true }, \u0026#34;BucketLevel\u0026#34;: { \u0026#34;ActivityMetrics\u0026#34;: { \u0026#34;IsEnabled\u0026#34;: true }, \u0026#34;PrefixLevel\u0026#34;: { \u0026#34;StorageMetrics\u0026#34;: { \u0026#34;IsEnabled\u0026#34;: true, \u0026#34;SelectionCriteria\u0026#34;: { \u0026#34;Delimiter\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MaxDepth\u0026#34;: 0, \u0026#34;MinStorageBytesPercentage\u0026#34;: null } } } } }, \u0026#34;Include\u0026#34;: { \u0026#34;Buckets\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;Regions\u0026#34;: [ \u0026#34;\u0026#34; ] }, \u0026#34;Exclude\u0026#34;: { \u0026#34;Buckets\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;Regions\u0026#34;: [ \u0026#34;\u0026#34; ] }, \u0026#34;DataExport\u0026#34;: { \u0026#34;S3BucketDestination\u0026#34;: { \u0026#34;Format\u0026#34;: \u0026#34;Parquet\u0026#34;, \u0026#34;OutputSchemaVersion\u0026#34;: \u0026#34;V_1\u0026#34;, \u0026#34;AccountId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Encryption\u0026#34;: { \u0026#34;SSES3\u0026#34;: {}, \u0026#34;SSEKMS\u0026#34;: { \u0026#34;KeyId\u0026#34;: \u0026#34;\u0026#34; } } } }, \u0026#34;IsEnabled\u0026#34;: true, \u0026#34;AwsOrg\u0026#34;: { \u0026#34;Arn\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;StorageLensArn\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Tags\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;\u0026#34; } ] } 色んな情報 (設定項目) がありますが、コストが発生するかどうかという観点では、下記の項目に注意が必要です。\nAccountLevel ActivityMetrics IsEnabled BucketLevel ActivityMetrics IsEnabled PrefixLevel StorageMetrics IsEnabled 上記の各 IsEnabled には true または false を指定しますが、それぞれのメトリクスの取得は有料となるので、無料で Storage Lens を使いたい場合は false を指定します。\nStorageLensConfiguration.AccountLevel.DataExport では、取得したメトリクスのデータを指定した S3 バケットに出力することができます。\nダッシュボードを作成する 続いては、 AWS CLI を使って Storage Lens のダッシュボードを作成してみます。使用するコマンドは put-storage-lens-configuration です。\n$ aws s3control put-storage-lens-configuration help ... SYNOPSIS put-storage-lens-configuration --config-id \u0026lt;value\u0026gt; --account-id \u0026lt;value\u0026gt; --storage-lens-configuration \u0026lt;value\u0026gt; [--tags \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] 今回は、無料で使えるダッシュボードを作成してみます。設定項目の概要としては下記のとおりです。\nメトリクスは無料で取得できる使用率メトリクス () のみ 対象のリージョンは 東京 ap-northeast-1 データは S3 バケットにエクスポート 以上の要件を満たすダッシュボードを作成するために、次のような JSON を config.json という名前で作成します。\n{ \u0026#34;Id\u0026#34;: \u0026#34;free-dashboard-sample\u0026#34;, \u0026#34;AccountLevel\u0026#34;: { \u0026#34;BucketLevel\u0026#34;: {} }, \u0026#34;Include\u0026#34;: { \u0026#34;Regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ] }, \u0026#34;DataExport\u0026#34;: { \u0026#34;S3BucketDestination\u0026#34;: { \u0026#34;Format\u0026#34;: \u0026#34;Parquet\u0026#34;, \u0026#34;OutputSchemaVersion\u0026#34;: \u0026#34;V_1\u0026#34;, \u0026#34;AccountId\u0026#34;: \u0026#34;************\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:s3:::storage-lens-export-************\u0026#34; } }, \u0026#34;IsEnabled\u0026#34;: true } ダッシュボードを作成します。\n$ aws s3control put-storage-lens-configuration \\ --account-id ${AWS_ACCOUNT_ID} \\ --config-id free-dashboard-sample \\ --storage-lens-configuration file://config.json \\ --region ap-northeast-1 実行後、特に出力はないので、 get-storage-lens コマンドで正しく作成されていることを確認します。\n$ aws s3control get-storage-lens-configuration \\ --account-id ${AWS_ACCOUNT_ID} \\ --config-id free-dashboard-sample \\ --region ap-northeast-1 { \u0026#34;StorageLensConfiguration\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;free-dashboard-sample\u0026#34;, \u0026#34;AccountLevel\u0026#34;: { \u0026#34;BucketLevel\u0026#34;: {} }, \u0026#34;Include\u0026#34;: { \u0026#34;Regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ] }, \u0026#34;DataExport\u0026#34;: { \u0026#34;S3BucketDestination\u0026#34;: { \u0026#34;Format\u0026#34;: \u0026#34;Parquet\u0026#34;, \u0026#34;OutputSchemaVersion\u0026#34;: \u0026#34;V_1\u0026#34;, \u0026#34;AccountId\u0026#34;: \u0026#34;************\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:s3:::storage-lens-export-************\u0026#34; } }, \u0026#34;IsEnabled\u0026#34;: true, \u0026#34;StorageLensArn\u0026#34;: \u0026#34;arn:aws:s3:ap-northeast-1:************:storage-lens/free-dashboard-sample\u0026#34; } } マネジメントコンソール上では次のような形で設定を確認することができます。\nまとめ Amazon S3 Storage Lens を AWS CLI で触ってみた話でした。\nre:Invent 前で怒涛のアップデート、新サービス・新機能の登場が続いていますが、AWS CLI ではほぼすぐにそれらのサービスを触ることができるようです。このあたりの対応スピードはすごいですね。\nただ、今回の Amazon S3 Storage Lens では v2 よりも v1 のほうが反映が早かったようです。(v1 は 1.18.181 で対応していましたが、その時の v2 の最新であった 2.1.2 では使えませんでした) 前に DynamoDB のリージョン間リストアが発表された際も、 v2 の対応は v1 よりもしばらくあとでした。\n新サービス・新機能が出た際には、まずは v1 で試すのが良さそうです。\n",
    "permalink": "https://michimani.net/post/aws-s3-storage-lens-with-cli/",
    "title": "Amazon S3 Storage Lens を AWS CLI で触ってみる"
  },
  {
    "contents": "JAWS-UG CLI専門支部 #171R S3基礎 通知 (Lambdaの自動実行) に参加したので、そのレポートです。\nconnpass のイベントページはこちら。\nこれまでの CLI 専門支部参加レポートはこちら。\n[レポート] JAWS-UG CLI 専門支部 #170R S3基礎 ライフサイクル に参加しました #jawsug_cli [レポート] JAWS-UG CLI 専門支部 #169R S3基礎 バージョニング に参加しました #jawsug_cli [レポート] JAWS-UG CLI 専門支部 #168R S3基礎 Webサイト\u0026amp;amp;ログ に参加しました #jawsug_cli [レポート] JAWS-UG CLI 専門支部 #167R EventBridge入門 に参加しました #jawsug_cli 目次 概要 音声ファイルから文字起こし やってみる 1. 音声ファイルアップロード用の S3 バケットを作成 2. 文字起こし結果を出力する S3 バケットを作成 3. Amazon Transcribe を使って文字起こしする Lambda 関数を作成 4. 1 で作成したバケットにイベント通知設定を追加 5. 確認 まとめ 概要 Amazon S3 はただのストレージではなく、非常に機能が多いストレージになってます。どこが Simple やねん というツッコミもあるくらいです。\n今回は S3 の機能の中の一つである イベント通知 に関する内容でした。イベント通知については下記の記事内でも書いてますが、S3 バケットに対する色んなイベントをトリガーにして他のサービスと連携することができる機能です。\nイベントの内容としては、次のようなものです。\nオブジェクトの作成 : s3:ObjectCreated:* オブジェクトの削除 : s3:ObjectRemoved:* オブジェクトの復元 : s3:ObjectRestore:Post, s3:ObjectRestore:Completed その他のイベントについては下記公式ドキュメントの サポートされるイベントタイプ を参照してください。\nAmazon S3 イベント通知の設定 - Amazon Simple Storage Service そして、これらのイベントをトリガーにして連携できるサービスとしては\nAmazon SNS のトピック Amazon SQS のキュー AWS Lambda の関数 を指定することができます。\n今回のハンズオンでは、オブジェクトの作成 (s3:ObjectCreated:*) イベントをトリガーにして Lambda 関数を起動し、その関数内では S3 から渡されたイベントを print() で出力し、その内容を CloudWatch Logs で確認するという一連の流れを試しました。\nS3 のイベントをトリガーに Lambda 関数を実行するというのは以前に JAWS-UG 初心者支部のハンズオンでもやった内容なので、ここではそのときにやったハンズオンを AWS CLI を使ってやってみることにします。\n音声ファイルから文字起こし 初心者支部でやったハンズオンは、 S3 バケットに音声ファイル (.mp3) をアップロードし、そのイベントをトリガーに Lambda 関数を実行。その Lambda 関数では Amazon Transcribe を使ってアップロードされた音声ファイルを文字起こしして、その結果を再度 S3 バケットに出力するという処理をしています。(実際に結果を S3 バケットに出力するのは Amazon Transcribe)\nさらに、文字起こしした結果が S3 バケットに出力されたことをトリガーにして、今度は Amazon Translate を使って翻訳をする Lambda 関数を実行するところまで実装する宿題もありました。が、今回はそこまでやりません。ちなみにその宿題を普通にやってみた話はこちらです。\nまた、その後の復習イベントで、この内容と同じものを AWS CDK を使って構築してみた話を LT で紹介しました。\nやってみる 前置きが長くなりましたが、早速 CLI でやっていきます。手順としては下記のとおりです。\n音声ファイルアップロード用の S3 バケットを作成 文字起こし結果を出力する S3 バケットを作成 Amazon Transcribe を使って文字起こしする Lambda 関数を作成 Lambda 関数用の IAM ロールの作成 関数本体の作成 S3 から Lambda を実行する権限を付与 1 で作成したバケットにイベント通知設定を追加 確認 1 のバケットに音声ファイルをアップロード 2 のバケットに文字起こし結果が出力されることを確認 これで構築されるのは、次のような構成です。\nでは、順番にやっていきます。\n1. 音声ファイルアップロード用の S3 バケットを作成 まずは音声ファイルをアップロードする S3 バケットを作成します。 s3api create-bucket コマンドを使います。\nS3 バケット名を変数に設定します。\n$ INPUT_BUCKET=\u0026#34;s3-handson-input-$(aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text)\u0026#34; \u0026amp;\u0026amp; echo ${INPUT_BUCKET} s3-handson-input-************ (************ はアカウント ID とします)\n対象のリージョンは、東京 (ap-northeast-1) とします。\n$ S3_BUCKET_LOCATION=\u0026#34;ap-northeast-1\u0026#34; バケットを作成します。\n$ aws s3api create-bucket \\ --bucket ${INPUT_BUCKET} \\ --create-bucket-configuration \u0026#34;LocationConstraint=${S3_BUCKET_LOCATION}\u0026#34; { \u0026#34;Location\u0026#34;: \u0026#34;http://s3-handson-input-************.s3.amazonaws.com/\u0026#34; } 2. 文字起こし結果を出力する S3 バケットを作成 1 と同様に、 Amazon Transcribe で文字起こしされた結果が出力される S3 バケットを作成します。\n$ OUTPUT_BUCKET=\u0026#34;s3-handson-output-$(aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text)\u0026#34; \u0026amp;\u0026amp; echo ${OUTPUT_BUCKET} s3-handson-output-************ $ aws s3api create-bucket \\ --bucket ${OUTPUT_BUCKET} \\ --create-bucket-configuration \u0026#34;LocationConstraint=${S3_BUCKET_LOCATION}\u0026#34; { \u0026#34;Location\u0026#34;: \u0026#34;http://s3-handson-output-************.s3.amazonaws.com/\u0026#34; } 3. Amazon Transcribe を使って文字起こしする Lambda 関数を作成 S3 のイベントによって起動される Lambda 関数を作成します。\n関数の実体となる .py ファイルを作成 関数の処理内容としては、S3 から渡されるイベントの内容から対象のオブジェクトのキーを取得し、そのキーをもとに Amazon Transcribe の文字起こし処理を呼ぶ というものです。今回は Python で実装した下記のような関数を使います。\nimport json import urllib.parse import boto3 import datetime s3 = boto3.client(\u0026#39;s3\u0026#39;) transcribe = boto3.client(\u0026#39;transcribe\u0026#39;) def lambda_handler(event, context): bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = urllib.parse.unquote_plus(event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;], encoding=\u0026#39;utf-8\u0026#39;) media_uri = f\u0026#39;https://{bucket}.s3-ap-northeast-1.amazonaws.com/{key}\u0026#39; try: transcribe.start_transcription_job( TranscriptionJobName= datetime.datetime.now().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) + \u0026#39;_Transcription\u0026#39;, LanguageCode=\u0026#39;en-US\u0026#39;, Media={ \u0026#39;MediaFileUri\u0026#39;: media_uri }, OutputBucketName=\u0026#39;s3-handson-output-************\u0026#39; ) except Exception as e: print(e) print(\u0026#39;Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.\u0026#39;.format(key, bucket)) raise e OutputBucketName には、 2 で作成したバケット名を指定します。\nこれを transcribe.py として保存し、下記のコマンドを実行して ZIP ファイルを生成します。\n$ LAMBDA_ZIP=\u0026#34;function.zip\u0026#34; $ FUNCTION_FILE=\u0026#34;transcribe.py\u0026#34; $ zip -j ${LAMBDA_ZIP} ${FUNCTION_FILE} adding: transcribe.py (deflated 46%) Lambda 関数にアタッチする IAM ロールを作成 続いて、 Lambda 関数にアタッチする IAM ロールを作成します。今回の Lambda 関数で必要になる権限は、 CloudWatch Logs へのログの書き込み、 Amazon Transcribe の実行権限、 S3 へのアクセス権限です。ということで、次のような IAM ポリシードキュメントを policy.json という名前で作成します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;transcribe:*\u0026#34;, \u0026#34;s3:*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } このポリシードキュメントを使って、 s3_handson_transcribe_policy という名前の IAM ポリシーを作成します。コマンドは iam create-policy です。ポリシーを作成する際には、その後の管理がしやすくなるように --path パラメータを指定します。\n$ LAMBDA_IAM_POLICY_NAME=\u0026#34;s3_handson_transcribe_policy\u0026#34; $ aws iam create-policy \\ --policy-name ${LAMBDA_IAM_POLICY_NAME} \\ --path \u0026#34;/michimani/sample/\u0026#34; \\ --policy-document file://policy.json { \u0026#34;Policy\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;s3_handson_transcribe_policy\u0026#34;, \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPA2FQKQ3TUYPLJBDU4T\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::************:policy/michimani/sample/s3_handson_transcribe_policy\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/michimani/sample/\u0026#34;, \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;AttachmentCount\u0026#34;: 0, \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, \u0026#34;IsAttachable\u0026#34;: true, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-11-12T12:59:08+00:00\u0026#34;, \u0026#34;UpdateDate\u0026#34;: \u0026#34;2020-11-12T12:59:08+00:00\u0026#34; } } この IAM ポリシーを用いて、 s3_handson_transcribe_role という名前の IAM ロールを作成します。まずは、 Lambda がロールを使用するための信頼ポリシードキュメントを作成します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } これを assume_role.json として保存しておきます。\nロール名を変数に設定して、 IAM ロールを作成します。コマンドは iam create-role です。\n$ LAMBDA_IAM_ROLE_NAME=\u0026#34;s3_handson_transcribe_role\u0026#34; $ aws iam create-role \\ --role-name ${LAMBDA_IAM_ROLE_NAME} \\ --assume-role-policy-document file://assume_role.json { \u0026#34;Role\u0026#34;: { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;RoleName\u0026#34;: \u0026#34;s3_handson_transcribe_role\u0026#34;, \u0026#34;RoleId\u0026#34;: \u0026#34;AROA2FQKQ3TUU6KJ3TVDJ\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::************:role/s3_handson_transcribe_role\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-11-12T13:17:00+00:00\u0026#34;, \u0026#34;AssumeRolePolicyDocument\u0026#34;: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } } } IAM ポリシーの ARN を変数に設定して、上で作成した IAM ロールにアタッチします。コマンドは iam attach-policy です。\n$ LAMBDA_IAM_POLICY_ARN=$( \\ aws iam list-policies \\ --scope Local \\ --path-prefix \u0026#34;/michimani/sample/\u0026#34; \\ --max-items 1000 \\ --query \u0026#34;Policies[?PolicyName==\\`${LAMBDA_IAM_POLICY_NAME}\\`].Arn\u0026#34; \\ --output text \\ ) \u0026amp;\u0026amp; echo ${LAMBDA_IAM_POLICY_ARN} arn:aws:iam::************:policy/michimani/sample/s3_handson_transcribe_policy $ aws iam attach-role-policy \\ --role-name ${LAMBDA_IAM_ROLE_NAME} \\ --policy-arn ${LAMBDA_IAM_POLICY_ARN} 作成した IAM ロールを Lambda 関数にアタッチする際に ARN が必要になるので、変数に設定しておきます。\n$ LAMBDA_IAM_ROLE_ARN=$( \\ aws iam get-role \\ --role-name ${LAMBDA_IAM_ROLE_NAME} \\ --query \u0026#39;Role.Arn\u0026#39; \\ --output text \\ ) \u0026amp;\u0026amp; echo ${LAMBDA_IAM_ROLE_ARN} arn:aws:iam::************:role/s3_handson_transcribe_role Lambda 関数を作成 諸々準備が整ったので、 Lambda 関数を作成します。コマンドは lambda create-function です。\n$ LAMBDA_FUNCTION_NAME=\u0026#34;s3-handson-tanscribe\u0026#34; $ aws lambda create-function \\ --function-name ${LAMBDA_FUNCTION_NAME} \\ --description \u0026#34;S3 イベント通知テスト用関数\u0026#34; \\ --runtime \u0026#34;python3.8\u0026#34; \\ --role ${LAMBDA_IAM_ROLE_ARN} \\ --handler \u0026#34;transcribe.lambda_handler\u0026#34; \\ --zip-file fileb://${LAMBDA_ZIP} { \u0026#34;FunctionName\u0026#34;: \u0026#34;s3-handson-tanscribe\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:ap-northeast-1:************:function:s3-handson-tanscribe\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;python3.8\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::************:role/s3_handson_transcribe_role\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;transcribe.lambda_handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 680, \u0026#34;Description\u0026#34;: \u0026#34;S3 イベント通知テスト用関数\u0026#34;, \u0026#34;Timeout\u0026#34;: 3, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-11-12T13:43:35.679+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;rrjImL4le+s5Hyz8rPlapksfo1DmUqpZnOzpYwfRWtc=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;1e1fcdd5-ba7f-4495-a8ee-7d7b71c4e3ee\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;LastUpdateStatus\u0026#34;: \u0026#34;Successful\u0026#34; } S3 バケットに Lambda の実行権限を付与 作成した Lambda 関数を 1 で作成したバケットが実行できるように権限を付与します。コマンドは lambda add-permission を使用しますが、事前に必要なパラメータの値を変数に設定しておきます。\nS3 バケットの ARN\n$ INPUT_BUCKET_ARN=\u0026#34;arn:aws:s3:::${INPUT_BUCKET}\u0026#34; \\ \u0026amp;\u0026amp; echo ${INPUT_BUCKET_ARN} arn:aws:s3:::s3-handson-input-************ 実行権限を付与します。\n$ aws lambda add-permission \\ --function-name ${LAMBDA_FUNCTION_NAME} \\ --statement-id \u0026#34;s3-invoke-lambda-function\u0026#34; \\ --action \u0026#34;lambda:InvokeFunction\u0026#34; \\ --principal \u0026#34;s3.amazonaws.com\u0026#34; \\ --source-arn ${INPUT_BUCKET_ARN} 4. 1 で作成したバケットにイベント通知設定を追加 1 で作成したバケットに対して、イベント通知の設定を追加します。まずはイベント通知設定用のドキュメントを作成します。その際、実行する Lambda 関数の ARN が必要なので、事前に取得しておきます。\n$ aws lambda get-function \\ --function-name ${LAMBDA_FUNCTION_NAME} \\ --query \u0026#39;Configuration.FunctionArn\u0026#39; \\ --output text arn:aws:lambda:ap-northeast-1:************:function:s3-handson-tanscribe 今回はオブジェクトの作成 (MP3 ファイル) をトリガーにして、 Lambda 関数を実行するので、次のような次のようなドキュメントになります。イベントの発生条件を制御するために、 Filter でサフィックスが .mp3 のオブジェクトのみをイベント発生の対象としています。\n{ \u0026#34;LambdaFunctionConfigurations\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;s3:ObjectCreated-lambda\u0026#34;, \u0026#34;LambdaFunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:ap-northeast-1:************:function:s3-handson-tanscribe\u0026#34;, \u0026#34;Events\u0026#34;: [ \u0026#34;s3:ObjectCreated:*\u0026#34; ], \u0026#34;Filter\u0026#34;: { \u0026#34;Key\u0026#34;: { \u0026#34;FilterRules\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;suffix\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;.mp3\u0026#34; } ] } } } ] } これを config.json として保存しておきます。その他、 Lambda 関数との連携設定については下記のドキュメントを参照してください。\nLambdaFunctionConfiguration - Amazon Simple Storage Service この JSON をもとに、 1 で作成した S3 バケットに対してイベント通知設定を追加します。コマンドは s3api put-bucket-notification-configuration を使います。\n$ aws s3api put-bucket-notification-configuration \\ --bucket ${INPUT_BUCKET} \\ --notification-configuration file://config.json ちなみに、該当のバケットに Lambda の実行権限 (前項で設定した権限) がない場合、 s3api put-bucket-notification-configration を実行しても下記のようなエラーとなり、イベント通知の設定が追加できません。\nAn error occurred (InvalidArgument) when calling the PutBucketNotificationConfiguration operation: Unable to validate the following destination configurations\n5. 確認 すべての準備が整ったので、 INPUT_BUCKET に MP3 ファイルをアップロードしてみます。使用する MP3 ファイルは、 Amazon Poly のページで用意されているサンプルファイルから HelloEnglish-Joanna.mp3 をダウンロードして使います。\nAmazon Polly（深層学習を使用したテキスト読み上げサービス）| AWS $ aws s3 cp ./HelloEnglish-Joanna.mp3 s3://${INPUT_BUCKET}/HelloEnglish-Joanna.mp3 upload: ./HelloEnglish-Joanna.mp3 to s3://s3-handson-input-************/HelloEnglish-Joanna.mp3 しばらくすると OUTPUT_BUCKET に文字起こし結果が出力されるので、確認します。\n$ aws s3 ls s3://${OUTPUT_BUCKET} 2020-11-13 00:12:51 2 .write_access_check_file.temp 2020-11-13 00:13:16 1878 20201112151225_Transcription.json JSON ファイルをダウンロードして中身を確認してみます。\n$ aws s3 cp s3://${OUTPUT_BUCKET}/20201112151225_Transcription.json ./res.json \\ \u0026amp;\u0026amp; cat res.json \\ | jq .\u0026#34;results.transcripts[0]\u0026#34; { \u0026#34;transcript\u0026#34;: \u0026#34;Hello. Do you speak a foreign language? One language is never enough.\u0026#34; } 文字起こしの結果が取得できました。\nまとめ JAWS-UG CLI専門支部 #171R S3基礎 通知 (Lambdaの自動実行) に参加したので、そのレポートでした。\nというか、今回はレポートではなく以前にマネジメントコンソールや CDK で作成した構成を AWS CLI でやってみた内容になってしまいました。\nやってみた感想としては、正直 AWS CDK で構築するのが一番楽だなという思いです。どのあたりが辛かったかと言うと、 IAM ポリシー、 IAM ロールまわりの設定です。マネジメントコンソールや CDK でよしなにやってくれている部分が大きいんだなと、あらためて感じました。\n逆に言うと、 AWS CLI でやろうとするとそのあたりをしっかり理解していないと設定できないので、 IAM の各要素 (ポリシー/ロール/ユーザー\u0026hellip;) に関して理解を深めるには一番良い方法だなと思います。 これまでの CLI 専門支部のハンズオンの中でも IAM による権限設定は毎回のように出てきますが、なんとなくコマンドのコピペで終わっていた部分も大いにあるので、しっかり理解しながら進めないといけないなと感じました。\n今回のように扱うサービスや機能 (S3 + Lambda) 自体に馴染みがある場合には、 IAM の権限設定まわりを注意しながらハンズオンやっていこうと思います。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-171r-s3-notification/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #171R S3基礎 通知 (Lambdaの自動実行) に参加しました #jawsug_cli"
  },
  {
    "contents": "JAWS-UG CLI専門支部 #170R S3基礎 ライフサイクル に参加したので、そのレポートです。\nconnpass のイベントページはこちら。\nこれまでの CLI 専門支部参加レポートはこちら。\n[レポート] JAWS-UG CLI 専門支部 #169R S3基礎 バージョニング に参加しました #jawsug_cli [レポート] JAWS-UG CLI 専門支部 #168R S3基礎 Webサイト\u0026amp;amp;ログ に参加しました #jawsug_cli [レポート] JAWS-UG CLI 専門支部 #167R EventBridge入門 に参加しました #jawsug_cli 目次 S3 の全体像、ストレージクラス ハンズオン 2.1 S3 ライフサイクルドキュメントの作成 3.3 S3 オブジェクトのライフサイクル確認 プラス α 既存のバケットにライフサイクルを設定する ストレージクラスの変更を Slack に通知する (できませんでした) まとめ S3 の全体像、ストレージクラス ※基本的には前回までと同内容 S3 Glacier もともとは S3 と Galcier は別のサービスだったが、今は S3 の一部\nAPI は glacier\n$ aws glacier abort-multipart-upload delete-vault get-vault-access-policy list-multipart-uploads set-data-retrieval-policy abort-vault-lock delete-vault-access-policy get-vault-lock list-parts set-vault-access-policy add-tags-to-vault delete-vault-notifications get-vault-notifications list-provisioned-capacity set-vault-notifications complete-multipart-upload describe-job initiate-job list-tags-for-vault upload-archive complete-vault-lock describe-vault initiate-multipart-upload list-vaults upload-multipart-part create-vault get-data-retrieval-policy initiate-vault-lock purchase-provisioned-capacity wait delete-archive get-job-output list-jobs remove-tags-from-vault S3 ストレージクラス オンラインストレージ S3 標準 (S3 Standard) S3 標準 低頻度アクセス (S3 Standard-IA) ※IA = Infrequent Access S3 Intelligent-Tiering S3 1ゾーン 低頻度アクセス (S3 One Zone-IA) テープストレージ (に似ている) S3 Glacier S3 Glacier Deep Archive それぞれに最小期間が設定されている S3 標準 と S3 Intelligent-Tiering は取り出し料金がかからない (他はかかる) 少し前までは Glacier 破産 なるものがあった 保持にかかるコストは低いが、取り出しにかかるコストが高い 事前のコスト計算は必須 料金 - Amazon S3 ｜AWS ライフサイクル - ストレージクラス間の移動 指定したオブジェクトを一定期間経過後に自動的に破棄や低コストのストレージクラスに移行 高いクラスから低いクラスへのみ移行可能 (逆は不可) プレフィックスで設定するので、 logs/ を低クラスへ、 tmp/ を破棄 といった設定が可能 ハンズオン 今回のハンズオンでは、新たに作成した S3 バケットに対して、プレフィックス別に複数のライフサイクルを設定し、各ルールが適用されるようなオブジェクトを作成しました。ライフサイクルの性質上、実際の挙動は最短でも翌日 (厳密には次の 00:00 UTC) にならないとわからないので、結果については翌日以降に確認します。\n今回も、ハンズオンの詳細な手順についてはイベントページの資料におまかせするとして、今回のメインであるライフサイクルの設定部分のコマンドについて書いておきます。\n2.1 S3 ライフサイクルドキュメントの作成 S3 バケットのライフサイクルは、複数の ルール を設定可能で、日付の計算に用いるタイムゾーンは UTC、起算日はオブジェクトの作成日となっています。公式ドキュメントでは次のように記載されています。\nAmazon S3 は、ルールに指定された日数をオブジェクトの作成時間に加算し、得られた日時を翌日の午前 00:00 UTC (協定世界時) に丸めることで、時間を算出します。たとえば、あるオブジェクトが 2014 年 1 月 15 日午前 10 時 30 分 (UTC) に作成され、移行ルールに 3 日と指定した場合、オブジェクトの移行日は 2014 年 1 月 19 日 0 時 0 分 (UTC) となります。\nライフサイクル設定の要素 - Amazon Simple Storage Service ハンズオンでは下記のような 3 つのルールを設定しました。\ntemporary-objects プレフィックス tmp/ が対象 1 日経過後 (次の次の 00:00 UTC) に 破棄 archive-objects プレフィックス archive/ が対象 0 日経過後 (次の 00:00 UTC) に S3 Glacier に移行 1 日経過後に 破棄 short-keep-objects プレフィックス logs/ が対象 30 日経過後に S3 標準 低頻度アクセス に移行 60 日経過後に S3 Intelligent Tiering に移行 90 日経過後に S3 1ゾーン 低頻度アクセス に移行 120 日経過後に S3 Glacier に移行 210 日経過後に S3 Glacier Deep Archive に移行 211 日経過後に 破棄 JSON で記述すると下記のような内容になります。ちなみに、公式ドキュメントでは XML での指定方法が書かれているが、 JSON での指定方法が書かれていない。(日本語だけでなく English でも。)\n{ \u0026#34;Rules\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;temporary-objects\u0026#34;, \u0026#34;Filter\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;tmp/\u0026#34; }, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Expiration\u0026#34;: { \u0026#34;Days\u0026#34;: 1 } }, { \u0026#34;ID\u0026#34;: \u0026#34;archive-objects\u0026#34;, \u0026#34;Filter\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;archive/\u0026#34; }, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Transitions\u0026#34;: [ { \u0026#34;Days\u0026#34;: 0, \u0026#34;StorageClass\u0026#34;: \u0026#34;GLACIER\u0026#34; } ], \u0026#34;Expiration\u0026#34;: { \u0026#34;Days\u0026#34;: 1 } }, { \u0026#34;ID\u0026#34;: \u0026#34;short-keep-objects\u0026#34;, \u0026#34;Filter\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;logs/\u0026#34; }, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Transitions\u0026#34;: [ { \u0026#34;Days\u0026#34;: 30, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD_IA\u0026#34; }, { \u0026#34;Days\u0026#34;: 60, \u0026#34;StorageClass\u0026#34;: \u0026#34;INTELLIGENT_TIERING\u0026#34; }, { \u0026#34;Days\u0026#34;: 90, \u0026#34;StorageClass\u0026#34;: \u0026#34;ONEZONE_IA\u0026#34; }, { \u0026#34;Days\u0026#34;: 120, \u0026#34;StorageClass\u0026#34;: \u0026#34;GLACIER\u0026#34; }, { \u0026#34;Days\u0026#34;: 210, \u0026#34;StorageClass\u0026#34;: \u0026#34;DEEP_ARCHIVE\u0026#34; } ], \u0026#34;Expiration\u0026#34;: { \u0026#34;Days\u0026#34;: 211 } } ] } S3 バケットへのルールの適用は s3api put-bucket-lifecycle-configuration コマンドを使います。\n$ aws s3api put-bucket-lifecycle-configuration \\ --bucket ${S3_BUCKET_NAME} \\ --lifecycle-configuration file://s3-bucket-lifecycle.json 各ストレージクラスには最小期間が設定されているので、例えば Glacier への移行日数 120 日に対して Glacier Deep Archive への移行日数を 200 日に設定しようとすると、 s3api put-bucket-lifecycle-configuration 実行時に下記のようなエラーが出ます。\nAn error occurred (InvalidArgument) when calling the PutBucketLifecycleConfiguration operation: \u0026lsquo;Days\u0026rsquo; in the \u0026lsquo;Transition\u0026rsquo; action for StorageClass \u0026lsquo;DEEP_ARCHIVE\u0026rsquo; for filter \u0026lsquo;(prefix=logs/)\u0026rsquo; must be 90 days more than \u0026lsquo;filter \u0026lsquo;(prefix=logs/)\u0026rsquo;\u0026rsquo; in the \u0026lsquo;Transition\u0026rsquo; action for StorageClass \u0026lsquo;GLACIER\u0026rsquo;\nライフサイクルが設定されると、マネジメントコンソールの S3 バケット詳細で次のように表示されます。\n余談ですが、バケット詳細画面の UI が変わってますね。\nまた、各ルールの詳細画面では日数の経過とストレージクラスの遷移がビジュアライズされた形で確認することができます。\n3.3 S3 オブジェクトのライフサイクル確認 ルールが適用されるオブジェクトについては、 s3api head-object コマンドで得られるメタデータの中に含まれる Expiration の値を確認することで、ライフサイクルを確認することができます。\n$ aws s3api head-object \\ --bucket ${S3_BUCKET_NAME} \\ --key tmp/2020-10-29.txt \\ --query \u0026#39;Expiration\u0026#39; \\ --output text expiry-date=\u0026#34;Sat, 31 Oct 2020 00:00:00 GMT\u0026#34;, rule-id=\u0026#34;temporary-objects\u0026#34; 211 日経過後に破棄される logs/ のオブジェクトについては、次のような出力が得られます。\n$ aws s3api head-object \\ --bucket ${S3_BUCKET_NAME} \\ --key logs/2020-10-29.log \\ --query \u0026#39;Expiration\u0026#39; \\ --output text expiry-date=\u0026#34;Sat, 29 May 2021 00:00:00 GMT\u0026#34;, rule-id=\u0026#34;short-keep-objects\u0026#34; また、今回のハンズオンで設定したルールの中で その挙動が最短で確認できるのは archive-objects のルールです。 \u0026quot;Days\u0026quot;: 0 で Glacier への移行を設定しているので、翌日の 09:00 JST には結果が確認できます。ということで、移行前のストレージクラスを事前に確認しておきます。date コマンドで実行時の日時も一緒に出力しています。\n$ date \u0026amp;\u0026amp; aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} \\ --prefix \u0026#34;archive/\u0026#34; \\ --query \u0026#34;Contents[].[join(\\`\\`, [Key, \\`: \\`, StorageClass])]\u0026#34; \\ --output text 2020年 10月29日 木曜日 22時07分14秒 JST archive/2020-10-29.txt: STANDARD (翌日)\n翌日の 09:00 JST 以降にあらためて確認してみます。\n$ date \u0026amp;\u0026amp; aws s3api list-objects-v2 \\ --bucket ${S3_BUCKET_NAME} \\ --prefix \u0026#34;archive/\u0026#34; \\ --query \u0026#34;Contents[].[join(\\`\\`, [Key, \\`: \\`, StorageClass])]\u0026#34; \\ --output text 2020年 10月30日 金曜日 22時00分50秒 JST archive/2020-10-29.txt: GLACIER ストレージクラスが S3 Glacier に移行しています。 09:00 JST (= 00:00 UTC) になってすぐに確認したときにはまだ S3 標準 のままでした。どうやらライフサイクルによるストレージクラスの移行には遅延が発生する場合があるようです。移行が完了していない場合でも、発生するコストの変更に関してはライフサイクルルールを満たした時点で適用されるみたいです。\nAmazon S3 ライフサイクルルールに遅延がある理由 プラス α ハンズオン中にいくつか疑問に思ったことがあったので、それらについて試してみます。\n既存のバケットにライフサイクルを設定する ハンズオンでは、新たに作成したバケットに対してライフサイクルを設定し、その後オブジェクトを追加しました。では、既にオブジェクトが存在するバケットに対してライフサイクルを設定した場合はどのような挙動になるのでしょうか。\n今回は、個人で運用している Web サービス (と言って良いのかはさておき) の ココイチ注文料金簡易カリキュレータ の CloudFront で生成しているログを対象に、ライフサイクルを設定してみます。\nログは既にとあるバケット (cf-logs-bucket (仮称)) に生成されていて、例えば 2019 年6月のログオブジェクトとストレージクラスは次のようになっています。date コマンドで実行時の日時も一緒に出力しています。\n$ date \u0026amp;\u0026amp; aws s3api list-objects-v2 \\ --bucket cf-logs-bucket \\ --prefix coco1.app/2019-06 \\ --max-keys 5 \\ --query \u0026#34;Contents[].[join(\\`\\`, [Key, \\`: \\`, StorageClass])]\u0026#34; \\ --output text 2020年 10月30日 金曜日 22時05分55秒 JST coco1.app/2019-06-09-08-17-46-3A0F0F0D5E81E231: STANDARD coco1.app/2019-06-09-08-18-35-A18FDA5904E7D007: STANDARD coco1.app/2019-06-09-08-19-59-53378BC489E1596B: STANDARD coco1.app/2019-06-09-08-20-53-D6015EC587DF38C1: STANDARD coco1.app/2019-06-09-08-22-23-4DB13667C68BCEBA: STANDARD このバケットに対して、 coco1.app/ というプレフィックスを対象に、今回のハンズオンと同様に次のようなライフサイクルを設定してみます。現時点 (2020/10/30) でオブジェクトが生成されてから 1 年以上経過しているので、設定する日数は次のようにします。\n{ \u0026#34;Rules\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;keep-log-objects\u0026#34;, \u0026#34;Filter\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;coco1.app/\u0026#34; }, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Transitions\u0026#34;: [ { \u0026#34;Days\u0026#34;: 360, \u0026#34;StorageClass\u0026#34;: \u0026#34;GLACIER\u0026#34; }, { \u0026#34;Days\u0026#34;: 540, \u0026#34;StorageClass\u0026#34;: \u0026#34;DEEP_ARCHIVE\u0026#34; } ], \u0026#34;Expiration\u0026#34;: { \u0026#34;Days\u0026#34;: 720 } } ] } 上記のルールを設定します。\n$ aws s3api put-bucket-lifecycle-configuration \\ --bucket cf-logs-bucket \\ --lifecycle-configuration file://coco1_log_lifecycle_rule.json 一つのオブジェクトに対して有効期限を確認してみます。\n$ aws s3api head-object \\ --bucket cf-logs-bucket \\ --key coco1.app/2019-06-09-08-17-46-3A0F0F0D5E81E231 \\ --query Expiration \\ --output text expiry-date=\u0026#34;Sun, 30 May 2021 00:00:00 GMT\u0026#34;, rule-id=\u0026#34;keep-log-objects\u0026#34; ルールが適用されているのがわかります。\n2020/10/31 は 2019/06/09 から 508 日経過しているので、上の条件から S3 Glacier に移行すると想定されます。\n(翌日)\nルールの条件適用の判定 (という言い方が正しいかは一旦スルー) は 00:00 UTC に行われるので、翌日の 09:00 JST 以降に確認してみます。\n$ date \u0026amp;\u0026amp; aws s3api list-objects-v2 \\ --bucket cf-logs-bucket \\ --prefix coco1.app/2019-06 \\ --max-keys 5 \\ --query \u0026#34;Contents[].[join(\\`\\`, [Key, \\`: \\`, StorageClass])]\u0026#34; \\ --output text 2020年 10月31日 土曜日 16時37分03秒 JST coco1.app/2019-06-09-08-17-46-3A0F0F0D5E81E231: GLACIER coco1.app/2019-06-09-08-18-35-A18FDA5904E7D007: GLACIER coco1.app/2019-06-09-08-19-59-53378BC489E1596B: GLACIER coco1.app/2019-06-09-08-20-53-D6015EC587DF38C1: GLACIER coco1.app/2019-06-09-08-22-23-4DB13667C68BCEBA: GLACIER 想定通り、ストレージクラスが S3 Glacier に移行しています。\nサポートされていないライフサイクル設定 ライフサイクルによるストレージクラスの移行には、サポートされていない (移行がされない) パターンがあります。それは、 対象となるオブジェクトのサイズが 128 KB 未満の場合 です。対象となるオブジェクトのサイズが 128 KB 未満の場合、次のようなストレージクラスの移行は実行されません。\nS3 標準 または S3 標準 低頻度アクセス から S3 Intelligent-Tiering への移行 S3 標準 から S3 標準 低頻度アクセス または S3 1ゾーン 低頻度アクセス への移行 これらの移行は、小さいサイズのオブジェクトにとっては費用対効果が悪いため、 Amazon S3 が移行させないようにしています。\nAmazon S3 のライフサイクルを使用したオブジェクトの移行 - Amazon Simple Storage Service ストレージクラスの変更を Slack に通知する (できませんでした) 今回のハンズオンでわかった通り、ライフサイクルルールによるストレージクラスの変更には多少時間がかかります。設定だけしてあとで忘れてしまう気がしたので、ストレージクラスが変わったときに Slack に通知できないかな？と考えました。S3 ではオブジェクトに対するさまざまなイベントを SNS 、 SQS 、 Lambda に通知することができるので、その機能を使って通知できないか試してみます。\nS3 のイベントについて公式ドキュメントを読んでみると、残念ながらストレージクラスの変更を通知するようなストレートなイベントはありませんでした。\nAmazon S3 イベント通知の設定 - Amazon Simple Storage Service じゃあストレージクラスの変更って裏でどう動いているのかなと思って調べてみると、どうやら元のオブジェクトを新しいストレージクラスとしてコピーしているようでした。というのも、ストレージクラスの変更は手動でも可能で、 AWS CLI を使って手動でストレージクラスを変更する際には次のような形で s3api copy-object コマンドを使うからです。\n$ aws s3api copy-object \\ --bucket ${S3_BUCKET_NAME} \\ --copy-source ${S3_BUCKET_NAME}/${OBJECT_KEY} \\ --key ${OBJECT_KEY} \\ --storage-class DEEP_ARCHIVE なので、 S3 イベントとしては s3:ObjectCreated:Copy を通知するようにしてみます。このあたりの内容は次回の CLI 専門支部で扱うようなので、今回はマネジメントコンソール上で設定してしまいます。\nもしかしたら COPY じゃないかもしれないので念のため PUT と POST についても通知対象にしています。通知用の Lambda 関数は、受け取ったイベントを整形して Slack に投げるだけの関数です。\nimport json from urllib.request import Request, urlopen SLACK_WEBHOOK_URL = \u0026#39;https://hooks.slack.com/services/********************\u0026#39; def lambda_handler(event, context): try: try: slack_message = \u0026#39;```\\n\u0026#39; + json.dumps(event, indent=2) + \u0026#39;\\n```\u0026#39; except Exception as e: print(e) slack_message = str(event) headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} data = bytes(json.dumps({\u0026#39;text\u0026#39;: slack_message}), \u0026#39;utf-8\u0026#39;) request = Request(SLACK_WEBHOOK_URL, data=data, headers=headers) urlopen(request) except Exception as e: print(e) イベント通知の設定後、ストレージクラスが変更されるであろう時間に通知がくる想定だったんですが、残念ながら通知は来ず。ライフサイクルルールによるストレージクラスの移行は、手動でやる場合とは動作が異なるのかもしれません。ストレージクラスの移行を通知する何か良い方法があれば教えていただきたいです。\nまとめ JAWS-UG CLI専門支部 #170R S3基礎 ライフサイクル の参加レポートでした。\n前回のバージョニングと比べると使い所は大いにありそうだなと思いました。ただし、 Glacier 破産という言葉もあるようなので、実際にライフサイクルを導入する際には対象となるオブジェクトのサイズや取り出し頻度などを考慮してコスト計算をしっかりしておく必要があります。\nCLI 専門支部のレポートブログもこれで 4 本目になりますが、なんとなく AWS CLI 力が高まってきたような気がしてます。来月から新しい会社での勤務が始まりますが、勤務時間には融通が効く (はずな) ので引き続き参加したいと思います。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-170r-s3-lifecycle/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #170R S3基礎 ライフサイクル に参加しました #jawsug_cli"
  },
  {
    "contents": "この三年半、つまり 2017年 5月から 2020年 10月までの諸々について振り返ってみたいと思います。今回の内容は、激寒自分語りポエムとなっています。\n目次 なぜ急に振り返るのか AWS との出会い AWS を使った開発 セミナーとか勉強会への参加 アウトプットの習慣化 他にも色々手を出してみた フロントエンドの実装 色んな言語での実装 まとめ なぜ急に振り返るのか なぜこの微妙な期間を振り返るのかというと、ちょうど今の会社に在籍していた期間だからです。過去形にしてますが、一応 10月いっぱいは籍があります。で、来月からは別の会社で引き続きバックエンドエンジニアとして開発をしていく予定です。\n三年半と言いながらも自身としてはもっと長い感じがしていて、凄い濃い期間だったなーということで振り返ってみることにしました。\nAWS との出会い 振り返りと言ってもこの要素が一番大きいので、ここが今回のメインです。\n今ではほぼ毎日のようにアップデート情報を確認したり JAWS-UG 等の勉強会に参加するようになりましたが、三年半前は AWS についてはほぼ知りませんでした。 EC2 くらいは名前を聞いたことがあるかなー程度で、レンタルサーバー的な感じでしょと思ってました。\nAWS を使った開発 今の会社に入ってから小さい規模のとある Web システムを作ってくれと言われて、インフラは AWS 使ってもいいよということで触らせてもらうことにしました。で、まず何をやったかと言うと、 EC2 インスタンスの中に LAMP 環境を作るっていう、まさにレンタルサーバー的な使い方をしたわけです。\n個人の AWS アカウントもこのときに作って色々試すようにもなりました。\nその Web システムを構築する際に、 DNS は Route 53 というサービスを使うらしい、 SSL 証明書は Certificate Manager を使うらしい、諸々の監視・通知には CloudWatch 、 SNS 、 Lambda というサービスを使うらしい、なるほど、わからん\u0026hellip; という感じで徐々に触るサービスが増えていきました。\n最近では、サービス名を見ればそれがどういうサービスなのかは何となく分かるようになってます。また、各サービスだけではなくて CDK とか Chalice とか Serverless Framework 等の周辺ツールも使うようになりました。マネジメントコンソールの辛さがわかるようになってきました。\n三年半もあればそんなもんやろというツッコミもあるかと思いますが、個人的には何も知らないレベルからだいぶ成長したなと思ってます。(自分には甘いです)\nAWS 認定も、アソシエイトレベルの 3 つを取得できたのは、成長の証かなと思います。(自分には甘いです)\nセミナーとか勉強会への参加 セミナーや勉強会に参加するようになったのも AWS を触り始めたことがきっかけになってます。\n初めて行ったセミナーは、 2017年に東京で開催された AWSome Day 2017 東京 です。これをきっかけに、 Developers.IO や AWS Loft でのハンズオンなどに参加するようになりました。 特に Developers.IO 2018 では、 AWS なんもわからんという感想とともに、クラスメソッドのエンジニアさんのレベルの高さ (技術力、アウトプット力) に感動したと言うかびっくりしたと言うか、とにかく凄いインパクトを受けたのを覚えています。\n今年に入ってからは各種イベントがオンラインで開催されるようになりました。オンライン開催の良いところ/悪いところはそれぞれあると思いますが、個人的には凄くありがたいなと感じてます。\nイベントによっては 21時スタートだったり、朝の 7時半スタートだったりと、業務後や業務前の参加しやすいタイミングで開催されるのが大変ありがたいです。\nアウトプットの習慣化 上にも書きましたが、クラスメソッドのエンジニアさんをはじめ、様々なイベントで登壇されている方々を見て、だいぶ遠い存在ながらも自分もいつかあんな風になれたらいいなーと思うようになりました。ただ、いきなり登壇なんてできないので、ブログでのアウトプットを続けてみることにしました。\nその結果、ブログの記事数は 2016年が 5 本 (Qiita で書いてました) だったのが、 2017年には 18 本、 2018年には 48 本、 2019年には 54 本、そして 2020年には 10月時点で 78 本と、記事数だけは着実に増えてきています。\nカテゴリとしても AWS に関する記事が 107 本ということで、 AWS との出会いがもたらした効果は大きなと思うばかりです。\nまた、今年はオンラインではありますが念願の LT デビューも果たしました。オフラインだと人前で緊張してしまうのがわかりきっているので、オンラインで LT の機会がやってきたのは個人的にはラッキーでした。\nこの LT をやってみて、自分のアウトプットに対して何かしらレスポンスをもらえるのは凄くありがたいということを感じたので、今後 自分が参加するイベントやセミナーなどでは、ブログや Twitter 実況等で何かしらの反応をしてきたいなと思うようになりました。\nTwitter のフォロワーさんには技術系以外にもバイク関係でフォローいただいている方々もいると思いますが、その方々に対してはノイズになってしまっているかと思います。すみません。\nブログに関してはまだまだ自分のメモ程度の内容から抜け出せない状態ではありますが、今後も続けていきたいと思ってます。\n他にも色々手を出してみた この三年半は AWS の占める割合が大きいですが、その他にも色々と手を出す機会をいただきました。\nフロントエンドの実装 これまでは基本的にサーバーサイドの実装ばかりしてきましたが、現職についてから短期間ではありますが Vue.js を使ったフロントエンドの実装をする機会をいただきました。いまだに掴めない部分はたくさんありますが、経験として少しでも触ることができたのは良かったです。個人としても、 Vue.js を使って変なツールを作ることもできました。\n色んな言語での実装 バックエンドの実装として触って来た言語は、 PHP が一番長く、 Java、 C# の経験がありましたが、この三年半では Python、 Go での実装を経験しました。両方とも Lambda の実装で使いましたが、 Python に関しては簡単な CLI ツールっぽいものを作る際には積極的に使うようになりました。\n他にも、 CDK の実装で TypeScript を少し使ったり、広く浅くですが色んな言語での実装を経験できたのは良かったです。\nちなみに次の職場では Go を使った開発をする予定です。\nまとめ 2017年 5月から 2020年 10月までの三年半を雑に振り返ってみました。\n結論としては、やっぱり AWS との出会いが一番大きいですね。\nイベント参加もブログも色んな言語での実装も、全部 AWS との出会いが元になっていますし、今回の転職にも大いに影響していると感じてます。次の会社でも AWS を使った開発は続きますし、個人でも情報は追いかけ続けるつもりです。\nまた、転職活動をする中でいろんな企業さんのエンジニアの方々をお話をさせていただいて、自分の足りない部分はまだまだたくさんあるということを痛感したので、選考途中でいただいたお言葉は、良いものも悪いものも全部受け止めて今後の成長の糧にしていきたいと思ってます。\nということで、今後も楽しみながら成長できたら良いな！というスタンスでがんばります。\nよっしーの欲しいものリスト ",
    "permalink": "https://michimani.net/post/other-retrospect-in-2017-to-2020/",
    "title": "この三年半を雑に振り返ってみる"
  },
  {
    "contents": "今年に入ってから少し Go 言語を使う機会があり、今後はもっと使っていくことになりそうということで最近は Go の勉強をしています。Go 言語には並行処理を簡単に扱うことができるという特徴があるので、今回はその並行処理に入門したいと思います。\n目次 概要 やってみる go キーワードを使った並行処理 goroutine とは sync.WaitGroup を使った goroutine の管理 まとめ 概要 今回は入門ということで、 go というキーワードを使った並行処理と、 sync.WaitGroup を使った並行処理の管理について触ってみます。ちなみに、内容としては O\u0026rsquo;Reilly の 「Go言語による並行処理」 という書籍をもとに勉強しています。\nGo言語による並行処理 やってみる ということで、とりあえずコードを書いてみます。\ngo キーワードを使った並行処理 まずは次のようなコードを書きました。\npackage main import \u0026#34;fmt\u0026#34; func printNum(n int) { fmt.Println(n) } func main() { fmt.Println(\u0026#34;Start main function...\u0026#34;) for i := 1; i \u0026lt;= 10; i++ { printNum(i) } fmt.Println(\u0026#34;Finished main function.\u0026#34;) } printNum() という、引数で受け取った数値を標準出力に出力するだけの関数を作って、 main() 関数内では 1 から 10 までの数値を for 文で回して printNum() に渡しています。このコードを実行すると、次のような出力が得られます。\n$ go run sample.go Start main function... 1 2 3 4 5 6 7 8 9 10 Finished main function. 想定通りの出力です。\nGo 言語では、並行で処理したい関数の前に go キーワードを書くことで、簡単に並行処理を実現することができます。ということで、上記のコードを次のように変更して、あらためて実行してみます。\nfor i := 1; i \u0026lt;= 10; i++ { - printNum(i) + go printNum(i) } $ go run sample.go Start main function... Finished main function. 1 7 5 6 はて。どういうことでしょう。\nこの現象が起こった理由の前に、 goroutine (ゴルーチン) という概念について簡単に触れておきます。\ngoroutine とは goroutine (ゴルーチン) とは、何なのか、冒頭に紹介した Go言語による並行処理 で書かれている説明文を引用します。\nゴルーチンはGoのプログラムでの最も基本的な構成単位です。したがって、それが何で、どのように動作するのかを理解することは重要です。事実、すべてのGoのプログラムには最低1つのゴルーチンがあります。それがメインゴルーチンです。\n\u0026hellip;\n“単純に言えば、ゴルーチンは他のコードに対し並行に実行している関数のことです（注意：必ずしも並列ではありません！）。ゴルーチンはgoキーワードを関数呼び出しの前に置くことで簡単に起動できます。”\ngoroutine は、 Go 言語におけるプログラムの基本単位ということがわかりました。これに加えて、 Go 言語での並行処理のモデルについても、同著の説明を引用します。\nGoはfork-joinモデルと呼ばれる並行処理のモデルに従っています。分岐（fork）という用語は、プログラムの任意の場所で、プログラムが子の処理を分岐させて、親と並行に実行させることを指しています。合流（join）という用語は、分岐した時点から先でこれらの並行処理の分岐が再び合流することを指します。\nmain() 関数、つまりメインの goroutine にて go キーワードを使って goroutine を生成すると、それはメインのプログラムから分岐した場所で実行されることになります。その結果、先ほどの出力では main() 関数の終了を示す文字列が出力されたあとに、 goroutine として分岐された printNum() の出力がされています。\nまた、分岐する形で作成された goroutine は、あくまでも実行のスケジューリングがされた状態なので、明確な実行タイミングはわかりません。そのため、出力される数値の順番もバラバラで、実行するたびに変わります。\nさらに、分岐した goroutine は元のプログラム (上のコードであれば main() 関数) が終了した時点で実行されていない場合は、実行されません。先ほどの実行結果で 1 から 10 までの数値がすべて出力されていないのはこのためです。\nGo 言語の並行処理は fork-join モデルということなので、分岐した goroutine の実行を待つためには呼び出し元で join する (合流ポイントを作成する) 必要があります。ただ単に処理を待つだけであれば time.Sleep() で待つこともできますが、確実ではありません。これを解決するのが、 sync.WaitGroup です。\nsync.WaitGroup を使った goroutine の管理 先ほどのコードを次のようなコードに変更します。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func printNum(n int, w *sync.WaitGroup) { defer w.Done() fmt.Println(n) } func main() { fmt.Println(\u0026#34;Start main function...\u0026#34;) var wg sync.WaitGroup for i := 1; i \u0026lt;= 10; i++ { wg.Add(1) go printNum(i, \u0026amp;wg) } wg.Wait() fmt.Println(\u0026#34;Finished main function.\u0026#34;) } 変更点としては、 sync.WaitGroup 型の wg を生成して、 printNum() 関数の実行前に wg.Add() で goroutine の起動を表しています。 printNum() 関数内では、関数の実行完了を WaitGroup に伝えるために、ポインタで渡された WaitGroup に対して w.Done() を実行しています。そして、 main() 関数の最後では、生成した goroutine の終了を待つように wg.Wait() を実行しています。\nこのコードを実行すると、次のような出力が得られます。\n$ go run sample.go Start main function... 1 3 7 5 6 8 9 10 4 2 Finished main function. 実行タイミングは約束されていないので順番はバラバラですが、 1 から 10 までの数値がすべて出力されています。\nまとめ Go 言語における並行処理入門として、 WaitGroup を使った goroutine の制御を試してみました。とりあえず並行処理の実行と それらを待つという処理はできたので、次は生成した goroutine 間でのメモリアクセス同期について sync.RWMutex の仕様について理解を深めたいと思います。\n",
    "permalink": "https://michimani.net/post/programming-introduction-to-concurrency-in-go/",
    "title": "Go 言語による並行処理に入門してみる - sync.WaitGroup 編"
  },
  {
    "contents": "前回の JAWS-UG CLI 専門支部で、シェルの for 文と AWS CLI コマンドを組み合わせた操作があったので、同じパターンを使って複数の EC2 インスタンスを起動・停止してみます。\n目次 概要 やってみる EC2 インスタンスの作成 まとめて起動・停止する for 文使わなくてもいいのでは まとめ 概要 AWS CLI を使って複数の EC2 インスタンスを起動・停止します。 \u0026ldquo;複数の\u0026rdquo; というのは、今回は \u0026ldquo;特定のタグキーと値を持つ EC2 インスタンス\u0026rdquo; ということにします。\n使用する AWS CLI のバージョンは、現時点 (2020/10/15) で最新の 2.0.56 です。\n$ aws --version aws-cli/2.0.56 Python/3.7.4 Darwin/19.6.0 exe/x86_64 やってみる 早速やってみます。手順としては、まず特定のタグキーと値を持つ EC2 インスタンスを複数準備して、その後 まとめて起動と停止を実施します。\nEC2 インスタンスの作成 EC2 インスタンスの作成には、 ec2 run-instances コマンドを使います。 今回 \u0026ldquo;特定のタグキーと値\u0026rdquo; として指定するのは StartStopByCLI というタグキーと TRUE という値とします。\nAMI イメージは Amazon Linux 2 を指定するので、最新のイメージ ID を取得します。\n$ EC2_IMAGE_ID=$(aws ec2 describe-images \\ --owners amazon \\ --filters \u0026#39;Name=name,Values=amzn2-ami-hvm-2.0.*-x86_64-gp2\u0026#39; \\ --query \u0026#39;reverse(sort_by(Images,\u0026amp;CreationDate))[1].ImageId\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; echo ${EC2_IMAGE_ID} ami-0053d11f74e9e7f52 ami-0053d11f74e9e7f52 という AMI イメージ ID が取得できました。この AMI イメージで複数の EC2 インスタンスを作成します。\nインスタンスタイプは、時間あたりのコストが低い t3a.nano を指定します。\n$ EC2_INSTANCE_TYPE=\u0026#34;t3a.nano\u0026#34; タグで指定する文字列もあらかじめ変数に保持しておきます。\n$ RESOURCE_TAG_SPEC=\u0026#34;ResourceType=instance,Tags=[{Key=StartStopByCLI,Value=TRUE}]\u0026#34; これらの条件で、 3 つのインスタンスを作成してみます。\n$ aws ec2 run-instances \\ --image-id ${EC2_IMAGE_ID} \\ --instance-type ${EC2_INSTANCE_TYPE} \\ --tag-specifications ${RESOURCE_TAG_SPEC} \\ --count 3 ステータスを確認します。\n$ aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].{InstanceID:InstanceId,Status:State.Name}\u0026#39; [ { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0a7f9d2965a2177f6\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-044041fb5a5d40623\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0d2ab038f784f9156\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34; } ] これで対象のインスタンスが作成できました。ちなみに、それ以外のインスタンスも含めてステータスを確認してみると、\n$ aws ec2 describe-instances \\ --query \u0026#39;Reservations[*].Instances[].{InstanceID:InstanceId,Status:State.Name,LaunchTime:LaunchTime}\u0026#39; [ { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0539ee02588c12499\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-09-10T09:03:47+00:00\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0a7f9d2965a2177f6\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-15T01:44:26+00:00\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-044041fb5a5d40623\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-15T01:44:26+00:00\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0d2ab038f784f9156\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-15T01:44:26+00:00\u0026#34; } ] 今回作成したインスタンスの他に、一つ停止しているインスタンスが存在しているという状態です。\nまとめて起動・停止する では、先ほど作成したインスタンスを対象にして、 CLI コマンドでまとめて起動・停止してみます。現時点でインスタンスは起動しているので、まずはまとめて停止させてみます。\nEC2 インスタンスの停止 EC2 インスタンスを停止するには、 ec2 stop-instances コマンドを使います。その際にインスタンス ID を指定する必要があるので、 ec2 describe-instances コマンドで取得したインスタンス ID を渡します。これを、シェルの for 文を使って一気にやってしまいます。\n対象のインスタンス ID のリストは次のコマンドでします。\n$ aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].InstanceId\u0026#39; \\ --output text [ \u0026#34;i-0a7f9d2965a2177f6\u0026#34;, \u0026#34;i-044041fb5a5d40623\u0026#34;, \u0026#34;i-0d2ab038f784f9156\u0026#34; ] これを for 文で回して、各インスタンスに対して ec2 stop-instance を実行します。\n$ for i in $( aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].InstanceId\u0026#39; \\ --output text ); do aws ec2 stop-instances \\ --instance-ids ${i} done ステータスを確認してみます。\n$ aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].{InstanceID:InstanceId,Status:State.Name}\u0026#39; [ { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0a7f9d2965a2177f6\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;stopped\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-044041fb5a5d40623\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;stopped\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0d2ab038f784f9156\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;stopped\u0026#34; } ] 停止できてますね。\nEC2 インスタンスの起動 では、今度は対象のインスタンスをまとめて起動してみます。起動するには ec2 start-instances コマンドを使います。\n$ for i in $( aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].InstanceId\u0026#39; \\ --output text ); do aws ec2 start-instances \\ --instance-ids ${i} done 今度は全インスタンスのステータスを確認してみます。\n$ aws ec2 describe-instances \\ --query \u0026#39;Reservations[*].Instances[].{InstanceID:InstanceId,Status:State.Name,LaunchTime:LaunchTime}\u0026#39; [ { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0539ee02588c12499\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-08T09:02:05+00:00\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0a7f9d2965a2177f6\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-15T02:01:28+00:00\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-044041fb5a5d40623\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-15T02:01:30+00:00\u0026#34; }, { \u0026#34;InstanceID\u0026#34;: \u0026#34;i-0d2ab038f784f9156\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;LaunchTime\u0026#34;: \u0026#34;2020-10-15T02:01:31+00:00\u0026#34; } ] 対象のインスタンスのみが起動しています。\nfor 文使わなくてもいいのでは 途中で気付きましたが、インスタンスの起動・停止をするコマンドは *-instances という形で複数形になっているんです。そして、インスタンス ID を指定するオプションも --instance-ids と、複数形になっています。つまり、今回のように for 文でインスタンスを一つずつ処理する必要はなかったわけです。\nつまり、次のようにすれば起動・停止のコマンドは一回実行するだけでいいんですよね。\n$ TARGET_INSTANCES=$( aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].InstanceId\u0026#39; ) \\ \u0026amp;\u0026amp; aws ec2 stop-instances \\ --instance-ids ${TARGET_INSTANCES} { \u0026#34;StoppingInstances\u0026#34;: [ { \u0026#34;CurrentState\u0026#34;: { \u0026#34;Code\u0026#34;: 64, \u0026#34;Name\u0026#34;: \u0026#34;stopping\u0026#34; }, \u0026#34;InstanceId\u0026#34;: \u0026#34;i-044041fb5a5d40623\u0026#34;, \u0026#34;PreviousState\u0026#34;: { \u0026#34;Code\u0026#34;: 16, \u0026#34;Name\u0026#34;: \u0026#34;running\u0026#34; } }, { \u0026#34;CurrentState\u0026#34;: { \u0026#34;Code\u0026#34;: 64, \u0026#34;Name\u0026#34;: \u0026#34;stopping\u0026#34; }, \u0026#34;InstanceId\u0026#34;: \u0026#34;i-0a7f9d2965a2177f6\u0026#34;, \u0026#34;PreviousState\u0026#34;: { \u0026#34;Code\u0026#34;: 16, \u0026#34;Name\u0026#34;: \u0026#34;running\u0026#34; } }, { \u0026#34;CurrentState\u0026#34;: { \u0026#34;Code\u0026#34;: 64, \u0026#34;Name\u0026#34;: \u0026#34;stopping\u0026#34; }, \u0026#34;InstanceId\u0026#34;: \u0026#34;i-0d2ab038f784f9156\u0026#34;, \u0026#34;PreviousState\u0026#34;: { \u0026#34;Code\u0026#34;: 16, \u0026#34;Name\u0026#34;: \u0026#34;running\u0026#34; } } ] } まとめ AWS CLI を使って特定のタグキーと値を持つ EC2 インスタンスをまとめて起動・停止してみた話でした。\n当初はシェルの for 文で各インスタンスごとに起動・停止を実行する想定でしたが、それぞれのコマンドのドキュメントを見れば複数インスタンスに対して同時に起動・停止を行うことができることがわかりました。一回の CLI コマンド実行で複数のインスタンスを処理できるのであればそのほうがコマンドの実行回数が少なくなるのでシンプルですが、 for を使った再帰処理の復習になったので、良かったことにします\u0026hellip;。\n最後に使用したインスタンスを削除しておしまいです。\n$ TARGET_INSTANCES=$( aws ec2 describe-instances \\ --filters Name=tag-key,Values=StartStopByCLI Name=tag-value,Values=TRUE \\ --query \u0026#39;Reservations[*].Instances[].InstanceId\u0026#39; ) \\ \u0026amp;\u0026amp; aws ec2 terminate-instances \\ --instance-ids ${TARGET_INSTANCES} ちなみに、同じようなことを以前に AWS SDK for Python (boto3) で実行した話については下記の記事で書いてます。\n",
    "permalink": "https://michimani.net/post/aws-start-stop-ec2-instances-using-aws-cli/",
    "title": "AWS CLI で複数の EC2 インスタンスをまとめて起動・停止する"
  },
  {
    "contents": "JAWS-UG CLI 専門支部 #169R S3基礎 バージョニング に参加したので、そのレポートです。\nconnpass のイベントページはこちら。\n前回、前々回のレポートはこちら。\n[レポート] JAWS-UG CLI 専門支部 #168R S3基礎 Webサイト＆ログ に参加しました #jawsug_cli - michimani.net [レポート] JAWS-UG CLI 専門支部 #167R EventBridge入門 に参加しました #jawsug_cli - michimani.net 目次 S3 の全体像とサービス概要 ハンズオン 1.3. IAMポリシーの作成 2. S3バケットのバージョニング有効化 3.1. ファイルの作成 3.5 S3オブジェクトの特定バージョンのダウンロード 3.6 S3オブジェクトの特定バージョンを削除 知らんかったぞ、それ クレデンシャルファイルに任意のファイルを指定する シェルの for 文 まとめ S3 の全体像とサービス概要 S3 の全体像とサービス概要については 前回 のレポートを参照してください。今回はその差分のみ。\nOutposts に Amazon S3 が対応 Amazon S3 on Outposts の一般提供開始に伴うオブジェクトストレージのオンプレミス環境への拡張 Outposts で Amazon S3 が利用可能に | Amazon Web Services ブログ 今回はオブジェクトに関する部分のハンズオン バージョニング自体はバケットに対して設定する オブジェクトはその影響を受ける S3 全体が名前空間というイメージ 故にバケット名はグローバルユニーク S3 はファイルシステムではない マネコンではオブジェクトのキーに含まれる / でフォルダ構造っぽく見せている バージョンが異なる別のオブジェクトが生成される (※重要) 差分が生成されるわけではない 特定のバージョンを削除しても他のバージョンに影響はない バージョンを指定しないと、一番最近のバージョンのオブジェクトが取得される バージョニングを使うか否か バケットの外 (Git や Mercurial) でバージョン管理 バケットの中身は使い捨て Web サイトなど バケットでバージョン管理 バケットの中身がマスターになる バージョニングを利用する S3 の機能 リテーションモード オブジェクトロック リーガルホールド ハンズオン 今回のハンズオンでは、バージョニングを有効にした S3 バケットに対してオブジェクトをアップロード・更新し、特定バージョンの取得や削除を行いました。\n今回も、ハンズオンの詳細な手順についてはイベントページの資料 及び下記のハンズオン資料におまかせするとして、個人的に気になった部分のコマンドについて書いておきます。\nハンズオン(簡易版): S3基礎 バージョニング 1.3. IAMポリシーの作成 IAM ポリシーを作成する際にパスを使用することで、その後の管理がやりやすくなります。\n$ aws iam create-policy help ... SYNOPSIS create-policy --policy-name \u0026lt;value\u0026gt; [--path \u0026lt;value\u0026gt;] --policy-document \u0026lt;value\u0026gt; [--description \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] iam create-policy コマンドの --path オプションを使って、次のように実行します。\n$ aws iam create-policy \\ --policy-name \u0026#34;michimani-sample-policy\u0026#34; \\ --path \u0026#34;/michimani/sample/\u0026#34; \\ --policy-document file://policy パスを使うことで、 list-policies コマンドの --path-prefix で結果を絞ることができます。\n$ aws iam list-policies \\ --path-prefix \u0026#34;/michimani/sample/\u0026#34; { \u0026#34;Policies\u0026#34;: [ { \u0026#34;PolicyName\u0026#34;: \u0026#34;michimani-sample-policy\u0026#34;, \u0026#34;PolicyId\u0026#34;: \u0026#34;ANPA2FQKQ3TXXXXXXXXXX\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::1234XXXXXXXX:policy/michimani/sample/michimani-sample-policy\u0026#34;, \u0026#34;Path\u0026#34;: \u0026#34;/michimani/sample/\u0026#34;, \u0026#34;DefaultVersionId\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;AttachmentCount\u0026#34;: 0, \u0026#34;PermissionsBoundaryUsageCount\u0026#34;: 0, \u0026#34;IsAttachable\u0026#34;: true, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-10-08T11:34:10+00:00\u0026#34;, \u0026#34;UpdateDate\u0026#34;: \u0026#34;2020-10-08T11:34:10+00:00\u0026#34; } ] } 2. S3バケットのバージョニング有効化 s3api put-bucket-versioning コマンドを使用します。\n$ aws s3api put-bucket-versioning help ... SYNOPSIS put-bucket-versioning --bucket \u0026lt;value\u0026gt; [--content-md5 \u0026lt;value\u0026gt;] [--mfa \u0026lt;value\u0026gt;] --versioning-configuration \u0026lt;value\u0026gt; [--expected-bucket-owner \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] --versioning-configuration オプションでバージョニングを有効にします。\n$ aws s3api put-bucket-versioning \\ --bucket ${S3_BUCKET_NAME} \\ --versioning-configuration Status=Enabled バージョニングのステータスを確認するには s3api get-bucket-versioning コマンドを使って、出力結果から Status の部分を抜き出します。\n$ aws s3api get-bucket-versioning \\ --bucket ${S3_BUCKET_NAME} \\ --query \u0026#39;Status\u0026#39; \\ --output text Enabled 無効化する場合は Status=Enabled として実行します。 (ハンズオンの手順 4)\n$ aws s3api put-bucket-versioning \\ --bucket ${S3_BUCKET_NAME} \\ --versioning-configuration Status=Suspended なお、バージョニングの設定を一度も触っていない場合、 get-buket-versioning では出力結果を得られず、 Status だけ抜出層としても None になります。\n$ aws s3api get-bucket-versioning \\ --bucket ${OTHER_S3_BUCKET_NAME} \\ --query \u0026#39;Status\u0026#39; \\ --output text None なので、上記の無効化は、 無効化 というよりも 一時停止 (= Suspended) というイメージです。\n3.1. ファイルの作成 バージョニングを有効にしたバケットに対してファイルをアップロードします。アップロードには s3 cp コマンドを使用します。\n$ aws s3 cp sample_file.txt s3://${S3_BUCKET_NAME} ちなみに、 sample_file.txt の中身は下記の内容です。\n$ cat sample_file.txt This is a sample file. (v1) マネジメントコンソールで確認すると、オブジェクトの詳細画面にバージョンを指定するプルダウンが表示されています。\nプルダウンからバージョンを指定すると、 バージョン ID が表示されます。\nバージョン ID を構成する文字列にはピリオド . が含まれる場合もあります。\nS3 オブジェクトを更新する場合、特に追加のパラメータなどは必要なく、先ほどと同じ s3 cp コマンドを使います。 sample_file.txt の中身の v1 の部分を変更して何度かアップロードを実行し、作成されたバージョンを確認してみます。オブジェクトのバージョン一覧を取得するには、 s3api list-object-versions コマンドを使用します。\n$ aws s3api list-object-versions help ... SYNOPSIS list-object-versions --bucket \u0026lt;value\u0026gt; [--delimiter \u0026lt;value\u0026gt;] [--encoding-type \u0026lt;value\u0026gt;] [--prefix \u0026lt;value\u0026gt;] [--expected-bucket-owner \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--starting-token \u0026lt;value\u0026gt;] [--page-size \u0026lt;value\u0026gt;] [--max-items \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] バケット名とオブジェクトキーを指定して実行。今回は、各バージョンのバージョン ID と最終更新日時を、最終更新日時の降順で出力してみます。\n$ aws s3api list-object-versions \\ --bucket ${S3_BUCKET_NAME} \\ --prefix sample_file.txt \\ --query \u0026#39;reverse(sort_by(Versions,\u0026amp;LastModified))[].{LM:LastModified,VID:VersionId}\u0026#39; [ { \u0026#34;LM\u0026#34;: \u0026#34;2020-10-08T12:02:16+00:00\u0026#34;, \u0026#34;VID\u0026#34;: \u0026#34;PWdCD9sEmem67CqNHA2cBMUSQCY6t5Sv\u0026#34; }, { \u0026#34;LM\u0026#34;: \u0026#34;2020-10-08T12:02:09+00:00\u0026#34;, \u0026#34;VID\u0026#34;: \u0026#34;b4234JLABtdRGlQ9AGreIuSsqoaf964n\u0026#34; }, { \u0026#34;LM\u0026#34;: \u0026#34;2020-10-08T12:01:59+00:00\u0026#34;, \u0026#34;VID\u0026#34;: \u0026#34;jfbONeI0pJ1h9unJgRrvzmIFBvUBa0xv\u0026#34; }, { \u0026#34;LM\u0026#34;: \u0026#34;2020-10-08T12:01:49+00:00\u0026#34;, \u0026#34;VID\u0026#34;: \u0026#34;baLmE30tSBAuiZ8lhQ3.LcyAf3P2fv7N\u0026#34; }, { \u0026#34;LM\u0026#34;: \u0026#34;2020-10-08T11:56:26+00:00\u0026#34;, \u0026#34;VID\u0026#34;: \u0026#34;sRfncD5dSroDAQYq4s3gAinHWQpQp4VW\u0026#34; } ] 3.5 S3オブジェクトの特定バージョンのダウンロード オブジェクトのダウンロードには s3api get-object コマンドを使用します。\n$ s3api get-object help ... SYNOPSIS get-object --bucket \u0026lt;value\u0026gt; [--if-match \u0026lt;value\u0026gt;] [--if-modified-since \u0026lt;value\u0026gt;] [--if-none-match \u0026lt;value\u0026gt;] [--if-unmodified-since \u0026lt;value\u0026gt;] --key \u0026lt;value\u0026gt; [--range \u0026lt;value\u0026gt;] [--response-cache-control \u0026lt;value\u0026gt;] [--response-content-disposition \u0026lt;value\u0026gt;] [--response-content-encoding \u0026lt;value\u0026gt;] [--response-content-language \u0026lt;value\u0026gt;] [--response-content-type \u0026lt;value\u0026gt;] [--response-expires \u0026lt;value\u0026gt;] [--version-id \u0026lt;value\u0026gt;] [--sse-customer-algorithm \u0026lt;value\u0026gt;] [--sse-customer-key \u0026lt;value\u0026gt;] [--sse-customer-key-md5 \u0026lt;value\u0026gt;] [--request-payer \u0026lt;value\u0026gt;] [--part-number \u0026lt;value\u0026gt;] [--expected-bucket-owner \u0026lt;value\u0026gt;] \u0026lt;outfile\u0026gt; オプションがたくさんありますが、今回使用するのは --version-id オプションです。\n--version-id で指定するのは s3api list-object-versions で確認できた VersionId ですが、今回は最新のものから数えて 3 番目にあたるバージョンのオブジェクトをダウンロードしてみます。対象のバージョン ID を次のようにして取得します。\n$ TARGET_VERSION_ID=$(\\ aws s3api list-object-versions \\ --bucket ${S3_BUCKET_NAME} \\ --prefix sample_file.txt \\ --query \u0026#39;reverse(sort_by(Versions,\u0026amp;LastModified))[2].VersionId\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; echo $TARGET_VERSION_ID jfbONeI0pJ1h9unJgRrvzmIFBvUBa0xv バージョン ID が取得できたので、そのバージョンを指定してオブジェクトをダウンロードします。3 番目ということで、オブジェクトの中身は This is a sample file. (v3) になっているはずなので、ダウンロード後に中身を出力するところまでやってみます。\n$ aws s3api get-object \\ --bucket ${S3_BUCKET_NAME} \\ --key sample_file.txt \\ --version-id ${TARGET_VERSION_ID} \\ ./out/sample_file_v3.txt \u0026gt; /dev/null \\ \u0026amp;\u0026amp; cat ./out/sample_file_v3.txt This is a sample file. (v3) 想定通りの内容でした。\n3.6 S3オブジェクトの特定バージョンを削除 特定バージョンの削除には s3api delete-object を --version-id オプションと合わせて使用します。\n$ aws s3api delete-object help ... SYNOPSIS delete-object --bucket \u0026lt;value\u0026gt; --key \u0026lt;value\u0026gt; [--mfa \u0026lt;value\u0026gt;] [--version-id \u0026lt;value\u0026gt;] [--request-payer \u0026lt;value\u0026gt;] [--bypass-governance-retention | --no-bypass-governance-retention] [--expected-bucket-owner \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] 今回は、先ほど取得したバージョンを削除します。\n$ aws s3api delete-object \\ --bucket ${S3_BUCKET_NAME} \\ --key sample_file.txt \\ --version-id ${TARGET_VERSION_ID} { \u0026#34;VersionId\u0026#34;: \u0026#34;jfbONeI0pJ1h9unJgRrvzmIFBvUBa0xv\u0026#34; } 続けて、あらためて最新のものから 3 番目にあたるバージョンのオブジェクトをダウンロードしてみます。今度は中身が This is a sample file. (v2) になるはずです。\n$ TARGET_VERSION_ID=$(\\ aws s3api list-object-versions \\ --bucket ${S3_BUCKET_NAME} \\ --prefix sample_file.txt \\ --query \u0026#39;reverse(sort_by(Versions,\u0026amp;LastModified))[2].VersionId\u0026#39; \\ --output text) \\ \u0026amp;\u0026amp; echo $TARGET_VERSION_ID baLmE30tSBAuiZ8lhQ3.LcyAf3P2fv7N $ aws s3api get-object \\ --bucket ${S3_BUCKET_NAME} \\ --key sample_file.txt \\ --version-id ${TARGET_VERSION_ID} \\ ./out/sample_file_v2.txt \u0026gt; /dev/null \\ \u0026amp;\u0026amp; cat ./out/sample_file_v2.txt This is a sample file. (v2) 想定通りの内容でした。\n知らんかったぞ、それ 今回に限らずハンズオンでは新しく知ることがたくさんあるので、この 知らんかったぞ、それ 項目では新しく知ったことを自分なりに深堀りしてみたいと思います。(恒例化したい)\nというわけで、今回もいくつか取り上げてみます。\nクレデンシャルファイルに任意のファイルを指定する 環境変数 AWS_SHARED_CREDENTIALS_FILE に任意のファイルを指定することで、 ~/.aws/credentials 以外のファイルをクレデンシャルファイルとして指定できます。\nAWS CLI を設定する環境変数 - AWS Command Line Interface 今回のようにハンズオン用のプロファイルを作成する際に ~/.aws/credentials を編集すると、ハンズオン後の掃除がしにくくなり、事故の原因にもなります。ハンズオン用のクレデンシャルファイルを作って AWS_SHARED_CREDENTIALS_FILE で指定すれば、 ~/.aws/credentials は汚れずに済むというわけです。\nシェルの for 文 AWS CLI というかシェルの使い方の話ですが。\n今回のハンズオンでは、シェルの for 文と CLI コマンドを組み合わせて、すべてのバージョンを削除していました。今までシェルの for 文を使ったことがなかったのと、 CLI コマンドとの組み合わせに感動しました。\u0026hellip;と言っても自分で良い例が思いつかないので、ハンズオンと同じくすべてのバージョンを削除してみます。\n削除前に、対象のバージョンの件数を確認しておきます。\n$ aws s3api list-object-versions \\ --bucket ${S3_BUCKET_NAME} \\ --prefix sample_file.txt \\ --query \u0026#39;length(Versions)\u0026#39; \\ --output text 4 for 文で削除します。\n$ for i in $( aws s3api list-object-versions \\ --bucket ${S3_BUCKET_NAME} \\ --prefix sample_file.txt \\ --query \u0026#39;Versions[].VersionId\u0026#39; \\ --output text ); do aws s3api delete-object \\ --bucket ${S3_BUCKET_NAME} \\ --key sample_file.txt \\ --version-id ${i} done { \u0026#34;VersionId\u0026#34;: \u0026#34;PWdCD9sEmem67CqNHA2cBMUSQCY6t5Sv\u0026#34; } { \u0026#34;VersionId\u0026#34;: \u0026#34;b4234JLABtdRGlQ9AGreIuSsqoaf964n\u0026#34; } { \u0026#34;VersionId\u0026#34;: \u0026#34;baLmE30tSBAuiZ8lhQ3.LcyAf3P2fv7N\u0026#34; } { \u0026#34;VersionId\u0026#34;: \u0026#34;sRfncD5dSroDAQYq4s3gAinHWQpQp4VW\u0026#34; } s3api list-object-versions コマンドの出力結果から --query オプションで各バージョンの VersionId を抜き出して、その値を s3api delete-object に渡しています。出力としては、 s3api delete-object で削除されたバージョンの VersionId が出力されます。\n念のため削除後の件数を確認しておきます。\n$ aws s3api list-object-versions \\ --bucket ${S3_BUCKET_NAME} \\ --prefix sample_file.txt \\ --query \u0026#39;length(Versions)\u0026#39; \\ --output text In function length(), invalid type for value: None, expected one of: [\u0026#39;string\u0026#39;, \u0026#39;array\u0026#39;, \u0026#39;object\u0026#39;], received: \u0026#34;null\u0026#34; まとめ JAWS-UG CLI 専門支部 #169R S3基礎 バージョニング の参加レポートでした。\nS3 のバージョニングって、そういえば普段の業務や個人的な利用でも有効にしたことがなかったので、今回その挙動について試すことができてよかったです。S3 のバージョニングは差分を保持する形ではなく、別のオブジェクトとして保持するので、大量にバージョンを生成する際にはストレージコストに注意する必要がありそうです。\n今回もシェルについて新たに for 文の知識を得たので、どんどん使っていってシェル芸を上達させたいと思います。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-169-s3-versioning/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #169R S3基礎 バージョニング に参加しました #jawsug_cli"
  },
  {
    "contents": "いきなりですが、 IAM ユーザーは自分自身を削除できるのでしょうか？ちょっとした疑問というか、どうなるんや？と思ったので、いくつかの方法で試してみました。\n目次 やること やってみる 1. マネジメントコンソールにログインして IAM のコンソールから削除 2. AWS CLI の iam delete-user コマンドで削除 3. マネジメントコンソールにログインして CloudFormation のコンソールからスタックを削除 まとめ やること 既存の IAM ユーザーで自分自身を削除することができるのか、いくつかの方法で試してみます。\n方法としては、次のとおりです。\nマネジメントコンソールにログインして IAM のコンソールから削除 ログインした IAM ユーザー自身で削除 スイッチロールして削除 AWS CLI の iam delete-user コマンドで削除 自身のプロファイルで削除 スイッチロールして削除 マネジメントコンソールにログインして CloudFormation のコンソールからスタックを削除 事前に CloudFormation で IAM ユーザーを作成しておきます ログインした IAM ユーザー自身で削除 スイッチロールして削除 やってみる では、それぞれやっていきます。\n1. マネジメントコンソールにログインして IAM のコンソールから削除 事前に will-be-deleted という IAM ユーザーを作成しておきます。作成する際に、今回 必要になる IAM ポリシーがアタッチされている IAM グループに所属させるものとします。\n$ aws iam get-user \\ --user-name will-be-deleted { \u0026#34;User\u0026#34;: { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;will-be-deleted\u0026#34;, \u0026#34;UserId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXMWMLT\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::12345678XXXX:user/will-be-deleted\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-10-07T09:21:20+00:00\u0026#34;, \u0026#34;Tags\u0026#34;: [] } } ログインした IAM ユーザー自身で削除 まずは will-be-deleted ユーザーでマネジメントコンソールにログインして、 IAM のコンソールから削除を試してみます。\n削除 ボタンを押すと削除が始まり、何事もなく削除できてしまいました。\n削除後は一旦 IAM ユーザー一覧画面が表示されますが、その後他のページへ移動したり何らかの操作を行うと、ログイン画面へ遷移しました。念のため確認しておきます。\n$ aws iam get-user \\ --user-name will-be-deleted An error occurred (NoSuchEntity) when calling the GetUser operation: The user with name will-be-deleted cannot be found. ということで、もう結論が出てしまった感はありますが、一応 残りのパターンについても試してみます。\nスイッチロールして削除 再度 IAM ユーザー will-be-deleted を作成・ログインして、 michimani-admin というロールにスイッチしてから will-be-deleted ユーザーを削除してみます。\nこの場合も問題なく削除できました。\n当該 IAM ユーザー自身で削除する場合と違って、スイッチロールして削除する場合は IAM ユーザー削除後もスイッチ後のロールでマネジメントコンソールを操作することができました。\nまた、右上のスイッチロール名をクリックしたときに出るメニューから will-be-deleted に戻る を選択しても、元の IAM ユーザーに戻ることもできず、ログイン画面に遷移するわけでもなく、そのままスイッチロールした状態が継続する形となりました。\n2. AWS CLI の iam delete-user コマンドで削除 続いては AWS CLI で削除を試してみます。\n今回も事前に IAM ユーザー will-be-deleted を作成しておき、アクセスキーおよびシークレットを作成しておきます。そして、プロファイル名として be-del という名前をつけておきます。\n~/.aws/credentials は下記のような内容。\n[be-del] aws_access_key_id=AKIA2FQK************ aws_secret_access_key=iis77+****************** 自身のプロファイルで削除 まずは自身のプロファイルで削除してみます。\n削除の前に確認。\n$ aws iam get-user \\ --user-name will-be-deleted \\ --profile be-del { \u0026#34;User\u0026#34;: { \u0026#34;Path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;UserName\u0026#34;: \u0026#34;will-be-deleted\u0026#34;, \u0026#34;UserId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXTXQKX\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::12345678XXXX:user/will-be-deleted\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2020-10-07T09:51:06+00:00\u0026#34; } } iam delete-user コマンドで削除します。\n$ aws iam delete-user \\ --user-name will-be-deleted \\ --profile be-del An error occurred (DeleteConflict) when calling the DeleteUser operation: Cannot delete entity, must remove users from group first. まずは IAM グループから remove しなさいと言われました。ということで、 IAM グループから remove して、必要な IAM ポリシーを IAM ユーザー自身にアタッチし、再度試してみます。\n$ aws iam delete-user \\ --user-name will-be-deleted \\ --profile be-del An error occurred (DeleteConflict) when calling the DeleteUser operation: Cannot delete entity, must detach all policies first. 次はアタッチされているポリシーをすべて削除しなさいと言われました。ということで、アタッチされたポリシーを削除します。でもこうすると\u0026hellip;\n$ aws iam delete-user \\ --user-name will-be-deleted \\ --profile be-del An error occurred (AccessDenied) when calling the DeleteUser operation: User: arn:aws:iam::12345678XXXX:user/will-be-deleted is not authorized to perform: iam:DeleteUser on resource: user will-be-deleted iam:DeleteUser の権限がないと言われました。それはそう。\nということで、自身のプロファイルでは自分自身を削除することはできないようです。\nスイッチロールして削除 では、 AWS CLI でスイッチロールして削除を試してみます。\nCLI でスイッチロールするために、 ~/.aws/config に次の内容を記載します。\n[profile michimani-admin] role_arn=arn:aws:iam::12345678XXXX:role/michimani-admin source_profile=be-del --profile で michimani-admin を指定して削除を試します。\n$ aws iam delete-user \\ --user-name will-be-deleted \\ --profile michimani-admin An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:iam::12345678XXXX:user/will-be-deleted is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::12345678XXXX:role/michimani-admin sts:AssumeRole の権限がないと言われました。じゃあ sts:AssumeRole のポリシーをアタッチすればよいかと言うと、そうすると今度はアタッチしたポリシーを削除してと言われてしまいます。\nということで、スイッチロールしても削除することはできないようです。\n3. マネジメントコンソールにログインして CloudFormation のコンソールからスタックを削除 続いては、 IAM ユーザーを CloudFormation で管理し、そのスタックを削除することで IAM ユーザーを削除してみます。今回用意した CloudFormation テンプレートは下記の内容です。\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;IAM User\u0026#34; Parameters: UserPassword: Type: String Group: Type: String Resources: IAMUserForDelete: Type: \u0026#34;AWS::IAM::User\u0026#34; Properties: LoginProfile: Password: !Ref UserPassword PasswordResetRequired: false UserName: will-be-deleted Groups: - !Ref Group このテンプレートで IAM ユーザ will-be-deleted を作成し、マネジメントコンソールからログインしてスタックを削除してみます。ちなみに、ここで所属させる IAM グループには必要な IAM ポリシーがアタッチされているものとします。\nログインした IAM ユーザー自身で削除 will-be-deleted でマネジメントコンソールにログインし、 CloudFormation のコンソールから該当のスタックを削除します。\n問題なく削除できました。そして、 IAM のコンソールから削除したときと同様に、別の操作を行うとログイン画面に遷移しました。\nスイッチロールして削除 will-be-deleted を再度作成 (こういうとき CFn だとすぐに再現できて良いですね) 、マネジメントコンソールにログインして先程と同様に michimani-admin ロールにスイッチします。そして、 CloudFormation のコンソールから該当のスタックを削除します。\nこちらも問題なく削除できました。そして、 IAM のコンソールから削除したときと同様に、スイッチロールは継続、元の IAM ユーザに戻ることはできない という状態になりました。\nまとめ IAM ユーザーは自分自身を削除できるのか、いくつかの方法で試してみた話でした。今回試した方法とその結果のまとめは下記の表のようになります。\n方法 削除可否 備考 IAM コンソールから ○ 削除後に別の操作を行うとログイン画面に遷移する IAM コンソールから with スイッチロール ○ 削除後もスイッチロールは継続、元の IAM ユーザには戻れない AWS CLI × 最終的に権限が足りなくなる AWS CLI with スイッチロール × 最終的に権限が足りなくなる CloudFormation のスタック削除 ○ 削除後に別の操作を行うとログイン画面に遷移する CloudFormation のスタック削除\nwith スイッチロール ○ 削除後もスイッチロールは継続、元の IAM ユーザには戻れない そもそも自分自身を削除しようとする場面はあまりない (退職とプロジェクトから抜ける時くらい？) と思いますが、もしやるならマネジメントコンソールからの操作であれば削除できそうです。\n",
    "permalink": "https://michimani.net/post/aws-delete-iam-user-by-myself/",
    "title": "IAM ユーザーは自分自身を削除できるのか色んな方法で試してみた"
  },
  {
    "contents": "AWS CDK にフォーカスしたオンラインイベント CDK Day が開催されたので、その参加レポ (という名のメモ) です。全部英語のセッションで字幕などもなかったので、なんとなく聞き取れた部分とスライドから読み取れた部分、関連するリンクなどをメモしています。\n目次 CDK Day タイムテーブル セッションレポート Keynote Lightning Talk: The cdk8s, the why and how - Matthew Bonig CDK 101 - Tatenda Gibson projen - a CDK for software project configuration - Elad Ben Israel Our Saas Journey with CDK - Ran Isenberg Building Real-time Back Ends on AWS with AppSync and CDK - nader dabit CDK and FedRAMP Compliance - Julia Jacobs Getting started with CDK for Terraform and Python - Charles McLaughlin Mix and Match CDK Constructs between AWS CDK, CDK8s and CDKTF - Sebastian Korfmann AWS config with CDK - chitranjali edpuganti Moving from CloudFormation YAML to CDK - Benefits, How to do it in a production system, and More! - Andrew Nguyen Serverless IoT with Greengrass, Ansible and AWS CDK - Luca Bianchi Multi-account and multi-region - Deploy your CDK app to multiple environments - Thorsten Höger The good, the bad and the ugly of CDK adoption in a big enterprise - Mat Jovanovic Prototyping a CDK for Azure - Andreas Heumaier まとめ CDK Day AWS CDK にフォーカスしたオンラインイベントで、 東部標準時 (EDT) の 9月30日 10:00 〜 15:00 に開催されました。時差の関係で、協定世界時 (UTC) では 30日 14:00 〜 19:00 、日本標準時 (JST) では 30日 23:00 から翌 10月1日 4:00 の開催でした。\nTwitter のハッシュタグは #CDKDay でした。\nタイムテーブル 時間 (日本時間) タイトル 11:00 PM Keynote 11:20 PM Lightning Talk: The cdk8s, the why and how - Matthew Bonig 11:30 PM CDK 101 - Tatenda Gibson 11:40 PM projen - a CDK for software project configuration - Elad Ben Israel 11:50 PM Our Saas Journey with CDK - Ran Isenberg 12:00 AM Building Real-time Back Ends on AWS with AppSync and CDK - nader dabit 12:30 AM CDK and FedRAMP Compliance - Julia Jacobs 1:00 AM Lunch 1:30 AM Getting started with CDK for Terraform and Python - Charles McLaughlin 2:00 AM Mix and Match CDK Constructs between AWS CDK, CDK8s and CDKTF - Sebastian Korfmann 2:10 AM AWS config with CDK - chitranjali edpuganti 2:20 AM Moving from CloudFormation YAML to CDK - Benefits, How to do it in a production system, and More! - Andrew Nguyen 2:30 AM Serverless IoT with Greengrass, Ansible and AWS CDK - Luca Bianchi 3:00 AM Multi-account and multi-region - Deploy your CDK app to multiple environments - Thorsten Höger 3:30 AM The good, the bad and the ugly of CDK adoption in a big enterprise - Mat Jovanovic 3:40 AM Prototyping a CDK for Azure - Andreas Heumaier 3:50 AM Closing Remarks セッションレポート ここからは各セッションのレポート (メモ) です。\nKeynote CDK for Terraform ( @build1point0 ) CDK for Terraform: Enabling Python \u0026amp;amp; TypeScript Support TypeScript 、 Python 本家 CDK とは独立している Lightning Talk: The cdk8s, the why and how - Matthew Bonig Matthew Bonig (@mattbonig) k8s manifest\n読みやすいけど、 not DRY Helm Charts\nテストしやすいけど、 すごく表面的 CDK を使うメリット\nmulti-lingual (TypeScript, Python, Java, C#) 読みやすい ユニットテストしやすい cdk8s と cdk8s+ は現在 α 版\nCDK 101 - Tatenda Gibson Tatenda_M (@Taity__m) Overview 2019/7 リリース JavaScript, TypeScript, Python, Java, C# Why CDK? CFn = Pain 新たに学ぶことが少ない コンストラクトによってハイレベルに抽象化された方法でリソースを定義できる Comparison with SAM SAM CDK YAML or JSON プログラム言語 local emulation をサポート local testing をサポートしていない サーバーレスリソースのみデプロイ すべてのリソースをデプロイ Next Steps\u0026hellip; A beginner\u0026amp;rsquo;s guide to CDK, by building an ECS cluster projen - a CDK for software project configuration - Elad Ben Israel Elad Ben-Israel (@emeshbi) eladb/projen eladb/projen: A new generation of project generators JS で記述された定義から、 package.json, tsconfig.json, GitHub workflows の定義などのプロジェクト構成ファイル群を生成 (LIVE Demo) Our Saas Journey with CDK - Ran Isenberg Ran Isenberg (@IsenbergRan) CyberArk lower level CFn class を使うことを恐れないで チーム内で Constructs を共有すればコードの重複を防いで開発時間を短縮できる About CyberArk Engineering – Medium Building Real-time Back Ends on AWS with AppSync and CDK - nader dabit GraphQL query language for API 構成要素 Schema Resolvers Data source Database Serverless Function HTTP endpoint AWS AppSync マネージドな GraphQL サービス IAM, Cognito, API Keys, OIDC によるセキュリティ Building AWS AppSync with CDK Install CDK packages (AppSync + other data sorce) Create configure an API スキーマの場所を指定 認証方法を指定 (API Key) xrayEnablesd: true Add data source Lambda 関数 Add resolvers to data source (LIVE Demo) CDK and FedRAMP Compliance - Julia Jacobs Julia Jacobs (@jewelsjacobs) Acxiom (IPG) CMS Developer Tools AWS での FedRAMP 対応 FedRAMP コンプライアンス – アマゾン ウェブ サービス (AWS) 対象範囲内のサービス – アマゾン ウェブ サービス (AWS) マイグレーション AUtherntication Firebase OAuth -\u0026gt; Amazon Cognito Data mLab mongoDB -\u0026gt; Heroku PostgreSQL -\u0026gt; RDS PostgreSQL API GraphQL Server Koa Heroku -\u0026gt; GraphQL Server Koa ECS/Fargate Frontend React/Redux Heroku -\u0026gt; React/Redux CloudFront セッション資料 AWS CDK and FedRAMP Compliance Getting started with CDK for Terraform and Python - Charles McLaughlin CDK for Terraform hashicorp/terraform-cdk: Define infrastructure resources using programming constructs and provision them using HashiCorp Terraform Python または TypeScript で実装 Python の場合 cdktf init --template=\u0026quot;python-pip\u0026quot; --local terraform-cdk/python.md at master · hashicorp/terraform-cdk TypeScript の場合 cdktf init --template=\u0026quot;typescript\u0026quot; --local terraform-cdk/typescript.md at master · hashicorp/terraform-cdk LIVE Demo では Static Webisite Hosting を有効にした S3 バケットを Python プロジェクトで作成 Mix and Match CDK Constructs between AWS CDK, CDK8s and CDKTF - Sebastian Korfmann Sebastian Korfmann (@skorfmann) CDK Construct Level\nLevel 3 - パターン Load balanced Fargate Service Level 2 - API ECS Fargate Service, Application Loadbalancer Level 1 - from CFn spec CfnEcsService, CfnEcsTaskDefinition, CfnApplicationLB, CfnTargetGroup, CfnRule skorfmann/cfn2tf Converts CloudFormation resources to Terraform resources. AWS config with CDK - chitranjali edpuganti CDK で AWS Config を構築する話 Moving from CloudFormation YAML to CDK - Benefits, How to do it in a production system, and More! - Andrew Nguyen Benefit CDK は CFn よりもコード量が大幅に少なくなる 開発ツールの恩恵を受けられる IDE サポート プログラム言語 パッケージ管理 (Maven, npm, pip) CDK コミュニティ コミュニティで知見をシェアできる Migration SQS + Lambda + SNS の例 マイグレーション CDK プロジェクトの作成 CDK で一部のリソースを構築する まずは SNS のみ構築 旧リソースを CDK で作成したリソースに置き換える Lambda の向き先を CDK で作成した SNS に変更 ステップ 2,3 を繰り返す SNS に続いて、 Lambda -\u0026gt; SQS の順に移行 旧リソースを削除する Done! more Andy Nguyen Serverless IoT with Greengrass, Ansible and AWS CDK - Luca Bianchi Luca Bianchi - #AWS Serverless Hero (@bianchiluca) RaspberryPi CM3 のコアセンサーについて\nAWS IoT Greengrass\nAWS IoT Greengrass（AWS をエッジデバイスへシームレスに拡張）| AWS IoT アーキテクチャ\nセンサーデバイス リモートボード ローカルビジネスロジック IoT トピック AWS IoT Greengrass core Lambda 関数 (Cloud へのプロキシ) (\u0026hellip; CDK の話は？)\nMulti-account and multi-region - Deploy your CDK app to multiple environments - Thorsten Höger Thorsten Hoeger (@hoegertn) Create Demo Project (HTTP API) API Gateway + Lambda + DynamoDB CodePipeline CDK Pipeline は developers preview (v1.64.1) aws-cdk/packages/@aws-cdk/pipelines at v1.64.1 · aws/aws-cdk Deploy to two different Region (and Account) cdk.Stage を extends した class (デモでは下記のような AppStage) を作成\nclass AppStage extend cdk.Stage { constructor(scope: cdk.Construct, id: string, props: cdk.StageProps) { super(scope, id, props); ... } } pipeline.addApplicationStage() の env.region で別リージョンを指定\npipeline.addApplicationStage( new AppStage(this, \u0026#39;app-dev-fra\u0026#39;, { env: { account: \u0026#39;XXXXXXXXXXXX\u0026#39;, region: \u0026#39;eu-central-1\u0026#39; } }) ); pipeline.addApplicationStage( new AppStage(this, \u0026#39;app-dev-fra\u0026#39;, { env: { account: \u0026#39;XXXXXXXXXXXX\u0026#39;, region: \u0026#39;eu-west-1\u0026#39; } }) ); Link Create a CI/CD Pipeline for your CDK App — Taimos GmbH Your access to my CDK brain — Taimos GmbH The good, the bad and the ugly of CDK adoption in a big enterprise - Mat Jovanovic Mat Jovanovic 🏂 (@MatJovanovic) 100+ AWS Account 5,500 CFn Stacks Mats Cloud Bad external colab いろんなツール (Terraform) 、言語 (Python, TypeScript, Java) 、アーキテクチャ (k8s) を使いたい人がいる Ugly it operations 一つの環境 (CDK + Python) に決めると怒る人がいる Prototyping a CDK for Azure - Andreas Heumaier Andreas Heumaier (@aheumaier) 多くの場合、 API リソースの定義に JSON スキーマを使う (Azure でも同じ) CDK で JSON スキーマを生成する @armkit/core を import Azure/armkit: The Cloud Development Kit for Azure Yetics/armkit: The Cloud Development Kit for Azure 共通のコードベースを使うことでミスをハマりどころを回避できる まとめ AWS CDK にフォーカスしたオンラインイベント CDK Day のレポート (という名のメモ) でした。\nCDK に関するセッションのみということで、非常に濃い CDK タイムでした。すべて英語のセッションで、時間帯も深夜帯ということでなかなか頭が回らない中で聞いていましたが、面白い話もいくつかありました。\n特に、 CDK for Terraform の話は Keynote や複数のセッションで触れられていました。個人的には Terraform 始めようと思っているところなので、 CDK for Terraform も合わせて使ってみようと思います。\n今回は AWS CDK についての話を聞くというのはもちろん、海外のセッションを聞いてみるというのも一つの目的でした。普段から英語のドキュメントを読むことも多いので英語のセッションもなんとなくは理解できるかなと思っていましたが、全然ダメでした。やはり Reading と Listening では勝手が全然違いますね\u0026hellip;ということで今回のメモは本当のメモレベルになってます。\nセッションの動画はあとからアップされる (?) みたいなので、気になる方はぜひそちらのほうを確認してみてください。\n",
    "permalink": "https://michimani.net/post/aws-report-of-cdk-day/",
    "title": "[レポート] CDK Day のセッション聴講メモ #CDKDay"
  },
  {
    "contents": "JAWS-UG CLI 専門支部 #168R S3基礎 Webサイト\u0026amp;ログ に参加したので、そのレポートです。\nconnpass のイベントページはこちら。\n前回のレポートはこちら。\n[レポート] JAWS-UG CLI 専門支部 #167R EventBridge入門 に参加しました #jawsug_cli - michimani.net 目次 S3 の概要 S3 の全体像 サービス概要 要素概要 ハンズオン 1.2. S3バケットACL更新 2.6. S3バケット Webサイトホスティング設定の作成 4. S3オブジェクトの確認 (ログの確認) CDK でやる場合との比較 CDK で AWS アカウント ID を扱う 知らんかったぞ、それ AWS アカウント ID の取得 まとめ S3 の概要 今回も前半部分は S3 の概要についての説明だったので、メモを残しておきます。\nS3 の全体像 マネジメントコンソールでは S3 API としては s3 、 s3control CLI としては s3 、 s3api 、 s3control s3control はパワーキット的な感じ バッチオペレーション など サービス概要 オブジェクトストレージ (所謂ブロックストレージとは違う) アクセス方法 ブラウザ、 http クライアント CLI、 SDK、マネジメントコンソール アクセス制御方法 パブリックアクセスブロック バケットポリシー バケット ACL 要素概要 インプット/アクセス AWS リソース 利用者 (一般) 利用者 with IAM ポリシー インプット側からオブジェクトまでのアクセス制御 バケット パブリックアクセス CORS Web サイトホスティング 署名付き URL バケットポリシー、バケット ACL オブジェクト オブジェクト ACL オブジェクト属性 オブジェクトロック アウトプット S3 バケット ライフサイクル S3 Gracier メトリクス CloudWatch 通知 SNS SQS Lambda ハンズオン 今回のハンズオンでは、 S3 の Static website hosting の機能を使って静的なサイトを構築し、そのサイトへのアクセスログを別のバケットに出力するという構成を AWS CLI で構築しました。\nハンズオンの詳細な手順についてはイベントページの資料 及び下記のハンズオン資料におまかせするとして、個人的に気になった部分のコマンドについて書いておきます。\nハンズオン(簡易版): S3基礎 Webサイト\u0026amp;amp;ログ — ハンズオン(簡易版): S3基礎 Webサイト\u0026amp;amp;ログ 1.2. S3バケットACL更新 アクセスログを出力するバケットに対して、下記のコマンドでバケット ACL を変更しました。\n$ aws s3api put-bucket-acl \\ --bucket ${S3_BUCKET_NAME} \\ --grant-write \u0026#39;URI=\u0026#34;http://acs.amazonaws.com/groups/s3/LogDelivery\u0026#34;\u0026#39; \\ --grant-read-acp \u0026#39;URI=\u0026#34;http://acs.amazonaws.com/groups/s3/LogDelivery\u0026#34;\u0026#39; ここで出てくる LogDelivery とは、 ACL を設定する際に S3 で予め定義されたグループのことだそうです。公式ドキュメントには下記のように書かれていました。\nバケットの WRITE 許可により、このグループはサーバーアクセスログ (「Amazon S3 サーバーアクセスのログ記録」を参照) をバケットに書き込むことができます。\nアクセスコントロールリスト (ACL) の概要 - Amazon Simple Storage Service 2.6. S3バケット Webサイトホスティング設定の作成 S3 バケットに対して Static website hosting の機能を有効にするために、 s3api put-bucket-website コマンドを使いました。 --website-configuration オプションで Static website hosting の設定をしますが、ハンズオンでは下記のような設定をしました。\n{ \u0026#34;ErrorDocument\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;error.html\u0026#34; }, \u0026#34;IndexDocument\u0026#34;: { \u0026#34;Suffix\u0026#34;: \u0026#34;index.html\u0026#34; } } その他、詳細な設定としては下記のようなものがあるようです。\n$ aws s3api put-bucket-website \\ --generate-cli-skeleton yaml-input Bucket: \u0026#39;\u0026#39; # [REQUIRED] The bucket name. ContentMD5: \u0026#39;\u0026#39; # The base64-encoded 128-bit MD5 digest of the data. WebsiteConfiguration: # [REQUIRED] Container for the request. ErrorDocument: # The name of the error document for the website. Key: \u0026#39;\u0026#39; # [REQUIRED] The object key name to use when a 4XX class error occurs. IndexDocument: # The name of the index document for the website. Suffix: \u0026#39;\u0026#39; # [REQUIRED] A suffix that is appended to a request that is for a directory on the website endpoint (for example,if the suffix is index. RedirectAllRequestsTo: # The redirect behavior for every request to this bucket\u0026#39;s website endpoint. HostName: \u0026#39;\u0026#39; # [REQUIRED] Name of the host where requests are redirected. Protocol: https # Protocol to use when redirecting requests. Valid values are: http, https. RoutingRules: # Rules that define when a redirect is applied and the redirect behavior. - Condition: # A container for describing a condition that must be met for the specified redirect to apply. HttpErrorCodeReturnedEquals: \u0026#39;\u0026#39; # The HTTP error code when the redirect is applied. KeyPrefixEquals: \u0026#39;\u0026#39; # The object key name prefix when the redirect is applied. Redirect: # [REQUIRED] Container for redirect information. HostName: \u0026#39;\u0026#39; # The host name to use in the redirect request. HttpRedirectCode: \u0026#39;\u0026#39; # The HTTP redirect code to use on the response. Protocol: https # Protocol to use when redirecting requests. Valid values are: http, https. ReplaceKeyPrefixWith: \u0026#39;\u0026#39; # The object key prefix to use in the redirect request. ReplaceKeyWith: \u0026#39;\u0026#39; # The specific object key to use in the redirect request. ExpectedBucketOwner: \u0026#39;\u0026#39; # The account id of the expected bucket owner. ちなみにこの --generate-cli-skeleton yaml-input を使うと、上記のようにコメント付きで情報が出力されるようです。\n4. S3オブジェクトの確認 (ログの確認) 今回のハンズオン手順では、 s3://{S3_BUCKET_NAME}-log/Logs/ にアクセスログが出力されるはずでしたが、途中の手順をミスっていたためログの出力先がバケット直下になっていました。\n$ aws s3api get-bucket-logging \\ --bucket ${S3_BUCKET_NAME} { \u0026#34;LoggingEnabled\u0026#34;: { \u0026#34;TargetBucket\u0026#34;: \u0026#34;handson-cli-s3-website-logging-website-1234XXXXXXXX-log\u0026#34;, \u0026#34;TargetPrefix\u0026#34;: \u0026#34;/\u0026#34; } } 本当は TargetPrefix が Logs/ になっているはずでした。\nこれの何が辛かった (辛くはないけど気持ち悪かった) かというと、マネジメントコンソールで見たときに無名のディレクトリ (プレフィックス) が存在する形になってしまいます。\nCLI で確認するとこんな感じ。\n$ aws s3 ls s3://${S3_BUCKET_NAME}-log/ PRE / こうなった場合、 CLI では次のようにして確認します。\n$ aws s3 ls s3://${S3_BUCKET_NAME}-log// 2020-09-24 11:17:40 636 2020-09-24-11-17-39-D5F59C25B3841CAF 2020-09-24 11:17:43 2981 2020-09-24-11-17-42-83D41566B32CB6A7 2020-09-24 11:17:47 720 2020-09-24-11-17-46-2283C60C4B76D482 2020-09-24 11:17:50 656 2020-09-24-11-17-49-F938F6B946395166 2020-09-24 11:17:52 720 2020-09-24-11-17-51-70439B9BAD87B553 2020-09-24 11:18:12 720 2020-09-24-11-18-11-2FEBA44AFEB0C91D 2020-09-24 11:18:15 720 2020-09-24-11-18-14-A7A7F94850ED3EAD 2020-09-24 11:19:10 720 2020-09-24-11-19-09-56EBF4C2C6EA75BB 2020-09-24 11:19:36 1387 2020-09-24-11-19-35-0F6E11B37CB0113E S3 はオブジェクトストレージであってファイルシステムではないので // という指定もおかしくはないのですが、ちょっと気持ち悪いですね。\nCDK でやる場合との比較 今回はハンズオン後の LT で、同じような構成を CDK で構築した場合との比較についての話がありました。CDK は詳細な部分が隠蔽されて楽な反面、裏で動いている処理 (設定) については、 CLI を使ったほうが確認しやすいという話でした。CDK でサクッと作れるものでも、抽象化されている部分を CLI の操作で学んでおくのは大事だなと感じました。\nで、実は自分も前に CDK を使って同じような構成を作っていました。\n確かに CDK だと簡単に作れてしまう構成を CLI でポチポチやるのは時間がかかります。が、その分 API レベルで何をやっているのかがわかるので、今回のハンズオンも勉強になりました。\nCDK で AWS アカウント ID を扱う CDK と CLI との比較に関する LT 後の話の中で、 今回のように S3 を扱うハンズオンの場合、バケット名の衝突を防ぐために AWS アカウント ID をバケット名に組み込むと幸せになれる という話がありました。\n後述しますが、今回のハンズオンでは sts get-caller-identity から AWS アカウント ID を取得してバケット名に組み込んでいましたが、 CDK でも同じことができないか調べてみました。\n結論から言うと、できました。\n今回のように AWS アカウント ID をバケット名に含む S3 バケットを作成するには、次のようにします。\nimport * as cdk from \u0026#39;@aws-cdk/core\u0026#39;; import s3 = require(\u0026#39;@aws-cdk/aws-s3\u0026#39;); export class S3WithAwsIdStack extends cdk.Stack { constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); const bucketName: string = `sample-bucket-${this.account}`; const bucket: s3.Bucket = new s3.Bucket(this, \u0026#39;SampleBucketWithAccountID\u0026#39;, { bucketName, publicReadAccess: true, websiteIndexDocument: \u0026#39;index.html\u0026#39;, websiteErrorDocument: \u0026#39;error.html\u0026#39;, removalPolicy: cdk.RemovalPolicy.DESTROY }); } } これで生成される CloudFormation テンプレートは下記のようになります。\nResources: SampleBucketWithAccountID62145D3E: Type: AWS::S3::Bucket Properties: BucketName: Fn::Join: - \u0026#34;\u0026#34; - - sample-bucket- - Ref: AWS::AccountId WebsiteConfiguration: ErrorDocument: error.html IndexDocument: index.html UpdateReplacePolicy: Delete DeletionPolicy: Delete cdk deploy すれば sample-bucket-XXXXXXXXXXXX という形で、バケット名に AWS アカウント ID を含んだ S3 バケットが作成されます。\n知らんかったぞ、それ 今回に限らずハンズオンでは新しく知ることがたくさんあるので、この 知らんかったぞ、それ 項目では新しく知ったことを自分なりに深堀りしてみたいと思います。(恒例化したい)\nというのを前回のレポートでも書いたので、今回も一つ取り上げてみます。\nAWS アカウント ID の取得 今回のハンズオンでは、 S3 バケット名が衝突しないようにバケット名に AWS アカウント ID を含むようになっており、 AWS アカウント ID を取得するために下記のようなコマンドを実行しました。\n$ AWS_ID=$( \\ aws sts get-caller-identity \\ --query \u0026#39;Account\u0026#39; \\ --output text \\ ) \\ \u0026amp;\u0026amp; echo ${AWS_ID} 実行しているコマンドは sts get-caller-identity で、その出力結果から AWS アカウント ID を --query で抜き出しています。\nこの sts get-caller-identity コマンドでそもそも何だ？というのを調べてみました。といっても、 CLI のヘルプを確認です。\n$ aws sts get-caller-identity help ... NAME get-caller-identity - DESCRIPTION Returns details about the IAM user or role whose credentials are used to call the operation. NOTE: No permissions are required to perform this operation. If an admin- istrator adds a policy to your IAM user or role that explicitly denies access to the sts:GetCallerIdentity action, you can still perform this operation. Permissions are not required because the same information is returned when an IAM user or role is denied access. To view an example response, see I Am Not Authorized to Per- form: iam:DeleteVirtualMFADevice in the IAM User Guide . See also: AWS API Documentation See \u0026#39;aws help\u0026#39; for descriptions of global parameters. SYNOPSIS get-caller-identity [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] DESCRIPTION にもあるように、 CLI コマンドを実行する IAM ユーザー または IAM ロールの詳細を取得できるコマンドのようです。 --query で結果を絞らずに実行すると、結果は下記の通り。\n$ aws sts get-caller-identity { \u0026#34;UserId\u0026#34;: \u0026#34;AROAJC7BA46AREMWOJDPW:i-12345abcde8cXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;1234XXXXXXXX\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::1234XXXXXXXX:assumed-role/handson-cloud9-environment-role/i-12345abcde8cXXXXX\u0026#34; } Cloud 9 環境で実行したので、ユーザーは EC2 インスタンス、 Arn はアタッチされている IAM ロールのものが出力されています。\nちなみにこのコマンドに関してもクラメソさんのブログがヒットしました。\n記事によると、2016 年 当時の AWS CLI バージョンは 1.10.18 だったようです。\nまとめ JAWS-UG CLI 専門支部 #168R S3基礎 Webサイト\u0026amp;ログ の参加レポートでした。\n今回は S3 の Static website hosting 機能を使ったサイトの構築とログの確認ということで、以前に CDK で同じような構成を作っていた内容でした。CDK だと少ない記述でサクッと作れて楽だなーと思っていたので、それを CLI でやるとなるとちょっと面倒なのでは？と思っていました。実際にやってみるとそう思う部分もありましたが、やはり CDK で抽象化されている部分について CLI の操作で確認することができたので、より理解が深まった気がします。\n個人的には同じ構成を違う方法で構築してみると更に理解が深まるという感覚があるので、今回のハンズオンも大変勉強になりました。\nしばらくは S3 に関する内容が続くようなので、意外と知らない S3 の機能詳細について勉強していけるといいなと思います。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-168-s3-website-log/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #168R S3基礎 Webサイト\u0026ログ に参加しました #jawsug_cli"
  },
  {
    "contents": "今年のはじめ頃に登場した GitHub CLI のバージョン 1.0 がリリースされました。 issue とか PR の操作ができるみたいなので試してみます。\n目次 GitHub CLI 1.0 is now available 使ってみる インストール ヘルプ確認 ログイン Repository - repo Issue - issue Pull Request - pr まとめ GitHub CLI 1.0 is now available これまでの最新バージョンは 0.12.0 でしたが、先日 1.0.0 がリリースされました。\nGitHub CLI 1.0 is now available - The GitHub Blog Release GitHub CLI 1.0.0 · cli/cli 使ってみる では、早速使ってみます。\n今回は、インストールから実際のコマンド実行 (Repository 、 Issue 、 Pull Request に関する操作) までやってみます。\nインストール macOS の場合、 Homebrew または MacPorts でインストールできます。今回は Homebrew でインストールします。\n$ brew install gh ... ==\u0026gt; Summary 🍺 /usr/local/Cellar/gh/1.0.0: 60 files, 16.2MB バージョンを確認します。\n$ gh --version gh version 1.0.0 (2020-09-16) https://github.com/cli/cli/releases/tag/v1.0.0 ヘルプ確認 --help オプションでヘルプを確認できます。\n$ gh --help Work seamlessly with GitHub from the command line. USAGE gh \u0026lt;command\u0026gt; \u0026lt;subcommand\u0026gt; [flags] CORE COMMANDS gist: Create gists issue: Manage issues pr: Manage pull requests release: Manage GitHub releases repo: Create, clone, fork, and view repositories ADDITIONAL COMMANDS alias: Create command shortcuts api: Make an authenticated GitHub API request auth: Login, logout, and refresh your authentication completion: Generate shell completion scripts config: Manage configuration for gh help: Help about any command FLAGS --help Show help for command --version Show gh version EXAMPLES $ gh issue create $ gh repo clone cli/cli $ gh pr checkout 321 ENVIRONMENT VARIABLES See \u0026#39;gh help environment\u0026#39; for the list of supported environment variables. LEARN MORE Use \u0026#39;gh \u0026lt;command\u0026gt; \u0026lt;subcommand\u0026gt; --help\u0026#39; for more information about a command. Read the manual at https://cli.github.com/manual FEEDBACK Open an issue using \u0026#39;gh issue create -R cli/cli\u0026#39; 各コマンドについても、 --help オプションで詳細を確認できます。例えば issue に関するコマンドのヘルプは次のような内容になってます。\n$ gh issue --help Work with GitHub issues USAGE gh issue \u0026lt;command\u0026gt; [flags] CORE COMMANDS close: Close issue create: Create a new issue list: List and filter issues in this repository reopen: Reopen issue status: Show status of relevant issues view: View an issue FLAGS -R, --repo OWNER/REPO Select another repository using the OWNER/REPO format INHERITED FLAGS --help Show help for command ARGUMENTS An issue can be supplied as argument in any of the following formats: - by number, e.g. \u0026#34;123\u0026#34;; or - by URL, e.g. \u0026#34;https://github.com/OWNER/REPO/issues/123\u0026#34;. EXAMPLES $ gh issue list $ gh issue create --label bug $ gh issue view --web LEARN MORE Use \u0026#39;gh \u0026lt;command\u0026gt; \u0026lt;subcommand\u0026gt; --help\u0026#39; for more information about a command. Read the manual at https://cli.github.com/manual ログイン 各コマンド (サブコマンド) を実行するためには GitHub アカウントでログインする必要があります。ログインには gh auth login コマンドを実行し、対話形式でログインを進めていきます。\nアカウントの種類 GitHub.com または GitHub Enterprise を選択します。今回は GitHub.com アカウントでログインします。\n? What account do you want to log into? [Use arrows to move, type to filter] \u0026gt; GitHub.com GitHub Enterprise Server ログインの方法 ログイン方法として、ブラウザを使用するか、認証トークンを入力するかを選択します。今回はブラウザを使います。\n? How would you like to authenticate? [Use arrows to move, type to filter] \u0026gt; Login with a web browser Paste an authentication token ブラウザを選択すると、ワンタイムコードが表示されるのでコピーします。 Enter キーを押すとブラウザでコード入力画面が表示されます。\n! First copy your one-time code: EFD5-CC5F - Press Enter to open github.com in your browser... コードを入力すると認証画面が表示されるので、認証します。\nデフォルトプロトコル リポジトリにアクセスする際のデフォルトプロトコルを選択します。今回は SSH を選択します。\n? Choose default git protocol [Use arrows to move, type to filter] HTTPS \u0026gt; SSH 以上でログインは完了です。\n✓ Logged in as michimani 試しにサブコマンドのヘルプを確認してみます。\n$ gh issue create --help Create a new issue USAGE gh issue create [flags] FLAGS -a, --assignee login Assign people by their login -b, --body string Supply a body. Will prompt for one otherwise. -l, --label name Add labels by name -m, --milestone name Add the issue to a milestone by name -p, --project name Add the issue to projects by name -t, --title string Supply a title. Will prompt for one otherwise. -w, --web Open the browser to create an issue INHERITED FLAGS --help Show help for command -R, --repo OWNER/REPO Select another repository using the OWNER/REPO format EXAMPLES $ gh issue create --title \u0026#34;I found a bug\u0026#34; --body \u0026#34;Nothing works\u0026#34; $ gh issue create --label \u0026#34;bug,help wanted\u0026#34; $ gh issue create --label bug --label \u0026#34;help wanted\u0026#34; $ gh issue create --assignee monalisa,hubot $ gh issue create --project \u0026#34;Roadmap\u0026#34; LEARN MORE Use \u0026#39;gh \u0026lt;command\u0026gt; \u0026lt;subcommand\u0026gt; --help\u0026#39; for more information about a command. Read the manual at https://cli.github.com/manual Repository - repo repo コマンドではリポジトリに関する操作を実行できます。\n$ gh repo --help Work with GitHub repositories USAGE gh repo \u0026lt;command\u0026gt; [flags] CORE COMMANDS clone: Clone a repository locally create: Create a new repository fork: Create a fork of a repository view: View a repository create まずは create サブコマンドでリポジトリを作成してみます。\n$ gh repo create sandbox-repo \\ --description \u0026#34;This is a sandbox repository.\u0026#34; \\ --public ? This will create \u0026#39;sandbox-repo\u0026#39; in your current directory. Continue? Yes ✓ Created repository michimani/sandbox-repo on GitHub ? Create a local project directory for michimani/sandbox-repo? No 実行すると、本当に作ってもよいか確認されるので Y で続けます。\n次に、ローカルにプロジェクトディレクトリを作るか聞かれます。今回は作りません。 (あとで clone を試すので)\nclone 動作は git clone と同じです。\n$ gh repo clone michimani/sandbox-repo Cloning into \u0026#39;sandbox-repo\u0026#39;... warning: You appear to have cloned an empty repository. $ cd sandbox-repo $ git remote -v origin\tgit@github.com:michimani/sandbox-repo.git (fetch) origin\tgit@github.com:michimani/sandbox-repo.git (push) ログイン時にデフォルトプロトコルに SSH を選択したので、リモートリポジトリの URL も SSH のものになっています。\nclone したら、とりあえず README.md だけ作成して push しておきます。\nview view サブコマンドでは、指定したリポジトリの説明、 README の内容を確認できます。\n$ gh repo view michimani/sandbox-repo michimani/sandbox-repo This is a sandbox repository. sandbox-repo This is a sandbox. View this repository on GitHub: https://github.com/michimani/sandbox-repo -w または --web オプションを付けると、リポジトリのページをブラウザで開きます。\nIssue - issue issue コマンドでは Issue に関する操作を実行できます。\n$ gh issue --help Work with GitHub issues USAGE gh issue \u0026lt;command\u0026gt; [flags] CORE COMMANDS close: Close issue create: Create a new issue list: List and filter issues in this repository reopen: Reopen issue status: Show status of relevant issues view: View an issue create まずは create サブコマンドで issue を作成します。\n$ gh issue create \\ --title \u0026#34;The first issue\u0026#34; \\ --body \u0026#34;This is the first issue\u0026#34; Creating issue in michimani/sandbox-repo https://github.com/michimani/sandbox-repo/issues/1 ちなみに、プロジェクトディレクトリ以外で上記のコマンドを実行すると fatal: not a git repository (or any of the parent directories): .git というエラーになります。\nlist list サブコマンドで issue の一覧を表示します。\n$ gh issue list Showing 1 of 1 open issue in michimani/sandbox-repo #1 The first issue about 2 minutes ago 下記のオプションでフィルタリングできます。\nオプション フィルタ対象 -a, \u0026ndash;assignee string Filter by assignee -A, \u0026ndash;author string Filter by author -l, \u0026ndash;label strings Filter by labels \u0026ndash;mention string Filter by mention -m, \u0026ndash;milestone number Filter by milestone number or title -s, \u0026ndash;state string Filter by state: {open|closed|all} (default \u0026ldquo;open\u0026rdquo;) view view サブコマンドで issue の詳細を表示します。\n$ gh issue view 1 The first issue Open • michimani opened about 8 minutes ago • 0 comments This is the first issue View this issue on GitHub: https://github.com/michimani/sandbox-repo/issues/1 表示されるのは issue のタイトルと説明です。\n-w または --web オプションを付けるとブラウザで issue のページを開きます。\nstatus status サブコマンドでは、リポジトリの 自分にアサインされた issue 、 自分にメンションされた issue 、 オープンな issue を表示します。\n$ gh issue status Relevant issues in michimani/sandbox-repo Issues assigned to you #2 The second issue less than a minute ago Issues mentioning you There are no issues mentioning you Issues opened by you #2 The second issue less than a minute ago #1 The first issue about 6 minutes ago close close サブコマンドで issue を close します。\n$ gh issue close 1 ✔ Closed issue #1 (The first issue) close された issue の一覧を確認してみます。\n$ gh issue list --state closed Showing 1 of 1 issue in michimani/sandbox-repo that matches your search #1 The first issue about 1 minute ago Pull Request - pr pr コマンドでは Pull Request に関する操作を実行できます。\n$ gh pr --help Work with GitHub pull requests USAGE gh pr \u0026lt;command\u0026gt; [flags] CORE COMMANDS checkout: Check out a pull request in git checks: Show CI status for a single pull request close: Close a pull request create: Create a pull request diff: View changes in a pull request list: List and filter pull requests in this repository merge: Merge a pull request ready: Mark a pull request as ready for review reopen: Reopen a pull request review: Add a review to a pull request status: Show status of relevant pull requests view: View a pull request create まずは create サブコマンドで Pull Request を作成します。\n$ gh pr create \\ --base master \\ --head sample-pr \\ --title \u0026#34;The first PR\u0026#34; \\ --body \u0026#34;This is the first pull request.\u0026#34; Creating pull request for sample-pr into master in michimani/sandbox-repo https://github.com/michimani/sandbox-repo/pull/3 --head で指定するブランチは、デフォルトで現在の作業ブランチが指定されるので省略可能です。\nlist list サブコマンドで Pull Request の一覧を表示します。\n$ gh pr list Showing 1 of 1 open pull request in michimani/sandbox-repo #3 The first PR sample-pr 下記のオプションでフィルタリングできます。\nオプション フィルタ対象 -a, \u0026ndash;assignee string Filter by assignee -B, \u0026ndash;base string Filter by base branch -l, \u0026ndash;label strings Filter by labels -s, \u0026ndash;state string Filter by state: {open|closed|merged|all} (default \u0026ldquo;open\u0026rdquo;) diff diff サブコマンドで Pull Request の差分を表示します。\n$ gh pr diff 3 diff --git a/README.md b/README.md index 759b875..e696948 100644 --- a/README.md +++ b/README.md @@ -3,3 +3,5 @@ sandbox-repo This is a sandbox. +1. hoge + review review サブコマンドでは、 Pull Request に対してレビュー (コメント) をすることができます。\n$ gh pr review 3 \\ --body \u0026#34;This is a review comment\u0026#34; \\ --comment - Reviewed pull request #3 --comment の代わりに --approve または --request-changes を指定すれば、変更要求、承認をすることができます。自分自身が作成した Pull Request に対しては --comment のみ使えます。\n$ gh pr review 3 \\ --body \u0026#34;This is a approve comment\u0026#34; \\ --approve failed to create review: Can not approve your own pull request merge merge サブコマンドで Pull Request をマージします。\n$ gh pr merge 3 \\ --squash ✔ Squashed and merged pull request #3 (The first PR) ✔ Deleted branch sample-pr and switched to branch master マージの方法はそれぞれ下記のコマンドで指定します。\nオプション マージ方法 -m, \u0026ndash;merge Merge the commits with the base branch -r, \u0026ndash;rebase Rebase the commits onto the base branch -s, \u0026ndash;squash Squash the commits into one commit and merge it into the base branch マージ元のブランチはマージ後に削除されるので、もし削除したくない場合は --delete-branch=false を指定します。\nview view サブコマンドでは Pull Request の概要 (タイトル、説明、ステータス) を確認できます。\n$ gh pr view 3 The first PR Merged • michimani wants to merge 1 commit into master from sample-pr This is the first pull request. View this pull request on GitHub: https://github.com/michimani/sandbox-repo/pull/3 まとめ GitHub CLI のバージョン 1.0 がリリースされたので、普段から使いそうなコマンドについて試してみた話でした。\n各コマンド、サブコマンドについては --help オプションで詳細を確認できます。コマンドやオプションもシンプルなので、コマンドやオプションの詳細に関してはヘルプを見ればほぼほぼわかります。\nローカルで作業しているディレクトリにある .git を元にしてコマンド実行対象のリポジトリが判定されるので、ローカルから作業ブランチを Push 、そのままローカルのターミナルから Pull Request の作成までできるのは便利だなと思いました。\nAWS と違って GitHub は GUI が頻繁に変わるようなことはないですが、 CLI で操作できればだいぶ作業を効率化できそうです。\n",
    "permalink": "https://michimani.net/post/development-get-started-to-use-github-cli/",
    "title": "GitHub CLI 1.0 がリリースされたので一通り触ってみる"
  },
  {
    "contents": "これまで色んなイベントで紹介されてきた AWS Amplify の機能である Amplify Console を、重い腰を上げて試してみることにしました。先日開催された 24時間の技術イベント JAWS SONIC 2020 でもいくつかのセッションで取り上げられていたので、それも後押しになっています。\nイベントページはこちら。\nJAWS SONIC 2020 \u0026amp;amp; MIDNIGHT JAWS 2020 目次 JAWS SONIC 2020 での関連セッション やること Hugo サイトを作成 Amplify Console でアプリ作成 アプリの作成 失敗しました Hugo に記事を追加して再ビルド カスタムドメイン設定 プルリクエスト環境の有効化 まとめ JAWS SONIC 2020 での関連セッション JAWS SONIC 2020 では下記のセッションで Amplify Console について触れられていました。(私が視聴していた分に限ります🙇🏻‍♂️)\n[JAWS-UG 新潟支部][9/12(土) 21:00 ～ 21:20]AWSの研修環境構築のためにAWS CDKとAmplify Console使った話 │ JAWS SONIC 2020 \u0026amp;amp; MIDNIGHT JAWS 2020 [JAWS-UG名古屋][9/13(日) 16:40 ～ 17:00]AWSサービスでJAMStackの構築をしてみた │ JAWS SONIC 2020 \u0026amp;amp; MIDNIGHT JAWS 2020 やること 今回は、 Amplify Console を使って Hugo で作ったサイトをデプロイしてみます。また、普通にデプロイするだけでなく、 カスタムドメイン設定 と プルリクエストのプレビュー も試してみます。\nHugo サイトを作成 まずは Hugo でサイトを作ります。基本的には Hugo 公式のクイックスタート通りです。\nQuick Start | Hugo Hugo は現時点で最新の 0.75.1 を使います。\n$ hugo version Hugo Static Site Generator v0.75.1/extended darwin/amd64 BuildDate: unknown 今回は hugo-amplify-console という名前のサイトを作成します。\n$ hugo new site hugo-amplify-console $ cd hugo-amplify-console Hugo のテーマは、チュートリアルと同じ ananke を使います。\n$ git init $ git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke $ echo \u0026#39;\\ntheme = \u0026#34;ananke\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml これで最低限のサイトは完成したので、 GitHub のリポジトリに Push しておきます。\n$ git add . $ git commit -m \u0026#34;initial commit\u0026#34; $ git remote add origin git@github.com:michimani/hugo-amplify-console.git $ git push origin master:master Amplify Console でアプリ作成 今回はマネジメントコンソールでポチポチやっていきます。\nアプリの作成 アプリの作成 ボタンをクリックして作成手順を進めます。\nステップ 1 : リポジトリの設定 まず最初に接続する Git リポジトリを選択します。今回は GitHub なので、 GitHub を選択して次へ。\nGitHub と Amplify の連携認証画面に遷移するので、認証して続けます。\nGitHub との認証が成功したら、接続する対象のリポジトリとブランチを選択します。今回は michimani/hugo-amplify-console と接続し、対象のブランチは master とします。\nステップ 2 : ビルド設定 次のステップはビルド設定の構成で、 ビルド設定 (ビルドするためのコマンド等) を記述します。 CodeBuild の buildspec.yml みたいな感じです。\nここで凄いなと思ったのが、ビルド設定は接続したリポジトリの内容から自動検出されるという点です。今回であれば Hugo のサイトなので、ビルドには hugo コマンドを実行し、成果物は public ディレクトリに生成されます。\n自動検出されたビルド設定はそれらを考慮した内容になっており、特に変更する必要もなくそのまま次へ進みます。\nステップ 3 : 確認 最後に内容を確認して、アプリを保存します。\n保存後は初回のデプロイが始まるので、見守ります。\n失敗しました 初回のデプロイは失敗しました。\nメッセージを見ると、どうやら記事が 1 つもないことでエラーになっているようです。とはいえ、順番にポチポチしていくだけでアプリの設定ができたのは、なんというか、あっけないですね。\nHugo に記事を追加して再ビルド エラーを解消するべく、 Hugo に記事を追加してみます。\n$ hugo new posts/initial-post.md draft は false に変更します。\n--- title: \u0026#34;Initial Post\u0026#34; date: 2020-09-17T07:20:57+09:00 - draft: true + draft: false --- 作成した記事を Push すると再度ビルドが実行され、今回は無事にデプロイが完了しました。\nデプロイ後の URL にアクセスして確認してみます。\nめちゃくちゃ簡単ですね。\nカスタムドメイン設定 デフォルトで生成される URL は https://{ブランチ名}.{ハッシュ}.amplify.com という形式になっているので、カスタムドメインを設定してみます。\nAmplify Console のアプリ画面には次のキャプチャのようなリストが表示されているので、この中から Add a custom domain with a free SSL certificate をクリックしてカスタムドメインの設定を進めます。\n右上の カスタムドメインの追加 をクリックしてドメイン追加に進みます。\n今回は既に Route 53 で管理しているドメインを使います。ドメインの入力フォームにフォーカスを当てると Route 53 で管理しているドメインが候補に出てくるので、選択して ドメインを検索 をクリックします。\nmichimani.net は既にこのブログで使っているドメインなので、 Exclude root をクリックして無効化しておきます。そして、今回は hugo-amplify.michimani.net というサブドメインを設定します。\nちなみに、 追加 をクリックしてドメイン設定を追加することができるので、例えば develop ブランチに dev.*** のサブドメインを設定することもできます。\n設定を保存すると、ドメイン管理画面に設定の進行状況が表示されます。\n設定の順番としては、\nSSL creation SSL configuration Domain activation となっており、最後の 3. Domain activation までは約 5 分くらいで完了しました。 3. Domain activation フェーズに移行すると Status details に下記のようなメッセージが表示されます。\nWe verified domain ownership. We are propagating your custom domain to our global content delivery network which could take up to 30 minutes.\nドメインの有効化には最大 30 分かかると書かれていますが、30 分経たずとも設定したドメインでアクセスできることが確認できました。\nめちゃくちゃ簡単ですね。\nプルリクエスト環境の有効化 今回の最後は、プルリクエストが作成された際にプルリクエストの内容でデプロイをするという設定です。\nカスタムドメインのときと同様に、アプリ画面のトップに表示された Enable pull request previews から設定を進めます。\nGitHub app をインストールするようダイアログが表示されるので、 Install GitHub app をクリックして進みます。\n別タブで GitHub 側の設定画面が開くので、諸々設定をしてマネジメントコンソール側に戻ります。すると、 Previews が表示されているので、対象のブランチを選択して Preview Stratus を Enabled に変更します。\nプルリクエストを作成してみる では、早速 master へのプルリクエストを作成してみます。変更内容は、先ほど作成した記事のタイトル変更です。\n--- - title: \u0026#34;Initial Post\u0026#34; + title: \u0026#34;Initial Post - Pull Request\u0026#34; date: 2020-09-17T07:20:57+09:00 draft: false --- PR を作成すると、キャプチャ画像のように GitHub 側でも Amplify Console のステータスが表示されます。\nチェックが完了すると、マネジメントコンソール側でも Previews に PR の内容が Preview URL とともに表示されています。\nアクセスすると、 PR の内容を確認することができます。\nめちゃくちゃ簡単ですね。\nまとめ Amplify Console を使って Hugo で作ったサイトをデプロイしてみた話でした。\nAmplify に関しては AWS のハンズオンで Amplify Framework の方だけは触ったことがありました (もしかしたら Amplify Console にも触れていたかも\u0026hellip;)。ただ、手順を追うので精いっぱいで、それ以降全く触れていませんでした。\n先日の JAWS SONIC 2020 でも Amplify Console に触れられていたセッションがいくつかあったので、今回 重い腰を上げて試してみたわけです。感想としては、諸々のことがめちゃくちゃ簡単でビックリ、です。\nこのブログも Hugo で作っており、デプロイは CodeBuild で、ドメインは Route 53 と ACM を使ってポチポチと設定しました。それに比べると Amplify Console ではそれぞれの設定を数ステップで完了することができるので、これだけでいいの？って思ってしまうくらいでした。\nAmplify と一口に言っても Amplify Framework と Amplify Console は別物なので、 Amplify Console だけサクッと使ってみるのもいいな〜と思いました。\n",
    "permalink": "https://michimani.net/post/aws-deploy-hugo-using-amplify-console/",
    "title": "Amplify Console を使って Hugo で作ったサイトをデプロイする #jawssonic2020"
  },
  {
    "contents": "先日開催された 24時間の技術イベント JAWS SONIC 2020 のセッション内で初めて知った Moto を試してみました。\nイベントページはこちら。\n│JAWS SONIC 2020 \u0026amp;amp; MIDNIGHT JAWS 2020 イベントは 24 時間開催ということで、 Twitter でも #jawssonic2020 のハッシュタグで大盛りあがりでした。運営のみなさま、登壇されたみなさま、本当にお疲れさまでした ＆ ありがとうございました！\n目次 JAWS-UG 長野支部のセッション Moto とは 使ってみる 対象のプロジェクト テストコード まとめ JAWS-UG 長野支部のセッション 今回試す Moto に関するセッションは、 JAWS-UG 長野支部の知野さん、寺田さん、春原さんによるセッション 「AWSをMotoでmockしてユニットテスト！」 でした。登壇資料は Speakerdeck で公開されています。\nMoto とは Moto とは、 AWS の各サービスをモックできる Python のライブラリです。 Moto を使うことで AWS の各サービスを操作する Python のプロジェクトで、 AWS にアクセスする部分のテストを簡単に実装することができます。\n使ってみる セッション内ではサーバーレス構成でのテストということで、 S3、 SQS、 SSM Parameter Store に関する例が紹介されていまいた。今回は、 EC2 に対する操作に関するテストを実装してみます。\n対象のプロジェクト 今回 対象にするプロジェクトは、だいぶ前に作った下記の CDK プロジェクトです。\n特定のタグを持つ EC2 インスタンスを自動起動・停止させる Lambda 関数と EventBridge のルールをデプロイする CDK プロジェクトです。今回はこのプロジェクト内の Lambda 関数を、 Moto を使ってテストしてみます。また、テストコードの実行には pytest を使います。\nこのアプリケーションの詳細については下記の記事で書いてます。\nテストコード では早速 実装したテストコードを見てみます。\nimport boto3 import importlib from moto import mock_ec2 MOCK_REGION = \u0026#39;ap-northeast-1\u0026#39; MOCK_AMI_ID = \u0026#39;/aws/service/ami-amazon-linux-latest/amzn-ami-hvm-x86_64-ebs\u0026#39; MOCK_INSTANCE_TYPE = \u0026#39;t3.nano\u0026#39; MOCK_INSTANCE_NAME = \u0026#39;mock_instance\u0026#39; @mock_ec2 class TestStartStopEc2: assec2 = importlib.import_module(\u0026#39;lambda.auto-start-stop-ec2\u0026#39;) ec2_client = None mock_instance = {} def setup_method(self, method): self.ec2_client = boto3.client(\u0026#39;ec2\u0026#39;, region_name=MOCK_REGION) res = self.ec2_client.run_instances(ImageId=MOCK_AMI_ID, InstanceType=MOCK_INSTANCE_TYPE, MinCount=1, MaxCount=1, TagSpecifications=[ { \u0026#39;ResourceType\u0026#39;: \u0026#39;instance\u0026#39;, \u0026#39;Tags\u0026#39;: [ { \u0026#39;Key\u0026#39;: \u0026#39;AutoStartStop\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;TRUE\u0026#39; }, { \u0026#39;Key\u0026#39;: \u0026#39;Name\u0026#39;, \u0026#39;Value\u0026#39;: MOCK_INSTANCE_NAME } ]} ]) self.mock_instance = { \u0026#39;instance_id\u0026#39;: res[\u0026#39;Instances\u0026#39;][0][\u0026#39;InstanceId\u0026#39;], \u0026#39;instance_name\u0026#39;: MOCK_INSTANCE_NAME } def teardown_method(self, method): self.ec2_client.terminate_instances( InstanceIds=[self.mock_instance[\u0026#39;instance_id\u0026#39;]]) def test_stop_ec2_instance(self): res = self.assec2.stop_instance(self.ec2_client, self.mock_instance) assert res is True def test_start_ec2_instance(self): res = self.assec2.start_instance(self.ec2_client, self.mock_instance) assert res is True Moto でモックするにはデコレータを付与するだけ Moto では、モックしたいサービスを操作する部分 (関数、クラス) に対してデコレータをつけることで、そのサービスをモックすることができます。今回はテストコードのクラス TestStartStopEc2 に対して EC2 をモックするための @mock_ec2 デコレータをつけます。\nsetup/teardown でモック用インスタンスの起動と削除 クラス内の setup_method() および teardown_method() は、それぞれテストメソッドの実行前後に実行されるメソッドです。なので、 setup_method() 内でモック用の EC2 インスタンスを起動し、 teardown_method() 内でそのインスタンスを削除 (terminate) しています。\nsetup_method() ではインスタンスを起動したあと、起動したインスタンスの InstanceId と Instance Name (Tags の Name の値) をクラス変数に保持し、後続のテストメソッド、および teardown_method() で使用できるようにしています。\n実行 テストの実行は pytest を使用します。\n$ python -m pytest ./lambda/test/test_start_stop_ec2.py -v ============================================== test session starts =============================================== platform darwin -- Python 3.8.5, pytest-6.0.2, py-1.9.0, pluggy-0.13.1 -- .venv/bin/python cachedir: .pytest_cache rootdir: /path/to/Projects/auto-start-stop-ec2 collected 4 items lambda/test/test_start_stop_ec2.py::TestStartStopEc2::test_describe_instances PASSED [ 25%] lambda/test/test_start_stop_ec2.py::TestStartStopEc2::test_get_target_ec2_instances PASSED [ 50%] lambda/test/test_start_stop_ec2.py::TestStartStopEc2::test_stop_ec2_instance PASSED [ 75%] lambda/test/test_start_stop_ec2.py::TestStartStopEc2::test_start_ec2_instance PASSED [100%] テストの実行もできました。\nまとめ AWS の各サービスをモックできる Python のライブラリ Moto を使って CDK プロジェクト内の Lambda 関数をテストしてみた話でした。\nデコレータをつけるだけで AWS の各サービスをモックできるのは、非常に簡単で便利だなと感じました。今後も Python でアプリケーションを実装する際には、 Moto の利用を考えたいと思います。\nJAWS SONIC 2020 のセッション内では他にも初めて知る内容や試してみたい内容があったので、記憶が新しいうちに試していこうと思います。\n",
    "permalink": "https://michimani.net/post/aws-test-lambda-func-using-moto/",
    "title": "Moto で CDK プロジェクト内の Lambda 関数をテストする #jawssonic2020"
  },
  {
    "contents": "JAWS-UG CLI 専門支部 #167R EventBridge入門 に参加したので、その参加レポートです。\nconnpass のイベントページはこちら。\n目次 Amazon EventBridge の概要 EventBridge EventBridge スキーマレジストリ ハンズオン 各手順で使用したコマンド 知らんかったぞ、それ Python 3 の json.tool --query オプション内での max_by まとめ Amazon EventBridge の概要 CloudWatch から分離 大きく分けて２つの API events EventBridge (旧 CLoudWatch Events) schemas EventBridge スキーマレジストリ データ構造の確認 ユースケース イベントの発生元 (他の AWS アカウントも含む) からのイベントを検知 AWS リソース (他の AWS アカウントも含む) にイベントを通知 イベントの構造化データ (スキーマ) を参照できるのが EventBridge スキーマレジストリ EventBridge イベントソース AWS リソースとしては約 90 のサービスに対応 他の AWS アカウントからのイベントには パーミッション の設定が必要 SaaS ベンダー (Auth0 、 Mackerel 、 Zendesk など) からのイベントでは、イベントバス内に パートナーイベントソース が作成される Amazon EventBridge の統合 パートナー企業だけが使える CLI コマンドがある 止めたり再開したりが可能 イベントターゲット 約 20 の AWS リソースに対応 イベント フォーマットが決まっている time 、 source 、 resource 、 detail-type 、 detail 、 event-bus-name AWS イベント - Amazon EventBridge AWS リソースからのイベントでは CloudTrail 経由でないと受け取れないイベントがある e.g) S3 のオブジェクト単位のイベント (バケット単位のイベントは直接通知できる) イベントバス デフォルトイベントバス 各アカウントに一つ (意識せず動いている) カスタムイベントバス アカウントあたり 100 個作成可能 イベントバスの作成 - Amazon EventBridge ルール イベントバス内に作成 一つのルールに登録できるターゲットは 5 つ ターゲット ターゲット ID 、 ターゲット ARN 、 IAM ロール その他、 EventBridge の制限 Amazon EventBridge のクォータ - Amazon EventBridge EventBridge スキーマレジストリ レジストリ スキーマの保存 リソースポリシー (S3 のバケットポリシー的なもの) スキーマ OpenAPI 3.0 形式 ディスカバラー スキーマの検索 ハンズオン 今回のハンズオンでは、 EBS のイベントを検知して、 Lambda 経由で CloudWatch Logs にイベントを書き込むという内容でした。詳細についてはイベントページに記載されている手順におまかせするとして、使ったコマンドとその出力について簡単に触れておきます。\n各手順で使用したコマンド EventBridge Registru のスキーマ確認 schemas describe-schema コマンドを使います。\n$ aws schemas describe-schema help ... SYNOPSIS describe-schema --registry-name \u0026lt;value\u0026gt; --schema-name \u0026lt;value\u0026gt; [--schema-version \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] 今回は EBS をイベントソースとするので、その際に発行されるイベントのスキーマを確認しました。\n$ aws schemas describe-schema \\ \u0026gt; --registry-name aws.events \\ \u0026gt; --schema-name aws.ec2@EBSVolumeNotification { \u0026#34;Content\u0026#34;: \u0026#34;{ \\\u0026#34;openapi\\\u0026#34; : \\\u0026#34;3.0.0\\\u0026#34;, \\\u0026#34;info\\\u0026#34; : { \\\u0026#34;version\\\u0026#34; : \\\u0026#34;1.0.0\\\u0026#34;, \\\u0026#34;title\\\u0026#34; : \\\u0026#34;EBSVolumeNotification\\\u0026#34; }, \\\u0026#34;paths\\\u0026#34; : { }, \\\u0026#34;components\\\u0026#34; : { \\\u0026#34;schemas\\\u0026#34; : { \\\u0026#34;AWSEvent\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;object\\\u0026#34;, \\\u0026#34;required\\\u0026#34; : [ \\\u0026#34;detail-type\\\u0026#34;, \\\u0026#34;resources\\\u0026#34;, \\\u0026#34;id\\\u0026#34;, \\\u0026#34;source\\\u0026#34;, \\\u0026#34;time\\\u0026#34;, \\\u0026#34;detail\\\u0026#34;, \\\u0026#34;region\\\u0026#34;, \\\u0026#34;version\\\u0026#34;, \\\u0026#34;account\\\u0026#34; ], \\\u0026#34;x-amazon-events-detail-type\\\u0026#34; : \\\u0026#34;EBS Volume Notification\\\u0026#34;, \\\u0026#34;x-amazon-events-source\\\u0026#34; : \\\u0026#34;aws.ec2\\\u0026#34;, \\\u0026#34;properties\\\u0026#34; : { \\\u0026#34;detail\\\u0026#34; : { \\\u0026#34;$ref\\\u0026#34; : \\\u0026#34;#/components/schemas/EBSVolumeNotification\\\u0026#34; }, \\\u0026#34;detail-type\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;resources\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;array\\\u0026#34;, \\\u0026#34;items\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; } }, \\\u0026#34;id\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;source\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;time\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34;, \\\u0026#34;format\\\u0026#34; : \\\u0026#34;date-time\\\u0026#34; }, \\\u0026#34;region\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;version\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;account\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; } } }, \\\u0026#34;EBSVolumeNotification\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;object\\\u0026#34;, \\\u0026#34;required\\\u0026#34; : [ \\\u0026#34;result\\\u0026#34;, \\\u0026#34;cause\\\u0026#34;, \\\u0026#34;event\\\u0026#34;, \\\u0026#34;request-id\\\u0026#34; ], \\\u0026#34;properties\\\u0026#34; : { \\\u0026#34;result\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;cause\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;event\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; }, \\\u0026#34;request-id\\\u0026#34; : { \\\u0026#34;type\\\u0026#34; : \\\u0026#34;string\\\u0026#34; } } } } } }\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Schema for event type EBSVolumeNotification, published by AWS service aws.ec2\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2019-12-01T00:31:47+00:00\u0026#34;, \u0026#34;SchemaArn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;SchemaName\u0026#34;: \u0026#34;aws.ec2@EBSVolumeNotification\u0026#34;, \u0026#34;SchemaVersion\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Tags\u0026#34;: {}, \u0026#34;Type\u0026#34;: \u0026#34;OpenApi3\u0026#34;, \u0026#34;VersionCreatedDate\u0026#34;: \u0026#34;2019-12-01T00:31:47+00:00\u0026#34; } Content の内容がイベントのスキーマで、整形すると下記のような内容になっています。\n{ \u0026#34;openapi\u0026#34;: \u0026#34;3.0.0\u0026#34;, \u0026#34;info\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;EBSVolumeNotification\u0026#34; }, \u0026#34;paths\u0026#34;: {}, \u0026#34;components\u0026#34;: { \u0026#34;schemas\u0026#34;: { \u0026#34;AWSEvent\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [ \u0026#34;detail-type\u0026#34;, \u0026#34;resources\u0026#34;, \u0026#34;id\u0026#34;, \u0026#34;source\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;detail\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;account\u0026#34; ], \u0026#34;x-amazon-events-detail-type\u0026#34;: \u0026#34;EBS Volume Notification\u0026#34;, \u0026#34;x-amazon-events-source\u0026#34;: \u0026#34;aws.ec2\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;detail\u0026#34;: { \u0026#34;$ref\u0026#34;: \u0026#34;#/components/schemas/EBSVolumeNotification\u0026#34; }, \u0026#34;detail-type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;resources\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;time\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;date-time\u0026#34; }, \u0026#34;region\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;version\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;account\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } }, \u0026#34;EBSVolumeNotification\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [ \u0026#34;result\u0026#34;, \u0026#34;cause\u0026#34;, \u0026#34;event\u0026#34;, \u0026#34;request-id\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;result\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;cause\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;event\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;request-id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } } } } ちなみに、 aws.events レジストリに含まれるスキーマの一覧は下記のコマンドで確認できます。\n$ aws schemas list-schemas \\ --registry-name aws.events \\ --query \u0026#34;Schemas[].SchemaName\u0026#34; EventBridge ルールの作成 events put-rule コマンドを使用します。\n$ aws events put-rule help ... SYNOPSIS put-rule --name \u0026lt;value\u0026gt; [--schedule-expression \u0026lt;value\u0026gt;] [--event-pattern \u0026lt;value\u0026gt;] [--state \u0026lt;value\u0026gt;] [--description \u0026lt;value\u0026gt;] [--role-arn \u0026lt;value\u0026gt;] [--tags \u0026lt;value\u0026gt;] [--event-bus-name \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] 今回は EBS の通知をイベントソースとするので、イベントパターンを {\u0026quot;source\u0026quot;:[\u0026quot;aws.ec2\u0026quot;],\u0026quot;detail-type\u0026quot;:[\u0026quot;EBS Volume Notification\u0026quot;]} とします。ここで指定する source および detail-type に関しては、先ほど取得したスキーマ内の Content の内容から確認できます。\n$ aws events put-rule \\ --name handson-cli-events-gettings-ebs-rule \\ --description \u0026#34;EBS event rule.\u0026#34; \\ --event-pattern \u0026#39;{\u0026#34;source\u0026#34;:[\u0026#34;aws.ec2\u0026#34;],\u0026#34;detail-type\u0026#34;:[\u0026#34;EBS Volume Notification\u0026#34;]}\u0026#39; EventBridge にターゲット登録 events put-targets コマンドを使用します。\n$ aws events put-targets help ... SYNOPSIS put-targets --rule \u0026lt;value\u0026gt; [--event-bus-name \u0026lt;value\u0026gt;] --targets \u0026lt;value\u0026gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] $ aws events put-targets \\ --rule handson-cli-events-gettings-ebs-rule \\ --targets \u0026#34;Id=lambda-handson-cli-events-gettings-ebs-function,Arn=arn:aws:lambda:ap-northeast-1:XXXXXXXXXXXX:function:handson-cli-events-gettings-ebs-function\u0026#34; --target ではイベントターゲットを指定しますが、今回は Lambda 関数を指定しています。 Id はターゲット ID になるもので、ルールに追加できるターゲットが 5 つまでということから、各ターゲットを識別するために付与します。Arn は Lambda 関数の ARN です。\nログの確認 CloudWatch Logs のログイベントを確認するには、ロググループ名とログストリーム名が必要です。\nLambda で作成されるロググループ名は /aws/lambda/{関数名} という形式になるので、今回は /aws/lambda/handson-cli-events-gettings-ebs-function となります。ログストリームは実行されるタイミングでハッシュ値を含んだ名前で作成されます。今回は直近のログイベントを取得したいので、ログストリームも直近のものを取得します。\n$ aws logs describe-log-streams \\ --log-group-name /aws/lambda/handson-cli-events-gettings-ebs-function \\ --query \u0026#39;max_by(logStreams[], \u0026amp;lastEventTimestamp).logStreamName\u0026#39; \\ --output text 2020/09/10/[$LATEST]544ac06f414a47f6b49395b56db4a762 ログイベントの内容は、タイムスタンプとメッセージだけ抜き出して出力します。\n$ aws logs get-log-events \\ --log-group-name /aws/lambda/handson-cli-events-gettings-ebs-function \\ --log-stream-name \u0026#39;2020/09/10/[$LATEST]544ac06f414a47f6b49395b56db4a762\u0026#39; \\ --limit 7 \\ --query \u0026#39;events[].[join(``, [ to_string(timestamp) ,`: `,message])]\u0026#39; \\ --output text ... 1599735257694: ## EVENT 1599735257694: {\u0026#39;version\u0026#39;: \u0026#39;0\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;672deda3-a5d2-84f9-97b9-2d7531855a2e\u0026#39;, \u0026#39;detail-type\u0026#39;: \u0026#39;EBS Volume Notification\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;aws.ec2\u0026#39;, \u0026#39;account\u0026#39;: \u0026#39;XXXXXXXXXXXX\u0026#39;, \u0026#39;time\u0026#39;: \u0026#39;2020-09-10T10:54:17Z\u0026#39;, \u0026#39;region\u0026#39;: \u0026#39;ap-northeast-1\u0026#39;, \u0026#39;resources\u0026#39;: [\u0026#39;arn:aws:ec2:ap-northeast-1:XXXXXXXXXXXX:volume/vol-02871700072f113f1\u0026#39;], \u0026#39;detail\u0026#39;: {\u0026#39;result\u0026#39;: \u0026#39;deleted\u0026#39;, \u0026#39;cause\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;event\u0026#39;: \u0026#39;deleteVolume\u0026#39;, \u0026#39;request-id\u0026#39;: \u0026#39;213c7669-dee3-4223-a994-6f17af23d650\u0026#39;}} ... ちなみに、タイムスタンプは文字列ではないためそのままでは join できないようです。 join したい場合は to_string で文字列に変換します。\n知らんかったぞ、それ 今回に限らずハンズオンでは新しく知ることがたくさんあるので、この 知らんかったぞ、それ 項目では新しく知ったことを自分なりに深堀りしてみたいと思います。(恒例化したい)\nPython 3 の json.tool JAWS-UG CLI 専門支部では JSON のフォーマット確認に jsonlint を利用していました。が、今回の手順では Python 3 に含まれる json.tool というモジュールを使っていました。使い方は簡単で、 JSON の出力をパイプで渡す形で、次のように実行します。\n$ cat ${JSON_FILE_PATH} | python3 -m json.tool 例えば次のような JSON ファイルを準備して、フォーマットチェックしてみます。\n$ cat sample.json { \u0026#34;key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;key3\u0026#34;: \u0026#34;value3\u0026#34; } $ cat sample.json | python3 -m json.tool { \u0026#34;key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;key3\u0026#34;: \u0026#34;value3\u0026#34; } 特にエラーがなければ JSON が出力されます。フォーマットが崩れていたりすると、エラーが出力されます。(2 つ目のカンマを削除してみます)\n$ cat sample.json | python3 -m json.tool Expecting \u0026#39;,\u0026#39; delimiter: line 4 column 3 (char 43) --query オプション内での max_by 直近のログストリーム名を取得する際に、 --query オプションの中で max_by というクエリを使っていました。これは AWS CLI の機能というよりは JMESPath の機能で、配列内の最大要素を返してくれます。\nJMESPath Specification — JMESPath | max_by 今回実行したのは次のようなコマンドでした。\n$ aws logs describe-log-streams \\ --log-group-name /aws/lambda/handson-cli-events-gettings-ebs-function \\ --query \u0026#39;max_by(logStreams[], \u0026amp;lastEventTimestamp).logStreamName\u0026#39; \\ --output text max_by でログストリームの lastEventTimestamp が最大のもの、つまり直近のものを取得するようなクエリになっていました。\n例えば、 Lambda でコードサイズが一番大きい関数の名前が取得したい場合は、次のようなコマンドになります。\n$ aws lambda list-functions \\ --query \u0026#39;max_by(Functions, \u0026amp;CodeSize).FunctionName\u0026#39; ちなみに、 max_by の逆は min_by です。\nまとめ JAWS-UG CLI 専門支部 #167R EventBridge入門 の参加レポートでした。\n今回は EventBridge 入門ということ、そもそも EventBridge とは何なのかについて再確認することができました。スキーマレジストリの存在については知らなかったのですが、イベントルール作成時に必要な情報が参照できるのは便利だなと思いました。\nCLI 専門支部では、各回で扱うサービスの内容はもちろん、 AWS CLI の --query および --filter オプションの便利な使い方を知ることもできます。ハンズオンの手順によっては実行するコマンドが多くてついていくのが大変ですが、毎回得るものがたくさんあるので めっちゃインプットしてるな〜 と感じます。\nインプットしたままで終わらないように、今後も参加レポ書いていきたいと思います。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-cli-167-eventbridge/",
    "title": "[レポート] JAWS-UG CLI 専門支部 #167R EventBridge入門 に参加しました #jawsug_cli"
  },
  {
    "contents": "Go 言語で実装されたアプリケーションから AWS リソースを操作する際には AWS SDK for Go を使いますが、今回は その SDK を使って CloudWatch Logs にログイベントを送信してみます。\n前に AWS CLI からログイベントを送信したときの記事はこちらです。\n目次 概要 AWS SDK for Go による実装 CloudWatch Logs クライアントの生成 ロググループの作成 ログストリームの生成 ログイベントの送信 ログストリームにログイベントが既に存在している場合 まとめ 概要 前回 の記事にも書きましたが、あらためて CloudWatch Logs を構成する 3 つの要素について簡単に書いておきます。\nロググループ アプリケーション単位や画面単位といったくくりでログをグループ分けしている要素です。例えば Lambda の実行ログに関しては、関数単位でロググループが作成されています。\nログストリーム ロググループ内でさらにログを分割している要素がログストリームです。同じアプリケーションであっても、別のバージョンや別日付などでストリームを分けることになると思います。Lambda の実行ログを例にすると、関数のバージョン単位、一定時間単位でストリームが分割されています。\nロググループ内には数の制限なくログストリームを作成することができます。一つだけでもいいです。ただし、最低でも日付ごととかで分割してあると、あとでログを閲覧するときに見つけやすいです。 また、ストリームを分けたとしても、マネジメントコンソール上ではストリームを横断したログの検索が可能なので、心配ありません。\nログイベント いわゆるログのことです。\nAWS SDK for Go による実装 では、 AWS SDK for Go を使った CloudWatch Logs へのログイベント送信の実装についてみていきます。\nちなみに、インポートするパッケージは次の通りです。\nimport ( \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/cloudwatchlogs\u0026#34; ) CloudWatch Logs クライアントの生成 まずは CloudWatch Logs のクライアントを生成します。\nregion := \u0026#34;ap-northeast-1\u0026#34; client := cloudwatchlogs.New( session.Must(session.NewSession()), aws.NewConfig().WithRegion(region), ) リージョンを指定してクライアントを生成します。\nロググループの作成 続いて、ロググループを作成します。作成には、 CreateLogGroup() を使います。\ngroupName := \u0026#34;/sample/group/name\u0026#34; var createLogGroupIn *cloudwatchlogs.CreateLogGroupInput = \u0026amp;cloudwatchlogs.CreateLogGroupInput{ LogGroupName: aws.String(groupName), } _, err := client.CreateLogGroup(createLogGroupIn) 既にロググループが存在している場合は、 ResourceAlreadyExistsException がエラーで返却されます。\ncloudwatchlogs - Amazon Web Services - Go SDK | CreateLogGroup ログストリームの生成 続いてはログストリームの作成です。作成には CreateLogStream() を使います。\ngroupName := \u0026#34;/sample/group/name\u0026#34; streamName := \u0026#34;sample-stream-name\u0026#34; var createLogStreamIn *cloudwatchlogs.CreateLogStreamInput = \u0026amp;cloudwatchlogs.CreateLogStreamInput{ LogGroupName: aws.String(groupName), LogStreamName: aws.String(streamName), } _, err := client.CreateLogStream(createLogStreamIn) ログストリームに関しても、既に存在している場合は、 ResourceAlreadyExistsException がエラーで返却されます。また、指定したロググループが存在していない場合には ResourceNotFoundException が返却されます。\ncloudwatchlogs - Amazon Web Services - Go SDK | CreateLogStream ログイベントの送信 最後に、ログイベントの送信です。送信には PutLogEvents() を使います。メソッド名からわかるように、複数のログイベントを送信することができます。\ngroupName := \u0026#34;/sample/group/name\u0026#34; streamName := \u0026#34;sample-stream-name\u0026#34; message := \u0026#34;This is a sample log message.\u0026#34; timestamp := time.Now().Unix() timestampDigits := len(strconv.FormatInt(timestamp, 10)) var multipyNum int64 = 10 for n := 0; n \u0026lt; (13 - timestampDigits); n++ { timestamp = timestamp * multipyNum } var logEvents []*cloudwatchlogs.InputLogEvent = []*cloudwatchlogs.InputLogEvent{ { Message: aws.String(message), Timestamp: aws.Int64(timestamp), }, } var putLogEventIn *cloudwatchlogs.PutLogEventsInput = \u0026amp;cloudwatchlogs.PutLogEventsInput{ LogGroupName: aws.String(groupName), LogStreamName: aws.String(streamName), LogEvents: logEvents, } _, err := client.PutLogEvents(putLogEventIn) CLI での操作のときも書きましたが、ログイベント用のタイムスタンプは 13 桁 である必要があります。 13 桁というか、 ミリ秒 まで含んだタイムスタンプというイメージです。\nCloudWatch Logs では、過去または未来のタイムスタンプをもつイベントは記録されません。\nIf the timestamp of log event is more than 2 hours in future, the log event is skipped. If the timestamp of log event is more than 14 days in past, the log event is skipped. CloudWatch Logs Agent Reference - Amazon CloudWatch Logs cloudwatchlogs - Amazon Web Services - Go SDK | PutLogEvents ログストリームにログイベントが既に存在している場合 既にログイベントが存在しているログストリームに対して追加でログイベントを送信する場合には、 InputLogEvent.SequenceToken に対して値を指定する必要があります。\nここで指定する値は、 PutLogEventsInput() のレスポンス内にある PutLogEventsOutput.NextSequenceToken です。が、連続したログイベント送信処理でない場合、この値をどこかに保持しておく必要があります。それは面倒なので、 DescribeLogStreams() で取得したログストリームの情報から取得します。\n例えば次のような関数を用意して、ログストリーム名から NextSequenceToken を取得できるようにしておきます。\nfunc GetNextSequenceToken(groupName string, streamName string) *string { var nextSeqToken string = \u0026#34;\u0026#34; var describeLogStreamsIn *cloudwatchlogs.DescribeLogStreamsInput = \u0026amp;cloudwatchlogs.DescribeLogStreamsInput{ LogStreamNamePrefix: aws.String(streamName), LogGroupName: aws.String(groupName), } out, err := client.DescribeLogStreams(describeLogStreamsIn) if err != nil { fmt.Println(err.Error()) return aws.String(\u0026#34;\u0026#34;) } if len(out.LogStreams) == 0 { return aws.String(\u0026#34;\u0026#34;) } for _, logStream := range out.LogStreams { if *logStream.LogStreamName == streamName { if logStream.UploadSequenceToken != nil { nextSeqToken = *logStream.UploadSequenceToken break } } } return aws.String(nextSeqToken) } これで値が取得できた かつ 空文字でない場合には、先ほどの InputLogEvent に SequenceToken を追加して PutLogEvents() を実行します。\nvar nextSeqToken *string = GetNextSequenceToken(groupName, streamName) if *nextSeqToken != \u0026#34;\u0026#34; { putLogEventIn.SequenceToken = nextSeqToken } まとめ AWS SDK for Go を使った CloudWatch Logs にログイベントを送信してみた話でした。\nSDK を使うとポインタ型への変換などが発生するので少しだけややこしいということで、今回紹介した処理をまとめたものを GitHub に置きました。\nGo での自作パッケージの構成についてはまだまだ全然わかってないのですが、とりあえず go get して使える状態にはなっています。 今後、ぼちぼち良い感じにしていけたら良いなーと思ってます。\n",
    "permalink": "https://michimani.net/post/aws-put-events-to-cloudwatch-logs-via-go-sdk/",
    "title": "AWS SDK for Go で CloudWatch Logs にログイベントを送信する"
  },
  {
    "contents": "JAWS-UG 初心者支部のオンラインイベント 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう に参加してきました。今回は監視編ということで、 WordPress でサイトを構築して CloudWatch で様々なメトリクスを確認するという内容でした。\n目次 概要 CloudFormation で WordPress 環境をサクッと構築 各メトリクスの確認 アラームの設定 WordPress のログ確認 知らんかったぞ、それ CloudWatch メトリクスグラフの現地時間表示 RDS のメトリクスはデフォルトで 1 分間隔 まとめ 概要 今回のハンズオンの概要です。資料は slideshare で公開されています。\nJAWS-UG 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう イベントページはこちら。\nJAWS-UG 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう - connpass CloudFormation で WordPress 環境をサクッと構築 今回のハンズオンでは、まず最初に CloudFormation で次のような構成を作ってしまいます。\nJAWS-UG 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう from Hiroki Uchida 今回のメインは 監視 なので、その土台となる WordPress の環境をサクッと一律で準備できてしまうのは、やはり CloudFormation の、というか IaC の良いところだなと思いました。とりあえず CloudFormation のスタック作成さえできればハンズオンの準備が整うので。\n各メトリクスの確認 今回作成したリソース (EC2, ELB, RDS) に関するメトリクスをマネジメントコンソールで確認しました。\nJAWS-UG 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう from Hiroki Uchida アラームの設定 メトリクスに対してアラームを設定して、 SNS 経由でメールアドレスに通知が飛んでくることを確認しました。\nJAWS-UG 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう from Hiroki Uchida WordPress のログ確認 WordPress のアクセスログ、エラーログを CloudWatch Logs 確認しました。\nJAWS-UG 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう from Hiroki Uchida WordPress のログ (Apache のログ) を CloudWatch Logs に送信するには、 CloudWatch Agent で次のように設定します。\n{ \u0026#34;logs\u0026#34;: { \u0026#34;logs_collected\u0026#34;: { \u0026#34;files\u0026#34;: { \u0026#34;collect_list\u0026#34;: [ { \u0026#34;file_path\u0026#34;: \u0026#34;/var/log/httpd/access_log\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;wordpress_access_log\u0026#34;, \u0026#34;log_stream_name\u0026#34;: \u0026#34;{instance_id}\u0026#34; }, { \u0026#34;file_path\u0026#34;: \u0026#34;/var/log/httpd/error_log\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;wordpress_error_log\u0026#34;, \u0026#34;log_stream_name\u0026#34;: \u0026#34;{instance_id}\u0026#34; } ] } } } } 今回は、上記の情報を含んだ CloudWatch Agent の設定ファイルを /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ に配置するようなスクリプトが EC2 インスタンスの User Data に記載されていました。\n知らんかったぞ、それ 今回のハンズオンでは「へぇ〜」「知らんかった」という部分があったのでメモしておきます。\nCloudWatch メトリクスグラフの現地時間表示 CloudWatch の各メトリクスをグラフ表示した際に、 X 軸の時刻は UTC になっています。これを現地時間に変換して表示できるというのを初めて知りました。\n厳密にいつからこの機能があったかというのはわからないのですが、クラメソさんの記事で 2017 年時点で紹介されていたので、自分が AWS を触り始めた頃からあったようです。\nさすがクラメソさん、その機能がいつ頃から存在していたかもわかってしまうんですね\u0026hellip;。\n当たり前ですが、公式ドキュメントにもしっかり記述がありました。\nCloudWatch ダッシュボードの時間範囲またはタイムゾーン形式を変更する - Amazon CloudWatch RDS のメトリクスはデフォルトで 1 分間隔 CloudWatch で取得しているメトリクスはデフォルトで 5 分間隔で、有料の詳細モニタリングを有効にすることで 1 分間隔で値を取得できるという認識でいました。\nインスタンスの詳細モニタリングを有効または無効にする - Amazon Elastic Compute Cloud これはそうなんですが、今回出てきた RDS に関するメトリクスに関してはデフォルトで 1 分間隔でデータを取得できるようです。\nAmazon RDS は、アクティブな各データベースのメトリクスを 1 分ごとに CloudWatch に自動送信します。CloudWatch の Amazon RDS メトリクスに対する追加料金は発生しません。\nAmazon RDS ​のモニタリングの概要です。 - Amazon Relational Database Service まとめ JAWS-UG 初心者支部のオンラインイベント 初心者支部 #31 監視編 サーバーのモニタリングの基本を学ぼう に参加してきました。\n久々に初心者支部のハンズオンに参加することができて、やっぱり決められた時間で集中して作業するというのはいいなと再確認しました。\nメトリクスのグラフを見て、メトリクスに対してアラート設定して、というのは普段からやっているのですが、それでも初めて知ることがあり勉強になりました。\n参加者からの質問にもありましたが、今回は Apache の access_log と error_log を別々のロググループに設定していましたが、そのあたりの設計思想に関してはオライリーから出ている「入門 監視」が参考になるということでした。この書籍に関しては以前に読んだのですが、あらためて読んでみようと思います。\nまた、今回のハンズオンは下記のハンズオンが元になっているようなので、合わせて確認してみると良いかもということでした。\nAWS Hands-on for Beginners - 監視編 サーバーのモニタリングの基本を学ぼう | AWS あらためて、家にいながらにしてこういったハンズオンイベントに参加できるのは大変ありがたいことだなと感じました。オンラインで開催していただいた運営の方々、ありがとうございました！\nあと全然関係ないですが、今回のハンズオンを進行していただいた AWS の内田さん ( @nikuyoshi ) の映りがめちゃくちゃキレイでした。\n",
    "permalink": "https://michimani.net/post/event-jaws-ug-beginner-31/",
    "title": "[レポート] JAWS-UG 初心者支部#31 「監視編 サーバーのモニタリングの基本を学ぼう」 #jawsug_bgnr"
  },
  {
    "contents": "みなさんスマブラやってますか？前回、キャプチャ画像から世界戦闘力の値を抜き出しましたが、今回はその続編です。\n前回の記事。\n概要 前回に引き続いて、今回は次のことを試します。\nキャラクターの判別 Amazon Rekognition の DetectText の制限 (仕様) への対応 検出された情報をまとめる キャラクターの判別 Amazon Rekognition の仕様として、日本語のテキストは抽出することができません。つまり、キャプチャ画像から対象のキャラクターを名前で判別することができません。名前での判別ができないので、キャラクターのサムネイル画像のバイト文字列で判別してみます。\n対象のキャラクターのサムネイルはこの部分です。\nキャプチャ画像のサイズは 1260x640 (px) なので、この部分の座標 (左上が (0,0)) は次のようになります。\nA : (911, 474) B : (959, 474) C : (959, 522) D : (911, 522) この部分をトリミングするには、 Python の opencv-python モジュールを使います。(Python のバージョンは 3.7.x とします)\n$ pip install opencv-python キャラクターの判別には、トリミングした画像のバイト文字列を使いますが、そのままだと長すぎるのでハッシュ値で判別することにします。\nトリミングして画像のハッシュ値を取得するコードは次の通り。\nimport cv2 import hashlib import os import sys THUMBNAIL_DIR = \u0026#39;./char_thumb\u0026#39; character_area = { \u0026#39;A\u0026#39;: {\u0026#39;X\u0026#39;: 911, \u0026#39;Y\u0026#39;: 474}, \u0026#39;B\u0026#39;: {\u0026#39;X\u0026#39;: 959, \u0026#39;Y\u0026#39;: 474}, \u0026#39;C\u0026#39;: {\u0026#39;X\u0026#39;: 959, \u0026#39;Y\u0026#39;: 522}, \u0026#39;D\u0026#39;: {\u0026#39;X\u0026#39;: 911, \u0026#39;Y\u0026#39;: 522}, } def get_character_hash(image_path): img = cv2.imread(image_path) trimed = img[ character_area[\u0026#39;A\u0026#39;][\u0026#39;Y\u0026#39;]: character_area[\u0026#39;D\u0026#39;][\u0026#39;Y\u0026#39;], character_area[\u0026#39;A\u0026#39;][\u0026#39;X\u0026#39;]: character_area[\u0026#39;C\u0026#39;][\u0026#39;X\u0026#39;]] trimed_bytes = trimed.tobytes() trimed_hash = hashlib.sha1(trimed_bytes).hexdigest() thumb_path = f\u0026#39;{THUMBNAIL_DIR}/{trimed_hash}.jpg\u0026#39; if os.path.exists(thumb_path) is False: cv2.imwrite(thumb_path, trimed) return trimed_hash if __name__ == \u0026#39;__main__\u0026#39;: image_path = sys.argv[1] character_hash = get_character_hash(image_path) print(character_hash) 次のようにして実行すると、キャラクター画像のハッシュ値が出力され、サムネイル用のディレクトリにトリミングした画像が生成されます。\n$ python trim_img.py 2020082121055400-0E7DF678130F4F0FA2C88AE72B47AFDF.jpg e6bc228bbea1f0d9034cbec36871df87628d9daf 同じキャラクターの別のキャプチャ画像で何度か試しましたが、ハッシュ値が異なることはなかったので、キャラクターはこれで判別できそうです。\nAmazon Rekognition の DetectText の制限 (仕様) への対応 Amazon Rekognition の DetectText の制限 (仕様) として、ひとつの画像から検出できるテキストの要素数には上限があります。\nA word is one or more ISO basic latin script characters that are not separated by spaces. DetectText can detect up to 50 words in an image.\nDetectText - Amazon Rekognition テキストの検出は画像の左上から始まっているようで、画像内にこの上限を超えるテキスト要素が存在している場合、画像の下の方のテキストは検出されないことになります。\n例えば、次のようなキャプチャ画像では世界戦闘力の部分にたどり着くまでに検出数の上限に達しており、世界戦闘力の値を検出することができません。\nこの問題を回避するために、キャプチャを 4 分割して右下の部分のみを検出対象画像にしたいと思います。\nこれを実現するためには、先程と同様に opencv-python モジュールでサムネイル画像をトリミングします。\nRekognition には画像のバイト文字列を渡すので、キャプチャ画像の右下をトリミングした画像を一旦生成し、バイト文字列を取得します。その後、トリミングした画像は不要になるので削除します。\u0026hellip;というのを次の関数で処理します。\nTMP_DIR = \u0026#39;./tmp\u0026#39; def get_bytes_of_image(image_path): img = cv2.imread(image_path) trimed = img[ 360: 720, 640: 1280 ] ts = str(time.time()) trimed_tmp_path = f\u0026#39;{TMP_DIR}/{ts}.jpg\u0026#39; cv2.imwrite(trimed_tmp_path, trimed) with open(trimed_tmp_path, mode=\u0026#39;rb\u0026#39;) as img: image_bytes = img.read() os.remove(trimed_tmp_path) return image_bytes 世界戦闘力の描画エリア調整 Rekognition にわたす画像が元のキャプチャの右下部分になったので、 DetectText のレスポンスから世界戦闘力を抜き出すための描画エリアを調整します。\nトリミングされた画像は 640x320 (px) になっており、 ABCD の各座標は次のようになっています。\nA : (366, 60) B : (576, 60) C : (576, 138) D : (366, 138) これを画像サイズに対する割合に変換します。\nA : (0.57187, 0.1875) B : (0.9, 0.1875) C : (0.9, 0.43125) D : (0.57187, 0.43125) あとは前回と同じです。\n検出された情報をまとめる 最後に、キャプチャ画像から得られた情報をひとつのオブジェクトとしてまとめます。まとめる情報としては次の通りです。\nキャラクターのハッシュ値 キャプチャ画像の取得日時 世界戦闘力の値 これらを JSON で出力するように改修したのが下記のコードです。\nimport boto3 import cv2 import datetime import hashlib import json import os import re import sys import time TMP_DIR = \u0026#39;./tmp\u0026#39; THUMBNAIL_DIR = \u0026#39;./char_thumb\u0026#39; reko = boto3.client(\u0026#39;rekognition\u0026#39;) target_area = { \u0026#39;A\u0026#39;: {\u0026#39;X\u0026#39;: 0.57187, \u0026#39;Y\u0026#39;: 0.1875}, \u0026#39;B\u0026#39;: {\u0026#39;X\u0026#39;: 0.9, \u0026#39;Y\u0026#39;: 0.1875}, \u0026#39;C\u0026#39;: {\u0026#39;X\u0026#39;: 0.9, \u0026#39;Y\u0026#39;: 0.43125}, \u0026#39;D\u0026#39;: {\u0026#39;X\u0026#39;: 0.57187, \u0026#39;Y\u0026#39;: 0.43125}, } character_area = { \u0026#39;A\u0026#39;: {\u0026#39;X\u0026#39;: 911, \u0026#39;Y\u0026#39;: 474}, \u0026#39;B\u0026#39;: {\u0026#39;X\u0026#39;: 959, \u0026#39;Y\u0026#39;: 474}, \u0026#39;C\u0026#39;: {\u0026#39;X\u0026#39;: 959, \u0026#39;Y\u0026#39;: 522}, \u0026#39;D\u0026#39;: {\u0026#39;X\u0026#39;: 911, \u0026#39;Y\u0026#39;: 522}, } def get_datetime_from_image_path(image_path): datetime_part = re.sub(r\u0026#39;.*(\\d{16}).*\u0026#39;, r\u0026#39;\\1\u0026#39;, image_path)[:14] dt = datetime.datetime.strptime(datetime_part, \u0026#39;%Y%m%d%H%M%S\u0026#39;) datetime_str = \u0026#39;{0:%Y-%m-%d %H:%M:%S}\u0026#39;.format(dt) return datetime_str def get_character_hash(image_path): img = cv2.imread(image_path) trimed = img[ character_area[\u0026#39;A\u0026#39;][\u0026#39;Y\u0026#39;]: character_area[\u0026#39;D\u0026#39;][\u0026#39;Y\u0026#39;], character_area[\u0026#39;A\u0026#39;][\u0026#39;X\u0026#39;]: character_area[\u0026#39;C\u0026#39;][\u0026#39;X\u0026#39;]] trimed_bytes = trimed.tobytes() trimed_hash = hashlib.sha1(trimed_bytes).hexdigest() thumb_path = f\u0026#39;{THUMBNAIL_DIR}/{trimed_hash}.jpg\u0026#39; if os.path.exists(thumb_path) is False: cv2.imwrite(thumb_path, trimed) return trimed_hash def get_bytes_of_image(image_path): img = cv2.imread(image_path) trimed = img[ 360: 720, 640: 1280 ] ts = str(time.time()) trimed_tmp_path = f\u0026#39;{TMP_DIR}/{ts}.jpg\u0026#39; cv2.imwrite(trimed_tmp_path, trimed) with open(trimed_tmp_path, mode=\u0026#39;rb\u0026#39;) as img: image_bytes = img.read() os.remove(trimed_tmp_path) return image_bytes def detect_text(image_path): image_bytes = get_bytes_of_image(image_path) return reko.detect_text( Image={ \u0026#39;Bytes\u0026#39;: image_bytes } ) def get_smash_power(image_path): smash_power = 0 detect_res = detect_text(image_path) for detected in detect_res[\u0026#39;TextDetections\u0026#39;]: polygon = detected[\u0026#39;Geometry\u0026#39;][\u0026#39;Polygon\u0026#39;] if polygon[0][\u0026#39;X\u0026#39;] \u0026gt; target_area[\u0026#39;A\u0026#39;][\u0026#39;X\u0026#39;] and polygon[0][\u0026#39;Y\u0026#39;] \u0026gt; target_area[\u0026#39;A\u0026#39;][\u0026#39;Y\u0026#39;] \\ and polygon[1][\u0026#39;X\u0026#39;] \u0026lt; target_area[\u0026#39;B\u0026#39;][\u0026#39;X\u0026#39;] and polygon[1][\u0026#39;Y\u0026#39;] \u0026gt; target_area[\u0026#39;B\u0026#39;][\u0026#39;Y\u0026#39;] \\ and polygon[2][\u0026#39;X\u0026#39;] \u0026lt; target_area[\u0026#39;C\u0026#39;][\u0026#39;X\u0026#39;] and polygon[2][\u0026#39;Y\u0026#39;] \u0026lt; target_area[\u0026#39;C\u0026#39;][\u0026#39;Y\u0026#39;] \\ and polygon[3][\u0026#39;X\u0026#39;] \u0026gt; target_area[\u0026#39;D\u0026#39;][\u0026#39;X\u0026#39;] and polygon[3][\u0026#39;Y\u0026#39;] \u0026lt; target_area[\u0026#39;D\u0026#39;][\u0026#39;Y\u0026#39;]: smash_power = int(detected[\u0026#39;DetectedText\u0026#39;].replace(\u0026#39;,\u0026#39;, \u0026#39;\u0026#39;)) return smash_power def generate_smash_power_item(image_path): # Datetime of data datetime_str = get_datetime_from_image_path(image_path) # Get Smash Power via Amazon Rekognition smash_power = get_smash_power(image_path) # Get character hash (and Generate character thumbnail) character_hash = get_character_hash(image_path) return { \u0026#39;character_hash\u0026#39;: character_hash, \u0026#39;capture_datetime\u0026#39;: datetime_str, \u0026#39;power\u0026#39;: smash_power } if __name__ == \u0026#39;__main__\u0026#39;: image_path = sys.argv[1] smash_power_item = generate_smash_power_item(image_path) print(json.dumps(smash_power_item, indent=2, ensure_ascii=False)) これを実行すると、次のような結果が得られます。\n$ python detect_smash_power.py 2019100921045700-0E7DF678130F4F0FA2C88AE72B47AFDF.jpg { \u0026#34;character_hash\u0026#34;: \u0026#34;06e200a624eab0babac0ea3fee6325efcb3dae05\u0026#34;, \u0026#34;capture_datetime\u0026#34;: \u0026#34;2019-10-09 21:04:57\u0026#34;, \u0026#34;power\u0026#34;: 1000958 } コードはこちら。\nUse Amazon Rekognition to detect world strength from captured images of Smash Bros SPECIAL. (part 2) まとめ Amazon Rekognition を使ってスマブラ SP の世界戦闘力を抜き出してみました、という 前回 の記事から、今回はキャラクターの判別とデータのまとめを試してみました。その際に Rekognition の制限 (仕様) にも対応してみました。\nこれでデータの生成準備が整ったので、キャラクターごとの世界戦闘力の推移をグラフ化できる未来も近そうです。\n",
    "permalink": "https://michimani.net/post/aws-extract-value-of-smash-blos-sp-power-by-rekognition-2/",
    "title": "続・Amazon Rekognition を使ってスマブラ SP のキャプチャ画像から世界戦闘力を抜き出す"
  },
  {
    "contents": "Serverless Meetup Japan Virtual #5 にオンラインで参加したので、その参加レポート・メモです。\nServerless Meetup Japan Virtual #5 - connpass Twitter のハッシュタグは #serverlessjp です。\n#2、#3 の様子はこちら。\nタイムテーブル Timeline Title Speaker 20:45-21:00 Social 21:00-21:05 Opening Talk 21:05-21:10 Forkwellさん告知!! 重本湧気さん 21:10-21:30 DynamoDB のインデックス再編事例 丹羽一智さん (Game Server Services) 21:30-21:40 Meetup Zoom参加メンバー 21:40-22:00 楽しいことにフォーカスしたい！上手なクラウドの使い方 中丸良さん (Google) 22:00-22:30 Meetup Zoom参加メンバー 22:30- Closing セッションレポート DynamoDB のインデックス再編事例 SEGA -\u0026gt; Nintendo -\u0026gt; 起業 GS2 というゲームに特化したサービス DynamoDB のテーブル構成のベストプラクティス 動機 インデックスの設計に失敗 LSI (Local Secondaly Index) はテーブル作成時にしか設定できない マルチテナント x マイクロサービスで、それぞれの組み合わせでテーブルを作成 結果としてテーブル数が 2,000 を超える 新しいテーブルを準備 流れは RDBMS のシャーディングと同じ 新しいテーブルへの同期 方法としては 2 通り DynamoDB Stream 作成・更新・削除イベントから Lambda を実行 トランザクションを使用 今回はこちらを採用 テーブル数が 2,000 もあり、Stream の制限があるのではないか？と思ったから インデックス問題を回避するために 先にインデックスを作る 属性名に __lsi_ をつけて先に作ってしまう 使いたくなったら値を入れていく 1 テーブルでいろんなエンティティを扱いやすくなった バックアップ テナントごとにポイントインタイムリカバリ Cloud Firestore とほぼ同じ触り心地になった Q\u0026amp;A Q. 副作用は？デメリットは？ A. 人間が見にくくなる。関連する属性を消すのが大変。 Q. GSI でやらない理由は？ A. コストの問題と、整合性の問題。GSI だと結果整合性、 LSI だと強整合性。 Q. マイクロサービスの観点からすると元々のほうが正しいのでは？ A. ゲームの用途だと特定のマイクロサービスだけロールバックするということはない。 Q. シングルテーブル設計が好きだが、設計に時間がかかる。新しい開発だとそのあたりの難しさは？ A. 新しいデータのとり方をしたくなったときにできない。 Q. 設計に掛けた時間は？ A. 2 週間くらい Q. コストは削減できた？ A. あまり削減できた印象はない。ただし、オンデマンド/プロビジョニングの管理はしやすくなった。 Q. DB アクセス用のリポジトリ自動生成について。 A. DDL をもとに、そのテーブルにアクセスするためのクラスができあがる。 Q. 本体のプログラム変更にかかった時間は？ A. インターフェースは同じなので、 DB アクセス部分の入れ替えのみで対応できた。 Q. 基本的にインデックスをもとにデータを引っ張ってくる？ A. そう。JOIN したいような場合は複数クエリを投げるスタンス。 Q. パフォーマンスは？ A. シングルテーブルになったとは言え、はやくなったわけではない。 Q. 苦手なクエリは？日付の範囲など。同じキーが並ぶ場合など。 A. コンポジットキーが苦手。LSI に値を連結させたものを保持する。\n範囲検索はレンジキーを使うが、他の条件 (他のキー、他の範囲) と組み合わさると難しくなる。\n同じキーが並ぶ場合については、キーが分散するようなキー設計をしている。 楽しいことにフォーカスしたい！上手なクラウドの使い方 (※ 大人の事情によりメモもローカルにとどめておきます\u0026hellip;)\nまとめ Serverless Meetup Japan Virtual #5 の参加メモでした。\n今回は Amazon DynamoDB の話と、 Google Cloud の話でした。 DynamoDB については、シングルテーブルでの (キー) 設計はすごく複雑で大変そうだなという印象を受けました。 DynamoDB については無理せずテーブルを分けていくという考えしかなかったので、シングルテーブルでの運用については調べてみようと思いました。\nzoom 枠の方々には AWS、Azure、GCP な人がそれぞれいらっしゃったので、各クラウドをまたいだ話を聞くことができたのもすごく勉強になりました。\n次回以降も楽しみです。今回も開催ありがとうございました。\n",
    "permalink": "https://michimani.net/post/event-serverless-meetup-japan-virtual-5/",
    "title": "[レポート] Serverless Meetup Japan Virtual #5 の参加メモ #serverlessjp"
  },
  {
    "contents": "みなさんスマブラやってますか？今回は、スマブラ SP の強さをはかる指標である 世界戦闘力 の値を Amazon Rekognition のテキスト抽出機能で抜き出してみます。\n概要 Nintendo Switch のゲーム 大乱闘スマッシュブラザーズ SPECIAL の世界戦闘力を、 Amazon Rekognition のテキスト抽出機能を使って、ゲーム内で撮影したキャプチャ画像から抜き出します。\nスマブラ SP オンライン対戦のキャラ選択画面で、任意のキャラを選択した状態でスクリーンキャプチャを撮ります。例えば下記のようなもの。\n右下に 3 つの戦闘力が表示されていますが、今回抜き出すのは一番上の戦闘力の値です。\nAmazon Rekognition Amazon Rekognition (以下、Rekognition) は、 AWS で提供されているディープラーニングベースの視覚分析サービスです。静止画や動画から物体やシーンの検出、顔分析、テキストの抽出などを行うことができます。\n今回はその中から テキスト抽出 - Detect Text の機能を使います。\nマネジメントコンソールで試す Rekognition の各機能はマネジメント今ソースから簡単に試すことができるので、まずはそれで試してみます。\nこの様に、画像内のテキストが抽出されます。 プレビュー内の青枠をホバーすると、右側の 結果 に表示されている文字列を確認することができます。\nまた、 レスポンス では実際の API レスポンスを JSON 形式で確認することができます。例えば、今回抜き出したい戦闘力部分のオブジェクトは次のようになっています。\n{ \u0026#34;DetectedText\u0026#34;: \u0026#34;7,816,946\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;WORD\u0026#34;, \u0026#34;Id\u0026#34;: 55, \u0026#34;ParentId\u0026#34;: 9, \u0026#34;Confidence\u0026#34;: 98.4032974243164, \u0026#34;Geometry\u0026#34;: { \u0026#34;BoundingBox\u0026#34;: { \u0026#34;Width\u0026#34;: 0.11172738671302795, \u0026#34;Height\u0026#34;: 0.04027777910232544, \u0026#34;Left\u0026#34;: 0.8187500238418579, \u0026#34;Top\u0026#34;: 0.6694444417953491 }, \u0026#34;Polygon\u0026#34;: [ { \u0026#34;X\u0026#34;: 0.8187500238418579, \u0026#34;Y\u0026#34;: 0.6694444417953491 }, { \u0026#34;X\u0026#34;: 0.930468738079071, \u0026#34;Y\u0026#34;: 0.668055534362793 }, { \u0026#34;X\u0026#34;: 0.930468738079071, \u0026#34;Y\u0026#34;: 0.7083333134651184 }, { \u0026#34;X\u0026#34;: 0.8187500238418579, \u0026#34;Y\u0026#34;: 0.7097222208976746 } ] } } DetectedText が抽出された文字列、 Geometry にはその文字列が画像のどの位置にあるかを示しています。 Geometry.Polygon では、その文字列が存在するエリア (長方形) の四角の座標を、画像のサイズに対する割合で示されています。\n一点注意が必要なのは、 日本語には対応していない ため日本語の文字列については正しく抽出することができないという点です。\nPython で実装 続いては、 boto3 (AWS SDK for Python) を使って戦闘力を抽出してみます。\n使うのは boto3 の Client.detect_text です。\nRekognition — Boto3 Docs 1.14.47 documentation | Client.detect_text テキスト抽出する対象の画像ファイルは S3 またはバイト文字列で指定します。今回はローカルにある画像ファイルを使うので、バイト文字列で指定します。\n対象となる戦闘力のエリアを確認 先ほどのマネジメントコンソールでの例を見てもらうとわかるように、抽出されたテキストから戦闘力の部分を判定する必要があります。レスポンスにはテキストが存在するエリアの四角の座標が含まれているので、この座標をもとにして目的の戦闘力を抜き出したいと思います。\n目的の戦闘力が存在するエリアは、だいたいこのあたりです。\nスマブラ SP のキャプチャ画像のサイズは 1260 x 720 (px) で、上の画像内に示した長方形 ABCD の各頂点の座標 (X, Y の始点は画像の左上) はだいたい下記の値でした。\nA : (1004.85, 460.33) B : (1216.26, 460.33) C : (1216.26, 530.83) D : (1004.85, 530.83) これを画像サイズに対する割合で表すと\nA : (0.785039062, 0.639347222) B : (0.950203125, 0.639347222) C : (0.950203125, 0.737263889) D : (0.785039062, 0.737263889) となります。\nレスポンス内の Geometry.Polygon に含まれるオブジェクトは、この ABCD の順番で対応しているので、各頂点が上で用意した範囲内に入っているものを、目的の戦闘力とすることができそうです。\n実装 実際のスクリプトは次のようになります。\nimport boto3 import sys reko = boto3.client(\u0026#39;rekognition\u0026#39;) target_area = { \u0026#39;A\u0026#39;: {\u0026#39;X\u0026#39;: 0.785039062, \u0026#39;Y\u0026#39;: 0.639347222}, \u0026#39;B\u0026#39;: {\u0026#39;X\u0026#39;: 0.950203125, \u0026#39;Y\u0026#39;: 0.639347222}, \u0026#39;C\u0026#39;: {\u0026#39;X\u0026#39;: 0.950203125, \u0026#39;Y\u0026#39;: 0.737263889}, \u0026#39;D\u0026#39;: {\u0026#39;X\u0026#39;: 0.785039062, \u0026#39;Y\u0026#39;: 0.737263889}, } def get_bytes_of_image(image_path): with open(image_path, mode=\u0026#39;rb\u0026#39;) as img: image_bytes = img.read() return image_bytes def detect_text(image_path): image_bytes = get_bytes_of_image(image_path) return reko.detect_text( Image={ \u0026#39;Bytes\u0026#39;: image_bytes } ) def get_smash_power(image_path): smash_power = \u0026#39;0\u0026#39; detect_res = detect_text(image_path) for detected in detect_res[\u0026#39;TextDetections\u0026#39;]: polygon = detected[\u0026#39;Geometry\u0026#39;][\u0026#39;Polygon\u0026#39;] if polygon[0][\u0026#39;X\u0026#39;] \u0026gt; target_area[\u0026#39;A\u0026#39;][\u0026#39;X\u0026#39;] and polygon[0][\u0026#39;Y\u0026#39;] \u0026gt; target_area[\u0026#39;A\u0026#39;][\u0026#39;Y\u0026#39;] \\ and polygon[1][\u0026#39;X\u0026#39;] \u0026lt; target_area[\u0026#39;B\u0026#39;][\u0026#39;X\u0026#39;] and polygon[1][\u0026#39;Y\u0026#39;] \u0026gt; target_area[\u0026#39;B\u0026#39;][\u0026#39;Y\u0026#39;] \\ and polygon[2][\u0026#39;X\u0026#39;] \u0026lt; target_area[\u0026#39;C\u0026#39;][\u0026#39;X\u0026#39;] and polygon[2][\u0026#39;Y\u0026#39;] \u0026lt; target_area[\u0026#39;C\u0026#39;][\u0026#39;Y\u0026#39;] \\ and polygon[3][\u0026#39;X\u0026#39;] \u0026gt; target_area[\u0026#39;D\u0026#39;][\u0026#39;X\u0026#39;] and polygon[3][\u0026#39;Y\u0026#39;] \u0026lt; target_area[\u0026#39;D\u0026#39;][\u0026#39;Y\u0026#39;]: smash_power = detected[\u0026#39;DetectedText\u0026#39;] return smash_power if __name__ == \u0026#39;__main__\u0026#39;: image_path = sys.argv[1] smash_power = get_smash_power(image_path) print(smash_power) 実行してみます。\n$ python detect_smash_power.py 2020082121055400-0E7DF678130F4F0FA2C88AE72B47AFDF.jpg 7,816,946 取れました。\nUse Amazon Rekognition to detect world strength from captured images of Smash Bros SPECIAL. まとめ Amazon Rekognition を使ってスマブラ SP の世界戦闘力を抜き出してみました。キャプチャ画像にはキャプチャした日時が含まれているので、その値も含めれば世界戦闘力の推移がグラフ化できそうです。\n",
    "permalink": "https://michimani.net/post/aws-extract-value-of-smash-blos-sp-power-by-rekognition/",
    "title": "Amazon Rekognition を使ってスマブラ SP のキャプチャ画像から世界戦闘力を抜き出す"
  },
  {
    "contents": "Go 歴半年くらいで、主に Serverless Framework で Lambda 関数を書くという用途で使っています。今回は Go の Web アプリケーションフレームワーク Revel を触ってみようということで、公式のチュートリアルをやってみます。\n目次 概要 公式チュートリアル インストール アプリケーション作成 起動 リクエストの処理フロー Hello World アプリを作ってみる まとめ 概要 Go の Web アプリケーションフレームワークである Revel の公式チュートリアルを一通りやってみます。\nGo のバージョンは 1.14.4 です。\n$ go version go version go1.14.4 darwin/amd64 公式チュートリアル ここからは、 Revel の公式チュートリアルをやっていきます。\nIntroduction | Revel - A Web Application Framework for Go! インストール まずは Revel フレームワークと Revel コマンドをインストールします。\n$ go get github.com/revel/revel $ go get github.com/revel/cmd/revel ヘルプとバージョンを確認しておきます。\n$ revel --help Usage: revel [OPTIONS] \u0026lt;command\u0026gt; Application Options: -v, --debug If set the logger is set to verbose --historic-run-mode If set the runmode is passed a string not json --historic-build-mode If set the code is scanned using the original parsers, not the go.1.11+ -X, --build-flags= These flags will be used when building the application. May be specified multiple times, only applicable for Build, Run, Package, Test commands --gomod-flags= These flags will execute go mod commands for each flag, this happens during the build process Help Options: -h, --help Show this help message Available commands: build clean new package run test version $ revel version Revel executing: displays the Revel Framework and Go version Revel Framework\t:\tUnknown\t(1.0.0 remote master branch) Revel Cmd\t:\t1.0.0\t(1.0.0 remote master branch) Revel Modules\t:\tUnknown\t(1.0.0 remote master branch) Go Location:/usr/local/bin/go go version go1.14.4 darwin/amd64 アプリケーション作成 続いて、 my-app という名前の Revel アプリケーションを作成します。\n$ revel new -a my-app Revel executing: create a skeleton Revel application Your application has been created in: /Users/hoge/Projects/go-revel/my-app You can run it with: revel run -a my-app 作成が完了すると、下記のようなディレクトリ構造が作られます。\n$ tree -a -L 2 . ├── .gitignore ├── README.md ├── app │ ├── controllers │ ├── init.go │ ├── routes │ ├── tmp │ └── views ├── conf │ ├── app.conf │ └── routes ├── go.mod ├── go.sum ├── messages │ └── sample.en ├── none ├── public │ ├── css │ ├── fonts │ ├── img │ └── js ├── target │ └── app └── tests └── apptest.go 起動 revel run コマンドでアプリケーションを起動します。\n$ cd my-app $ revel run -a my-app Revel executing: run a Revel application WARN 19:51:15 harness.go:175: No http.addr specified in the app.conf listening on localhost interface only. This will not allow external access to your application Changed detected, recompiling Parsing packages, (may require download if not cached)... Completed INFO 19:51:25 app run.go:34: Running revel server INFO 19:51:25 app plugin.go:9: Go to /@tests to run the tests. Revel engine is listening on.. localhost:59472 Revel proxy is listening, point your browser to : 9000 Time to recompile 10.823098593s コンパイルが完了したあと http://localhost:9000 にアクセスすると、ページが表示されます。\n右に見えている三角アイコンをクリックすると、デバッグ情報を確認することができます。\nちなみに Revel では Go のプログラムを変更した時点で自動で再コンパイルが実行されます。\nリクエストの処理フロー では、実際のリクエストが処理される流れを見てみます。\nルーティング まず、リクエストのルーティングは conf/routes で設定します。\n# Routes Config # # This file defines all application routes (Higher priority routes first) # module:testrunner # module:jobs GET / App.Index App.Index とは、対応するコントローラーのメソッドを表します。\nコントローラー 前述の通り、 conf/routes 内でルーティングに対応するコントローラーのメソッドが指定されています。 App.Index とは app/controllers/app.go 内の Index() メソッドを指します。\npackage controllers import ( \u0026#34;github.com/revel/revel\u0026#34; ) type App struct { *revel.Controller } func (c App) Index() revel.Result { return c.Render() } テンプレートファイル このルーティングに対応するテンプレートファイルは app/views/App/Index.html となります。\n{{set . \u0026#34;title\u0026#34; \u0026#34;Home\u0026#34;}} {{template \u0026#34;header.html\u0026#34; .}} \u0026lt;header class=\u0026#34;jumbotron\u0026#34; style=\u0026#34;background-color:#A9F16C\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;span6\u0026#34;\u0026gt; {{template \u0026#34;flash.html\u0026#34; .}} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{template \u0026#34;footer.html\u0026#34; .}} コントローラーからテンプレートへのデータ渡し コントローラーからテンプレートへデータを渡すには、コントローラーとテンプレートをそれぞれ次のように変更します。\nコントローラー\nfunc (c App) Index() revel.Result { greeting := \u0026#34;Aloha World\u0026#34; return c.Render(greeting) } テンプレート\n\u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{.greeting}}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; Hello World アプリを作ってみる 続いて、簡単なフォームを用いたアプリケーションを作成してみます。\nテンプレートファイルの変更・追加 既存のテンプレートファイル app/views/App/Index.html に下記のフォームを追加します。\n\u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;span6\u0026#34;\u0026gt; {{template \u0026#34;flash.html\u0026#34; .}} + \u0026lt;form action=\u0026#34;/App/Hello\u0026#34; method=\u0026#34;GET\u0026#34;\u0026gt; + \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;myName\u0026#34; /\u0026gt;\u0026lt;br/\u0026gt; + \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Say hello!\u0026#34; /\u0026gt; + \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{template \u0026#34;footer.html\u0026#34; .}} そして、フォーム送信後に表示されるページのテンプレートファイルを、下記の内容で app/views/App/Hello.html という名前で作成します。\n{{set . \u0026#34;title\u0026#34; \u0026#34;Hello page\u0026#34;}} {{template \u0026#34;header.html\u0026#34; .}} \u0026lt;h1\u0026gt;Hello {{.myName}}\u0026lt;/h1\u0026gt; \u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;Back to form\u0026lt;/a\u0026gt; {{template \u0026#34;footer.html\u0026#34; .}} コントローラーにメソッド追加 続いて、コントローラー app/controllers/app.go に下記のメソッドを追加します。\nfunc (c App) Hello(myName string) revel.Result { return c.Render(myName) } ルーティングの追加 最後に、 conf/routes にルーティングを追加します。\nGET / App.Index + GET /App/Hello App.Hello これで準備が整ったので、フォームに値を送信してみます。\nバリデーションの追加 今作ったフォームにバリデーションを追加してみます。バリデーション項目としては、必須チェックと最低文字数チェックです。\nまずはコントローラー app/controllers/app.go の Hello() メソッドを次のように変更します。\nfunc (c App) Hello(myName string) revel.Result { + c.Validation.Required(myName).Message(\u0026#34;Your name is required!\u0026#34;) + c.Validation.MinSize(myName, 3).Message(\u0026#34;Your name is not long enough!\u0026#34;) + + if c.Validation.HasErrors() { + c.Validation.Keep() + c.FlashParams() + return c.Redirect(App.Index) + } return c.Render(myName) } エラーメッセージの表示は、テンプレートファイル app/views/App/Index.html にインポートされている app/views/flash.html 内で行われます。\n{{if .flash.success}} \u0026lt;div class=\u0026#34;alert alert-success\u0026#34;\u0026gt; {{.flash.success}} \u0026lt;/div\u0026gt; {{end}} {{if or .errors .flash.error}} \u0026lt;div class=\u0026#34;alert alert-danger\u0026#34;\u0026gt; {{if .flash.error}} {{.flash.error}} {{end}} \u0026lt;ul style=\u0026#34;margin-top:10px;\u0026#34;\u0026gt; {{range .errors}} \u0026lt;li\u0026gt;{{.}}\u0026lt;/li\u0026gt; {{end}} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; {{end}} そして、フォーム部分を次のように変更します。\n\u0026lt;form action=\u0026#34;/App/Hello\u0026#34; method=\u0026#34;GET\u0026#34;\u0026gt; - \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;myName\u0026#34; /\u0026gt;\u0026lt;br/\u0026gt; + {{with $field := field \u0026#34;myName\u0026#34; .}} + \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;{{$field.Name}}\u0026#34; value=\u0026#34;{{$field.Flash}}\u0026#34;/\u0026gt;\u0026lt;br/\u0026gt; + {{end}} \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Say hello!\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; まとめ Go の Web アプリケーションフレームワーク Revel の公式チュートリアルをやってみた話でした。\nRevel を使ったサンプルアプリケーションはいくつか公開されているので、次回は DB へのアクセスも含めたサンプルアプリケーションを触ってみようと思います。\n",
    "permalink": "https://michimani.net/post/programming-get-started-revel-go-web-framework/",
    "title": "Go の Web アプリケーションフレームワーク Revel を試してみた"
  },
  {
    "contents": "プログラムを書いているときに、特定の値がリストなどのオブジェクトに含まれているかどうかをチェックする場面はよくあると思います。久々に入門 Python 3 を読んでいたらタイトルのような内容がコラム的に書いてあったので、実際にどれくらい速いのか試してみました。\n前提 Python 3.7 値が含まれているかどうかのみチェックする 値の重複、順序は考慮しない やってみる 下記のようなスクリプトを用意して、連続して実行してみます。\nitem_in_list.py\nfrom decimal import Decimal import sys import time TARGET_ITEM = \u0026#39;text_00001000\u0026#39; num = 1 text_list = list() def item_in_list(target_item, target_list): in_start = time.time() print(target_item in target_list) in_proccess = time.time() - in_start print(f\u0026#39;[item in list]: {Decimal(in_proccess)}\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: num = int(sys.argv[1]) for n in range(0, num): text_list.append(f\u0026#39;text_{str(n).zfill(8)}\u0026#39;) item_in_list(TARGET_ITEM, text_list) item_is_subset.py\nfrom decimal import Decimal import sys import time TARGET_ITEM = \u0026#39;text_00001000\u0026#39; num = 1 text_set = set() def item_is_subset(target_item, target_set): sub_start = time.time() print({target_item} \u0026lt; text_set) sub_proccess = time.time() - sub_start print(f\u0026#39;[is subset]: {Decimal(sub_proccess)}\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: num = int(sys.argv[1]) for n in range(0, num): text_set.add(f\u0026#39;text_{str(n).zfill(8)}\u0026#39;) item_is_subset(TARGET_ITEM, text_set) check.sh\npython item_in_list.py $1 python item_is_subset.py $1 あらかじめ NUM で指定した数の要素を持つ list と set を生成しておいて、それぞれに対して特定の要素 TARGET_ITEM を含むかどうかをチェックし、かかった時間を計測します。\nとりあえず要素数が 10 の場合で実行してみます。\n$ sh check.sh 10 False [item in list]: 0.000027179718017578125 False [is subset]: 0.00002574920654296875 この時点でも少しだけ set のほうが速いです。ただ、何度か実行していると 10 回に 1 回くらいは list のほうが速くなる時がありました。\n要素数 = 100 $ sh check.sh 100 False [item in list]: 0.000029087066650390625 False [is subset]: 0.000027179718017578125 要素数 = 10000 $ sh check.sh 10000 True [item in list]: 0.0000660419464111328125 True [is subset]: 0.0000431537628173828125 要素数 = 1000000 $ sh check.sh 1000000 True [item in list]: 0.000080108642578125 True [is subset]: 0.0000650882720947265625 要素数 = 100000000 $ sh check.sh 100000000 True [item in list]: 0.0016210079193115234375 True [is subset]: 0.00047206878662109375 まとめ set のほうが速そう。\n",
    "permalink": "https://michimani.net/post/programming-in-list-vs-is-subset-python/",
    "title": "[Python] 特定の値が存在するかチェックするときは list より set のほうが速い"
  },
  {
    "contents": "このブログでは静的サイトジェネレーターの Hugo を使っていますが、各記事の OGP 画像とアイキャッチ画像は手動で作っていました。ただ、記事を書く頻度が高くなってくると面倒な作業になってくるので、 tcardgen というツール (Go パッケージ) を使って自動生成してみます。\nなお、このブログを書く際に下記のブログ記事を参考にさせていただきました。\n目次 tcardgen とは OGP 画像を生成する テンプレートの準備 フォントファイルの準備 config ファイルの準備 生成 各記事内での設定 スクリプト化しておくと便利 まとめ tcardgen とは tcardgen とは、Front Matter として toml または yaml フォーマットで記述されたタイトルやカテゴリ名、タグ名、執筆者名などをもとに Twitter Card 用の画像 (OGP) を生成してくれるツール (Go のパッケージ) です。 Twitter Card Image Generator の略称のようです。\nソースは GitHub で公開されています。\n使用するには、下記のコマンドでインストールします。\n$ go get github.com/Ladicle/tcardgen OGP 画像を生成する では、早速 OGP 画像を生成していきたいと思います。手順としては、下記の 4 つに分けています。\nテンプレートの準備 フォントファイルの準備 config ファイルの準備 生成 順番にやっていきましょう。\nテンプレートの準備 まずは、生成される OGP 画像のベースとなるテンプレート画像を準備します。 GitHub リポジトリの examples 内にあるテンプレートを参考にして、サイズは 1200 x 628 (px) で作成します。\nこのファイルを、 _ogp_template.png というファイル名で ./static/images/og ディレクトリに保存しておきます。\nちなみにこれと同じレイアウトのテンプレートについては、 PSD ファイルで作ってあるのでよかったら使ってください。\n[ダウンロード] hugo_ogp_template.psd フォントファイルの準備 続いて、 OGP 画像内の文字にあてるフォントファイルを用意します。GitHug リポジトリでは Kinto というフォントを使用していますが、今回は HackGen (白源) というフォントを使用してみます。\nこのフォントはプログラミング向けのフォントとして公開されており、個人的にはめちゃくちゃ読みやすくて好きなフォントです。\nGitHub からダウンロードして、今回は HackGen-Regular.ttf と HackGen-Bold.ttf を ./static/fonts/hackgen/HackGen ディレクトリに保存します。\nconfig ファイルの準備 続いて、 config ファイルを作成します。この config ファイル内では、 OGP 画像内の文字の位置や大きさ、使用するフォントを指定します。 yaml 形式で作成します。\n今回は、先ほど作成したテンプレート画像に合うように、次のような config ファイルを作成しました。\ntitle: fontSize: 65 fontStyle: Bold info: start: pX: 96 pY: 536 fontSize: 30 fontStyle: Regular tags: start: pX: 1190 pY: 495 fontStyle: Regular fontSize: 0 これを tcargen.yaml というファイル名で、 Hugo プロジェクトの直下に保存します。\n生成 では実際に OGP 画像を生成してみます。と、その前に、まずは生成する対象の記事を作成します。\n$ post/test/test-post-at-2020-07-31.md /my/local/path/michimani.net/content/post/test/test-post-at-2020-07-31.md created 後ほど詳細については書きますが、私の環境では次のような Markdown ファイルが生成されます。\n--- title: \u0026#34;Test Post at 2020 07 31\u0026#34; date: 2020-07-31T17:51:11+09:00 draft: false author: [\u0026#34;michimani\u0026#34;] categories: [\u0026#34;Test\u0026#34;] tags: [\u0026#34;Hugo\u0026#34;, \u0026#34;tcardgen\u0026#34;] archives: [\u0026#34;2020\u0026#34;, \u0026#34;2020-07\u0026#34;] eyecatch: \u0026#34;/images/og/test-post-at-2020-07-31.png\u0026#34; ogimage: \u0026#34;/images/og/test-post-at-2020-07-31.png\u0026#34; comments: true adsense: false description: \u0026#34;\u0026#34; url: \u0026#34;/post/test-post-at-2020-07-31/\u0026#34; --- tcardgen を使って OGP 画像を生成する場合、 categories と tags には何かしら指定する必要があるので、一旦次のように変更します。\n- categories: [\u0026#34;\u0026#34;] - tags: [\u0026#34;\u0026#34;] + categories: [\u0026#34;Test\u0026#34;] + tags: [\u0026#34;Hugo\u0026#34;, \u0026#34;tcardgen\u0026#34;] これで準備が整ったので、次のコマンドを実行して OGP 画像を生成します。\ntcardgen \\ --fontDir ./static/fonts/hackgen/HackGen \\ --outDir static/images/og \\ --template static/images/og/_ogp_template.png \\ --config tcargen.yaml \\ ./content/post/test/test-post-at-2020-07-31.md すると、次のような画像が生成されます。\nファイル名は記事 Markdown ファイルのファイル名の拡張子が .png に変わる形で test-post-at-2020-07-31.png となります。\n生成されるディレクトリは --outDir で指定したディレクトリとなります。\n各記事内での設定 各記事内での設定ですが、私の場合は記事のテンプレートとなる archetypes/default.md を次のような内容にしています。\n--- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} draft: false author: [\u0026#34;michimani\u0026#34;] categories: [\u0026#34;\u0026#34;] tags: [\u0026#34;\u0026#34;] archives: [\u0026#34;{{ dateFormat \u0026#34;2006\u0026#34; .Date }}\u0026#34;, \u0026#34;{{ dateFormat \u0026#34;2006-01\u0026#34; .Date }}\u0026#34;] eyecatch: \u0026#34;/images/og/{{ .Name }}.png\u0026#34; ogimage: \u0026#34;/images/og/{{ .Name }}.png\u0026#34; comments: true adsense: false description: \u0026#34;\u0026#34; url: \u0026#34;/{{ .Type }}/{{ .Name }}/\u0026#34; --- eyecatch については各記事のタイトル部分に表示するために使っており、 ogimage は各テーマの下の layout/partials/head.html 内で次のように指定するために使っています。 (これは indigo テーマの場合なので、もしかするとテーマによっては構造が違うかもしれません)\n\u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;{{ .Site.BaseURL }}{{ .Params.ogimage }}\u0026#34;\u0026gt; archetypes/default.md 内でこのように書いておくことで、 tcardgen によって OGP 画像を生成すると自動的にその記事の OGP 画像として設定されることになります。\nスクリプト化しておくと便利 OGP 画像を生成するためにはコマンドの実行が必要になりますが、オプションがいくつかあって毎回実行するのはミスが発生する可能性もあります。\nなので、次のようなシェルスクリプトを作成しておいて、 OGP 画像作成時には対象の記事 Markdown へのパスを指定するだけにしています。\nif [ $# != 1 ] || [ $1 = \u0026#34;\u0026#34; ]; then echo \u0026#34;One parameters are required\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;string: path to markdown file of target post\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;example command\u0026#34; echo \u0026#34;\\t$ sh ./scripts/gen_ogp.sh ./content/post/test/test.md\u0026#34; exit fi TARGET_POST_PATH=$1 tcardgen \\ --fontDir ./static/fonts/hackgen/HackGen \\ --outDir static/images/og \\ --template static/images/og/_ogp_template.png \\ --config tcargen.yaml \\ $TARGET_POST_PATH まとめ tcardgen というツール (Go パッケージ) を使って OGP 画像を自動生成してみた話でした。\n冒頭で紹介したカックさん ( @kakakakakku ) のブログで tcardgen の存在を知ったのですが、見た瞬間から「あ、これ便利なやつや」と思って導入を考えていました。\nこれまでは、例えば AWS に関する記事であれば、その中に出てくるサービスのアイコンを使った OGP 画像を作ったりしていました。しかし、毎回作成していると結構な時間を消費するので、今後は今回紹介した方法で作成するようにしようと思っています。\nHugo で作ったブログで各記事の OGP 画像を楽に生成したいと考えている方には、導入をおすすめします！\n",
    "permalink": "https://michimani.net/post/development-generate-ogp-image-by-tcardgen-in-hugo/",
    "title": "[Hugo] tcardgen を使って OGP 画像を自動生成する"
  },
  {
    "contents": "Serverless Meetup Japan Virtual #3 にオンラインで参加したので、その参加レポート・メモです。\nServerless Meetup Japan Virtual #3 - connpass Twitter のハッシュタグは #serverlessjp です。\n前回の様子はこちら。\n目次 タイムテーブル セッションレポート GCP の Network Endpoint Groups の新しい機能 「Serverless NEG」を試してみる AWS CDK/AppSync/Athena/S3 select あたり まとめ タイムテーブル Timeline Title Speaker 20:45-21:00 Social ／ 21:00-21:05 Opening Talk 吉田真吾 さん 21:05-21:25 GCP の Network Endpoint Groups の新しい機能 「Serverless NEG」を試してみる 五十嵐透 さん 21:25-21:35 Meetup Zoom参加メンバー 21:35-21:40 Forkwellさん告知!! 永田りさ さん 21:40-22:00 AWS CDK/AppSync/Athena/S3 select あたり 和田祐介 さん 22:00-22:30 Meetup Zoom参加メンバー 22:30- Closing セッションレポート GCP の Network Endpoint Groups の新しい機能 「Serverless NEG」を試してみる Network Endpoint Groups とは GCP のリソース エンドポイントまたはサービスのグループを指定する構成オブジェクト GCP の一部のロードバランサのバックエンド (アプリケーションの手前) として使用する NEG と略される 公式ドキュメント : ネットワーク エンドポイント グループの概要 | 負荷分散 | Google Cloud NEG は Compute Engine リソース (Network services リソースではない) NEG の種類 Zonal (GA) VM インスタンスや GKE 上の Pod を指定 Internet (GA) Google Cloud の外部にあるサービスを指定 Serverless (Beta) Serverless NEG とは NEG の新しい機能 2020/07/08 に Beta 版がリリース GCP のサーバレスサービスをロードバランサのバックエンドサービスとして指定できる ロードバランサにつけられる機能の恩恵を受けられる Cloud Armor (WAF) Cloud CDN これらをマネージドなサービスとして使える Serverless NEG を実際に動かしてみる Fowerding rules -\u0026gt; Target proxies -\u0026gt; Backend serviceies Serverless NEG は現時点で Beta なので今ソースでは見れない (作れない) CLI から操作する まとめ Serverless NEG によって GCP でサーバレスなシステムを作りやすくなった Q\u0026amp;A Q. AWS だと ALB のターゲットグループという感じで、ターゲットに Lambda を置いたり、同時に EC2 を置いたりもできたが、 NEG でもできる？ A. できる。Serverless NEG と Zonal NEG を一緒に使えば実現できる。 Q. いま運用しているシステムで使えるところはある？ A. 現在は Zonal を主に使っている。が、 Cloud Run と App Engine を同じドメインで使うことはないので\u0026hellip;。ただユースケースとしては色々考えられそう。オンプレからのリフト\u0026amp;シフトするときなどにも使えそう。 Q. パスで重み付けは可能か？ A. 現時点ではわからない。が、必要そうな機能なので今後に期待。 その他 Cloud Armor がつけられることにより IP 制限が可能になった。 Cloud Run は作成した時点でエンドポイントが作られるので、それを NEG 経由のアクセスのみに制限できればいい AWS CDK/AppSync/Athena/S3 select あたり サーバーレスで ETL AWS Glue (なんか変換する人？) Amazon Athena (S3 のデータに対してクエリ発行してくれる人？) 名前は知っているが、アプリケーションに組み込むイメージがあまり沸かない AWS Glue 構成要素 データカタログ (Table) スキーマの定義 データの実体は S3, DynamoDB などに持たせて、 Glue はスキーマの定義のみ持つ この Table に対して Athena などからクエリ発行 サーバレス ETL (Job) ETL (データ変換処理) の本体 スクリプトは Python Shell または Spark 大規模向けのジョブ 小規模だと、起動時間と課金額がネック 最低実行時間が 10 分 ざっくり、最低でも数万円/月 代わりに Step Functions + Lambda で構築するという選択肢 (肌感で 5,000円/月 くらい) オーケストレーショ (Workflow) クローラとジョブを組み合わせて一連の処理を定義 Step Functions と同じような機能 構成要素は CloudFormation のリソース名称を確認するとわかりやすい Athena との関係 Amazon Athena データソースを S3 として、初期状態で Glue のデータカタログ (Table) と統合されている S3 をデータソースとする Glue のデータカタログにクエリを発行し、結果を S3 にエクスポートする 具体的なユースケース 今回 \u0026ldquo;ファイル\u0026rdquo; に焦点を当てた理由\n複数のサービスの情報から一つのサービスのバックエンドに見せかける仕組みが今後増えそう いろんなサービスを組み合わせて生活していく 大抵のサービスでは CSV 出力が可能 Webhook や EventBridge などいろんな連携方法があるが、ファイルを入力とする連携も残る チケット ID からセッションデータを引き当てる\nRDB であれば各テーブルに CSV データを入れて、取得時に JOIN サーバレスだと DynamoDB S3 Select ゴール : S3 to S3 の ETL 今回は小規模なので、 Glue Job は使わず、 Step Functions + Lambda で構築 CDK でリソースを作成してみる TypeScript などでコードから CloudFormation テンプレートを生成 アプリケーションエンジニアがインフラを書く Athena を使うには、まず Glue Table リソースを作成する Q\u0026amp;A Q. 今回の方法だと、どれくらいのデータ量までなら？ A. 300 MB くらいなら。行数なら 100 万行くらい。 Q. ファイル to ファイルの ETL という前提がなければ、最初に考えうる構成は？ A. データストアが S3 ではなく DynamoDB になるかなと。 Q. その場合だと Glue 等でやりづらくなるポイントは？ A. クエリ結果が S3 に出力されるので、もう一度そこから取得する必要が出てくる。 Q. CDK に切り替えたきっかけは？ A. もともとベータ版で使っていたが、 GA のタイミングで 100 % 切り替えた。(from SAM) Q. みんなで？ A. まずは個人で使って、それを横展開。チームで使う場合に教材になるようなものが欲しかったため。 その他 CloudFormation のリソース名で確認するのは凄く良い S3 ファーストの考え方は凄く良い S3 Select をゴールにするためにアーキテクチャを考えるユーザも多い 圧縮されたファイルに対してクエリ実行できる良さ まとめ Serverless Meetup Japan Virtual #3 の参加メモでした。\n今回は GCP の話と AWS の話ということで、 Serverless というキーワードで複数のクラウドの話を聞くことができて楽しかったです。\nGCP の方はほぼ使用経験がないのですが、 AWS だとこの部分にあてはまるのかなーとかを考えながら聞いているのは面白かったです。 AWS だけでなく他のクラウドについても触ってみると、より AWS の理解が深まるのかなと思いました。ただ、やりたいことが増えていって沼にハマってしまいそうという思いもあります。\n和田さんの話の中では、 Glue というサービスの構成要素の話がすごくわかりやすかったです。サービスがどんな機能を持っていて、どのようなリソースで構成されているかを知るためには CloudFormation のリソース名を確認してみるというのも、あーなるほど！と思いました。\nCDK に関しては、個人的にも IaC するうえで凄く便利なツールだと思っています。\nあと、 S3 Select の話もありましたが、その便利さの恩恵を受けるためにアーキテクチャを設計することもあるという話があり、相当便利な機能なんだろうなと思いました。\n今回もサーバレスに関する面白い話を聞けてよかったです！次回も楽しみです。\n",
    "permalink": "https://michimani.net/post/event-serverless-meetup-japan-virtual-3/",
    "title": "[レポート] Serverless Meetup Japan Virtual #3 の参加メモ #serverlessjp"
  },
  {
    "contents": "AWS CLI を使って Amazon S3 に格納されているオブジェクトのメタデータを変更します。思っていた方法と違ったので、備忘録のための書き残しておきます。\n目次 TL;DR やってみる 対象のオブジェクトを追加 オブジェクトの情報確認 メタデータの更新 メタデータの削除 まとめ TL;DR s3api copy-object コマンドを使う update-metadata みたいなコマンドはない 変更 (追加・削除) というよりは 上書き という感覚でやる やってみる AWS CLI のバージョンは、 v2 の現時点の最新バージョンで実行します。\n$ aws --version aws-cli/2.0.35 Python/3.7.4 Darwin/19.5.0 botocore/2.0.0dev39 対象のオブジェクトを追加 まずはメタデータを変更する対象となるオブジェクトを追加します。ファイルは何でも良いので、今回は下記のような JSON ファイルを追加します。\n$ cat sample.json | jq . { \u0026#34;message\u0026#34;: \u0026#34;AWS CLI ha iizo\u0026#34; } 今回の対象バケット名は s3-metadata-test-michimani とします。\n$ aws s3 cp ./sample.json s3://s3-metadata-test-michimani/sample.json upload: ./sample.json to s3://s3-metadata-test-michimani/sample.json ちなみに s3 コマンドは S3 に対する API 高レベルで抽象化したコマンドで、今回のようにバケットにオブジェクトを追加したり、逆にローカルにオブジェクトをコピーしてきたりする場合には楽に使えます。\n$ aws s3 help NAME s3 - DESCRIPTION This section explains prominent concepts and notations in the set of high-level S3 commands provided. オブジェクトの情報確認 オブジェクトの追加ができたら、現時点でのオブジェクトの情報を確認します。確認には、 s3api head-object コマンドを使います。\n$ aws s3api head-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T22:30:41+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 35, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Metadata\u0026#34;: {} } s3api コマンドは先程の s3 コマンドとは違い、 S3 に対する操作を行う各 API を実行するコマンドになっています。 S3 を操作するコマンドにはもう一つ s3control コマンドがありますが、このコマンドではバッチオペレーションやアクセスポイントに関する操作を実行することができます。\nメタデータの更新 では、先程追加したオブジェクトに対してメタデータの更新 (追加・削除) を実行してみます。\nなお、 AWS CLI では S3 オブジェクトに対してメタデータを更新 (追加・削除) する update-metadata みたいなコマンドはありません。なので、 s3api copy-object コマンドを使います。\n$ aws s3api copy-object help ... SYNOPSIS copy-object [--acl \u0026lt;value\u0026gt;] --bucket \u0026lt;value\u0026gt; [--cache-control \u0026lt;value\u0026gt;] [--content-disposition \u0026lt;value\u0026gt;] [--content-encoding \u0026lt;value\u0026gt;] [--content-language \u0026lt;value\u0026gt;] [--content-type \u0026lt;value\u0026gt;] --copy-source \u0026lt;value\u0026gt; [--copy-source-if-match \u0026lt;value\u0026gt;] [--copy-source-if-modified-since \u0026lt;value\u0026gt;] [--copy-source-if-none-match \u0026lt;value\u0026gt;] [--copy-source-if-unmodified-since \u0026lt;value\u0026gt;] [--expires \u0026lt;value\u0026gt;] [--grant-full-control \u0026lt;value\u0026gt;] [--grant-read \u0026lt;value\u0026gt;] [--grant-read-acp \u0026lt;value\u0026gt;] [--grant-write-acp \u0026lt;value\u0026gt;] --key \u0026lt;value\u0026gt; [--metadata \u0026lt;value\u0026gt;] [--metadata-directive \u0026lt;value\u0026gt;] [--tagging-directive \u0026lt;value\u0026gt;] [--server-side-encryption \u0026lt;value\u0026gt;] [--storage-class \u0026lt;value\u0026gt;] [--website-redirect-location \u0026lt;value\u0026gt;] [--sse-customer-algorithm \u0026lt;value\u0026gt;] [--sse-customer-key \u0026lt;value\u0026gt;] [--sse-customer-key-md5 \u0026lt;value\u0026gt;] [--ssekms-key-id \u0026lt;value\u0026gt;] [--ssekms-encryption-context \u0026lt;value\u0026gt;] [--copy-source-sse-customer-algorithm \u0026lt;value\u0026gt;] [--copy-source-sse-customer-key \u0026lt;value\u0026gt;] [--copy-source-sse-customer-key-md5 \u0026lt;value\u0026gt;] [--request-payer \u0026lt;value\u0026gt;] [--tagging \u0026lt;value\u0026gt;] [--object-lock-mode \u0026lt;value\u0026gt;] [--object-lock-retain-until-date \u0026lt;value\u0026gt;] [--object-lock-legal-hold-status \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] バケット名、コピー元オブジェクト、コピー先オブジェクトが必須オプションになっています。メタデータを更新 (追加・削除) する場合は、 --cache-control や --metadata オプションに加えて --metadata-directive オブションを使って実行します。\nCacheControl の追加 まずはキャッシュに関するメタデータを追加してみます。\n$ aws s3api copy-object \\ --bucket s3-metadata-test-michimani \\ --copy-source s3-metadata-test-michimani/sample.json \\ --key sample.json \\ --cache-control \u0026#34;public, max-age=31536000\u0026#34; \\ --metadata-directive REPLACE オプションの指定で注意したいのは、 --copy-source オプションで指定する値には バケット名も含める必要がある 点です。\nまた、 --metadata-directive を指定しないと、下記のようなエラーになります。\nAn error occurred (InvalidRequest) when calling the CopyObject operation: This copy request is illegal because it is trying to copy an object to itself without changing the object\u0026rsquo;s metadata, storage class, website redirect location or encryption attributes.\n--metadata-directive オプションに対する値は COPY または REPLACE を指定します。\n--metadata-directive (string) Specifies whether the metadata is copied from the source object or replaced with metadata provided in the request. Possible values: o COPY o REPLACE REPLACE を指定すると、 copy-object 実行時に指定したメタデータで上書きすることができます。この 上書き には注意が必要なので、これについては後ほど書きます。\nでは、念のためメタデータが追加できたか確認してみます。\n$ aws s3api head-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T22:56:15+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 35, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;CacheControl\u0026#34;: \u0026#34;public, max-age=31536000\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: {} } CacheControl のメタデータが追加されています。\nカスタムメタデータを追加する 次に、先程の CacheControl に 加えて ユーザ独自のカスタムメタデータを追加してみます。カスタムメタデータを追加する場合は、 --metadata オプションでキーと値を指定します。\n今回は、 stage というキーで dev という値を持つメタデータを追加してみます。\n$ aws s3api copy-object \\ --bucket s3-metadata-test-michimani \\ --copy-source s3-metadata-test-michimani/sample.json \\ --key sample.json \\ --metadata stage=dev \\ --metadata env=dev,env-name=development { \u0026#34;CopyObjectResult\u0026#34;: { \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T23:06:06+00:00\u0026#34; } } 確認してみます。\n$ aws s3api head-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T23:11:08+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 35, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: { \u0026#34;env\u0026#34;: \u0026#34;dev\u0026#34;, \u0026#34;env-name\u0026#34;: \u0026#34;development\u0026#34; } } カスタムメタデータは正しく追加されましたが、先程追加した CacheControl が削除されてしまいました。これは --metadata-directive REPLACE を指定したために元のメタデータを上書きするからです。\n元のメタデータを保持したまま別のメタデータを追加する場合には、あらためて元のメタデータも指定する必要があるようです。\n$ aws s3api copy-object \\ --bucket s3-metadata-test-michimani \\ --copy-source s3-metadata-test-michimani/sample.json \\ --key sample.json \\ --cache-control \u0026#34;public, max-age=31536000\u0026#34; \\ --metadata-directive REPLACE \\ --metadata env=dev,env-name=development { \u0026#34;CopyObjectResult\u0026#34;: { \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T23:23:22+00:00\u0026#34; } } 確認してみます。\n$ aws s3api head-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T23:23:22+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 35, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;CacheControl\u0026#34;: \u0026#34;public, max-age=31536000\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: { \u0026#34;env\u0026#34;: \u0026#34;dev\u0026#34;, \u0026#34;env-name\u0026#34;: \u0026#34;development\u0026#34; } } 無事にメタデータを追加することができました。\n--metadata オプションでは、上記のように キー=値 という形式で指定する他、 JSON 形式でも指定できます。\n--metadata (map) A map of metadata to store with the object in S3. key -\u0026gt; (string) value -\u0026gt; (string) Shorthand Syntax: KeyName1=string,KeyName2=string JSON Syntax: {\u0026#34;string\u0026#34;: \u0026#34;string\u0026#34; ...} メタデータの削除 メタデータを削除する場合は、削除したいメタデータを 指定せずに a3api copy-object コマンドを実行します。\n例えば、 カスタムメタデータの stage-name を削除したい場合、次のように実行します。\n$ aws s3api head-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json { \u0026#34;AcceptRanges\u0026#34;: \u0026#34;bytes\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-28T23:25:14+00:00\u0026#34;, \u0026#34;ContentLength\u0026#34;: 35, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;6f1d414f6389f754c0a93fab4727f2d9\\\u0026#34;\u0026#34;, \u0026#34;CacheControl\u0026#34;: \u0026#34;public, max-age=31536000\u0026#34;, \u0026#34;ContentType\u0026#34;: \u0026#34;binary/octet-stream\u0026#34;, \u0026#34;Metadata\u0026#34;: { \u0026#34;env\u0026#34;: \u0026#34;dev\u0026#34; } } 削除 というよりは、やはり 上書き という意識で実行するのが良さそうです。\nまとめ AWS CLI を使って Amazon S3 に格納されているオブジェクトのメタデータを変更してみた話でした。\nやる前には、 update-metadata や put-metadata みたいなコマンドがあってそれを使えばいいと思っていましたが、そうではなかったようです。追加や削除をする場合でも、コピーして 上書きする というのがポイントかなと思いました。\n",
    "permalink": "https://michimani.net/post/aws-modify-metadata-of-s3-object/",
    "title": "AWS CLI で S3 オブジェクトのメタデータを変更する"
  },
  {
    "contents": "Amazon CloudFront の新機能として、 Cache Policy と Origin Request Policy というものが使えるようになりました。まだ日本語ドキュメントが準備されていないみたいなのですが、英語のドキュメントを元に、どのようなものなのか確認してみたいと思います。\n目次 概要 Cache Policy と Origin Request Policy Understanding cache policies Understanding origin request policies AWS CLI での操作 マネジメントコンソールでの操作 ポリシーの利用方法 料金 まとめ 概要 今朝 (現地時間だと 7/22) の AWS Recent Annoucements (What\u0026rsquo;s New) で、次のような記事がありました。\nAmazon CloudFront announces Cache Key and Origin Request Policies この記事では、次のように書かれています。\nFurther, you can configure the cache key and origin request settings independently as account-level policies that can be easily applied across multiple distributions.\nつまりどういうことかというと、これまでは 各 distribution の 各 behavior でキャッシュの時間やオリジンへのヘッダ、 Cookie の伝達などを設定していましたが、それらをアカウントレベルで管理できる ポリシー を作成できるということみたいです。\nCache Policy と Origin Request Policy その ポリシー とはどんなものなのでしょうか。ここからは公式のドキュメントを見ながら確認していきます。\nWorking with policies - Amazon CloudFront Understanding cache policies Controlling the cache key - Amazon CloudFront Cache Policy では、キャッシュキーとして含まれる値 (クエリ文字列、 HTTP ヘッダ、 Cookie) の制御と、キャッシュ時間 (Default、 Minimum、 Maximum) を設定できます。Cache Policy は自分で作成することもできますが、あらかじめ用意されている Managed Cache Policy を利用することもできます。\nCache Policy で設定できるのは次の項目です。\nInfo Name : Cache Policy の名前です。 Comment : Cache Policy に対するコメントです。 TTL settings Cache-Control および Expires ヘッダをもとに動作するキャッシュ時間を、次の 3 つの値で設定します。\nMinimum TTL Maximum TTL Default TTL Cache key contents キャッシュキーとして含む値を設定します。下記の項目について設定します。\nHeaders None Whitelist Cookies All None Whitelist All-Except Query string All None Whitelist All-Except また、クライアント (ビューア) が compressed object をサポートしている場合にそれらをキャッシュするかを設定します。\nCache compressed objects (uses Accept-Encoding header) 以上が Cache Policy で設定できる項目です。これまで各 behavior で設定していたものと同じですね。\nUnderstanding origin request policies Controlling origin requests - Amazon CloudFront Origin Request Policy では、オリジンへのリクエスト時に送信する情報を制御します。 Origin Request Policy は自分で作成することもできますし、 Cache Policy と同様にあらかじめ用意されている Managed Origin Request Policy を利用することもできます。\nOrigin Request Policy で設定できるのは次の項目です。\nInfo Name : Origin Request Policy の名前です。 Comment : Origin Request Policy に対するコメントです。 Origin request contens オリジンに対して送信する情報を、下記の項目で設定します。\nHeaders None Whitelist All viewer headers and whitelisted CloudFront-* headers All viewer headers Cookies All None Whitelist Query strings All None Whitelist 以上が Origin Request Policy で設定できる項目です。こちらも、これまで各 behavior で設定していたものと同様です。\nAWS CLI での操作 Cache Policy および Origin Request Policy については、 AWS SDK や AWS CLI を使って操作することができます。例えば、それぞれを作成する場合は、 cloudfront create-cache-policy 、 create-origin-request-policy コマンドを使います。なお、 AWS CLI のバージョンについては、この記事を書いている時点で最新の 2.0.33 および 1.18.103 で使えるようです。\n$ aws --version aws-cli/2.0.33 Python/3.7.4 Darwin/19.5.0 botocore/2.0.0dev3 $ (aws-cli-v1) $ aws --version aws-cli/1.18.103 Python/3.7.7 Darwin/19.5.0 botocore/1.17.26 Cache Policy の作成 $ aws cloudfront create-cache-policy help ... SYNOPSIS create-cache-policy --cache-policy-config \u0026lt;value\u0026gt; [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] ... $ aws cloudfront create-cache-policy --generate-cli-skeleton { \u0026#34;CachePolicyConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;DefaultTTL\u0026#34;: 0, \u0026#34;MaxTTL\u0026#34;: 0, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;ParametersInCacheKeyAndForwardedToOrigin\u0026#34;: { \u0026#34;EnableAcceptEncodingGzip\u0026#34;: true, \u0026#34;HeadersConfig\u0026#34;: { \u0026#34;HeaderBehavior\u0026#34;: \u0026#34;whitelist\u0026#34;, \u0026#34;Headers\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } }, \u0026#34;CookiesConfig\u0026#34;: { \u0026#34;CookieBehavior\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;Cookies\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } }, \u0026#34;QueryStringsConfig\u0026#34;: { \u0026#34;QueryStringBehavior\u0026#34;: \u0026#34;allExcept\u0026#34;, \u0026#34;QueryStrings\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } } } } } Origin Request Policy の作成 $ aws cloudfront create-origin-request-policy help ... SYNOPSIS create-origin-request-policy --origin-request-policy-config \u0026lt;value\u0026gt; [--cli-input-json \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] ... $ aws cloudfront create-origin-request-policy --generate-cli-skeleton { \u0026#34;OriginRequestPolicyConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;HeadersConfig\u0026#34;: { \u0026#34;HeaderBehavior\u0026#34;: \u0026#34;whitelist\u0026#34;, \u0026#34;Headers\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } }, \u0026#34;CookiesConfig\u0026#34;: { \u0026#34;CookieBehavior\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;Cookies\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } }, \u0026#34;QueryStringsConfig\u0026#34;: { \u0026#34;QueryStringBehavior\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;QueryStrings\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } } } } ちなみに、既にマネージドなポリシーはそれぞれに存在しているので、 cloudfront list-cache-policies 、 cloudfront list-origin-request-policies を実行するとそれらの情報を取得することができます。\n$ aws cloudfront list-cache-policies { \u0026#34;CachePolicyList\u0026#34;: { \u0026#34;MaxItems\u0026#34;: 100, \u0026#34;Quantity\u0026#34;: 4, \u0026#34;Items\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;CachePolicy\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;658327ea-f89d-4fab-a63d-7e88639e58f6\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;CachePolicyConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Default policy when CF compression is enabled\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Managed-CachingOptimized\u0026#34;, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000, \u0026#34;MinTTL\u0026#34;: 1, \u0026#34;ParametersInCacheKeyAndForwardedToOrigin\u0026#34;: { \u0026#34;EnableAcceptEncodingGzip\u0026#34;: true, \u0026#34;HeadersConfig\u0026#34;: { \u0026#34;HeaderBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;CookiesConfig\u0026#34;: { \u0026#34;CookieBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;QueryStringsConfig\u0026#34;: { \u0026#34;QueryStringBehavior\u0026#34;: \u0026#34;none\u0026#34; } } } } }, { \u0026#34;Type\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;CachePolicy\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;4135ea2d-6df8-44a3-9df3-4b5a84be39ad\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;CachePolicyConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Policy with caching disabled\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Managed-CachingDisabled\u0026#34;, \u0026#34;DefaultTTL\u0026#34;: 0, \u0026#34;MaxTTL\u0026#34;: 0, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;ParametersInCacheKeyAndForwardedToOrigin\u0026#34;: { \u0026#34;EnableAcceptEncodingGzip\u0026#34;: false, \u0026#34;HeadersConfig\u0026#34;: { \u0026#34;HeaderBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;CookiesConfig\u0026#34;: { \u0026#34;CookieBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;QueryStringsConfig\u0026#34;: { \u0026#34;QueryStringBehavior\u0026#34;: \u0026#34;none\u0026#34; } } } } }, { \u0026#34;Type\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;CachePolicy\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;b2884449-e4de-46a7-ac36-70bc7f1ddd6d\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;CachePolicyConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Default policy when compression is disabled\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Managed-CachingOptimizedForUncompressedObjects\u0026#34;, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000, \u0026#34;MinTTL\u0026#34;: 1, \u0026#34;ParametersInCacheKeyAndForwardedToOrigin\u0026#34;: { \u0026#34;EnableAcceptEncodingGzip\u0026#34;: false, \u0026#34;HeadersConfig\u0026#34;: { \u0026#34;HeaderBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;CookiesConfig\u0026#34;: { \u0026#34;CookieBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;QueryStringsConfig\u0026#34;: { \u0026#34;QueryStringBehavior\u0026#34;: \u0026#34;none\u0026#34; } } } } }, { \u0026#34;Type\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;CachePolicy\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;08627262-05a9-4f76-9ded-b50ca2e3a84f\u0026#34;, \u0026#34;LastModifiedTime\u0026#34;: \u0026#34;1970-01-01T00:00:00Z\u0026#34;, \u0026#34;CachePolicyConfig\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;Policy for Elemental MediaPackage Origin\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;Managed-Elemental-MediaPackage\u0026#34;, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;ParametersInCacheKeyAndForwardedToOrigin\u0026#34;: { \u0026#34;EnableAcceptEncodingGzip\u0026#34;: true, \u0026#34;HeadersConfig\u0026#34;: { \u0026#34;HeaderBehavior\u0026#34;: \u0026#34;whitelist\u0026#34;, \u0026#34;Headers\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ \u0026#34;origin\u0026#34; ] } }, \u0026#34;CookiesConfig\u0026#34;: { \u0026#34;CookieBehavior\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;QueryStringsConfig\u0026#34;: { \u0026#34;QueryStringBehavior\u0026#34;: \u0026#34;whitelist\u0026#34;, \u0026#34;QueryStrings\u0026#34;: { \u0026#34;Quantity\u0026#34;: 4, \u0026#34;Items\u0026#34;: [ \u0026#34;aws.manifestfilter\u0026#34;, \u0026#34;start\u0026#34;, \u0026#34;end\u0026#34;, \u0026#34;m\u0026#34; ] } } } } } } ] } } マネジメントコンソールでの操作 CloudFront のダッシュボードを開くと、サイドメニューに Policies というメニューが増えています。そこで、 Cache Policy および Origin Request Policy を管理することができます。\nそれぞれの作成画面は次のようになっています。\nCache Policy の作成画面 Origin Request Policy の作成画面 ポリシーの利用方法 作成、またはあらかじめ用意されているマネージドな Cache Policy および Origin Request Policy は、各 behavior の設定にて、利用するかどうかを設定します。\nAWS CLI の場合 AWS CLI の場合、 cloudfront create-distribution コマンドで作成しますが、 skeleton を確認してみると DefaultCacheBehavior の中に CachePolicyId と OriginRequestPolicyId という属性が追加されていました。\n{ \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;TrustedSigners\u0026#34;: { \u0026#34;Enabled\u0026#34;: true, \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] }, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;https-only\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;POST\u0026#34; ], \u0026#34;CachedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;POST\u0026#34; ] } }, \u0026#34;SmoothStreaming\u0026#34;: true, \u0026#34;Compress\u0026#34;: true, \u0026#34;LambdaFunctionAssociations\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ { \u0026#34;LambdaFunctionARN\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;EventType\u0026#34;: \u0026#34;origin-response\u0026#34;, \u0026#34;IncludeBody\u0026#34;: true } ] }, \u0026#34;FieldLevelEncryptionId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;CachePolicyId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;OriginRequestPolicyId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: true, \u0026#34;Cookies\u0026#34;: { \u0026#34;Forward\u0026#34;: \u0026#34;whitelist\u0026#34;, \u0026#34;WhitelistedNames\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } }, \u0026#34;Headers\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] }, \u0026#34;QueryStringCacheKeys\u0026#34;: { \u0026#34;Quantity\u0026#34;: 0, \u0026#34;Items\u0026#34;: [ \u0026#34;\u0026#34; ] } }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 0, \u0026#34;MaxTTL\u0026#34;: 0 }, } マネジメントコンソールの場合 マネジメントコンソールの場合は、 behabior の設定画面で Cache and origin request settings という項目が追加されているので、そこで Use a cache policy and origin request policy を選択すると、どのポリシーを利用するか設定することができます。\nUse legacy cache settings を選択した場合は、これまで通り各 behavior で設定をします。\n料金 各ポリシーの管理、利用には追加の料金は発生しません。発生するのは、これまで通り CloudFront の各種料金のみです。\nThere is no additional fee for using this feature. Regular CloudFront charges apply.\nAmazon CloudFront announces Cache Key and Origin Request Policies まとめ Amazon CloudFront の新機能、 Cache Policy と Origin Request Policy について調べてみた話でした。\nキャッシュおよびオリジンへのリクエストについて、これまでは各 behavior で個別に設定する必要があり、たとえ同じ設定を当てたいとしてもすべてを個別に変更する必要がありました。今回の Cache Policy と Origin Request Policy を使うことで、異なる behavior でも設定を統一したいという場合に、その管理が簡単になりそうです。\nそれぞれにマネージドなポリシーもあらかじめ用意されているため、特にこだわりがない場合にはそれらを利用することで CloudFront の設定自体がより簡単になるのかなと思いました。CloudFront 自体の仕様については以前に簡単にまとめた記事があるので、よかったら参考にしてみてください。\n",
    "permalink": "https://michimani.net/post/aws-cloudfront-cache-key-and-origin-request-policy/",
    "title": "CloudFront で Cache Policy と Origin Request Policy が使えるようになったみたいなので調べてみた"
  },
  {
    "contents": "AWS Chalice で API Gateway のカスタムドメイン名の作成がサポートされたので、実際に試してみたいと思います。\n目次 概要 これまで やってみる Chalice プロジェクトの作成 一旦デプロイしてみる Route 53 と ACM でドメインと SSL 証明書の設定 Chalice でカスタムドメインの設定 Route 53 のレコード作成 カスタムドメインでリクエストしてみる まとめ 概要 Python でサーバレスアプリケーションを作成するためのマイクロフレームワークである AWS Chalice ですが、今回バージョン 1.16.0 にて API Gateway のカスタムドメイン名作成がサポートされました。今まではカスタムドメイン名の作成は手動で実施する必要があったのですが、今回はそれが Chalice の機能として実装されたことになります。\n既に AWS ブログにも実際の手順について記載されているので、今回はそれを元にして実際に試してみたいと思います。\nConfiguring custom domain names with AWS Chalice | AWS Developer Blog aws/chalice: Python Serverless Microframework for AWS これまで 今回のアップデートがくるまでは、 Chalice で構築した API に対してカスタムドメインを当てる場合、マネジメントコンソールや CLI や CloudFormation などで自分で設定をする必要がありました。\n過去に Chalice で todo を管理する簡単な API を作ったのですが、カスタムドメインを当てるために次のような手順を踏んでいました。\nchalice deploy を実行 作成された API Gateway に対してカスタムドメイン名を作成 Route 53 で ALIAS レコード作成 2, 3 については CloudFormation テンプレートを作成して、シェルスクリプト経由でデプロイをするようにしていました。手作業ではなく CFn で構築できるようにしておくと楽といえば楽なのですが、できれば Chalice の機能としてカスタムドメイン名の作成ができれば良いなーとは思っていました。\nmichimani/todo-app: A sample of ToDo app. やってみる では、AWS ブログを元に実際にやってみます。その前に、 Chalice を最新版 1.16.0 にアップデートしておきます。\n$ pip install chalice --upgrade $ chalice --version chalice 1.16.0, python 3.7.7, darwin 19.5.0 Chalice プロジェクトの作成 まずは Chalice プロジェクトの作成です。\n$ chalice new-project custom-domain-app $ cd custom-domain-app $ tree -a -L 2 . ├── .chalice │ └── config.json ├── .gitignore ├── app.py └── requirements.txt 初期状態では、 / というエンドポイントで {'hello': 'world'} と返すような app.py になっています。\nfrom chalice import Chalice app = Chalice(app_name=\u0026#39;custom-domain-app\u0026#39;) @app.route(\u0026#39;/\u0026#39;) def index(): return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} chalice local コマンドでローカル実行できるので、試してみます。\n$ chalice local Serving on http://127.0.0.1:8000 Restarting local dev server. Serving on http://127.0.0.1:8000 別のターミナルを開いてアクセスしてみます。\n$ http http://127.0.0.1:8000/ HTTP/1.1 200 OK Content-Length: 17 Content-Type: application/json Date: Tue, 21 Jul 2020 21:18:13 GMT Server: BaseHTTP/0.6 Python/3.7.7 { \u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34; } ちなみに、ここで使っている http コマンドは、 HTTPie というツールです。 cURL よりも直感的でわかりやすいコマンド、オプションが特徴です。\n一旦デプロイしてみる カスタムドメインを設定する前に、まずはデプロイしてみます。\nAWS ブログではエンドポイントを Regional に変更してデプロイしていましたが、今回はデフォルトのまま Edge タイプでデプロイします。また、実行対象のリージョンは バージニア北部 (us-east-1) とします。\n$ export AWS_DEFAULT_REGION=us-east-1 $ chalice deploy Creating deployment package. Creating IAM role: custom-domain-app-dev Creating lambda function: custom-domain-app-dev Creating Rest API Resources deployed: - Lambda ARN: arn:aws:lambda:us-east-1:123456789012:function:custom-domain-app-dev - Rest API URL: https://1234567893.execute-api.us-east-1.amazonaws.com/api/ API Gateway のエンドポイントが出力されるので、確認してみます。\n$ http https://1234567893.execute-api.us-east-1.amazonaws.com/api/ HTTP/1.1 200 OK Connection: keep-alive Content-Length: 17 Content-Type: application/json Date: Tue, 21 Jul 2020 21:43:26 GMT Via: 1.1 60c021dff092d29bb692026a19f1de3b.cloudfront.net (CloudFront) X-Amz-Cf-Id: jcQfmHIp5XfuvOAMg4RKJECU8pIsVMW9qOcuMFMnkq7uTW3bvPXi2Q== X-Amz-Cf-Pop: NRT20-C4 X-Amzn-Trace-Id: Root=1-5f17617d-1e5da61a1164ffc4a7ef10d4;Sampled=0 X-Cache: Miss from cloudfront x-amz-apigw-id: QCwrrEtVoAMFUSw= x-amzn-RequestId: 8614edfa-4709-4e01-997d-21b703af194e { \u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34; } Chalice のデプロイは他のフレーム枠と違って CloudFormation を使っていないので、めちゃくちゃ早いです。\nRoute 53 と ACM でドメインと SSL 証明書の設定 続いてカスタムドメインを設定していきますが、 Chalice プロジェクトでの設定の前に Route 53 でホストゾーン作成と、 ACM で SSL 証明書を発行します。\n今回はエンドポイントのタイプを Edge にしているので、 SSL 証明書はバージニア北部 (us-east-1) で発行します。\n手順については割愛しますが、今回は下記のドメイン、及び SSL 証明書を利用します。\nmichimani.net chalice-api.michimani.net に対する SSL 証明書 ここでひとつ注意が必要なのは、 ワイルドカード証明書では chalice deploy 時にエラーになる という点です。\n$ aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\\`michimani.net.\\`]\u0026#34; [ { \u0026#34;Id\u0026#34;: \u0026#34;/hostedzone/Z3911234567890\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;michimani.net.\u0026#34;, \u0026#34;CallerReference\u0026#34;: \u0026#34;E2XXXXXX-XXXX-XXXX-XXXX-123456789012\u0026#34;, \u0026#34;Config\u0026#34;: { \u0026#34;PrivateZone\u0026#34;: false }, \u0026#34;ResourceRecordSetCount\u0026#34;: 28 } ] $ aws acm list-certificates \\ --query \u0026#34;CertificateSummaryList[?DomainName==\\`chalice-api.michimani.net\\`]\u0026#34; \\ --region us-east-1 [ { \u0026#34;CertificateArn\u0026#34;: \u0026#34;arn:aws:acm:us-east-1:123456789012:certificate/96123456-xxxx-xxxx-xxxx-f401234567890\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;chalice-api.michimani.net\u0026#34; } ] ホストゾーンの設定と SSL 証明書の発行が完了したら、それぞれ hosted zone ID (Z3911234567890) と CertificateArn (arn:aws:acm:us-east-1:123456789012:certificate/c3610514-3ef7-4478-9778-a5ab59ad168) をメモしておきます。\nChalice でカスタムドメインの設定 続いては Chalice プロジェクトでカスタムドメインの設定をします。\n.chalice/config.json に対して次のような設定を追記します。\n{ \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;app_name\u0026#34;: \u0026#34;custom-domain-app\u0026#34;, \u0026#34;stages\u0026#34;: { \u0026#34;dev\u0026#34;: { + \u0026#34;api_gateway_custom_domain\u0026#34;: { + \u0026#34;domain_name\u0026#34;: \u0026#34;chalice-api.michimani.net\u0026#34;, + \u0026#34;certificate_arn\u0026#34;: \u0026#34;arn:aws:acm:us-east-1:123456789012:certificate/96123456-xxxx-xxxx-xxxx-f401234567890\u0026#34; + }, \u0026#34;api_gateway_stage\u0026#34;: \u0026#34;api\u0026#34; } } } デプロイしてみます。\n$ chalice deploy Creating deployment package. Updating policy for IAM role: custom-domain-app-dev Updating lambda function: custom-domain-app-dev Updating rest API Creating custom domain name: chalice-api.michimani.net Creating api mapping: / Resources deployed: - Lambda ARN: arn:aws:lambda:us-east-1:123456789012:function:custom-domain-app-dev - Rest API URL: https://1234567893.execute-api.us-east-1.amazonaws.com/api/ - Custom domain name: HostedZoneId: Z2FDTNDATAQYW2 AliasDomainName: 1234567890q58.cloudfront.net これで API Gateway のカスタムドメイン名が作成されましたので、あとはこの出力にある HostedZoneId と AliasDomainName を使って Route 53 に ALIAS レコードを作成します。\nちなみに、 SSL 証明書としてワイルドカード証明書を指定した場合は、下記のようなエラーになりました。\n$ chalice deploy Creating deployment package. Updating policy for IAM role: custom-domain-app-dev Updating lambda function: custom-domain-app-dev Updating rest API Creating custom domain name: chalice-api.michimani.net Traceback (most recent call last): ... An error occurred (BadRequestException) when calling the CreateDomainName operation: The specified SSL certificate doesn\u0026#39;t exist, isn\u0026#39;t in us-east-1 region, isn\u0026#39;t valid, or doesn\u0026#39;t include a valid certificate chain. (Service: AmazonCloudFront; Status Code: 400; Error Code: InvalidViewerCertificate; Request ID: 65485f0b-7063-4eef-a4ce-2c45e56128cf) Route 53 のレコード作成 レコードの作成には AWS CLI の route53 change-resource-record-sets コマンドを使います。\nまずは --generate-cli-skeleton オプションで、パラメータのテンプレートを確認します。\n$ aws route53 change-resource-record-sets --generate-cli-skeleton { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;ChangeBatch\u0026#34;: { \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Changes\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;MX\u0026#34;, \u0026#34;SetIdentifier\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Weight\u0026#34;: 0, \u0026#34;Region\u0026#34;: \u0026#34;us-west-1\u0026#34;, \u0026#34;GeoLocation\u0026#34;: { \u0026#34;ContinentCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;CountryCode\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;SubdivisionCode\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Failover\u0026#34;: \u0026#34;SECONDARY\u0026#34;, \u0026#34;MultiValueAnswer\u0026#34;: true, \u0026#34;TTL\u0026#34;: 0, \u0026#34;ResourceRecords\u0026#34;: [ { \u0026#34;Value\u0026#34;: \u0026#34;\u0026#34; } ], \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: true }, \u0026#34;HealthCheckId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;TrafficPolicyInstanceId\u0026#34;: \u0026#34;\u0026#34; } } ] } } 今回使うのは Changes の部分です。先程 取得したカスタムドメインの HostedZoneId と AliasDomainName を使って次のような JSON を作っておきます。\n{ \u0026#34;Changes\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;chalice-api.michimani.net\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;DNSName\u0026#34;: \u0026#34;1234567890q58.cloudfront.net\u0026#34;, \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } } ] } ResourceRecordSet.AliasTarget.DNSName に AliasDomainName 、ResourceRecordSet.AliasTarget.HostedZoneId に HostedZoneId を、それぞれ指定します。\nそして、 --change-batch オプションでこの JSON を指定して実行します。このとき --hosted-zone-id で指定するのは、 Route 53 に登録したドメインの hosted zone ID です。\n$ aws route53 change-resource-record-sets \\ --hosted-zone-id Z3911234567890 \\ --change-batch \\ \u0026#39;{ \u0026#34;Changes\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;chalice-api.michimani.net\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;DNSName\u0026#34;: \u0026#34;1234567890q58.cloudfront.net\u0026#34;, \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } } ] }\u0026#39; カスタムドメインでリクエストしてみる 最後に、設定したカスタムドメインで API にリクエストしてみます。\n$ http https://chalice-api.michimani.net/ HTTP/1.1 200 OK Connection: keep-alive Content-Length: 17 Content-Type: application/json Date: Tue, 21 Jul 2020 22:08:34 GMT Via: 1.1 3c7a01dc859868cee354c75bcf600744.cloudfront.net (CloudFront) X-Amz-Cf-Id: lHTkFHXDMngc5e5KtOvf4bbKS4jGBJaY8VJkYXT-Jo4XQ4vLeFvMfQ== X-Amz-Cf-Pop: NRT20-C4 X-Amzn-Trace-Id: Root=1-5f176761-350c758e6c6201c99ab9607f;Sampled=0 X-Cache: Miss from cloudfront x-amz-apigw-id: QC0XTEFIoAMFd_A= x-amzn-RequestId: 0d12b19d-6fa1-4237-a6e1-a1b979f3b455 { \u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34; } 無事にレスポンスが返ってきました。\nまとめ AWS Chalice で API Gateway のカスタムドメイン名の作成がサポートされたので試してみたという話でした。\nカスタムドメイン名の作成がサポートされたとは言っても、そのドメイン名を Route 53 で ALIAS レコードとして作成する必要はあります。手順でいうと最後の `` コマンドを実行するところです。なので、そのあたりはシェルスクリプトなり何なりを自作してコマンド化したいところです。\nとは言え、これまではカスタムドメイン名の作成も手作業でやる必要があったことを考えると、今回のアップデートですごく便利になったなという印象です。\n途中でも書きましたが、 Chalice でのリソース作成・更新・削除は非常に高速なので、サクッと API を作りたいという場面ではとても強力なツールです。今回やってみた内容についても、証明書の発行部分を含めても 15 分程度で試すことができるかなと思います。リソースの削除も chalice delete コマンドでサラッと消えていってくれます。\nAWS 環境でサーバレスなアプリケーションを構築するツールには SAM や Serverless Framework がありますが、個人的にはシンプルで高速な Chalice がお気に入りです。Chalice については以前にも記事を書いたので、こちらも参考にしていただけると幸いです。\n",
    "permalink": "https://michimani.net/post/aws-chalice-add-support-custom-domain/",
    "title": "AWS Chalice で API Gateway のカスタムドメイン名作成がサポートされたので試してみた"
  },
  {
    "contents": "AWS CLI には、出力結果を制御する --query というオプションがあります。これまで AWS CLI の出力結果を加工する際には jq コマンドを使用していたのですが、今回はそれらを --query オプションで置き換えるべく、色々触ってみようと思います。\n目次 概要 --query オプション 要素の件数を出力する 特定の値だけを出力する 出力結果をソートする まとめ 概要 今回は AWS CLI の --query について、公式ドキュメントを元にその動作・使い方を確認してみます。また、これまで使っていた jq コマンドで同様の出力結果を得るためにはどうしていたかも合わせて確認してみます。\n--query オプション 公式ドキュメントには次のように説明されています。\nAWS CLI は、\u0026ndash;query オプションによって、組み込みの JSON ベースの出力フィルタリング機能を提供します。\u0026ndash;query パラメータは、JMESPath の仕様に準拠している文字列を受け入れます。\nAWS CLI からのコマンド出力の制御 - AWS Command Line Interface JMESPath については次のページを参照してください。\nJMESPath — JMESPath 公式ドキュメントでは EBS を取得する ec2 describe-volumes コマンドに対する出力結果について書かれていますが、今回は Lambda に対する lambda list-functions コマンドで色々試してみます。\nひとまず、オプション無しで実行した場合の結果を見てみます。\n$ aws lambda list-functions { \u0026#34;Functions\u0026#34;: [ { \u0026#34;FunctionName\u0026#34;: \u0026#34;SEStoSlack\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:SEStoSlack\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;python3.7\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/read-write-s3-objects\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;lambda_function.lambda_handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 4042, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 180, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2019-11-21T02:34:36.825+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;123456789012FBC+dJ1XxbQZwJgeExxykdDJ0UZ4WxI=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;VpcConfig\u0026#34;: { \u0026#34;SubnetIds\u0026#34;: [], \u0026#34;SecurityGroupIds\u0026#34;: [], \u0026#34;VpcId\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;425ad537-1734-4509-88c7-9a7e96d62d5d\u0026#34; }, { \u0026#34;FunctionName\u0026#34;: \u0026#34;CFBasicAuthenticattion\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:CFBasicAuthenticattion\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;nodejs10.x\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/service-role/lambda_edge_exection\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;index.handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 561, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 3, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2019-10-21T01:03:51.582+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;123456789012JHZPsxKj0uAtf5hKJJeDmuchKo9EPpQ=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;VpcConfig\u0026#34;: { \u0026#34;SubnetIds\u0026#34;: [], \u0026#34;SecurityGroupIds\u0026#34;: [], \u0026#34;VpcId\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;61627b30-9c51-4fb4-8637-2eff1e126ab8\u0026#34; }, { \u0026#34;FunctionName\u0026#34;: \u0026#34;amplify-boyaki-production-UpdateRolesWithIDPFuncti-WDZSC7TXY5G8\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:amplify-boyaki-production-UpdateRolesWithIDPFuncti-WDZSC7TXY5G8\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;nodejs10.x\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/amplify-boyaki-production-185237-authRole-idp\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;index.handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 2850, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 300, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-05-29T09:59:03.177+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;123456789012x9TDi44KnZHySIe5hjE8uVRgyJJoPss=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;7434a66b-daf0-416a-bef5-e0e8385ba6d4\u0026#34; }, { \u0026#34;FunctionName\u0026#34;: \u0026#34;CFRedirectIndexDocument\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:CFRedirectIndexDocument\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;nodejs12.x\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/Lambda@Edge_service_role\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;index.handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 986, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 5, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-14T06:04:22.489+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;123456789012nqnsmKKR0TMbr1IvscKqk7Za7xRXzxo=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;VpcConfig\u0026#34;: { \u0026#34;SubnetIds\u0026#34;: [], \u0026#34;SecurityGroupIds\u0026#34;: [], \u0026#34;VpcId\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;f013317f-9a05-4d69-83da-26aea503770b\u0026#34; }, { \u0026#34;FunctionName\u0026#34;: \u0026#34;amplify-boyaki-production-185-UserPoolClientLambda-4AHDAEXH1A3K\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:amplify-boyaki-production-185-UserPoolClientLambda-4AHDAEXH1A3K\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;nodejs10.x\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/upClientLambdaRole185237-production\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;index.handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 2451, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 300, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-05-29T09:58:01.018+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;123456789012cywCXQVnJQOSVla0JU3I/FIpcZnDVLQ=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;e28bf64c-c58c-483a-b600-8b6424357df0\u0026#34; }, { \u0026#34;FunctionName\u0026#34;: \u0026#34;SNStoSlack\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:123456789012:function:SNStoSlack\u0026#34;, \u0026#34;Runtime\u0026#34;: \u0026#34;python3.7\u0026#34;, \u0026#34;Role\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/service-role/toSlack\u0026#34;, \u0026#34;Handler\u0026#34;: \u0026#34;lambda_function.lambda_handler\u0026#34;, \u0026#34;CodeSize\u0026#34;: 889, \u0026#34;Description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Timeout\u0026#34;: 63, \u0026#34;MemorySize\u0026#34;: 128, \u0026#34;LastModified\u0026#34;: \u0026#34;2019-07-25T00:44:09.944+0000\u0026#34;, \u0026#34;CodeSha256\u0026#34;: \u0026#34;123456789012g0hoF+8LqRIimBXOmzkWVZk/VHLvI78=\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;VpcConfig\u0026#34;: { \u0026#34;SubnetIds\u0026#34;: [], \u0026#34;SecurityGroupIds\u0026#34;: [], \u0026#34;VpcId\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;Environment\u0026#34;: { \u0026#34;Variables\u0026#34;: { \u0026#34;HookUrl\u0026#34;: \u0026#34;https://hooks.slack.com/services/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\u0026#34;, \u0026#34;slackChannel\u0026#34;: \u0026#34;dev\u0026#34; } }, \u0026#34;TracingConfig\u0026#34;: { \u0026#34;Mode\u0026#34;: \u0026#34;PassThrough\u0026#34; }, \u0026#34;RevisionId\u0026#34;: \u0026#34;3b562f87-05cc-48ef-8879-ff9e4cf67b9b\u0026#34; } ] } 6 個の関数が出力されました。\nでは、ここからは --query オプションで出力結果を制御してみます。\n要素の件数を出力する まずは、出力結果の要素の件数を出力してみます。今回であれば Lambda 関数の件数を出力します。\n$ aws lambda list-functions \\ --query \u0026#34;length(Functions)\u0026#34; 6 この出力を jq コマンドで得るためには次のようにします。\n$ aws lambda list-functions \\ | jq \u0026#34;.Functions | length\u0026#34; 6 jq コマンドではパイプ | を使用して加工していくため、 --query オプションとは書き方が異なってきます。\n特定の値だけを出力する 続いて、各 Lambda 関数の情報のうち、 関数名 と ラインタイム だけを出力してみます。\n$ aws lambda list-functions \\ --query \u0026#34;Functions[].{FnName:FunctionName,FnRuntime:Runtime}\u0026#34; [ { \u0026#34;FnName\u0026#34;: \u0026#34;SEStoSlack\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;python3.7\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;CFBasicAuthenticattion\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;amplify-boyaki-production-UpdateRolesWithIDPFuncti-WDZSC7TXY5G8\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;CFRedirectIndexDocument\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs12.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;amplify-boyaki-production-185-UserPoolClientLambda-4AHDAEXH1A3K\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;SNStoSlack\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;python3.7\u0026#34; } ] この出力結果を jq コマンドで得る場合は次のようにします。\n$ aws lambda list-functions \\ | jq \u0026#34;[.Functions[] | {FnName:.FunctionName,FnRuntime:.Runtime}]\u0026#34; ここでも jq コマンド内でパイプ | を使う必要があり、やや複雑です。\n出力結果をソートする 次に、出力結果を特定の属性の値でソートしてみます。今回はランタイムの値でソートします。\n$ aws lambda list-functions \\ --query \u0026#34;sort_by(Functions, \u0026amp;Runtime)[].{FnName:FunctionName,FnRuntime:Runtime}\u0026#34; [ { \u0026#34;FnName\u0026#34;: \u0026#34;CFBasicAuthenticattion\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;amplify-boyaki-production-UpdateRolesWithIDPFuncti-WDZSC7TXY5G8\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;amplify-boyaki-production-185-UserPoolClientLambda-4AHDAEXH1A3K\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;CFRedirectIndexDocument\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs12.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;SEStoSlack\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;python3.7\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;SNStoSlack\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;python3.7\u0026#34; } ] 降順にする場合は次のようにします。\n$ aws lambda list-functions \\ --query \u0026#34;reverse(sort_by(Functions, \u0026amp;Runtime))[].{FnName:FunctionName,FnRuntime:Runtime}\u0026#34; [ { \u0026#34;FnName\u0026#34;: \u0026#34;SNStoSlack\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;python3.7\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;SEStoSlack\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;python3.7\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;CFRedirectIndexDocument\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs12.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;amplify-boyaki-production-185-UserPoolClientLambda-4AHDAEXH1A3K\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;amplify-boyaki-production-UpdateRolesWithIDPFuncti-WDZSC7TXY5G8\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; }, { \u0026#34;FnName\u0026#34;: \u0026#34;CFBasicAuthenticattion\u0026#34;, \u0026#34;FnRuntime\u0026#34;: \u0026#34;nodejs10.x\u0026#34; } ] ソートを jq コマンドで実現する場合は、次のようにします。\n$ aws lambda list-functions \\ | jq \u0026#34;[(.Functions | sort_by(.Runtime))[] | {FnName:.FunctionName,FnRuntime:.Runtime}]\u0026#34; 降順にする場合は次のようにします。\n$ aws lambda list-functions \\ | jq \u0026#34;[(.Functions | sort_by(.Runtime) | reverse)[] | {FnName:.FunctionName,FnRuntime:.Runtime}]\u0026#34; こちらも --query を使用したほうがシンプルで直感的に書ける気がします。\nまとめ AWS CLI には、出力結果を制御する --query というオプションについて、 jq コマンドで同じ出力を得る場合を比較してみました。\njq コマンドに慣れている場合はついついそのまま使いがちですが、環境によっては別途インストールする必要があります。その点、 --query オプションについては AWS CLI のオプションなので、 CLI を利用する環境では必ず利用できます。 CLI を用いた操作を手順書として残しておく場合、余計なツールのインストールをする必要がなく、 AWS から提供されているものという面からも、 --query オプションを使ったほうが良さそうです。\u0026hellip;\u0026hellip;という話を JAWS-UG CLI 専門支部で話されており、それ以降は jq の代わりに積極的に --query を使うようになっています。\nJAWS-UG CLI専門支部 - connpass AWS を使うにあたって、マネジメントコンソールから操作をすることが多いと思いますが、 AWS を触り始めた初期の段階から AWS CLI での操作に慣れておくのは凄く良いことだなと思います。私自身も、もっと初期の頃から CLI での操作をしていればよかったと思います。これについてはクラスメソッドさんのテックイベント (DevelopersIO 2020 CONNECT) でもセッションが公開されていました。\n動画内では jq コマンドを使用されていますが、 AWS CLI いいぞ！ という思いは伝わってくると思います。\n[レポート] Developers.IO 2020 CONNECT というイベントについて #devio2020 - michimani.net AWS では、マネジメントコンソールの UI は頻繁に変わってしまいますが、中で使われている API は変わりません。なので、 AWS CLI を使いこなすことは AWS を理解する近道にもなるのではないかと思っています。\n",
    "permalink": "https://michimani.net/post/aws-cli-query-option/",
    "title": "AWS CLI の --query オプションと jq コマンド"
  },
  {
    "contents": "6/16 〜 7/7 までの期間でオンラインで開催されていた Developers.IO 2020 CONNECT について、感じたことを書いていきたいと思います。\n目次 Developers.IO 2020 CONNECT セッションの動画配信 セッションの内容 イベントに参加して思ったこと オンラインでの開催について 各配信方法について 印象に残った動画 ケーススタディで学ぶ企業運営〜クラスメソッドの新型コロナ対応〜 AWS環境でIaCを使い始めて運用するためのメリットデメリットと注意点 AWS CDK \u0026#43; Step Functions 入門 まとめ Developers.IO 2020 CONNECT Developers.IO 2020 CONNECT は、クラスメソッド株式会社が主催して開催された技術イベントです。クラメソさんは毎年 Developers.IO というイベントを開催されていて、 AWS に関する内容を中心に様々な技術情報が発信されるイベントです。私自身も 2018 と 2019 に参加し、非常にたくさんの情報を得ることができました。 これまでオフラインで開催されていた当イベントですが、昨今の情勢を踏まえて今年はオンラインでの開催となりました。オンラインでも開催していただいたことに感謝いたします。ありがとうございました！\nセッションの動画配信 オンラインでの開催ということですが、コンテンツとしては次の 2 パターンありました。\nライブセッション オンデマンド型ビデオセッション ライブセッション は、事前に申し込みをして時間になったら zoom に接続してセッションを視聴するという形です。 オンデマンド型ビデオセッション は、あらかじめ録画・編集されたセッション動画が YouTube にて配信されて、それを視聴するという形です。 ライブセッション に関しては後日編集後のアーカイブ動画が YouTube にアップされていたので、実質全てのセッションを YouTube で視聴できる形となっていました。\nセッションの内容 Developers.IO 2020 CONNECT では、 6/16 〜 7/7 のイベント期間中の下記の日程で、それぞれのテーマに関するライブセッション、ビデオセッションの配信が行われました。\n6/16 AWSとクラスメソッドの文化 6/19 データ分析を支える技術 6/23 ソーシャル、IoT、クラウドがもたらすUX 6/26 機械学習の現在地と、サーバーレス、IaC、ほか 6/30 ビジネスとマネジメント 7/1 Modern Application Development 7/7 クラスメソッド、17年目の\u0026quot;Still Day 1\u0026quot; これらのテーマについて、 AWS に限らず非常にたくさんのセッションが配信されていました。 詳細についてはイベントページをご覧ください。\nDevelopers.IO 2020 CONNECT | クラスメソッド イベントに参加して思ったこと ここからはセッション個別のレポート・感想ではなく、イベント全体に関する感想を中心に書いていきたいと思います。\nオンラインでの開催について あらためまして、オンラインでの開催ありがとうございました！オフラインでのイベントが開催しにくく色んなイベントが延期や中止になる中で、オンラインという開催方法をとっていただけたのはとても嬉しいです。幸いにも技術的なイベントに関してはオンラインで開催されるものも多くなってきており、一人のエンジニアとしては自分の技術研鑽や他のつよつよエンジニアさんを見て刺激を受けたりできるので、オンラインで色んなイベントや勉強会を開催していただいている方々には感謝しかありません。\n各配信方法について 冒頭にも書きましたが、 Developers.IO 2020 CONNECT では 2 パターンの配信方法がありました。\nライブセッション オンデマンド型ビデオセッション それぞれについて思ったことを書いていきます。\nライブセッション ライブセッションは、事前に申し込みをして、セッション日時になったら zoom を使ってセッションに参加するという形でした。ライブセッションでは、その日のテーマに沿った内容を 3 〜 4 つのセッションで話されていました。 1 つのセッションは 45 分で、セッション間には休憩を挟んで、進行役の方が各アナウンス・進行を行うという形で、基本的にはオフラインイベントと同じような感じでした。\nオンラインでの特徴としては、セッションに関する質問を随時 zoom のチャットで受け付けていて、セッション終了後にまとめて答えるという部分でしょうか。この方法を取ることで、セッション中に疑問に思ったことをその時点で質問することができるというメリットと、言葉ではなく文字で質問することで質問内容が正しく伝わるというメリットがあると思いました。また、 Q\u0026amp;A タイムでは進行役の方が質問をあらためて読んでいただいたので、よりフレンドリーな Q\u0026amp;A タイムだったと思います。わりと長い質問文も噛まずに読まれていたので、すごいなと思ってました。\nライブセッションでは映像や音声の乱れ、画面共有が上手くいかないといったハプニングが起こりがちですが、そのようなこともなく快適に視聴することができました。ライブで視聴できなかったとしても、後日 編集後の動画を YouTube にアップしていただいていたので、それもありがたかったです。\nオンデマンド型ビデオセッション オンデマンド型ビデオセッションでは、事前に録画された動画を YouTube にアップされていました。これについても、その日のテーマに合った動画が 10:00 になると一斉に公開されるという形で、毎日たくさんのセッション動画がアップされていました。イベント期間を通じてアップされた動画本数は、なんと 90 本 (ライブセッションのアーカイブも含む) でした。すごい。\nセッション動画を YouTube で配信することで、視聴する側としては自分の好きな時間に、好きなデバイスで、好きな再生速度で、何度でも、気になる部分だけでも視聴できるので、これまた大変ありがたいなと感じました。\nセッション動画については特にフォーマットが決まっているわけではなさそうで、動画を作成されるエンジニアさんにもろもろ委ねられているような感じでした。基本的にはスライドを映しながら話していくというスタイルですが、いろんな効果音やエフェクトを駆使した動画や、中には VTuber としてキャラクターを登場させた動画もありました。クラメソさんのエンジニアの方がに対しては、これまでのイベント登壇やブログの内容から高い技術力を持っている方々だなという印象は持っていましたが、今回のイベント期間で 動画編集に関する技術力もハンパないなと感じました。普通、オンラインイベントやるから動画作るってなったとしても、こんなにたくさんの、しかもクオリティの高い動画が出てくるでしょうか。すごいを通り越してちょっと怖いです。\n印象に残った動画 ここからは、イベント期間に配信された動画の中から個人的に印象に残った動画を 3 つ紹介したいと思います。まだ見ていない動画もたくさんあるので、時間をつくって見ていこうと思っています。\nケーススタディで学ぶ企業運営〜クラスメソッドの新型コロナ対応〜 初日のライブセッションの動画です。\n昨年末から年初、そして最近までのコロナの影響に対して、クラスメソッドが実施してきた対応に関するセッションでした。具体的な日付とともに、国内外の感染状況とそれに対する会社としての対応について詳細に話されていました。外から見ていても、全社的な在宅勤務導入や各種手当の付与など、非常に早い判断で動かれているなという印象を受けていました。実際にクラスメソッドに所属している方々からしても、会社として社員を守る行動を早い段階からどんどん実施していってくれることに関しては安心感があったのではないかと思います。社長の横田さんのテレワーク環境構築ブログも印象的でした。\n社長として最低限のテレワーク環境を整えてみた | Developers.IO AWS環境でIaCを使い始めて運用するためのメリットデメリットと注意点 個人的に IaC するようになってからはまだ日が浅いのですが、 CDK や CloudFormation を使ってリソースをコードで管理することについてはとても楽しいというなという感想です。ただし、ちょっとした変更の際にもコードを変更して、 GitHub で管理して\u0026hellip;と、ちょっと煩雑だなと感じる場面もあります。\nこのセッション動画では、そもそも IaC を導入したほうがいいのか? ということを考えるきっかけとなりました。もちろん導入したほうが後々のことを考えると幸せになれるとは思うのですが、管理するリソースの規模感や後々までメンテしていく気持ちの強さなども重要だと感じました。\nその他、 CDK、 CloudFormation、 Terraform についてもざっくりとその特徴について話されていました。\nAWS CDK + Step Functions 入門 最近いろんなところで Step Functions の話を聞くのでそろそろ触らないと行けないなーと思っていたところにちょうど公開されたセッション動画でした。\nStep Functions の概要から、 Task、 Pass、 Parallel といった各 State に関するわかりやすい説明・使い所が紹介されていて、入門にはもってこいな内容でした。この動画を見たあとに公式のチュートリアルをいくつかやってみたのですが、各 State の組み合わせ次第ではいろんなことができそうだなというふんわりとしたイメージはできました。今後はもう少し深堀りして試していきたいと思います。\nまとめ 6/16 〜 7/7 までの期間でオンラインで開催されていた Developers.IO 2020 CONNECT について、感じたことを書きました。\n技術的な情報はもちろんですが、クラスメソッドという会社についてもあらためて色々と知る機会になったので、とても良いイベントだったなというのが全体の感想です。正直、いろんな情報がいろんなエンジニアの方々から発信されることで、世の中にはすごいエンジニアさんがいっぱいいるんだ\u0026hellip;と謎のダメージを受けてしまうこともありましたが、そういった方々の知見を無料で得られるというのは本当にありがたいことだなと感じました。\nあと、ブログでの発信に加えて動画配信もするようになったクラメソさんの情報発信力は本当にすごいなと、あらためて感じました。\nあらためまして、オンラインでの開催ありがとうございました！\n",
    "permalink": "https://michimani.net/post/event-developersio-2020-connect/",
    "title": "[レポート] Developers.IO 2020 CONNECT というイベントについて #devio2020"
  },
  {
    "contents": "Serverless Meetup Japan Virtual #2 にオンラインで参加したので、その参加レポート・メモです。\nServerless Meetup Japan Virtual #2 - connpass Twitter のハッシュタグは #serverlessjp です。\n目次 タイムテーブル セッションレポート Access to multiple microservices on AWS Azure Static Web Apps で実現する Serverless CMS RDS Proxyの話 まとめ ※登壇資料が追加されれば追記します\nタイムテーブル Timeline Title Speaker 20:45-21:00 Social ／ 21:00-21:05 Opening Talk 吉田真吾 さん 21:05-21:10 Forkwellさん告知!! 永田りさ さん 21:10-21:30 Access to multiple microservices on AWS 中山桂一 さん 21:30-21:40 Meetup Zoom参加メンバーの方々 21:40-22:00 Azure Static Web Apps で実現する Serverless CMS 三宅和之 さん 22:00-22:15 Meetup Zoom参加メンバーの方々 22:15-22:20 RDS Proxyの話 亀田治伸 さん 22:30- Closing セッションレポート Access to multiple microservices on AWS マイクロサービスしてますか？ 小さく分割された、自立したサービス 他のサービスと連携するインターフェース デプロイも独立 データストアも独立 障害があったとしても他のサービスに影響を与えない フロントエンドから見たマイクロサービスの課題 たくさんの API を把握する必要がある 欲しい情報を得るために複数の API リクエストが必要 分散されていることにより 非同期になりがち 認証・認可が複雑 モニタリングも複雑 これらの課題を解決するのが AWS AppSync マネージドな GraphQL サービス 様々なデータソースを選択可能 GraphQL エンドポイントが一つ Query, Mutation, Subscription の 3 つのオペレーションが可能 1 回のクエリに複数のリクエスト REST API だと欲しい情報それぞれに API リクエストが必要 欲しい情報をクライアントが指定 選択可能なデータソース Lambda この時点で実質すべてのデータソースに対応可能 DynamoDB トランザクションにも対応 Elasticserch Service Aurora Serverless HTTP Endpoint SigV4 対応 フロントエンドからは AppSync のみにリクエスト AppSync の裏に API Gateway を置く Cognito 認証 IAM 認証 Pipeline Resolver 認証 JWT トークンを使用した認証などを取り入れたい場合 API Gateway の Lambda オーソライザーと似たような形 GraphQL リゾルバのデバッグ CloudWatch Logs Insight Subscription 決済サービスにリクエスト 結果は非同期で API にコールバックされる データストアのデータを更新したら AppSync に Mutation する必要がある X-Ray を使ったモニタリング まとめ フロントエンドをシンプルにしやすい 細やかな認証を扱いやすい CloudWatch Logs Insight や X-Ray をあわせて使うと捗る Q\u0026amp;A Q. フロントエンドからの処理はシンプルになるが、その後ろのレイテンシなどが気になる A. キャッシュを有効可する (裏では ElastiCache を使っている) Q. マイクロサービスをたくさん使うとインターフェースの齟齬が出てくるが、AppSync を入れた場合に E2E テストはどうするか。 A. 具体的な知見はない。が、普通の E2E テストで対応。マッピングの管理は辛い。 Q. REST API から AppSync に移ったときに、スキーマの型が少ないのが辛くないか？ A. 力技で頑張る。Swagger は階層も一つのオブジェクトとして使えるが、 GraphQL のスキーマ定義だと辛い。 Q. BFF だけ切り出すというアカウント戦略について A. すべてのサービスが一つのアカウントに入っている状態。ただし、サービスが増えてくるのであればアカウントは分けたほうがよいと考えている。 Azure Static Web Apps で実現する Serverless CMS Microsoft MVP (for Microsoft Azure) Azure Static Web Apps (preview) 静的サイトのホスティング GitHub Actions と統合された CI/CD Azure Functions によるサーバレス API の統合 やっと出てきてくれたという感じ Netlify みたいな感じ Azure Static Web Apps – App service | Microsoft Azure デプロイの仕組み GitHub Actions に統合されている 特に特別な設定は必要ない ホスティングサービスというよりも、 CI/CD に工夫が詰まっている 統合された API リポジトリに BFF (APIs) を共存させると GitHub Actions が良しなにデプロイしてくれる Azure Function (Node.js) で実装 ユースケース SPA SSG : Static Site Generator (JAM Stack 含む) Static Site Generator 動的コンテンツを事前にレンダリング Jekyll, Gatsby, HUGO, Next.js, Nuxt.js など Git ベースの SSG コンテンツは Git リポジトリで管理 Markdown からコンテンツを作成・更新 DB, CMS が不要 nuxt/content による SSG Nuxt.js の Git-based Headless CMS モジュール Markdown 内で Vue コンポーネントを利用可能 PR でステージ環境が作成される API-basses SSG (JAM Stack) コンテンツは Headless CSM で管理 管理画面だけ提供されている ビルドプロセスで CMS の API からコンテンツを取得 一般ユーザがコンテンツを更新するのに向いている API-first content platform to build digital experiences | Contentful WYSIWYG で編集可能 WordPress から移行してくる人が多い microsoft/static-web-apps-gallery-code-samples: A gallery of awesome projects deployed on Azure Static Web Apps 🎉 GitHub Actions に癖がある ビルドとデプロイが分割できない 凝ったことをしようとすると辛い E2E テスト Lint Q\u0026amp;A Q. Static Web Apps の API の機能で一番欲しい機能は? A. Azure Functions にはいろんな機能があるが、 コールドスタートが可能なプランがあればいい。というか、無いと辛い。フロントエンドでコールドスタートは辛い。 Q. 今は GitHub Actions のみだが、 GitLab などにも対応するか? A. issue には上がっている。対応はプッシュしているが、時間はかかるかもしれない。 Q. デプロイ先は Azure ストレージ? A. Azure ストレージではない。特別な場所が用意されている。もろもろ隠蔽されている。Firebase も隠蔽されている。Amplify は S3 とか諸々作成される。 Q. SSL は? A. 無償。カスタムドメインに対しても。(今だけかも) RDS Proxyの話 当初から抱えていた Lambda の問題 Lambda から VPC への通信 今は EC2 も RDS も VPC 内に作成する必要がある Lambda は VPC の外 VPC の中に入るには ENI を経由する必要がある パフォーマンスの問題 2019 年 9 月にパフォーマンス改善 RDB へのコネクション爆発 Lambda は水平スケーリング コネクションを張るのは重い (セッションハンドリングなど) Lambda でコネクションプーリングをはると RDS が待ちっぱなしになる RDS Proxy VPC 内に作成される Aurora, MySQL, PostgreSQL に対応 Aurora Serverless には非対応 本当にフロントエンドに使えるかどうかは検証次第 メリット コネクションの開閉回数を減らす -\u0026gt; DB リソースの適切な管理 RDS フェイルオーバー時間の短縮 (最大 66%) DNS 伝達の遅延解消 Secret Manager , IAM との連携で認証の管理 高負荷時にはスロットリング Q\u0026amp;A Q. 導入は進んでいる? A. 認知はされ始めていると思うが、プロダクションへの導入はまだ。挙動、癖、パフォーマンスについては検証・判断が必要。 Q. ユーザからの期待度は? A. 導入には POC を経てから。使っていただいてフィードバックをもらったほうが改善が進む。 Q. Lambda 以外からのユースケースは? A. 水平スケーリングするサービス (コンテナなど) でコネクションプールが枯渇するようなことがあるなら使う余地がある。あくまでも RDS 側のサービスなので、 RDS の気持ちになってプーリングサイズを決めることができる。 まとめ Serverless Meetup Japan Virtual #2 の参加メモでした。\nAppSync については、 Amplify ハンズオンで少し触った程度なので、今後 GraphQL も含めて勉強していきたいなと思っているところです。\nAzure Static Web Apps については、 Azure (というか Microsoft) ならではの GitHub (Actions) との連携がとても工夫されているなという印象でした。さまざまな静的サイトのホスティング、精製方法について知ることができました。\nRDS Proxy については、 過去のアンチパターンを解消することができるものとして、サーバレスの環境を大きく変えるものだなという印象です。先日開催された下記のイベントでは濃い話を聞くことができました。\nサーバーレスアンチパターン今昔物語 - connpass 普段 AWS の話を聞くことが多いので、 Azure の話を聞くことができたの非常に面白かったです。\nオンライン開催、登壇されたみなさまおつかれさまでした! ありがとうございました。\n",
    "permalink": "https://michimani.net/post/event-serverless-meetup-japan-virtual-2/",
    "title": "[レポート] Serverless Meetup Japan Virtual #2 の参加メモ #serverlessjp"
  },
  {
    "contents": "macOS で Nintend Switch の映像をキャプチャしたいなと思って色々探していたところ、 AVerMedia の GC553 というキャプチャボードにたどり着きました。非常に簡単に映像をキャプチャできるので、 macOS で使えるキャプチャボードを探している方にはオススメのアイテムです。\n目次 GC553 の概要 主な特徴 外観 GC553 の使い方 接続 配信ソフトでの検出 遅延について まとめ GC553 の概要 正式名称は AVerMedia Live Gamer Ultra GC553 となっています。\nここでは、主な特徴と製品の外観について製品サイトの内容を引用しつつ紹介していきます。\nLive Gamer ULTRA（GC553） | 比較する | AVerMedia 主な特徴 主な特徴としては次のとおりです。\n4K 60fps HDR パススルー対応 4K 30fps/1080p 120fps 録画対応 超低遅延 USB 外付けタイプ (USB バスパワー) Windows/macOS 対応 コンパクトな本体 (12.6 x 66.2 x 26 (mm), 116g) 録画・配信ソフト付属 (Windows/macOS 対応) 動画編集ソフト付属 (Windows 対応) 色々ありますが、今回重視したのは パススルー対応 と macOS 対応 です。4K、 HDR 対応といった恩恵は Switch では享受できませんが、 PS4 とかならその恩恵を受けられそうです。\n外観 外箱 内箱 付属品 GC553 本体 説明書 HDMI ケーブル (1.5m) USB 3.1 Type-C to Type-A ケーブル (1m) 動画編集ソフトのライセンスキー GC553 本体 GC553 の使い方 では GC553 をどのように使うのか見ていきます。\n接続 接続は非常に簡単です。図にすると次のような接続となります。\nGC553 の HDMI IN に Switch からの HDMI を接続し、 GC553 の HDMI OUT から外部ディスプレイに接続します。\nMacBook Pro へは USB 3.1 ケーブルで接続しますが、 GC553 側が Type-C で MacBook Pro 側が Type-A になるので、別途変換アダプタは必要になります。\n配信ソフトでの検出 今回は 配信ソフトとして OBS を使うことにします。が、 OBS 自体の詳しい使い方については割愛します。\nOBS ではデバイスの追加から Live Gamer Ultra-Video という名前を探して追加します。場合によっては #2 のような値が付くようです。\nあとはつかしたデバイスを有効にして録画や配信をすればオッケーです。\n遅延について スマブラ SP のような格闘ゲームでは遅延が大きな問題となります。ここれは、 GC553 を使った場合の遅延について説明します。\nパススルーなので遅延なし 接続説明の図の中にも書きましたが、 GC553 にはパススルー機能があるので Switch からの映像・音声は HDMI OUT で接続したディスプレイに対して遅延無しで送信されます。なので、プレイ中は普段どおり外部ディスプレイを見ながらプレイすれば遅延を感じることはありません。\nPC 側での検出には遅延が発生 これも図の中に書いていますが、 GC553 から USB 3.1 で接続した PC (今回であれば MacBook Pro) に対しては映像・音声ともに遅延が発生します。(もちろんここでの映像と音声の間にずれは発生しません)\nなので、PC を見ながらのプレイは厳しいでしょう。特にスマブラのような数フレームが勝負を分けるようなゲームでは厳しいです。\nただし、遅延があると言っても 1 秒未満のごく僅かな遅延なので、シビアな入力を求められることがない RPG などであれば特に気にはならないかなと思います。\nまとめ macOS に対応したキャプチャボード AVerMedia GC553 の紹介でした。\nキャプチャボードというものに対しては接続や設定が複雑なイメージがありましたが、 GC553 は接続するだけで専用のドライバも不要なので、非常に簡単にゲームの映像を取り込むことができます。\nまた、 OBS 等の配信ソフトを使用すれば YouTube 等へのライブ配信も簡単に行うことができます。\nもし macOS でゲーム配信を考えている方がいれば、選択肢として GC553 を候補に入れてみてはどうでしょうか。\n",
    "permalink": "https://michimani.net/post/gadget-capture-nintendo-switchvideo-on-mac-using-gc553/",
    "title": "[レビュー] macOS 対応キャプチャボード！AVerMedia GC553 を使ってみた"
  },
  {
    "contents": "AWS CLI を使って EC2 のセキュリティグループを操作してみます。\n目次 前提 VPC の作成 セキュリティグループの作成 ec2 create-security-group ec2 describe-security-groups インバウンドルールの追加・削除 ec2 authorize-security-group-ingress ec2 revoke-security-group-ingress まとめ 前提 今回は default ではない VPC に対してセキュリティグループの追加や削除、ルールの追加、削除を試してみます。\n内容的には、 以前に JAWS-UG CLI 専門支部で実施されていたハンズオンに含まれている内容です。\nJAWS-UG CLI専門支部 #156R EC2基礎(VPC) - connpass また、今回使う AWS CLI のバージョンは、現時点 (2020/07/14) で最新の 2.0.30 とします。\n$ aws --version aws-cli/2.0.30 Python/3.7.4 Darwin/19.5.0 botocore/2.0.0dev34 VPC の作成 まずは今回操作するセキュリティグループを作成する VPC を作成します。\n$ aws ec2 create-vpc help ... SYNOPSIS create-vpc --cidr-block \u0026lt;value\u0026gt; [--amazon-provided-ipv6-cidr-block | --no-amazon-provided-ipv6-cidr-block] [--ipv6-pool \u0026lt;value\u0026gt;] [--ipv6-cidr-block \u0026lt;value\u0026gt;] [--dry-run | --no-dry-run] [--instance-tenancy \u0026lt;value\u0026gt;] [--ipv6-cidr-block-network-border-group \u0026lt;value\u0026gt;] [--tag-specifications \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] ... --cidr-block で CIDR ブロックを指定して、あとのオプションはデフォルトのままとします。\n$ aws ec2 create-vpc \\ --cidr-block 10.0.0.0/16 { \u0026#34;Vpc\u0026#34;: { \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;DhcpOptionsId\u0026#34;: \u0026#34;dopt-cc5618a8\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-0f55f781234567890\u0026#34;, \u0026#34;OwnerId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;InstanceTenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Ipv6CidrBlockAssociationSet\u0026#34;: [], \u0026#34;CidrBlockAssociationSet\u0026#34;: [ { \u0026#34;AssociationId\u0026#34;: \u0026#34;vpc-cidr-assoc-0359f7b1234567890\u0026#34;, \u0026#34;CidrBlock\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;CidrBlockState\u0026#34;: { \u0026#34;State\u0026#34;: \u0026#34;associated\u0026#34; } } ], \u0026#34;IsDefault\u0026#34;: false } } セキュリティグループの作成 続いてセキュリティグループを作成します。\nec2 create-security-group $ aws ec2 create-security-group help ... SYNOPSIS create-security-group --description \u0026lt;value\u0026gt; --group-name \u0026lt;value\u0026gt; [--vpc-id \u0026lt;value\u0026gt;] [--tag-specifications \u0026lt;value\u0026gt;] [--dry-run | --no-dry-run] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] ... --vpc-id は必須ではありませんが、 省略すると default VPC が対象となる ので、ここでは明示的に VPC を指定します。\n$ aws ec2 create-security-group \\ --description \u0026#34;SG for michimani\u0026#34; \\ --group-name michimani-demo-sg \\ --vpc-id vpc-0f55f781234567890 { \u0026#34;GroupId\u0026#34;: \u0026#34;sg-03af1891234567890\u0026#34; } ec2 describe-security-groups 作成したセキュリティグループを確認してみます。\n$ aws ec2 describe-security-groups help ... SYNOPSIS describe-security-groups [--filters \u0026lt;value\u0026gt;] [--group-ids \u0026lt;value\u0026gt;] [--group-names \u0026lt;value\u0026gt;] [--dry-run | --no-dry-run] [--cli-input-json | --cli-input-yaml] [--starting-token \u0026lt;value\u0026gt;] [--page-size \u0026lt;value\u0026gt;] [--max-items \u0026lt;value\u0026gt;] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] ... --group-names でグループ名での取得もできますが、これは default VPC が対象となります。試しに先ほど作成したセキュリティグループの名前で実行してみると InvalidGroup.NotFound エラーとなります。\n$ aws ec2 describe-security-groups \\ --group-names michimani-demo-sg An error occurred (InvalidGroup.NotFound) when calling the DescribeSecurityGroups operation: The security group \u0026#39;michimani-demo-sg\u0026#39; does not exist in default VPC \u0026#39;vpc-1234567890aaaaaaa\u0026#39; なので、 --group-ids でグループ ID を指定して取得します。\n$ aws ec2 describe-security-groups \\ --group-ids sg-03af1891234567890 { \u0026#34;SecurityGroups\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;SG for michimani\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;michimani-demo-sg\u0026#34;, \u0026#34;IpPermissions\u0026#34;: [], \u0026#34;OwnerId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-03af1891234567890\u0026#34;, \u0026#34;IpPermissionsEgress\u0026#34;: [ { \u0026#34;IpProtocol\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;UserIdGroupPairs\u0026#34;: [] } ], \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-0f55f781234567890\u0026#34; } ] } アウトバウンドルール (IpPermissionsEgress) として、すべてのプロトコル (-1) に対して 全開放 (0.0.0.0/0) されています。インバウンドルールはありません。\nインバウンドルールの追加・削除 続いては、作成したセキュリティグループのインバウンドルールを追加・削除してみます。\nec2 authorize-security-group-ingress インバウンドルールを作成します。\n$ aws ec2 authorize-security-group-ingress help ... SYNOPSIS authorize-security-group-ingress [--group-id \u0026lt;value\u0026gt;] [--group-name \u0026lt;value\u0026gt;] [--ip-permissions \u0026lt;value\u0026gt;] [--dry-run | --no-dry-run] [--protocol \u0026lt;value\u0026gt;] [--port \u0026lt;value\u0026gt;] [--cidr \u0026lt;value\u0026gt;] [--source-group \u0026lt;value\u0026gt;] [--group-owner \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] ... ここでもセキュリティグループの指定はグループ名でも可能ですが、その場合の対象は default VPC となります。 なので、ここでもグループ ID で指定します。また、 http と https でのアクセスを許可するため、 80 と 443 ポートそれぞれにルールを作成します。\n$ aws ec2 authorize-security-group-ingress \\ --group-id sg-03af1891234567890 \\ --protocol tcp \\ --port 80 \\ --cidr 0.0.0.0/0 $ aws ec2 authorize-security-group-ingress \\ --group-id sg-03af1891234567890 \\ --protocol tcp \\ --port 443 \\ --cidr 0.0.0.0/0 ec2 authorize-security-group-ingress コマンドは特に出力がないので、 ec2 describe-security-groups コマンドで確認してみます。\n$ aws ec2 describe-security-groups \\ --group-ids sg-03af1891234567890 { \u0026#34;SecurityGroups\u0026#34;: [ { \u0026#34;Description\u0026#34;: \u0026#34;SG for michimani\u0026#34;, \u0026#34;GroupName\u0026#34;: \u0026#34;michimani-demo-sg\u0026#34;, \u0026#34;IpPermissions\u0026#34;: [ { \u0026#34;FromPort\u0026#34;: 80, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 80, \u0026#34;UserIdGroupPairs\u0026#34;: [] }, { \u0026#34;FromPort\u0026#34;: 443, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 443, \u0026#34;UserIdGroupPairs\u0026#34;: [] } ], \u0026#34;OwnerId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;GroupId\u0026#34;: \u0026#34;sg-03af1891234567890\u0026#34;, \u0026#34;IpPermissionsEgress\u0026#34;: [ { \u0026#34;IpProtocol\u0026#34;: \u0026#34;-1\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;UserIdGroupPairs\u0026#34;: [] } ], \u0026#34;VpcId\u0026#34;: \u0026#34;vpc-0f55f781234567890\u0026#34; } ] } インバウンドルール (IpPermissions) が追加されています。\nec2 revoke-security-group-ingress 先ほど作成したインバウンドルールを削除します。\n$ aws ec2 revoke-security-group-ingress help ... SYNOPSIS revoke-security-group-ingress [--group-id \u0026lt;value\u0026gt;] [--group-name \u0026lt;value\u0026gt;] [--ip-permissions \u0026lt;value\u0026gt;] [--dry-run | --no-dry-run] [--protocol \u0026lt;value\u0026gt;] [--port \u0026lt;value\u0026gt;] [--cidr \u0026lt;value\u0026gt;] [--source-group \u0026lt;value\u0026gt;] [--group-owner \u0026lt;value\u0026gt;] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton \u0026lt;value\u0026gt;] [--cli-auto-prompt \u0026lt;value\u0026gt;] ... ここでも --group-name の扱いは同じです。\nプロトコルやポート、 CIDR ブロックを指定して個別に削除することもできますが、一つ一つ削除するのは面倒です。今回は対象のセキュリグループ内のインバウンドルールをできるだけ少ないオプションで削除することを目標にしてみます。\nグループ ID のみを指定 $ aws ec2 revoke-security-group-ingress \\ --group-id sg-03af1891234567890 An error occurred (MissingParameter) when calling the RevokeSecurityGroupIngress operation: Missing source specification: include source security group or CIDR information エラーになりました。どうやらグループ ID のみの指定では削除できないようです。 CIDR を含める必要があると書かれています。\nグループ ID と CIDR を指定 $ aws ec2 revoke-security-group-ingress \\ --group-id sg-03af1891234567890 \\ --cidr 0.0.0.0/0 An error occurred (InvalidParameterValue) when calling the RevokeSecurityGroupIngress operation: Invalid value \u0026#39;null\u0026#39; for protocol. VPC security group rules must specify protocols explicitly. これもエラーとなりました。次は プロトコルを指定しろと書かれています。\nグループ ID と CIDR とプロトコルを指定 $ aws ec2 revoke-security-group-ingress \\ --group-id sg-03af1891234567890 \\ --protocol tcp \\ --cidr 0.0.0.0/0 An error occurred (InvalidParameterValue) when calling the RevokeSecurityGroupIngress operation: Invalid value \u0026#39;Must specify both from and to ports with TCP/UDP.\u0026#39; for portRange. これもエラーになりました。今度はポート (ポートレンジ) を指定する必要があるようです。\u0026hellip;ただ、これにポートの指定を加えると、結局個別に削除するのと同じになります。\nグループ ID とプロトコルとポートを指定 $ aws ec2 revoke-security-group-ingress \\ --group-id sg-03af1891234567890 \\ --protocol tcp \\ --port 80 エラーは出ませんでしたが、確認してみるとまだインバウンドルールは残っています。\n$ aws ec2 describe-security-groups \\ --group-ids sg-03af1891234567890 \\ --query \u0026#34;SecurityGroups[].IpPermissions\u0026#34; [ [ { \u0026#34;FromPort\u0026#34;: 80, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 80, \u0026#34;UserIdGroupPairs\u0026#34;: [] }, { \u0026#34;FromPort\u0026#34;: 443, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 443, \u0026#34;UserIdGroupPairs\u0026#34;: [] } ] ] グループ ID とプロトコルとポートと CIDR を指定 $ aws ec2 revoke-security-group-ingress \\ --group-id sg-03af1891234567890 \\ --protocol tcp \\ --port 80 \\ --cidr 0.0.0.0/0 $ aws ec2 describe-security-groups \\ --group-ids sg-03af1891234567890 \\ --query \u0026#34;SecurityGroups[].IpPermissions\u0026#34; [ [ { \u0026#34;FromPort\u0026#34;: 443, \u0026#34;IpProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;IpRanges\u0026#34;: [ { \u0026#34;CidrIp\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;Ipv6Ranges\u0026#34;: [], \u0026#34;PrefixListIds\u0026#34;: [], \u0026#34;ToPort\u0026#34;: 443, \u0026#34;UserIdGroupPairs\u0026#34;: [] } ] ] ということで、結局 グループ ID 、プロトコル、ポート、 CIDR を指定して個別に削除することになりました。\nまとめ AWS CLI を使って EC2 のセキュリティグループを操作してみた話でした。テレワーク中の問題として、自宅ネットワークの IP が頻繁に変わってしまう方もいると思います。そのたびにインバウンドルールの変更が発生してしまうと、マネジメントコンソールで作業するのは辛いので、 CLI でサクッとやってしまうのが楽ですね。\nただ、できることなら VPN 経由でアクセスするなど、頻繁にインバウンドルールを変更するのは避けたほうがいいと思いますが。\n",
    "permalink": "https://michimani.net/post/aws-handle-security-group-via-cli/",
    "title": "AWS CLI でセキュリティグループを触ってみる"
  },
  {
    "contents": "Google Analytics の Reporting API v4 を使って定期的に PV ランキングを生成して、その結果を Amazon S3 にエクスポートする処理を AWS Lambda で実装し、諸々の構成を AWS CDK を使って管理するようなものを作ってみました。\nPV 数の取得までは以前にもやっていたので、今回はそれを Lambda で実装、結果を S3 にエクスポートするようにしてみました。\n目次 概要 前提 実装 Lambda 関数 CDK デプロイ Python の外部モジュールのインストール CDK のデプロイ このプロジェクトの使い所 まとめ 概要 Google Analytics Reporting API で直近一週間の各ページごとの PV 数を取得 上位 5 件について、ページの URL とタイトルを保持した情報を JSON 形式にして S3 バケットに PUT この処理を、 Amazon EventBridge で定期的に実行 以上の処理をする構成を、 AWS CDK を使って実装しました。\n構成図としては次のようになります。\nPUT 対象となる S3 バケットは既に存在しているものとして、 CDK での管理から除外しています。また、 Google のサービスアカウントキーについては、ローカル環境から AWS CLI で SSM のパラメータストアに PUT するものとして、こちらも CDK での管理から除外しています。\nなので CDK で管理するリソースは、 EventBridge のルールと Lambda 関数 (とそれに付随する IAM ポリシー) ということになります。\n前提 Reporting API v4 が有効になっている サービスアカウントが作成されている サービスアカウントの認証情報 (client_secrets.json) が手元にある 上記の準備については下記の公式レイファレンスを参照してください。\nはじめてのアナリティクス Reporting API v4: サービス アカウント向け Python クイックスタート 実装 詳細については GitHub のリポジトリを参照してもらえればと思うので、ここでは Lambda 関数の中身と CDK の記述について簡単に説明します。\nLambda 関数 Lambda 関数は Python 3.7 で実装しています。関数の中身の前に、 CDK プロジェクト内でのディレクトリ構成について書いておきます。\nCDK プロジェクト内でのディレクトリ構成 今回は Google Analytics の Reporting API を使うということで、下記のモジュールをインストールしています。\ngoogle-api-python-client google-auth-httplib2 google-auth-oauthlib また、 Reporting API では一回で各ページのタイトルまで取得できないので、上位 5 件のページタイトルを取得するために requests モジュールもインストールしています。\nリポジトリ内のディレクトリ構成としては、直下に lambda ディレクトリを作成し、その中は次のような構成としています。\nlambda ├── dist ├── requirements.txt └── src └── fetch_rank.py あとで書きますが、デプロイ時には dist ディレクトリを Lambda のデプロイパッケージ用のディレクトリとして使ってデプロイをします。\n関数の実装内容 Lambda 関数の実装は次のようになっています。\nfrom google.oauth2 import service_account from googleapiclient.discovery import build import boto3 import json import logging import os import re import traceback import requests SCOPES = [\u0026#39;https://www.googleapis.com/auth/analytics.readonly\u0026#39;] CLIENT_SECRET_SSM_KEY = os.environ.get(\u0026#39;CLIENT_SECRET_SSM_KEY\u0026#39;) VIEW_ID = os.environ.get(\u0026#39;VIEW_ID\u0026#39;) OUT_S3_BUCKET = os.environ.get(\u0026#39;OUT_S3_BUCKET\u0026#39;) OUT_JSON_KEY = os.environ.get(\u0026#39;OUT_JSON_KEY\u0026#39;) SITE_BASE_URL = os.environ.get(\u0026#39;SITE_BASE_URL\u0026#39;) s3 = boto3.resource(\u0026#39;s3\u0026#39;) ssm = boto3.client(\u0026#39;ssm\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def get_ssm_param(key): # type: (str) -\u0026gt; str \u0026#34;\u0026#34;\u0026#34; Get parameter fron SSM Parameter Store. Args: key: string for SSM parameter store key Returns: string \u0026#34;\u0026#34;\u0026#34; response = ssm.get_parameters( Names=[ key, ], WithDecryption=True ) return response[\u0026#39;Parameters\u0026#39;][0][\u0026#39;Value\u0026#39;] def initialize_analyticsreporting(): # type: () -\u0026gt; build \u0026#34;\u0026#34;\u0026#34;Initializes an Analytics Reporting API V4 service object. Returns: An authorized Analytics Reporting API V4 service object. \u0026#34;\u0026#34;\u0026#34; client_secret_string = get_ssm_param(CLIENT_SECRET_SSM_KEY) client_secret = json.loads(client_secret_string) credentials = service_account.Credentials.from_service_account_info( client_secret, scopes=SCOPES) # Build the service object. analytics = build(\u0026#39;analyticsreporting\u0026#39;, \u0026#39;v4\u0026#39;, credentials=credentials, cache_discovery=False) return analytics def get_report(analytics): # type: (build) -\u0026gt; dict \u0026#34;\u0026#34;\u0026#34;Queries the Analytics Reporting API V4. Args: analytics: An authorized Analytics Reporting API V4 service object. Returns: The Analytics Reporting API V4 response. \u0026#34;\u0026#34;\u0026#34; return analytics.reports().batchGet( body={ \u0026#39;reportRequests\u0026#39;: [ { \u0026#39;viewId\u0026#39;: VIEW_ID, \u0026#39;dateRanges\u0026#39;: [{\u0026#39;startDate\u0026#39;: \u0026#39;7daysAgo\u0026#39;, \u0026#39;endDate\u0026#39;: \u0026#39;yesterday\u0026#39;}], \u0026#39;metrics\u0026#39;: [{\u0026#39;expression\u0026#39;: \u0026#39;ga:pageviews\u0026#39;}], \u0026#39;dimensions\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;ga:pagePath\u0026#39;}] }] } ).execute() def calc(response): # type: (dict) -\u0026gt; () \u0026#34;\u0026#34;\u0026#34;Calculate page views of each page path. Args: response: The Analytics Reporting API V4 response. \u0026#34;\u0026#34;\u0026#34; calc_res = dict() pv_summary = [] report = response.get(\u0026#39;reports\u0026#39;, [])[0] for report_data in report.get(\u0026#39;data\u0026#39;, {}).get(\u0026#39;rows\u0026#39;, []): # get page path page_path = report_data.get(\u0026#39;dimensions\u0026#39;, [])[0] # ignore query parameters page_path = re.sub(r\u0026#39;\\?.+$\u0026#39;, \u0026#39;\u0026#39;, page_path) # get page view page_view = int(report_data.get(\u0026#39;metrics\u0026#39;, [])[0].get(\u0026#39;values\u0026#39;)[0]) if page_path in calc_res: calc_res[page_path] += page_view else: calc_res[page_path] = page_view for path in calc_res: pv_summary.append({ \u0026#39;page_path\u0026#39;: path, \u0026#39;page_views\u0026#39;: calc_res[path] }) # sort by page views pv_summary.sort( key=lambda path_data: path_data[\u0026#39;page_views\u0026#39;], reverse=True) return pv_summary def report_to_rank(report, count=5): # type: (list) -\u0026gt; list \u0026#34;\u0026#34;\u0026#34; Convert report data to ranking data Args: report: list object to convert count: number of ranking post. default is 5 Returns: list \u0026#34;\u0026#34;\u0026#34; if count == 0 or len(report) \u0026lt; count: count = 5 rank_tmp = report[:count] rank = list() try: for rt in rank_tmp: post_url = SITE_BASE_URL + rt[\u0026#39;page_path\u0026#39;] rank.append({ \u0026#39;post_url\u0026#39;: post_url, \u0026#39;post_title\u0026#39;: get_post_title(post_url)}) except Exception: print(\u0026#39;An error occured in getting post title process.\u0026#39;) print(traceback.format_exc()) return rank def get_post_title(post_url): # type: (str) -\u0026gt; str \u0026#34;\u0026#34;\u0026#34; Get post title from post url Args: post_url: URL of the post Returns: string \u0026#34;\u0026#34;\u0026#34; post_title = \u0026#39;\u0026#39; try: res = requests.get(post_url) body = res.text post_title = re.sub(r\u0026#39;[\\s\\S]+\u0026lt;title\u0026gt;(.*)\u0026lt;\\/title\u0026gt;[\\s\\S]+\u0026#39;, r\u0026#39;\\1\u0026#39;, body) except Exception: print(f\u0026#39;Failed to get post title of \u0026#34;{post_url}\u0026#34;\u0026#39;) print(traceback.format_exc()) return post_title def put_to_s3(data, key): # type: (dict, str) -\u0026gt; () \u0026#34;\u0026#34;\u0026#34; Put object to S3 bucket Args: data: dict or list object to put as JSON. key: object key \u0026#34;\u0026#34;\u0026#34; try: s3obj = s3.Object(OUT_S3_BUCKET, key) body = json.dumps(data, ensure_ascii=False) s3obj.put( Body=body, ContentType=\u0026#39;application/json;charset=UTF-8\u0026#39;, CacheControl=\u0026#39;public, max-age=1209600\u0026#39;) except Exception: logger.error(\u0026#39;Put count data failed: %s\u0026#39;, traceback.format_exc()) def main(event, context): analytics = initialize_analyticsreporting() response = get_report(analytics) summary = calc(response) rank = report_to_rank(summary) put_to_s3(rank, OUT_JSON_KEY) if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#39;Running at local...\\n\\n\u0026#39;) analytics = initialize_analyticsreporting() response = get_report(analytics) summary = calc(response) rank = report_to_rank(summary) print(json.dumps(rank, indent=2, ensure_ascii=False)) ほぼ 前回 と同じですが、上位 5 件のページタイトルについては requests を使って取得しています。\nまた、 事前に client_secrets.json の情報を SSM パラメータストアに登録しておき、 Lambda 関数内で取得して利用しています。\nそして、最後には S3 バケットに PUT しています。\nローカルで実行する際には、 S3 バケットに PUT するのではなく標準出力に生成された JSON を出力します。\nCDK 続いて CDK のメインとなるスタックの記述についてです。 CDK については TypeScript で記述しています。\n環境依存の値については、次のような stac-config.json にまとめて、そこから値を取得して利用しています。\n{ \u0026#34;lambda\u0026#34;: { \u0026#34;env\u0026#34;: { \u0026#34;client_secret_ssm_key\u0026#34;: \u0026#34;google-client-secret\u0026#34;, \u0026#34;view_id\u0026#34;: \u0026#34;00000000\u0026#34;, \u0026#34;out_s3_bucket\u0026#34;: \u0026#34;\u0026lt;aleady-exists-your-bucket\u0026gt;\u0026#34;, \u0026#34;out_json_key\u0026#34;: \u0026#34;data/rank.json\u0026#34;, \u0026#34;site_base_url\u0026#34;: \u0026#34;\u0026lt;your-site-base-url\u0026gt;\u0026#34; } }, \u0026#34;event_bridge\u0026#34;: { \u0026#34;cron_expression\u0026#34;: \u0026#34;0 15 * * ? *\u0026#34; } } 今回はシンプルな構成なので、スタックは分割せず一つのファイル lib/export-pv-rank-stack.ts で書いています。\nimport * as cdk from \u0026#39;@aws-cdk/core\u0026#39;; import lambda = require(\u0026#39;@aws-cdk/aws-lambda\u0026#39;); import iam = require(\u0026#39;@aws-cdk/aws-iam\u0026#39;); import events = require(\u0026#39;@aws-cdk/aws-events\u0026#39;); import targets = require(\u0026#39;@aws-cdk/aws-events-targets\u0026#39;); import fs = require(\u0026#39;fs\u0026#39;); export class ExportPvRankStack extends cdk.Stack { constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); const stackConfig = JSON.parse(fs.readFileSync(\u0026#39;stack-config.json\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})); // Lambda function const lambdaFn = new lambda.Function(this, \u0026#39;fetchRank\u0026#39;, { code: new lambda.AssetCode(\u0026#39;lambda/dist\u0026#39;), runtime: lambda.Runtime.PYTHON_3_7, handler: \u0026#39;fetch_rank.main\u0026#39;, timeout: cdk.Duration.seconds(300), environment: { \u0026#39;CLIENT_SECRET_SSM_KEY\u0026#39;: stackConfig[\u0026#39;lambda\u0026#39;][\u0026#39;env\u0026#39;][\u0026#39;client_secret_ssm_key\u0026#39;], \u0026#39;VIEW_ID\u0026#39;: stackConfig[\u0026#39;lambda\u0026#39;][\u0026#39;env\u0026#39;][\u0026#39;view_id\u0026#39;], \u0026#39;OUT_S3_BUCKET\u0026#39;: stackConfig[\u0026#39;lambda\u0026#39;][\u0026#39;env\u0026#39;][\u0026#39;out_s3_bucket\u0026#39;], \u0026#39;OUT_JSON_KEY\u0026#39;: stackConfig[\u0026#39;lambda\u0026#39;][\u0026#39;env\u0026#39;][\u0026#39;out_json_key\u0026#39;], \u0026#39;SITE_BASE_URL\u0026#39;: stackConfig[\u0026#39;lambda\u0026#39;][\u0026#39;env\u0026#39;][\u0026#39;site_base_url\u0026#39;], } }); lambdaFn.addToRolePolicy(new iam.PolicyStatement({ actions: [ \u0026#39;s3:PutObject\u0026#39;, \u0026#39;ssm:DescribeParameters\u0026#39;, ], resources: [\u0026#39;*\u0026#39;] })); lambdaFn.addToRolePolicy(new iam.PolicyStatement({ actions: [ \u0026#39;ssm:GetParameter\u0026#39;, \u0026#39;ssm:GetParameters\u0026#39;, \u0026#39;ssm:GetParameterHistory\u0026#39;, \u0026#39;ssm:GetParametersByPath\u0026#39;, ], resources: [\u0026#39;arn:aws:ssm:*\u0026#39;] })); // EventBridge rule const fetchPvRanking = new events.Rule(this, \u0026#39;FetchPvRanking\u0026#39;, { schedule: events.Schedule.expression(`cron(${stackConfig.event_bridge.cron_expression})`) }); fetchPvRanking.addTarget(new targets.LambdaFunction(lambdaFn)); } } stac-config.json では Lambda 関数用の環境変数も保持しているので、 Lambda 関数オブジェクトを生成する際に environment で指定しています。\n前述しましたが、今回は外部モジュールを使用しているため、それらを Lambda のデプロイパッケージに含める必要があります。 CDK では 次のようにパッケージとするディレクトリを指定することで実現できます。\nnew lambda.AssetCode(\u0026#39;lambda/dist\u0026#39;) これを new lambda.Function() の code に渡せばオッケーです。\nデプロイ CDK のデプロイ自体は cdk synth からの cdk deploy コマンドで完了しますが、その前に少し準備をします。\nPython の外部モジュールのインストール 今回は Python の外部モジュールを使用するので、それらをデプロイパッケージ用のディレクトリ lambda/dist にインストールします。\n$ pip3 install --upgrade -r ./lambda/requirements.txt -t ./lambda/dist/ 本来であれば Lambda の実行環境である Amazon Linux 上でインストールしたモジュールを使うべきですが、今回は OS 依存となるようなモジュールを含まないためローカル環境でインストールしています。\nちなみに、画像処理用のモジュールである Pillow については OS 依存となるため、しっかりと環境を用意してインストールする必要があります。\nあとは、 lambda/src 内にある関数本体となるスクリプトも lambda/dist にコピーしておきます。\n$ cp -f ./lambda/src/fetch_rank.py ./lambda/dist/ これで準備は完了です。\nCDK のデプロイ $ cdk synth で CLoudFormation テンプレートを生成して\n$ cdk deploy で デプロイします。\n今回は IAM に関する変更・追加が発生するため、デプロイ時に確認されます。\nこのプロジェクトの使い所 最後に、このプロジェクトの使い所について説明します。\nブログでよくある「最近読まれている記事」みたいなものを静的サイトでも実現したいと思い、Google Analytics Reporting API v4 を使って直近一週間のページごとの PV 数を取得してみました。取得したデータを JSON とかで保持しておけば、前日までのデータで PV ランキングが作成できそうです。\nこれまた 前回 のブログで冒頭で書いていた内容なんですが、まさにこれを実現するためなんです。\n静的サイトではあらかじめ各ページが HTML で生成されているため、動的な情報を表示するためには工夫が必要です。 SNS でのシェア数なども同じで、実際にこのブログでは各記事の はてブ数 を定期的に取得して JSON ファイルとして生成し、ページ表示時に JavaScript でその値を取得・表示させています。\n今回はこれと同様の考えで、閲覧数ランキングを作りたいという思いで作りました。なので、同じように静的サイトでページの閲覧数ランキングを作りたいという方には参考になるかもしれません。\nまとめ Google Analytics の Reporting API v4 を使って PV ランキングを生成して、その結果を Amazon S3 にエクスポートする処理を AWS Lambda で実装し、諸々の構成を AWS CDK を使って管理するようなものを作ってみた話でした。\n静的サイトで動的な情報を表示させるときの参考になれば幸いです。\n",
    "permalink": "https://michimani.net/post/aws-export-pv-rank-to-s3-by-cdk-project/",
    "title": "Google Analytics Reporting API を定期実行する Lambda を CDK で構築してみた"
  },
  {
    "contents": "Amazon S3 について説明する機会があり、あらためて仕様とユースケースについて調べたのでブログ記事としてまとめてみました。\n概要 前提 Amazon S3 の概要 用語の説明 セキュリティ ストレージクラス 料金 Amazon S3 のユースケース 容量無制限のストレージ 静的ウェブサイトのホスティング イベント通知による他サービスとの連携 オブジェクトに対して一括処理する S3 バッチオペレーション まとめ 前提 AWS のストレージサービス Amazon S3 がどういったサービスなのか、公式ドキュメントの内容をもとにして自分なりに仕様とユースケースについてまとめてみます。\nCloudFront のキャッシュ仕様に引き続いて、あらためて調べてみたシリーズ(?) です。\nAmazon S3 の概要 Amazon Simple Storage Service はインターネット用のストレージサービスです。また、ウェブスケールのコンピューティングを開発者が簡単に利用できるよう設計されています。\nAmazon S3 とは - Amazon Simple Storage Service 公式ドキュメントにはこのように説明されています。\n既存のサーバにマウントするわけではなく、 AWS CLI や SDK を用いてデータにアクセスします。(マウントしたように扱うことができるサードパーティのツールも存在しますが、今回は純粋な S3 の説明ということで触れません)\nインターネットに公開されたストレージなので、格納したデータには https://examplebucket.s3.us-west-2.amazonaws.com/images/sample.jpg のような URL でアクセスすることができます。できてしまいます。後述しますが、データへのアクセス権限設定を誤ると誰でもアクセスできてしまうので注意が必要です。\n用語の説明 S3 の詳細な説明の前に、このあと出てくる用語の説明です。\nバケット S3 にデータを格納するためのコンテナ (名前空間) のことです。バケット名は、全 AWS アカウントおよび全リージョンを通じて一意な値となります。\nオブジェクト S3 に格納するデータそのものです。 S3 に格納されると言っても、いずれかのバケットに格納されるという意味です。\nキー バケット内のオブジェクトの固有の識別子です。\nリージョン バケットを保持するリージョンです。そのバケットを使用するサービスの地理的な条件や、複数リージョンへのオブジェクトの分散など、使用用途に合わせてリージョンを選択します。\nこれらの情報は上で挙げた https://examplebucket.s3.us-west-2.amazonaws.com/images/sample.jpg にすべて含まれています。\nexamplebucket がバケット名、 us-west-2 がリージョン、 images/sample.jpg がキー、 sample.jpg がオブジェクトとなります。\nセキュリティ 上にも書いたように、 S3 に格納されたオブジェクトにはインターネット上からアクセスすることができます。できてしまいます。(大事なことなので)\nそのため、適切にアクセス権限を設定しておかないと情報漏えいの危険もあるということです。\nとは言っても、現時点 (2020/07) では新たに S3 バケットを作成する際、デフォルトでバケットとオブジェクトへのパブリックアクセスはすべてブロックの設定となります。そのため、インターネットからはアクセスできず、 AWS CLI や SDK (以下、特定のアプリケーション) からのアクセスのみ可能となります。\nしかし、後述する S3 の機能である 静的ウェブサイトのホスティング を利用する場合はどうしてもパブリックアクセスを許可する必要があり、 IP アドレス等によるアクセス制限をしたい場合があります。また、パブリックアクセスは無効にしていた場合でも、特定のアプリケーションからのアクセス方法を制限したい場合もあります。(オブジェクトの読み取りだけ可能、など)\nそのような場合には バケットポリシー と呼ばれる JSON 形式の設定により、アクセスを制御します。下記がバケットポリシーの例です。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::examplebucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;aws:SourceIp\u0026#34;: [ \u0026#34;1.xxx.xxx.xxx/32\u0026#34;, \u0026#34;2.xxx.xxx.xxx/32\u0026#34;, \u0026#34;3.xxx.xxx.xxx/32\u0026#34; ] } } }, { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::examplebucket/*\u0026#34; } ] } 詳細な説明は省略しますが、上記のバケットポリシーでは examplebucket バケットに対して特定の IP アドレスからのみアクセスを許可しています。\nデフォルトではパブリックアクセスはすべてブロックとなりますが、要件によっては部分的に公開する必要があったりするので、その際には適切にアクセス権限を設定する必要があります。\nその他、 S3 のセキュリティに関しては公式のドキュメントも参照してください。\nAmazon S3 のセキュリティ - Amazon Simple Storage Service また、先日 IAM ポリシーの Condition で S3 バケットへのアクセスを制御してみた記事も書きました。\nストレージクラス S3 には様々な性質のオブジェクトを格納する場面があります。例えば、静的サイトのように高頻度でアクセスされるオブジェクト、逆に、過去のログファイルのように頻繁にアクセスする必要はないがいつか必要になる時が来るオブジェクト のような性質です。\nこれらの性質に合わせて、 S3 には次のような ストレージクラス が提供されています。\n汎用/低頻度アクセス S3 標準 S3 Inteligent-Tiering S3 標準 - 低頻度アクセス S3 1 ゾーン - 低頻度アクセス アーカイブ S3 Glacier S3 Glacier Deep Archive 上の例で挙げた静的サイトやその他 頻繁にアクセスする必要があるオブジェクトは S3 標準 クラスを利用し、あまりアクセスすることがないオブジェクト、アーカイブとして残しておきたいオブジェクトなどは、リストの下の方にあるストレージクラスを使用します。\nストレージクラスによる違いは、主に 可用性 、 取り出し時間 、 料金 です。\n\u0026ldquo;S3 標準\u0026rdquo; クラスは一番可用性が高く、発生する料金も一番高いです。逆に \u0026ldquo;S3 1 ゾーン - 低頻度アクセス\u0026rdquo; クラスでは可用性は低くなりますが、発生する料金は S3 標準に比べて安くなっています。\nまた、アーカイブ用のストレージクラスでは発生する料金が大幅に安くなりますが、格納したオブジェクトの取り出しに 数分 〜 数時間 かかります。長期保存が必要なデータの保持には、アーカイブ用のストレージクラスを利用することでコストの削減になります。\nその他の違いや可用性の細かい値については公式ドキュメントを参照してください。\nストレージクラス - Amazon S3 ｜AWS 料金 S3 を利用する上で発生する料金は、次の 4 種類あります。\nストレージ リクエストとデータの取り出し データ転送 マネジメントとレプリケーション マネジメントとレプリケーション については S3 の機能の中でも Advanced な機能に対して発生する料金なので、今回は説明を割愛します。それ以外の 3 種類について、それぞれどのような料金が発生するのか確認してみます。\nまた、前項でストレージクラスによって料金が変わると書きましたが、ここでは S3 標準 クラスの東京リージョン (ap-northeast-1) での料金について触れます。\nストレージ S3 バケットに格納しているオブジェクトの 合計サイズ に発生する料金です。\n料金 (1 GB あたり) 50 TB まで 0.025 USD 50 TB を超えて 500 TB まで 0.024 USD 500 TB 以降 0.023 USD 例えば、オブジェクトのサイズ合計が 300 TB あった場合の料金は次のようになります。\n(50 * 1024 * 0.025) + (250 * 1024 * 0.024) = 7424 1 USD = 105 円とすると、毎月 779,520 円 の料金が発生することとになります。\nリクエストとデータの取り出し S3 バケット内のオブジェクトに対する リクエスト回数 に対して発生する料金です。\nリクエストの種類 料金 (1000リクエストあたり) PUT, COPY, POST, LIST 0.0047 USD GET, SELECT, その他 0.00037 USD AWS CLI や SDK からのリクエストはもちろんですが、マネジメントコンソール上でオブジェクトを閲覧する際にも GET リクエストとしてカウントされます。ただ、 GET, SELECT などの読み取りに関するリクエストに関しては料金単価が非常に低いので、あまり意識する必要はないかなという印象です。\nデータ転送 S3 とパブリックなインターネット、及び他の AWS サービスとの間に発生する データ転送量 に対して発生する料金です。ただし、次のデータ転送については料金が発生しません。\nインターネットから S3 へのデータ転送 S3 と同一リージョンにある Amazon EC2 インスタンスへのデータ転送 Amazon CloudFront へのデータ転送 なので、料金が発生するデータ転送のパターンとしては次のとおりです。\nS3 からパブリックインターネットへのデータ転送 S3 から別リージョンにある他の AWS サービスへのデータ転送 インターネットへのデータ転送については、後述する静的サイトのウェブホスティング機能や、バイナリのダウンロード用にオブジェクトの URL を公開しているときには注意したいです。\n別リージョンにある他のサービスへの転送については、たとえ同じ国であってもリージョンが違えば発生します。なので、東京リージョンと大阪リージョン (ローカル) 間でも料金が発生します。\nS3 ではこういった場面で料金が発生します。その他、料金の詳細については下記の公式ドキュメントを参照してください。\n料金 - Amazon S3 ｜AWS Amazon S3 のユースケース では、実際に S3 をどのように使うかを紹介していきます。\n容量無制限のストレージ S3 の最大の特徴は、やはり 容量無制限 という点です。各オブジェクトの最大サイズは 5 TB となっていますが、バケットの合計サイズには制限がありません。そのため、書類、配信用コンテンツ、分析用データなど、容量を気にせずにどんどん格納することができます。\nまた、 AWS の様々なサービス1 から出力されるログデータは S3 に保存できるようになっています。なので、ログデータはとりあえず S3 に溜め込んで、その後 各種分析用のサービス2 を用いて分析する、というところまでがユースケースになります。\nもちろん、 とりあえず のデータ置き場として使うのもありだとは思いますが、前項で触れたように料金については事前に試算をしておいたほうが良さそうです。\nファイルシステムではない 一点注意しておきたいのが、 S3 はストレージサービスですが ファイルシステムではない ということです。 S3 に格納するデータは、バケット名とオブジェクトのキーで識別されます。なので、最初に出てきた images/sample.jpg というキーはディレクトリ構造を表しているわけではなく、あくまでも images/sample.jpg というキーでオブジェクトを識別しています。\nたとえば次のようなキーで識別されるオブジェクトが存在するとします。\nimages/red.jpg images/brack.jpg images/white.jpg これらは 同じ images ディレクトリにある というよりは、 同じ images/ プレフィックスを持っている と表現するのが正しいです。 S3 のマネジメントコンソール上ではディレクトリをたどっていくようにオブジェクトにアクセスできますが、あくまでもそれは便宜上ということです。\nこれを理解すると、 S3 に配置した静的サイトの index.html が補完されない理由が理解してもらえるかと思います。\n静的ウェブサイトのホスティング S3 の任意のバケットに静的サイトを格納し、静的ウェブサイトのホスティング機能を利用することで、 S3 のみで静的サイトのホスティングをすることができます。この機能を利用すると、前述した index.html が補完されない問題 (仕様) は解消されます。 ただし、次のような制限・注意点があります。\nドメイン名はバケット名とリージョンからなる値となる 独自ドメインを使用する場合は SSL 化できない パブリックアクセスが有効になる 社内向けに公開するような静的サイトの公開や、ハンズオン等の場合には便利な機能です。セキュリティの項目でも触れましたが、この機能を利用するとパブリックアクセスが有効になるため、適切にアクセス権限の設定をする必要があります。\nこの機能を使った静的ウェブサイトのホスティングについては以前に書いたこちらの記事も参考にしてみてください。\nAmazon S3 での静的ウェブサイトのホスティング - Amazon Simple Storage Service イベント通知による他サービスとの連携 ストレージでイベント？ と思われるかもしれませんが、この機能が S3 の面白いところだなと個人的には思っています。\nS3 で発生するイベントには、下記のようなものがあります。\n新しいオブジェクトの作成イベント : s3:ObjectCreated:* オブジェクト削除イベント : s3:ObjectRemoved:* オブジェクト復元イベント : s3:ObjectRestore:Post, s3:ObjectRestore:Completed (レプリケーションイベント) これらのイベントをトリガーにして、任意の Lambda 関数を実行したり、SNS 経由で Slack やメールに通知をしたりすることができます。\n具体的には次のような構成が考えられます。\nバケットに画像が Put される s3:ObjectCreated:Put イベント発生 イベントをトリガーに Lambda 関数を実行 複数サイズの画像生成、バケットへ Put その他、簡単な翻訳のパイプラインを構築することもできるので、興味のある方は AWS の金澤さんが作成されたこちらのハンズオン資料をもとに試してみてはいかがでしょうか。\nちなみに、このハンズオンの内容を AWS CDK でやってみたというタイトルで LT もさせていただきました。(人生初 LT でした)\nLT ネタの元記事になったのは以下の記事です。\nAmazon S3 イベント通知の設定 - Amazon Simple Storage Service オブジェクトに対して一括処理する S3 バッチオペレーション S3 バッチオペレーション は、S3 バケットに格納されたオブジェクトに対して一括で処理を実行することができる機能です。\n例えば次のような処理が考えられます。\nオブジェクトのコピー オブジェクトに対する特定のタグ、メタデータを付与 画像オブジェクトをリサイズ、またはサムネイル用のオブジェクトを生成 バッチオペレーションでは、次の手順で処理を実行します。\nインベントリレポートの作成 ジョブの作成 ジョブの実行 インベントリレポートの作成 では、一括処理の対象となるオブジェクトの一覧を作成します。\nジョブの作成 では、対象となるオブジェクトの一覧 (マニフェスト) と、実際に実行する処理を指定します。実際に実行する処理としては、バッチオペレーション側であらかじめ用意されている オブジェクトのコピー 、 タグの置き換え 、 ACL の置き換え 、 復元 および Lambda 関数の呼び出し を選択できます。 Lambda 関数を呼び出すことで、対象となっているオブジェクトそれぞれに対して任意の処理を実行することができます。\n以前にバッチオペレーションで画像オブジェクトをリサイズしてみた記事を書いたので、こちらも参考にしてみてください。\n基本: S3 バッチオペレーション - Amazon Simple Storage Service まとめ AWS のストレージサービス Amazon S3 の概要とユースケースについて書きました。\nS3 のリリースは 2006 年ということで、 AWS の歴史のほぼ一番最初から存在しているサービスです。なんとなく容量無制限のストレージだという認識がある方がほとんどだと思いますが、ユースケースの中でで紹介したイベント通知による他サービスとの連携に関しては、れっきとしたサーバレスアーキテクチャの一部を担うサービスだということを示しています。\nAWS の基本的なサービスとして挙げられることが多い Amazon S3 ですが、ただのストレージではなく、いろんな機能が詰まったサービスだなと感じました。\nAmazon S3 とは - Amazon Simple Storage Service 監視用の Amazon CloudWatch, AWS CloudTrail, ネットワーク関連では Amazon CloudFront, Elastic Load Balancing など\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n保存したログに対して SQL クエリを使って分析できる Amazon Athena など\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-about-amazon-s3/",
    "title": "Amazon S3 の仕様とユースケースについてあらためて調べてみた"
  },
  {
    "contents": "AWS 認定ソリューションアーキテクト - プロフェッショナルの勉強をしている中でディザスタリカバリ (DR) に関する話が出てきたので、今回は AWS 上での DR 構成 4 パターンについて簡単にまとめてみます。\n目次 ディザスタリカバリ (DR) とは DR 対策 4 パターン バックアップ \u0026amp;amp; リストア パイロットライト ウォームスタンバイ マルチサイト (ホットスタンバイ) まとめ 参考 ディザスタリカバリ (DR) とは ディザスタリカバリ (disaster recovery - DR) とは、直訳すると 災害からの復旧 という意味です。 AWS では「Design for Failure」という設計原則があり、 公式ドキュメントにも次のように記載されています。\n自然災害から機械的障害、人的ミスまで多岐にわたる原因により、こうした小さな障害が起こります。物理インフラストラクチャが長期間使用できない場合に備えて、稼働を継続できるようにするための予防的なクラウド災害復旧戦略が必要です。\n災害対策 | AWS DR 対策 4 パターン この DR 対策には RTO 1 、 RPO 2 という要件とコストとのバランスを考慮して、どのような構成にするかを検討します。今回はその構成の例として 4 パターンについてまとめてみます。\nバックアップ \u0026amp; リストア 定期的にシステムのバックアップを作成し、障害発生時にはバックアップからシステムを復旧する方法です。 AWS では、バックアップデータをリージョン A にある S3 に保存し、バックアップデータをリージョン B の S3 (DR 用) にレプリケートしておきます。構成イメージとしては次のようになります。\nバックアップは日次や月次などのタイミングで取得することになるため、 RPO は 最終バックアップ保存時点まで となります。また、障害発生時にはバックアップデータから復元 (リストア) をするため、 RTO は 長く なります。一方で、 DR 対策のために用意するのは S3 へのバックアップだけのため、コストは 低く なります。\nまとめ 適当なタイミングでバックアップを取得し、別リージョンにバックアップデータをレプリケート 障害発生時にはバックアップからリストア RPO は最終バックアップ時点まで RTO は長い コストは低い パイロットライト DR 用にスペックの低い DB を起動しておいて、通常時はデータの同期のみを行います。障害発生時には、 DR 用のリージョンでアプリケーションを起動し、 DB のスペックを上げて対応します。そして元のリージョンの復旧作業を行います。構成イメージは次のようになります。\nデータについては適宜 別リージョンの DB に同期するため、 RPO は 最終データ同期時点まで となります。障害発生時には DR 用のリージョンでのアプリケーション起動、 DB のスケールアップを実施する必要があるため、 RTO は やや長く なります。DR 対策のために用意するのはスペックの低い DB のみとなるため、コストは やや低く なります。\nまとめ 別リージョンに低スペックの DB を立て、適宜データ同期 障害発生時には DR 用リージョンでアプリケーションを起動、 DB をスケールアップ RPO は最終データ同期時点まで RTO はやや長い コストはやや低い ウォームスタンバイ DR 用のリージョンで、スペックのみ低い同構成でシステムを常時起動しておきます。障害発生時にはアプリケーションおよび DB のスケールアップ、 Route 53 で DNS を切り替えて対応します。そして元のリージョンの復旧作業を行います。構成イメージは次のとおりです。\nデータについてはパイロットライトと同様になので RPO は 最終データ同期時点まで となります。障害発生時には、 DB のスケールアップとアプリケーションのスケールアップ、 DNS の切り替えをする必要があります。ただし、新たにアプリケーションを起動するわけではないので RTO は やや短く なります。一方で、低スペックとは言え同構成のシステムを別リージョンに用意する必要があるためコストは やや高く なります。\nまとめ 低スペックで同構成のシステムを別リージョンに用意 障害発生時には DB とアプリケーションのスケールアップ、 DNS 切り替えを実施 RPO は最終データ同期時点まで RTO はやや短い コストはやや高い マルチサイト (ホットスタンバイ) DR 用として同スペック、同構成のシステムを常時起動しておきます。障害発生時には Route 53 で DNS の切り替えのみを実施して対応します。\nデータについてはパイロットラン、ウォームスタンバイと同様なので RPO は 最終データ同期時点まで となります。障害発生時には DNS の切り替えのみで対応できるので、 RTO は 短く なります。しかし、別リージョンに全く同じシステム (スペック、構成) を常時起動しておく必要があるため、コストは 高く なります。\nまとめ 同スペック、同構成のシステムを別リージョンに用意 障害発生時には DNS の切り替えのみ実施 RPO は最終データ同期時点まで RTO は短い コストは高い まとめ AWS 上での DR 構成 4 パターンについてまとめてみました。\nRTO、RPO とコストの兼ね合いについては DR 対策を講じる対象のシステムの性質や重要性によって議論が必要になるので、実際に DR 対策をする場合にはそのあたりの数値をしっかりと決めて置く必要がありそうです。\n試験対策としては、各パターンのメリット・デメリット、長所・短所を理解して問題文からどのパターンが最適なのかを判断できるようにしておきたいと思います。\n参考 AWS認定ソリューションアーキテクト-プロフェッショナル ~試験特性から導き出した演習問題と詳細解説 RTO (Recovery Time Objective) 目標復旧時間\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRPO (Recovery Point Objective) 目標復旧時点\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-architecture-for-disaster-recovery/",
    "title": "AWS 上でのディザスタリカバリ (DR) 構成 4 パターン"
  },
  {
    "contents": "AWS 認定ソリューションアーキテクト - プロフェッショナルの勉強をしている中で、 IAM の Condition によるアクセス制御の話が出てきたので、 S3 へのアクセスで実際に試してみようと思います。\n目次 IAM ポリシーの作成 Condition エレメント ListObjectsV2 リクエストを実行 AmazonS3ReadOnlyAccess ポリシーをアタッチした IAM ユーザで実行 Condition 付きの IAM ポリシーをアタッチした IAM ユーザで実行 まとめ IAM ポリシーの作成 アクセス制御用の IAM ポリシーを作成します。今回は次のような制御を考えます。\nS3 に対する ListObjects または ListObjectsV2 API で取得できるオブジェクトの最大数を 2 とする ListObjects または ListObjectsV2 API を実行するためには s3:ListBucket のアクセス許可が必要なので、このアクションに対して Condition エレメントで条件を指定します。\nCondition エレメント IAM ポリシーの Condition エレメントでは、予め用意されている StringEquals や NumericGreaterThan といった条件演算子によって, リクエストに含まれるコンテキストの値と条件の一致をチェックします。\n条件演算子は次のような種類に分けられています。\n文字列 数値 日付 Bool バイナリ IP アドレス ARN IfExists 条件キーの有無 今回の条件は 「S3 に対する ListObjects または ListObjectsV2 API で取得できるオブジェクトのキー数の最大数を 2 とする」 なので、文字列条件演算子の NumericLessThanEquals を使用して次のように記述します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:Get*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::iam-condition-test-bucket\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::iam-condition-test-bucket\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;NumericLessThanEquals\u0026#34;: { \u0026#34;s3:max-keys\u0026#34;: \u0026#34;2\u0026#34; } } } ] } この JSON の内容で IAM ポリシーを作成します。\ns3:max-keys の条件演算子を指定すると、 ListObjects または ListObjectsV2 で max-keys オプションを指定する必要があります。指定しない場合はそもそも条件に一致しないことになります。\nIf the key that you specify in a policy condition is not present in the request context, the values do not match.\nIAM JSON Policy Elements: Condition Operators - AWS Identity and Access Management その他、 Condition の詳細については下記の公式ドキュメントも合わせて。\nIAM JSON ポリシーの要素:Condition - AWS Identity and Access Management ちなみに、 arn:aws:s3:::iam-condition-test-bucket は今回の確認用に先に作成しておいた S3 バケットで、バケット内にはオブジェクトが 4 つ格納されています。\n$ aws s3api create-bucket \\ --bucket iam-condition-test-bucket \\ --create-bucket-configuration LocationConstraint=ap-northeast-1 { \u0026#34;Location\u0026#34;: \u0026#34;http://iam-condition-test-bucket.s3.amazonaws.com/\u0026#34; } $ aws s3api list-objects-v2 \\ --bucket iam-condition-test-bucket \\ --query \u0026#34;Contents[].[Key]\u0026#34; \\ --output text sample_0.json sample_1.json sample_2.json sample_3.json ListObjectsV2 リクエストを実行 今回は AWS CLI で ListObjectsV2 リクエストを実行してみます。\nAmazonS3ReadOnlyAccess ポリシーをアタッチした IAM ユーザで実行 まずは S3 への Read Only 権限 (AmazonS3ReadOnlyAccess) を持つ IAM ユーザで ListObjectsV2 を実行してみます。\n$ aws s3api list-objects-v2 \\ --bucket iam-condition-test-bucket { \u0026#34;Contents\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;sample_0.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:43:05+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;sample_1.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:42:12+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;sample_2.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:42:17+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;sample_3.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:42:51+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; } ] } バケット内のオブジェクトがすべて取得できました。\nCondition 付きの IAM ポリシーをアタッチした IAM ユーザで実行 では、先ほど作成した IAM ポリシーを適当な IAM ユーザにアタッチして ListObjectsV2 を実行してみます。\n$ aws s3api list-objects-v2 \\ --bucket iam-condition-test-bucket An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied AccessDenied エラーとなり、オブジェクトの一覧が取得できませんでした。\nなので、条件に合うように --max-keys オプションを指定して実行してみます。\n$ aws s3api list-objects-v2 \\ --bucket iam-condition-test-bucket \\ --max-keys 2 { \u0026#34;IsTruncated\u0026#34;: true, \u0026#34;Contents\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;sample_0.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:43:05+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;sample_1.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:42:12+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; } ], \u0026#34;Name\u0026#34;: \u0026#34;iam-condition-test-bucket\u0026#34;, \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MaxKeys\u0026#34;: 2, \u0026#34;EncodingType\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;KeyCount\u0026#34;: 2, \u0026#34;NextContinuationToken\u0026#34;: \u0026#34;17DiagPib4DkESmD7teHiRKAUOX3+2+PwEfdQ/orj+JJiIY9+KzSt0FY73/EOModsutwA/GSThwc=\u0026#34; } 取得できました。残りのオブジェクトを取得するためには、レスポンスの NextContinuationToken を使って次のようにリクエストします。\n$ aws s3api list-objects-v2 \\ --bucket iam-condition-test-bucket \\ --max-keys 2 \\ --continuation-token 17DiagPib4DkESmD7teHiRKAUOX3+2+PwEfdQ/orj+JJiIY9+KzSt0FY73/EOModsutwA/GSThwc= { \u0026#34;IsTruncated\u0026#34;: false, \u0026#34;Contents\u0026#34;: [ { \u0026#34;Key\u0026#34;: \u0026#34;sample_2.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:42:17+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; }, { \u0026#34;Key\u0026#34;: \u0026#34;sample_3.json\u0026#34;, \u0026#34;LastModified\u0026#34;: \u0026#34;2020-07-05T09:42:51+00:00\u0026#34;, \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;ff2238a281fccd41c7698ef7a1e2236d\\\u0026#34;\u0026#34;, \u0026#34;Size\u0026#34;: 21, \u0026#34;StorageClass\u0026#34;: \u0026#34;STANDARD\u0026#34; } ], \u0026#34;Name\u0026#34;: \u0026#34;iam-condition-test-bucket\u0026#34;, \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;MaxKeys\u0026#34;: 2, \u0026#34;EncodingType\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;KeyCount\u0026#34;: 2, \u0026#34;ContinuationToken\u0026#34;: \u0026#34;17DiagPib4DkESmD7teHiRKAUOX3+2+PwEfdQ/orj+JJiIY9+KzSt0FY73/EOModsutwA/GSThwc=\u0026#34; } 今回の条件は \u0026quot;s3:max-keys\u0026quot;: \u0026quot;2\u0026quot; なので、次のリクエストでは AccessDenied になります。\n$ aws s3api list-objects-v2 \\ --bucket iam-condition-test-bucket \\ --max-keys 3 An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied まとめ IAM の Condition エレメントを使用して S3 への ListObjects または ListObjectsV2 API のリクエストを制御してみました。\nCondition を使用したアクセス制御としては、 EC2 インスタンスに特定のタグがなければ起動できないようにする例がありました。\nユーザーのアクセスを特定の EC2 リソースに制限する また、今回は個人の AWS アカウント内での制御でしたが、 AWS Organizations の SCP (サービスコントロールポリシー) を使うことで組織内の AWS アカウントに対しても統一されたアクセス制御をすることができます。\nサービスコントロールポリシー - AWS Organizations このあたりは SAP の試験でもよく出てくる部分みたいなので、しっかり理解していきたいところです。\n",
    "permalink": "https://michimani.net/post/aws-iam-access-control-by-condition-element/",
    "title": "IAM の Condition によるアクセス制御を試してみる"
  },
  {
    "contents": "AWS SDK for Go で S3 \u0008バケットの格納されたオブジェクトのメタデータを取得しようとしたところ、メタデータのキーが大文字 (先頭とハイフンの直後) になっていました。\u0026hellip;という話です。\n目次 オブジェクトの作成 HeadObject API でオブジェクトの情報を取得 CLI で取得してみる SDK for Python (boto3) で取得してみる SDK for Go で取得してみる まとめ オブジェクトの作成 まずは適当なバケットにオブジェクトを作成します。\n$ cat ./sample.json { \u0026#34;text\u0026#34;: \u0026#34;This is a sample JSON.\u0026#34; } $ aws s3api put-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json \\ --body ./sample.json \\ --metadata custom-metadata-1=value1,custom-metadata-2=value2,Custom-Metadata-3=value3 \\ --cache-control \u0026#34;public, max-age=1209600\u0026#34; \\ --content-type \u0026#34;application/json;charset=UTF-8\u0026#34; { \u0026#34;ETag\u0026#34;: \u0026#34;\\\u0026#34;2670dbe970a150ec5f0d4b0e6b9a69cc\\\u0026#34;\u0026#34; } ちなみに、 --metadata オプションで大文字、小文字を変えて指定していますが、これは無視されます。\nマネジメントコンソールで確認すると、次のようになっています。\nHeadObject API でオブジェクトの情報を取得 S3 の HeadObject API を使用して、オブジェクトのメタデータを取得します。\n今回は SDK for Go 以外に、 AWS CLI と SDK for Python (boto3) でも試してみます。\nCLI で取得してみる まずは AWS CLI で取得してみます。\n$ aws --version aws-cli/2.0.26 Python/3.7.4 Darwin/19.5.0 botocore/2.0.0dev30 $ aws s3api head-object \\ --bucket s3-metadata-test-michimani \\ --key sample.json \\ --query \u0026#34;Metadata\u0026#34; { \u0026#34;custom-metadata-1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;custom-metadata-2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;custom-metadata-3\u0026#34;: \u0026#34;value3\u0026#34; } SDK for Python (boto3) で取得してみる 次に、 AWS SDK for Python (boto3) で取得してみます。\n$ python -V Python 3.7.7 取得するためのコードは次のとおりです。\nimport boto3 target_bucket = \u0026#39;s3-metadata-test-michimani\u0026#39; target_key = \u0026#39;sample.json\u0026#39; s3 = boto3.client(\u0026#39;s3\u0026#39;) def print_s3_metadata(bucket, key): out_object = s3.head_object( Bucket=bucket, Key=key ) for key in out_object[\u0026#39;Metadata\u0026#39;]: val = out_object[\u0026#39;Metadata\u0026#39;][key] print(f\u0026#39;{key} : {val}\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: print_s3_metadata(target_bucket, target_key) 実行してみると\n$ python python/main.py custom-metadata-1 : value1 custom-metadata-2 : value2 custom-metadata-3 : value3 boto3 の場合はメタデータのキーは小文字です。\nS3 — Boto3 Docs 1.14.13 documentation | S3.Client.head_object SDK for Go で取得してみる 最後に、 AWS SDK for Go を使って取得してみます。\n$ go version go version go1.14.4 darwin/amd64 取得するためのコードは次のとおりです。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/s3\u0026#34; ) var s3sess = s3.New(session.New(), \u0026amp;aws.Config{ Region: aws.String(\u0026#34;ap-northeast-1\u0026#34;), }) var targetBucket string = \u0026#34;s3-metadata-test-michimani\u0026#34; var targetKey string = \u0026#34;sample.json\u0026#34; // PrintS3Metadata is a function to print S3 metadata. func PrintS3Metadata(bucket string, key string) { outObject, _ := s3sess.HeadObject(\u0026amp;s3.HeadObjectInput{ Bucket: aws.String(bucket), Key: aws.String(key), }) for key, val := range outObject.Metadata { fmt.Printf(\u0026#34;%s : %s\\n\u0026#34;, key, *val) } } func main() { PrintS3Metadata(targetBucket, targetKey) } 実行してみると\n$ go run go/main.go Custom-Metadata-3 : value3 Custom-Metadata-1 : value1 Custom-Metadata-2 : value2 という感じで、キーの先頭の文字、およびハイフン直後の文字が大文字になっています。これについては、ドキュメントに次のように書かれています。\nBy default unmarshaled keys are written as a map keys in following canonicalized format: the first letter and any letter following a hyphen will be capitalized, and the rest as lowercase.\nまた、次のようにも書かれていました。\nSet `aws.Config.LowerCaseHeaderMaps` to `true` to write unmarshaled keys to the map as lowercase.\nこれに従ってコードを次のように変更しました。\nvar s3sess = s3.New(session.New(), \u0026amp;aws.Config{ - Region: aws.String(\u0026#34;ap-northeast-1\u0026#34;), + Region: aws.String(\u0026#34;ap-northeast-1\u0026#34;), + LowerCaseHeaderMaps: aws.Bool(true), }) これで再度実行してみましたが、\n$ go run go/main.go Custom-Metadata-3 : value3 Custom-Metadata-1 : value1 Custom-Metadata-2 : value2 結果は変わりませんでした。(順番が違うのは Go 言語の map の仕様です。)\nこの aws.Config.LowerCaseHeaderMaps の設定による挙動の違いについては、残念ながら調べてみてもそれらしい情報が得られませんでした。もしこの記事をご覧の方でご存じの方がいらっしゃればコメント等で教えていただけると幸いです。\ns3 - Amazon Web Services - Go SDK | S3.HeadObject まとめ AWS SDK for Go で S3 \u0008バケットの格納されたオブジェクトのメタデータを取得しようとしたところ、メタデータのキーが大文字になっているという話でした。\nSDK for Go でメタデータをゴニョゴニョするときに少しハマったので書きましたが、こうなっているので気をつけましょうというくらいの薄い内容です。 aws.Config.LowerCaseHeaderMaps の挙動についてはよくわからなかったので、もしご存じの方がいらっしゃればコメントいただけると幸いです！\n",
    "permalink": "https://michimani.net/post/aws-s3-metadata-keys-are-capitalized-using-go-sdk/",
    "title": "AWS SDK for Go で S3 のメタデータを取得するとキーが大文字だった"
  },
  {
    "contents": "AWS 上で静的サイトをホスティングする際に Amazon S3 と Amazon CloudFront をセットで使うことは多いと思います。ただ単にホスティングするだけであれば S3 だけでも可能ですが、 CloudFront を前段に置くことで色々とメリットがあります。今回は個人的にメリットだと思う部分について簡単にまとめてみたいと思います。\n目次 概要 キャッシュによる表示速度の高速化 独自ドメインの SSL 対応 AWS Certificate Manager について 他にもメリットはあるもよう まとめ 概要 AWS 上で静的サイトをホスティングする際に、 S3 のみを利用する場合に比べて CloudFront をセットで使うことで得られるメリットについてまとめてみます。\n今回触れるメリットは、次のような内容です。\nキャッシュによる表示速度の高速化 独自ドメインの SSL 対応 それぞれについて簡単にまとめていきます。\nキャッシュによる表示速度の高速化 CloudFront は AWS が提供している CDN サービスです。\nエンドユーザーにコンテンツをより低いレイテンシーで届けるため、Amazon CloudFront では 42 か国 84 都市にある 216 の POP (Point Of Presence) (205 のエッジロケーションと 11 のリージョン別エッジキャッシュ) のグローバルネットワークを使用しています。\n特徴 - Amazon CloudFront | AWS このように、世界中にエッジサーバ (エッジロケーション) があり、クライアントに近いエッジサーバからキャッシュしたコンテンツを配信することができます。\nまあ、これは CloudFront というか CDN というサービス自体による恩恵です。とは言え、クライアントに対して高速にコンテンツを配信できるというのは大きなメリットです。\nCloudFront では Cookie や クエリ文字列によるキャッシュ制御も柔軟に設定できるので、上手く利用していきたいところです。\n独自ドメインの SSL 対応 S3 のみの場合、 S3 + CloudFront の場合ともに、各サービスが提供しているドメインでは SSL/TSL 暗号化が有効になっており、 https でコンテンツを配信することができます。S3 であれば https://\u0026lt;bucket-name\u0026gt;.s3-ap-northeast-1.amazonaws.com 、 CloudFront であれば https://\u0026lt;distribution-id\u0026gt;.cloudfront.net のようなドメインとなっています。\nしかし、独自ドメインで SSL/TSL 経由でコンテンツを配信しようと思うと、 CloudFront を利用する必要があります。\nS3 のみで独自ドメインを利用することもできます1が、その場合は SSL 証明書を配置する場所がなく、 SSL/TSL 経由での配信ができません。 CloudFront を利用すれば、 ACM (AWS Certificate Manager) で作成した証明書をディストリビューションに設定することができるので、独自ドメインで SSL/TSL 経由での配信が可能になります。\nAWS Certificate Manager について ACM についても簡単におさらいしておきます。\nAWS Certificate Manager (以下、ACM) は SSL/TLS 証明書を管理できるサービスで、管理している証明書を AWS の各種サービスで利用することができます。また、 ACM での証明書発行は無料で、且つ更新作業も不要です。個人的に証明書の管理ってかなり面倒だと思っているので、 ACM はとてもありがたいサービスだと思っています。\nACM で管理している証明書を利用できるサービスは次のとおりです。\nAmazon CloudFront Amazon API Gateway Elastic Load Balancing つまり、 AWS で Web サービス (サイト) や API を SSL/TLS 経由で配信する場合は、上記のサービスに ACM で管理した証明書を配置するか、 EC2 などに直接証明書を配置することになります。(後者の場合は証明書の更新を自分たちでやる必要があります)\n今回のように、 S3 で・独自ドメインで・SSL/TLS 経由で 静的サイトをホスティングする場合には、 CloudFront を利用するしか方法がないということになります。\nAWS Certificate Manager（SSL/TLS 証明書のプロビジョン、管理、およびデプロイ）| AWS 他にもメリットはあるもよう この記事を書くにあたってあらためて S3 の仕様について調べていたのですが、どうやら S3 にもスロットリングの仕様があるようです。\nS3 では、バケット内の一つのプレフィックスにつき 1 秒間に 3,500 件の PUT/COPY/POST/DELETE 、5,500 件の GET/HEAD と、それぞれリクエストスロットリングが設定されています。これを超えるリクエストが発生すると、 S3 は 503 Slow Dwon エラーを返す場合があるようです。\nCloudFront を S3 の前段に置けばこのスロットリングを回避することもできそうです。\nベストプラクティスの設計パターン: Amazon S3 パフォーマンスの最適化 - Amazon Simple Storage Service またコスト面でもメリットはありそうです。 S3 のみの場合、 CloudFront 経由の場合ともにインターネットへのデータ転送にかかる料金はほぼ同じですが、 CloudFront を利用すれば S3 オブジェクトへのリクエスト回数を減らすことができるので、その分はコスト削減ができそうです。ただし、 S3 オブジェクトへのリクエストにかかる料金は 1,000 リクエストあたり 0.00037 USD (東京リージョン) と かなり少額なので、そもそもリクエスト数が少ない場合にはあまりコスト削減は見込めないと思います。\nまとめ AWS 上で静的サイトをホスティングする際に Amazon S3 と Amazon CloudFront をセットで使うことで得られるメリットについて簡単にまとめてみました。\nS3 だけでホスティングできるという手軽さはありますが、表示速度の高速化や独自ドメインでの SSL 対応など、得られるメリットは大きいと思います。ただし、 CloudFront の設定変更には多少時間がかかるので、色々設定をいじって変更して\u0026hellip;という作業が頻繁に発生する場合にはちょっとストレスを感じるかもしれません。とは言っても、一時期に比べてそれらにかかる時間も相当短縮されているのでストレスもだいぶ軽減されています。\nSlashing CloudFront change propagation times in 2020 – recent changes and looking forward | Networking \u0026amp;amp; Content Delivery CloudFrontの作成や更新時間が約5倍高速になりました | Developers.IO CloudFront まで設定するハンズオンはあまり無いので面倒だと思われることもあるかもしれませんが、得られるメリットは大きいのでぜひ S3 と CloudFront はセットで使ってみてください。\nStatic website hosting を有効にして Route 53 で独自ドメインを設定\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-advantages-of-using-s3-and-cloudfront/",
    "title": "なぜ S3 の前段に CloudFront を置くと良いのか"
  },
  {
    "contents": "オンラインで開催された「JAWS-UG コンテナ支部 #17 ECS/Fargate PV 1.4 ローンチ記念！」のレポ (メモ) です。私のコンテナに対するレベル感は、 最近少しずつ Docker 触り始めてコンテナ系の情報も追っているもののよくわからん という状態です。よくわからんなりにも話を聴いてみようということで参加しました。\nちなみにコンテナ支部のイベントは初参加でした。\nJAWS-UGコンテナ支部 #17 ECS/Fargate PV 1.4 ローンチ記念！ - connpass ハッシュタグ #jawsug_ct 目次 タイムテーブル セッション コンテナとコンテナのつなぎかた on ECS 金融系サービスでECS/Fargateを設計するということ FargateでService、Cron、RunTask基盤を運用する ホスティングサービスのインフラ環境を再構築！〜AWS Fargateのおかげで幸せになれた話〜 全体の感想 その他の資料 タイムテーブル 時間 内容 登壇者 (敬称略) 17:55 - 19:00 Black Belt Webinar \u0026ldquo;AWS コンテナサービスアップデート\u0026rdquo; の裏つぶやき コンテナ支部運営のみなさん 19:00 - 19:10 休憩 \u0026mdash; 19:10 - 19:15 支部案内など コンテナ支部運営 19:15 - 19:40 コンテナとコンテナのつなぎかた on ECS Masatoshi Hayashi / AWSJ 19:40 - 20:05 金融系サービスでECS/Fargateを設計するということ Masaya ARAI 20:05 - 20:15 休憩 \u0026mdash; 20:15 - 20:40 FargateでService、Cron、RunTask基盤を運用する Shohei Koyama 20:40 - 21:05 ホスティングサービスのインフラ環境を再構築！〜AWS Fargateのおかげで幸せになれた話〜 Takayuki Yoshioka 21:05 - 21:10 締めのご挨拶 コンテナ支部運営 ※ Black Belt Webinar “AWS コンテナサービスアップデート” の裏つぶやき については配信ではなく、 Twitter 上でたくさんツイートされていました。 #jawsug_ct セッション 各セッションの聴講メモです。発表資料も、公開されているものに関しては随時追加していきます。\nコンテナとコンテナのつなぎかた on ECS 三種類のコンテナ連携方法 タスク内接続 サービス関連系 イベント駆動連携 時間経過、コンテナが発行したイベントなど タスク内コンテナ接続 サイドカー、アンバサダー、アダプターとか呼ばれる ユースケース ログ関連機能をサイドカーにする ドメインと関係のないログのルーティング処理をアプリコンテナの外に出す サービス間接続 マイクロサービスの必要性とは\n高度なモジュール化という選択肢 あえてサービスに分けるのは、インテグレーション時に問題が発生してくるタイミング (調整に時間がかかる) 一つのライブラリを更新したら壊れた Fargate に移行したい サービス間の接続とは、インテグレーション問題の解決\n接続のあまり良くないパターン\nコンテナ内部の実装に暗黙的に依存 DB スキーマの共有など これはインテグレーションの範囲が絞れていない、サービス間接続のモチベーションを満たさない 一般的なサービス間の接続\nドキュメント化された API で接続 サービス内の実装に依存しない どこでコンテナが動いているかを管理するためにサービスディスカバリ ECS Service Discovery\nサービス同士を API でつなぐ ヘルスチェックが通ったらサービス登録 Cloud Map にサービス IP 登録 -\u0026gt; Route 53 に DNS レコード登録 独立性の高いサービス同士を統一した方法でつなぎたい\nいろんな状況 (EC2, EKS, ECS, Python, Go, Java\u0026hellip;) サービスメッシュ (App Mesh) で解決する App Mesh\nサービス間通信の標準化 サービスが独立しても接続しやすい 同期的なつなぎ方ばかりしていると、一つのサービスに負担が集中する\nサービス間に Event Bus を挟む イベント駆動連携 EventBridge \u0026amp; SQS\nマネージドな Event Bus スキーマを定義してコードを生成、サービス接続しやすい SQS と連携することで、ピークが読めないワークロードに対応 SNS \u0026amp; SQS\nスキーマの定義などはできないが、シンプルに高スループットな連携が可能 Kinesis Data Stream\nイベントのリプレイがしやすい 依存関係が課題になりやすい\nどのキューがどのサービスで、どういう目的で タイムアウトの整合性 エラー時にどうなるか サービス間の処理の流れが明示的でないのでわかりにくい サービスを追加するときに、他のサービスへの影響が発生 Step Functions で依存関係を整理 依存関係の管理を Step Functions に移譲することで各サービスの独立性が高まる あるステップを他の実装に差し替えるということがやりやすい まとめ なるべくシンプルに 必要に応じて様々なパターンを検討 サービスの独立性を保つことを意識する 質疑 Cloud Map だけでは要件が達成できない場合に App Mesh を使う Blue/Green デプロイ 金融系サービスでECS/Fargateを設計するということ 金融サービスでの ECS/Fargate を利用した事例と設計ポイント\n金融サービスの変化 安定性 x セキュリティ x ガバナンス\n社会インフラ 対サイバー攻撃 伝統的な統制ルール, FISC 法改正や FinTech 企業の登場による金融サービスの拡張が進んでいる\nECS/Fargate という選択 FinTech 領域ではユーザファーストな対応が必要 ユーザ価値の向上にリソースを集中 頻繁な改良 (変更) をしたい 運用保守作業をなるべく減らしたい 品質を維持したい -\u0026gt; コンテナによる解決 データプレーンとして EC2, Fargate の選択肢 コントロールプレーンとして ECS, EKS の選択肢 EC2 だと運用負荷が大きい -\u0026gt; Fargate Fargate は価格改定や Savings Plans によって料金もおさえられる 設計に必要な視点の整理 何を拠り所にして設計をすればよいか 安定性 x セキュリティ x ガバナンス については Fargate でも意識的に満たさないといけない FISC 安全対策基準への遵守 Well-Architected Framework 特定のワークロードに特化した W-A Lens 金融に特化した Lens を FISC に次ぐ 2 つめのベースラインとして設計 ECS/Fargate 設計の要所 FISC の安全対策基準 W-A FSI Lens AWS Well-Architected Framework Financial Services Industry Lens ネットワーク トラフィックを可能な限りプライベートに保つ ECR, S3, CloudWatch には VPC エンドポイントでアクセス アプリケーション クレデンシャル情報の保護 Secret Manager, パラメータストアのシークレットに保存 タスク定義内で環境変数に コンテナイメージ、タスク定義の書き方 ベースイメージはどこから？テスト済み？ 必要最低限のパッケージのみのコンテナを作成 CI/CD パイプラインの整備 脆弱性スキャンの有効可 データの保存場所 CI/CD で使う中間ファイルをもつ S3 へのアクセス制御 まとめ ECS/Fargate を安全に金融サービスで扱うための設計 FISC, W-A FSI Lens 質疑 VPC エンドポイントに対応してないサービスの辛さ 現状は特にない。ECR も最近対応して、 ECS まわりはほぼほぼ閉じたネットワークで動かせるようになった FargateでService、Cron、RunTask基盤を運用する ECS, Fargate で十分 Service : サーバー RunTask : 任意のタイミングで実行。ワンショット Cron : 定期実行 Fargate のタスク開始方法 Service RunTask Cron は EventBrige から Fargate の RunTask API Terraform 基盤の共通化 シンプルな設計 スケーラブル スパイクを予想している場合はオートスケールの値を アーキテクチャ図は draw.io で作成 (.draw.io, .png を Git 管理) Terraform module で共通化 変化が多いところは別ソリューションを考える 新規サービスを追加する際でも Dockerfile があれば小一時間で作れる ecs-deploy という内製ツール (Go) Slack でデプロイ メンテナンスの ON/OFF も Slack で ログ リアルタイムは Datadog, 長期保存 (監査用) は S3 モニタリング ecs-update-notify (Go) タスクが切り替わったとき、切り替わらなかったときに通知 sioncojp/ecs-update-notify | GitHub Fargate になったからと言って簡単になるわけではない。センスが必要 ホスティングサービスのインフラ環境を再構築！〜AWS Fargateのおかげで幸せになれた話〜 インフラ環境の再構築 構築 RedMica : Redmine の次期バージョンの新機能を先行して利用可能 redmica/redmica: The future Redmine you can get today — yet another distribution of a flexible project management software named Redmine | GitHub Before EC2 メイン After メインの Web サーバを Fargate on ECS 踏み台サーバ (EC2) 設定ファイルは S3 に保持 再構築のポイント データの永続化 DB, ログ, 添付ファイル S3 と EFS が候補 S3 はアプリの変更が必要、 EFS は変更不要 ランニングコストは S3 のほうが安い そもそも開発当時は Fargate の EFS 連携ができなかった (最近のアップデートで連携可能に) redmica/redmica_s3: Uses Amazon S3 for storing attachments | GitHub ユーザごとに ECS Service 起動 コストメリットがない ユーザごとにタスク定義するのが煩雑 -\u0026gt; 却下 Apartment (Gem) の利用 メンテナンス問題 migration 関連でつらそう -\u0026gt; 却下 Apache + Passenger プロセスごとにポート切り替え Apache の設定情報は S3 に保存 なぜ ECS ? 当時 EKS が東京リージョンになかった クラスターの料金が高かった なぜ Fargata ? EC2 の管理が面倒、リソースの計算も面倒 管理コストが低い EC2 と比較してランニングコストが高い、サーバに入れない サーバに入れないことは、調査範囲を限定できるという意味ではメリット ECS になれるまでは EC2 タイプも立てておく 運用 CodePipeline GitLab から CodeCommit にミラーリング CodeBuild (build) CodeBuild (test) Dockerfile ENTORYPOINT に DB 設定系のコマンドを仕込んでおく Step Functions サービスアカウントの新規登録 サービスアカウントの変更 サービス停止、データ削除 Migration 問題 rails db:migrate をどうやって実行するか 当初はパイプラインに migration フローを入れていた S3 からサイトごとの設定情報を取得、Step Functions の並列処理でサイトの数だけ migration を実行 全体の感想 「JAWS-UG コンテナ支部 #17 ECS/Fargate PV 1.4 ローンチ記念！」のレポ (メモ) でした。\n冒頭に書いた通り、コンテナほんとになんもわからん状態でしたが、わからないなりに追っていた AWS におけるコンテナに関する情報が、なんとなく自分の中で少し繋がったかなという印象です。今回はコンテナ支部のイベントだったんですが、セッションの中で何度も Step Functions の話題が出てきていたので、重い腰を上げて使ってみたいなと思いました。\nFargate, ECS に関しては先日レビューを書いた「みんなの AWS」内でハンズオンをやったので、今回のイベントを受けてもう一度試してみるとなにか発見がありそうな気がしています。\nあと印象的だったのが、金融系のサービスで Fargate を使う話です。 FISC と AWS Well-Architected Framework の FSI Lens に沿った設計をしていくことで、自ずと適切な設計・構成になっていくんだなと感じました。金融系は安全性・セキュリティ・ガバナンスに関して厳しい規定がありますが、その他の Web アプリケーションでも Well-Architected Framework に沿った設計をしていくことはとても大事だなと感じました。\nオンライン開催だと全然わからない分野のイベントでも気軽に参加できるので良いなと思ってます。運営のみなさま、オンラインでの開催ありがとうございました！\nその他の資料 connpass のイベントページにて登壇資料、動画、および参加レポートがまとめられています。\nJAWS-UGコンテナ支部 #17 ECS/Fargate PV 1.4 ローンチ記念！ - 資料一覧 - connpass ",
    "permalink": "https://michimani.net/post/event-jaws-ug-container-17/",
    "title": "[レポート] JAWS-UG コンテナ支部 #17 ECS/Fargate PV 1.4 ローンチ記念！ にオンラインで参加しました #jawsug_ct"
  },
  {
    "contents": "DeepL の有料プラン (DeepL Pro) が利用可能になり、 API で翻訳などの DeepL のサービスを利用することが出来るようになりました。今回は Python で DeepL の API を実行してみます。\n目次 DeepL Pro 前提 Python で DeepL API を試す Translating text Monitoring usage まとめ DeepL Pro DeepL の有料プラン (DeepL Pro) には次の 3 つがあります。\n個人向け Starter Advanced Ultimate チーム向け Starter Advanced Ultimate 開発者向け DeepL API それぞれ次のような料金設定になっています。(単位は 円)\n個人向け 年払い (月換算) 月払い Starter 9,000 (750) 1,200 Advanced 30,000 (2,500) 3,800 Ultimate 60,000 (5,000) 7,500 チーム向け 年払い (月換算) 月払い Starter 9,000 (750) 1,200 Advanced 30,000 (2,500) 3,800 Ultimate 60,000 (5,000) 7,500 ※ ユーザ 1 人あたりの料金です。\n開発者向け 月払い + 利用分 DeepL API 630 ※ 630 円/月 の基本料金に加えて、翻訳済みのテキスト 1,000,000 文字あたり 2,500 円の料金が追加されます。\n個人向け・チーム向けの各プランの機能差については公式のプラン表を参照してください。\nDeepL Pro 前提 今回は DeepL Pro の 開発者向け プランを利用します。\nAPI の実行に必要な auth_token については、プラン契約後のマイページで確認・取得済みとします。\nまた、今回は Python 3.x で実装します。\nPython で DeepL API を試す DeepL API では、大きく分けて 3 種類の API が用意されています。\nTranslating text Translating Documents (beta) Other functions 今回はこれらのうち、 Translating text と、 Other functions に含まれる Monitoring usage の API を試してみます。\nTranslating text いわゆる普通の文字列翻訳です。\nエンドポイントは https://api.deepl.com/v2/translate で、メソッドは POST 、他の翻訳サービスの API とほぼ同じで、 翻訳対象のテキスト 、 ターゲットとなる言語 、 API のトークン (auth_key) が必須のパラメータとなっています。\nただしリクエスト時の Content-Type は application/x-www-form-urlencoded となっています。その他、 API の詳細については下記のドキュメントを参照してください。\nDeepL API | Translating text Python のサンプルコードはこんな感じになります。\nimport argparse import json import os import sys import urllib.parse import urllib.request with open(os.path.dirname(os.path.abspath(__file__)) + \u0026#39;/../config.json\u0026#39;) as j: config = json.load(j) AUTH_KEY = config[\u0026#39;auth_key\u0026#39;] DEEPL_TRANSLATE_EP = \u0026#39;https://api.deepl.com/v2/translate\u0026#39; T_LANG_CODES = [\u0026#34;DE\u0026#34;, \u0026#34;EN\u0026#34;, \u0026#34;FR\u0026#34;, \u0026#34;IT\u0026#34;, \u0026#34;JA\u0026#34;, \u0026#34;ES\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;PL\u0026#34;, \u0026#34;PT-PT\u0026#34;, \u0026#34;PT-BR\u0026#34;, \u0026#34;PT\u0026#34;, \u0026#34;RU\u0026#34;, \u0026#34;ZH\u0026#34;] S_LANG_CODES = [\u0026#34;DE\u0026#34;, \u0026#34;EN\u0026#34;, \u0026#34;FR\u0026#34;, \u0026#34;IT\u0026#34;, \u0026#34;JA\u0026#34;, \u0026#34;ES\u0026#34;, \u0026#34;NL\u0026#34;, \u0026#34;PL\u0026#34;, \u0026#34;PT\u0026#34;, \u0026#34;RU\u0026#34;, \u0026#34;ZH\u0026#34;] p = argparse.ArgumentParser() p.add_argument(\u0026#39;-m\u0026#39;, \u0026#39;--message\u0026#39;, help=\u0026#39;text to translate. (Default: Hello World.)\u0026#39;, default=\u0026#39;Hello World.\u0026#39;) p.add_argument(\u0026#39;-t\u0026#39;, \u0026#39;--target\u0026#39;, help=f\u0026#39;target language code (Default: JA). allowed lang code : {str(T_LANG_CODES)}\u0026#39;, default=\u0026#39;JA\u0026#39;) p.add_argument(\u0026#39;-s\u0026#39;, \u0026#39;--source\u0026#39;, help=f\u0026#39;source language code (Default: auto). allowed lang code : {str(S_LANG_CODES)}\u0026#39;, default=\u0026#39;\u0026#39;) args = p.parse_args() def translate(text, s_lang=\u0026#39;\u0026#39;, t_lang=\u0026#39;JA\u0026#39;): headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/x-www-form-urlencoded; utf-8\u0026#39; } params = { \u0026#39;auth_key\u0026#39;: AUTH_KEY, \u0026#39;text\u0026#39;: text, \u0026#39;target_lang\u0026#39;: t_lang } if s_lang != \u0026#39;\u0026#39;: params[\u0026#39;source_lang\u0026#39;] = s_lang req = urllib.request.Request( DEEPL_TRANSLATE_EP, method=\u0026#39;POST\u0026#39;, data=urllib.parse.urlencode(params).encode(\u0026#39;utf-8\u0026#39;), headers=headers ) try: with urllib.request.urlopen(req) as res: res_json = json.loads(res.read().decode(\u0026#39;utf-8\u0026#39;)) print(json.dumps(res_json, indent=2, ensure_ascii=False)) except urllib.error.HTTPError as e: print(e) if __name__ == \u0026#39;__main__\u0026#39;: t_lang = args.target s_lang = args.source text = args.message if t_lang not in T_LANG_CODES: print(( f\u0026#39;ERROR: Invalid target language code \u0026#34;{t_lang}\u0026#34;. \\n\u0026#39; f\u0026#39;Alloed lang code are following. \\n{str(T_LANG_CODES)}\u0026#39; )) sys.exit(1) if s_lang != \u0026#39;\u0026#39; and s_lang not in S_LANG_CODES: print(( f\u0026#39;WARNING: Invalid source Language code \u0026#34;{s_lang}\u0026#34;. \\n\u0026#39; \u0026#39;The source language is automatically determined in this request. \\n\u0026#39; f\u0026#39;Allowed source lang code are following. \\n{str(S_LANG_CODES)} \\n\\n\u0026#39; )) s_lang = \u0026#39;\u0026#39; translate(text, t_lang=t_lang, s_lang=s_lang) このスクリプト translate.py と、 auth_key を記述した config.json を下記のように配置します。\n├── config.json └── src └── translate.py config.json の中身は下記のようにします。\n{ \u0026#34;auth_key\u0026#34;: \u0026#34;your-deelpl-api-auth-token\u0026#34; } そして、次のように実行すると、翻訳結果が出力されます。\n$ python3 src/translate.py -m \u0026#34;これは DeepL API のサンプルプログラムです。\u0026#34; -t EN { \u0026#34;translations\u0026#34;: [ { \u0026#34;detected_source_language\u0026#34;: \u0026#34;JA\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;This is a sample program for the DeepL API.\u0026#34; } ] } Monitoring usage 続いては、 DeepL API の使用状況を取得する API を試してみます。\nエンドポイントは https://api.deepl.com/v2/usage 、メソッドは POST 、パラメータは auth_key のみです。\nDeepL API | Monitoring usage Python のサンプルスクリプトは下記のようになります。\nimport json import os import urllib.parse import urllib.request with open(os.path.dirname(os.path.abspath(__file__)) + \u0026#39;/../config.json\u0026#39;) as j: config = json.load(j) AUTH_KEY = config[\u0026#39;auth_key\u0026#39;] DEEPL_TRANSLATE_EP = \u0026#39;https://api.deepl.com/v2/usage\u0026#39; def monitor_usage(): headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/x-www-form-urlencoded; utf-8\u0026#39; } params = { \u0026#39;auth_key\u0026#39;: AUTH_KEY } req = urllib.request.Request( DEEPL_TRANSLATE_EP, method=\u0026#39;POST\u0026#39;, data=urllib.parse.urlencode(params).encode(\u0026#39;utf-8\u0026#39;), headers=headers ) try: with urllib.request.urlopen(req) as res: res_json = json.loads(res.read().decode(\u0026#39;utf-8\u0026#39;)) print(json.dumps(res_json, indent=2, ensure_ascii=False)) except urllib.error.HTTPError as e: print(e) if __name__ == \u0026#39;__main__\u0026#39;: monitor_usage() これを実行すると、現在の翻訳文字数の合計と制限文字数が取得できます。\n$ python3 src/usage.py { \u0026#34;character_count\u0026#34;: 192, \u0026#34;character_limit\u0026#34;: 1000000 } まとめ DeepL API を Python で実行してみた話でした。この記事で紹介したサンプルについては GitHub に置いています。\n翻訳精度が Google 翻訳よりも優れているという評価もある DeepL ですが、有料プランとして API も利用可能になったことで Web アプリやネイティブアプリでの利用もしやすくなりました。翻訳制度に関しては翻訳する文章の性質 (専門用語が多い、話し言葉が多い など) によってしっかり吟味する必要がありますが、新しい選択肢として候補に入れておいても良さそうです。\n",
    "permalink": "https://michimani.net/post/programming-using-deepl-api-by-python/",
    "title": "Python で DeepL API を使ってみた"
  },
  {
    "contents": "一つ前の記事で Google Apps Script を使って Slack のステータスを変更しましたが、今回はそれを Python 製の CLI ツールっぽいものとして作ってみた話です。\n目次 概要 実装 config.json change_status.py Docker イメージで実行 まとめ 概要 コマンドラインから Slack のステータスを変更するコードを Python で実装します。 CLI ツールというほどしっかりしたものではないため、 「っぽいもの」 ということにしておきます。\n基本的には前回の記事で書いたスクリプトを Python に書き換えるのですが、セットする絵文字やメッセージをコマンドラインからオプションとして渡せるような形で実装をしています。\n実装 構成としては、 Slack の API や対象のユーザ ID などを記述した設定ファイル config.json と、実際の処理をする Python スクリプト change_status.py の 2 つを次のように配置します。\n$ tree . . ├── config.json └── src └── change_status.py config.json config.json の中身は次のような構造にします。\n{ \u0026#34;slack\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;XXXXXXXX\u0026#34;, \u0026#34;api_token\u0026#34;: \u0026#34;xoxp-********-********-********\u0026#34;, \u0026#34;default_status_emoji\u0026#34;: \u0026#34;:ghost:\u0026#34;, \u0026#34;default_status_message\u0026#34;: \u0026#34;I\u0026#39;m a ghost.\u0026#34; } } user_id : ステータス変更の対象となる Slack ユーザの ID です。 api_token : Slack API を利用するための token です。 スコープとして users.profile:write が必要になります。 default_status_emoji : 絵文字指定オプションを省略した際に使用する絵文字コードです。 default_status_message : メッセージ指定オプションを省略した際に使用するメッセージです。 change_status.py change_status.py は次のように実装します。ランタイムは Python 3 です。\nimport argparse import json import os import urllib.parse import urllib.request # define config value with open(os.path.dirname(os.path.abspath(__file__)) + \u0026#39;/../config.json\u0026#39;) as j: config = json.load(j) USER_ID = config[\u0026#39;slack\u0026#39;][\u0026#39;user_id\u0026#39;] API_TOKEN = config[\u0026#39;slack\u0026#39;][\u0026#39;api_token\u0026#39;] DEFAULT_STATUS_EMOJI = config[\u0026#39;slack\u0026#39;][\u0026#39;default_status_emoji\u0026#39;] DEFAULT_STATUS_MESSAGE = config[\u0026#39;slack\u0026#39;][\u0026#39;default_status_message\u0026#39;] PROFILE_SET_URL = \u0026#39;https://slack.com/api/users.profile.set\u0026#39; # CLI option setting p = argparse.ArgumentParser() p.add_argument(\u0026#39;-e\u0026#39;, \u0026#39;--emoji\u0026#39;, help=\u0026#39;Slack emoji code for status icon. (e.g) :ghost:\u0026#39;, default=DEFAULT_STATUS_EMOJI) p.add_argument(\u0026#39;-m\u0026#39;, \u0026#39;--message\u0026#39;, help=\u0026#39;message for status.\u0026#39;, default=DEFAULT_STATUS_MESSAGE) args = p.parse_args() def set_status(emoji, message): # type: (str) -\u0026gt; () \u0026#34;\u0026#34;\u0026#34;Set Slack status.\u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;Authorization\u0026#39;: \u0026#39;Bearer %s\u0026#39; % API_TOKEN, \u0026#39;X-Slack-User\u0026#39;: USER_ID, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json; charset=utf-8\u0026#39; } params = { \u0026#39;profile\u0026#39;: { \u0026#39;status_emoji\u0026#39;: emoji, \u0026#39;status_text\u0026#39;: message } } req = urllib.request.Request( PROFILE_SET_URL, method=\u0026#39;POST\u0026#39;, data=json.dumps(params).encode(\u0026#39;utf-8\u0026#39;), headers=headers ) with urllib.request.urlopen(req) as res: print(json.dumps(json.loads(res.read().decode(\u0026#39;utf-8\u0026#39;)), indent=2)) if __name__ == \u0026#39;__main__\u0026#39;: status_emoji = args.emoji status_message = args.message set_status(status_emoji, status_message) 特に変わったところは無いですが、コマンドラインから実行する際に引数とオプションを楽に扱うことが出来るように argparse モジュールを使用しています。\nファイル上部にある下記の部分で、コマンドラインオプションの設定をしています。\n# CLI option setting p = argparse.ArgumentParser() p.add_argument(\u0026#39;-e\u0026#39;, \u0026#39;--emoji\u0026#39;, help=\u0026#39;Slack emoji code for status icon. (e.g) :ghost:\u0026#39;, default=DEFAULT_STATUS_EMOJI) p.add_argument(\u0026#39;-m\u0026#39;, \u0026#39;--message\u0026#39;, help=\u0026#39;message for status.\u0026#39;, default=DEFAULT_STATUS_MESSAGE) args = p.parse_args() これで、 -e または --emoji オプション、 -m または --message オプションを使用して絵文字とメッセージそれぞれの値を受け取ることができます。例えば -e オプションで指定された値をプログラム内で参照するには、 args.emoji のようにして使います。\nargparse については下記の記事を参考にしました。\n[テンプレ付き]PythonでCLIツールを作るときのTips | Developers.IO argparse を使用すると、 -h オプションで実行することで add_argument() の help で指定した文字列が良い感じに出力されます。\n$ python3 src/change_status.py -h usage: change_status.py [-h] [-e EMOJI] [-m MESSAGE] optional arguments: -h, --help show this help message and exit -e EMOJI, --emoji EMOJI Slack emoji code for status icon. (e.g) :ghost: -m MESSAGE, --message MESSAGE message for status. Docker イメージで実行 次のコマンドを叩いて実行します。\n$ python3 src/change_status.py ただ、これだと Python 3 の環境が無いとダメなので、 Docker イメージで実行できるようにしてみました。\n次のような Dockerfile を用意します。今回は Python 3.8.3 のイメージを使用します。\nFROM python:3.8.3 COPY . /app ENTRYPOINT [ \u0026#34;python\u0026#34;, \u0026#34;/app/src/change_status.py\u0026#34; ] これをビルドして\n$ docker build -t michimani/chss . 実行します。\n$ docker run michimani/chss オプションを指定する場合は次のように実行します。\n$ docker run michimani/chss -e \u0026#34;:monkey:\u0026#34; -m \u0026#34;I am a monkey.\u0026#34; このように、 Docker は CLI っぽく使うことが出来るのが良いですね。必要なプロセスを必要なときにだけ起動して、不要になったら削除するという Docker の主とした使い方です。 \u0026hellip; まあこれは先日レビューしたみんなの Python の 第 2 章に書かれていたことなんですが。\nまとめ Slack のステータスを変更する CLI ツールっぽいものを Python で作ってみた話でした。 ソースコードは GitHub に置いているので、もしよかったら使ってみてください。\n用途としては、毎日の勤務開始・終了と一緒に使うくらいですかね\u0026hellip;。\n",
    "permalink": "https://michimani.net/post/programming-set-slack-status-by-python/",
    "title": "Slack のステータスを変更する CLI ツールっぽいものを Python で作ってみた"
  },
  {
    "contents": "ふと、 Google カレンダーの予定と Slack のステータスを連携させたいなと思い、 Google Apps Script でその処理を書いてみた話です。\n概要 先に書いておきますが、冒頭で書いた 「Google カレンダーの予定と Slack のステータスを連携させたい」 というのは実現できませんでした。そういう便利なものは既に存在しているので、もし連携させたいなと思っている方は使ってみてください。\nGoogle Calendar と Slack を連携させる | Slack なので、今回はただ単に Google Apps Script (以下、GAS) で Slack のステータスを変更してみたという話になります。いわゆる供養ブログです。\n(やりたかったこと) GAS を実行するためのトリガーには次の 2 通りがあります。\n時間主導型 カレンダーから 時間主導型 は、いわゆる cron みたいな形式で実行タイミングを指定します。\nカレンダーから は、 Google カレンダーと連動したトリガーとなります。この 連動 を、勝手に良いように解釈して 「予定の開始とか終了もトリガーにできるんじゃね？」 と思っていたのが間違いでした。 GAS のトリガーとして指定できるカレンダーの連動とは、 予定の変更 のようです。\nGAS で Slack のステータス変更 では、供養する GAS の紹介です。\nSlack のステータス変更は POST https://slack.com/api/users.profile.set のエンドポイントにリクエストします。\nこのエンドポイントではユーザ名やメールアドレスも変更できますが、ステータスを変更する場合は次のような payload を post します。\n{ \u0026#34;profile\u0026#34;: { \u0026#34;status_emoji\u0026#34;: \u0026#34;:ghost:\u0026#34;, \u0026#34;status_text\u0026#34;: \u0026#34;status message\u0026#34; } } status_emoji にはステータスの絵文字コードを指定します。存在しない絵文字を指定した場合は profile_status_set_failed_not_valid_emoji エラーとなります。 staus_emoji 、 status_text ともに空文字を指定すると、ステータスをリセットすることができます。\nusers.profile.set method | Slack また、 GAS で外部 API にリクエストを送信するためには UrlFetchApp.fetch() を使用します。\nClass UrlFetchApp | Apps Script | Google Developers まとめ Google Apps Script で Slack のステータスを変更してみた話でした。\nとくにまとめる内容もないですが、便利な仕組みはだいたい既に存在しているなと思いました。(小並)\n",
    "permalink": "https://michimani.net/post/programming-set-slack-status-via-google-apps-script/",
    "title": "Google Apps Script で Slack のステータスを変更する"
  },
  {
    "contents": "このブログ (Hugo) のビルドとデプロイには AWS CodePipeline を使っており、パイプラインの実行が完了するまでに 2 〜 3 分かかっていました。せっかく Hugo のビルドが高速なのに全体でそんなに時間がかかっているのは残念だったので、なんとかして時間短縮できないかなーというのが今回の話です。\n目次 やること CodePipeline を使ったビルド・デプロイフロー やったこと CodeBuild だけを使ったビルド・デプロイフロー CodeBuild だけにすることで発生するメリット・デメリット まとめ やること CodePipeline を使用した Hugo のビルドおよびデプロイの時間を、なんとかして短縮したいと思います。 \u0026lsquo;なんとかして\u0026rsquo; といっても何が原因がわからないと手のつけようがないので、とりあえず現状の構成をおさらいしておきます。\nCodePipeline を使ったビルド・デプロイフロー これまでは次のような構成で Hugo のビルドとデプロイを行っていました。\nCodePipeline では、 GitHub への Push とトリガーにして次の 3 つのフェーズで処理を実行していました。\nGitHub からソースを取得 CodeBuild で Hugo のビルド、 S3 バケットへデプロイ Lambda 関数を実行し、 Slack へパイプラインの実行結果を通知 この中でメインなのはもちろん \u0026ldquo;2. CodeBuild で Hugo のビルド、 S3 バケットへデプロイ\u0026rdquo; ですが、その処理自体の所要時間は約 1 分程度でした。\nソースの取得は CodeBuild 単体でもできますし、昨年 12 月のアップデートにより、各 Code シリーズから SNS トピックおよび今年の 4 月に GA となった AWS Chatbot 経由で通知を行うことが出来るようになりました。\nじゃあもう CodeBuild だけでいいやん。となるわけです。\nやったこと CodePipeline の使用をやめて、 CodeBuild だけでビルド・デプロイフローを完結するように構成を変更しました。\nCodeBuild だけを使ったビルド・デプロイフロー CodeBuild だけを使ったビルド・デプロイフローは、次のような構成になります。\nGitHub への Push を CodeBuild で検出し、ビルドを開始します。そして、結果の通知は AWS Chatbot によって Slack に通知します。この構成にすることで、全体の処理時間は約 1 分となりました。\nただし、通知の内容に関しては以前よりも簡素なものになりました。以前は 通知用の Lambda 関数を用意して、実行中のパイプラインの情報をゴニョゴニョして次のような通知を Slack に投げていました。\nこれが、 AWS Chatbot 経由になると次のような通知内容になります。\nま、ビルド終わったかどうかがわかれば良いので、十分といえば十分ですね。\nCodeBuild だけにすることで発生するメリット・デメリット CodeBuild だけにすることで発生するメリットとデメリットのおさらいです。\nメリット まず、メリットとしては下記の 3 点が挙げられます。\nフロー全体の時間短縮 CodePipeline の費用削減 Lambda の費用削減 フロー全体の時間短縮はもちろんですが、使用しなくなった CodePipeline 、 Lambda の費用削減も達成することができます。特に CodePipeline に関しては、毎月の無料枠としてアクティブなパイプライン 1 つは無料ですが、それ以降はパイプライン 1 つあたり 1 USD/month の費用が発生します。既に 2 つ以上のパイプラインがアクティブな場合は 1 USD の削減になりますし、アクティブなパイプラインがこれだけだった場合は、貴重な無料枠を空けることができます。\n\u0026hellip;と、この内容は以前にも書いていました。\nデメリット 今回のフローに関していてば、デメリットは無いと思います。あえて上げるとすれば、 Slack への通知内容が簡素になったくらいでしょうか。\nまとめ Hugo のビルドとデプロイに CodePipeline をやめて CodeBuild だけを使うようにした話でした。\n今回のようにソースが GitHub で管理されている場合には 1 CodeBuild で Push 等のイベントをトリガーにすることができるので、 CodePipeline でフローを開始する必要がなくなります。また、 Code シリーズからの通知、 Chatbot の GA により通知に関しても CodeBuild 単体で行うことが出来るようになったので、ここでも CodePipeline および通知用の Lambda 関数が不要となります。\nこのブログのような個人のちょっとしたプロジェクトや、実際のプロジェクトでも簡素なものに関しては CodeBuild だけでも十分なビルドフローを構築できそうです。\nソースが CodeCommit の場合には直接トリガーすることができないため、 EventBridge 経由で Lambda を実行し、その中で該当の CodeBuild を実行する といった流れが必要になります。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/aws-only-use-codebuild-for-building-hugo/",
    "title": "Hugo のビルドとデプロイに CodePipeline をやめて CodeBuild だけを使うようにした"
  },
  {
    "contents": "AWS には 170 を超えるサービスがありますが、それらのサービスをどのように組み合わせて使えばよいのかは、実際に動くものを作ってみないと理解が出来ない部分が大きいです。今回紹介する 「みんなの AWS 〜AWSの基本を最新アーキテクチャでまるごと理解！」 では、 AWS の基本から最新情報も含めた AWS の使いどころとポイントが詰まっています。購入から少し時間が経ってしまいましたが、レビューを書いておきます。\n届いた #みんなのAWS #awsforeveryone pic.twitter.com/oy9yyghbwi\n\u0026mdash; よっしー Lv.854 (@michimani210) April 20, 2020 目次 書籍の概要 著者 書籍の目次 各章の概要と感想 1 章 AWS の基礎知識 2 章 AWS で作る Web サービス 3 章 サーバーレスプラットフォームで作るモバイル向けアプリケーション 4 章 AWS で作るデータの収集・可視化 まとめ 書籍の概要 AWS の基礎と最新情報が詰まった一冊になっています。詳細な目次は下にも書きますが、本書では次の 4 つの章で構成されています。\nAWS の基礎知識 AWS で作るWeb サービス サーバーレスプラットフォームで作るモバイル向けアプリケーション AWS で作るデータの収集・可視化 AWS の全てのサービスに関する情報が書かれているわけではありませんが、一般的な Web アプリケーション構築、サーバーレスアーキテクチャでのアプリケーション構築、データ分析について、それぞれ AWS のサービスを利用したプラクティスについて書かれています。\nAWS の基礎知識については、その前段階にあたる「クラウドとは」という内容から話が始まるため、これから AWS を、クラウドを始めようと思っている方にも読みやすい内容になっています。\nまた、本書内で紹介されているコードやハンズオンパートで使用するコードについては GitHub で公開されています。\nclassmethod/aws-for-everyone: 「みんなのAWS」ソースコード置き場 著者 技術評論社から出版されている本書ですが、著者は AWS のプレミアコンサルティングパートナーで、「やってみた」系技術メディアの Developers.IO を運営されているクラスメソッドに所属しているエンジニアの方々です。 AWS について何か調べたことがある方なら、その技術ブログに一度はお世話になっていると思います。\n普段ブログを書かれている方々が書いた書籍ということで、非常に読みやすい文章構成になっています。\nクラスメソッド発「やってみた」系技術メディア | Developers.IO 書籍の目次 本書の目次は下記のようになっています。\n1 章 AWS の基礎知識\n1.1 クラウドとは 1.2 AWS のベストプラクティス 1.3 最低限押さえておくべきアカウント開設時のセキュリティ 1.4 AWS における監視 (モニタリング) 1.5 AWS を学習するコツ 2 章 AWS で作る Web サービス\n2.1 本性で解説するアプリケーションの全体構成と利用する AWS サービス 2.2 AWS のネットワーク基礎 2.3 アプリケーション構築・運用手段としてのコンテナ関連サービス 2.4 CI/CD を実現する Code シリーズ 2.5 モニタリング：障害監視、リソース監視 2.6 アプリケーションセキュリティ 2.7 コードによるインフラの運用管理 2.8 CloudFormation を利用したコンテナアプリケーション構築 3 章 サーバーレスプラットフォームで作るモバイル向けアプリケーション\n3.1 サーバーレスアーキテクチャとは 3.2 サーバーレスを実現する AWS サービス 3.3 構築するアプリケーションの全体構成 3.4 クラウド開発キット (AWS CDK) の準備 3.5 バックエンドアプリケーション (API) の構築 3.6 フロントエンドアプリケーションの作成 3.7 フロントエンドアプリケーションの配信 3.8 サーバーレスプリケーションのモニタリング 4 章 AWS で作るデータの収集・可視化\n4.1 AWS で作るデータ収集基盤 4.2 データ分析の基本知識と AWS サービス 4.3 データレイクを構築する 4.4 データウェアハウスを構成し、グラフ表示する 4.5 機械学習を導入する 4.6 構築したシステム (AWS リソース) を削除する 各章の概要と感想 ここからは各章の概要を簡単に書いていきます。\n1 章 AWS の基礎知識 この章は、 IT インフラの歴史からクラウドの登場、 そして AWS の登場についての内容から始まります。 AWS に関しては、 AWS Well-Architected フレームワーク に沿った内容で構成のポイントに触れられています。\nAWS Well-Architected – 安全で効率的なクラウド対応アプリケーション アカウント開設時のセキュリティ設定に関連して、 IAM の概要と使い方、権限設定の考え方についても解説されています。監視については、 AWS での監視の核となる Amazon CloudWatch の各用語 (メトリクス、ダッシュボード、 Logs Insight など) について説明されています。\nAWS を少し触ったことがある方にとっては基本的な部分のおさらいをすることができますし、 AWS が初めてだという人にとっても、導入としてわかりやすい内容になっています。\n2 章 AWS で作る Web サービス この章では、実際に AWS のサービスを組み合わせて Web アプリケーションを構築していきます。章の各節では Web アプリケーションの構築に使用するサービスの詳細説明と、インフラのコード化、 CI/CD についても触れられています。章の最後では、ハンズオン形式で Web アプリケーションを構築していく構成になっています。\nここで構築する Web アプリケーションは、下図のようないわゆる三層の Web アプリケーションになっています。\nご覧の通り、コンピューティングリソースには Amazon EC2 ではなく Amazon Fargate が利用されています。この構成を構築するハンズオンはマネジメントコンソールでポチポチするのではなく、すべて CloudFormation を使用して構築していきます。\nサンプルとはいえ、実際そのままでも使える構成のアプリケーションになっていて、 CLoudFormation のテンプレート、スタック作成のセルスクリプトについても本番運用でそのまま利用できそうな内容になっています。個人的にはインフラのコード化の部分がすごく参考になったと感じていて、この部分の執筆を担当されている濱田さん (ハマコーさん) の IaC のノウハウがすごく詰まっていて良いなと思います。\n3 章 サーバーレスプラットフォームで作るモバイル向けアプリケーション この章では、サーバレスアーキテクチャの概要 (メリット、選択する理由) の話から始まり、 AWS でサーバレスを実現するためのサービスが紹介されています。そして、実際にそれらのサービスを用いたモバイル向け Web アプリケーションを構築するハンズオンが用意されています。\n構築するアプリケーションのバックエンドは Amazon API Gateway 、 AWS Lambda 、 Amazon DynamoDB を利用し、フロントエンドには Vue.js でビルドしたアプリケーションを Amazon S3 に配置し、 Amazon CloudFront から配信する構成になっています。そして、それらは AWS CDK で管理するようになっており、サーバレスアーキテクチャだけでなく CDK についても学習することができます。\nもちろんこの章で使用するコードについても GitHub で公開されています。\nここ最近はサーバレスという言葉をたくさん聞くようになって、個人的にも Lambda や DynamoDB はよく使うようになっています。 AWS CDK に関しては昨年 7 月に GA となって以来ものすごいスピードでアップデートがされていて、注目のフレームワークになっています。個人的には AWS CDK で IaC 人生をスタートしたと言っても過言ではないので、実際に CDK を使ったアプリケーションの構築を経験することができたのは良かったです。\n4 章 AWS で作るデータの収集・可視化 この章では、 IoT デバイスから送信されるデータの収集基盤の構築、 AWS Glue 、 Amazon Athena 、 Amazon Redshift 、 Amazon QuickSight といったデータ分析サービスを利用したデータの分析方法について書かれています。\n前半部分は、 AWS SAM (AWS Serverless Application Model) を使って IoT デバイスから送信されるデータの収集基盤を構築し、データの取得、格納の流れを確認できます。 IoT デバイスを実際に準備するのは難しいため、 EC2 上に仮想の IoT デバイス (アプリケーション) を構築し、データを IoT Core で受け取り Kinesis Data Firehose で処理するといった構成を構築します。\n後半部分では、データレイク、データウェアハウスの用語の説明からはじまり、データウェアハウスを構築するために必要な前処理である ETL (Extract-Transform-Load) についても説明されています。その後、実際にデータレイク、データウェアハウスを構築し、 Amazon QuickSight でデータを参照するというハンズオンが用意されています。\n正直、データ分析に関してはわからない部分が多すぎるので、実際に手を動かして分析基盤を構築できるのは良いなと思いました。あらためて時間を作ってハンズオンはやり直してみます。\nまとめ AWS の使いどころとポイントがわかる「みんなの AWS」を読んでみた話でした。\n技術書に関しては、出版時点で既に情報が古くなっている可能性もあり、特に AWS に関しては日々のアップデートスピードがはやいので書籍を買うのは躊躇していました。しかし、日々のアップデートを追いかけるためにはそもそも今がどういう状態なのかを知る必要があります。そのためにはある時点での情報がぎっしり詰まっている書籍を買って、しっかり理解するというのは大事なことだなと感じました。\nこの「みんなの AWS」 に関しては出版イベントが開催され、 (ほぼ) 全著者集合で出版までの流れやそれぞれの執筆部分に対する想いについて語られていました。イベント参加者からの質問でも\nあえて紙ベースで今後古い情報になっていくであろう書籍を買う意義って何なんでしょうか？\nという質問があり、その答えとして\n書籍は体系立てられた教材で学習するのに向いていると考えています。そのため、基礎がしっかりしている方、応用を知りたい方、最新情報に触れたい方などは、インターネットの公式ドキュメント等で学習すると良いと考えています。\nという回答をされていました。\n「みんなのAWS〜AWSの基本を最新アーキテクチャでまるごと理解！」 全著者集合トークイベントのQ\u0026amp;amp;Aをまとめました！（動画あり） | Developers.IO 「みんなの AWS」では、この 体系立てられた教材で学習する という部分がしっかりできる書籍になっているなと感じました。ハンズオンで使用するコードも本番運用を意識された内容になっているので、公開されているサンプルコードを見るだけでも勉強になります。\n書籍内では基本的な部分とポイントとなる部分が書かれていますが、より詳細な部分に関する情報については Developers.IO の関連記事の URL が記載されているので、それも親切だなと思いました。\nそういう意味では、 AWS をこれから始めようと思っている方にはもちろんですが、私のように既に AWS は触っているけど実際の運用や構築方法に自信がないという方にもおすすめの書籍だと思います。\n",
    "permalink": "https://michimani.net/post/bookreview-aws-for-everyone/",
    "title": "[書評] AWS の使いどころとポイントがわかる「みんなの AWS」を読んでみた #awsforeveryone"
  },
  {
    "contents": "Amazon EventBridge を使って AWS Lambda を日本時間の毎月 1 日の 00:00 に実行する方法についてです。\n目次 ※追記 (2021/12/13) 概要 EventBridge でのスケジュール設定 Lambda 関数での日付判定 まとめ ※追記 (2021/12/13) 以降 色々と書いてますが、結論としては下記のように cron 式を指定します。\ncron(0 15 L * ? *) Amazon EventBridge の cron 式では、月末を示す L というワイルドカードを使用することができます。なので、上記のように指定することで、日本時間の月初の 00:00 を指定することができます。\nThe L wildcard in the Day-of-month or Day-of-week fields specifies the last day of the month or week.\nCreating an Amazon EventBridge rule that runs on a schedule - Amazon EventBridge コメントで教えていただきました。ありがとうございました！\n概要 今回のポイントは、 「日本時間の毎月 1 日の 00:00」 という部分です。\nAmazon EventBridge では、一般的な cron の構文を使ってイベントをスケジュールすることができます。ただし、この cron で指定する日時のタイムゾーンは UTC となっているため、 JST との時差を考慮して指定する必要があります。\nJST は +0900 なので、 cron では実際に日本時間で実行したい時間の 9 時間前の日時を指定します。なので、日本時間の毎月 1 日の 00:00 に実行したい場合は、 UNIX 時間の月末の 15:00 を指定することになります。しかし、 cron では 月末 を指定する方法がないため、実行する Lambda 関数側で少し手を加える必要があります。\n特徴 - Amazon EventBridge | AWS EventBridge でのスケジュール設定 cron で月末を指定する方法がないとはいえ、月末になり得る日は限られています。なので、 EventBridge では次のような cron 式でスケジュール設定をしておきます。\ncron(0 15 28-31 * ? *) こうすることで、次のようなスケジュールでイベントが発生します。\nSun, 28 Jun 2020 15:00:00 GMT Mon, 29 Jun 2020 15:00:00 GMT Tue, 30 Jun 2020 15:00:00 GMT Tue, 28 Jul 2020 15:00:00 GMT Wed, 29 Jul 2020 15:00:00 GMT Thu, 30 Jul 2020 15:00:00 GMT Fri, 31 Jul 2020 15:00:00 GMT Fri, 28 Aug 2020 15:00:00 GMT Sat, 29 Aug 2020 15:00:00 GMT Sun, 30 Aug 2020 15:00:00 GMT Mon, 31 Aug 2020 15:00:00 GMT ... これで各月の月末を網羅することができますが、月末でない日にもイベントが発生してしまいます。なので、実際に月末かどうか (日本時間では 1 日かどうか) を、実行する Lambda 関数側で判定します。\nLambda 関数での日付判定 Lambda 関数内では、実際の処理を行う前に日付の判定を行い、実行時間が日本時間の 1 日でなければそこで処理を終了するようにしています。今回は Go で書いてみます。\nまとめ Amazon EventBridge を使って AWS Lambda を日本時間の毎月 1 日の 00:00 に実行する方法についてでした。 毎日、毎週や 1 日の 09:00 以降の月次であれば cron で解決しますが、毎月 1 日の 09:00 までの実行には Lambda 側で判定が必要になります。\n",
    "permalink": "https://michimani.net/post/aws-run-lambda-at-1st-day-of-month/",
    "title": "AWS Lambda を毎月 1 日の 00:00 (JST) に実行する"
  },
  {
    "contents": "最近は Go を書くことが多いので、備忘録として AWS SDK for Go を使って DynamoDB のテーブルから特定の属性 (Attributes) のみを取得する方法について書いておきます。\n目次 Go で DynamoDB を操作する やること すべての属性を含めて Scan する 特定の属性のみを取得するように Scan する まとめ Go で DynamoDB を操作する Go で DynamoDB を操作する方法についてぐぐってみると、 guregu/dynamo というライブラリを使った例が多く出てきます。DynamoDB に限らず、 Go の SDK で AWS のリソースを操作する際にはポインタ型を扱う必要があるため、実装が複雑だと感じてしまいがちです。 guregu/dynamo を使用すると、非常にかんたんな記述で DynamoDB を操作することができます。\nguregu/dynamo: expressive DynamoDB library for Go ただし、今回のような場合や少し複雑な使い方をする場合は AWS SDK for Go を使ったほうが柔軟な実装ができます。\nやること DynamoDB のサンプルテーブルの Thread テーブルに対して、次の 2 つの方法でデータを取得してみます。\nすべての属性を含めて Scan する 特定の属性のみを取得するように Scan する サンプルデータについては公式のデータを使用します。\nサンプルテーブルとデータ - Amazon DynamoDB ここからは実際のコードを抜粋して書きますが、全体については Gist に置いていますのでそちらを参照してください。\nSample script to scan DynamoDB table via AWS SDK for Go. すべての属性を含めて Scan する package main import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/dynamodb\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/dynamodb/dynamodbattribute\u0026#34; ) // Thread is a struct of a Thread item type Thread struct { ForumName string `dynamodbav:\u0026#34;ForumName\u0026#34; json:\u0026#34;forum_name\u0026#34;` Subject string `dynamodbav:\u0026#34;Subject\u0026#34; json:\u0026#34;subject\u0026#34;` Answered int `dynamodbav:\u0026#34;Answered\u0026#34; json:\u0026#34;answered\u0026#34;` LastPostedBy string `dynamodbav:\u0026#34;LastPostedBy\u0026#34; json:\u0026#34;last_posted_by\u0026#34;` LastPostedDateTime string `dynamodbav:\u0026#34;LastPostedDateTime\u0026#34; json:\u0026#34;last_posted_date_time\u0026#34;` Message string `dynamodbav:\u0026#34;Message\u0026#34; json:\u0026#34;message\u0026#34;` Replies int `dynamodbav:\u0026#34;Replies\u0026#34; json:\u0026#34;replies\u0026#34;` Tags []string `dynamodbav:\u0026#34;Tags\u0026#34; json:\u0026#34;tags\u0026#34;` Views int `dynamodbav:\u0026#34;Views\u0026#34; json:\u0026#34;views\u0026#34;` } var tableName string = \u0026#34;Thread\u0026#34; var region string = \u0026#34;ap-northeast-1\u0026#34; var db = dynamodb.New(session.New(), \u0026amp;aws.Config{ Region: aws.String(region), }) // Scan is a function to scan table. func Scan() []Thread { var threads []Thread = []Thread{} scanOut, err := db.Scan(\u0026amp;dynamodb.ScanInput{ TableName: aws.String(tableName), }) if err != nil { fmt.Println(err.Error()) return threads } for _, scanedThread := range scanOut.Items { var threadTmp Thread _ = dynamodbattribute.UnmarshalMap(scanedThread, \u0026amp;threadTmp) threads = append(threads, threadTmp) } return threads } // ThreadsToJSONString is a function to convert Thread object to JSON string. func ThreadsToJSONString(threads interface{}) string { j, err := json.Marshal(threads) if err != nil { fmt.Println(err.Error()) return \u0026#34;\u0026#34; } var buf bytes.Buffer jerr := json.Indent(\u0026amp;buf, j, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) if jerr != nil { fmt.Println(jerr.Error()) return \u0026#34;\u0026#34; } return buf.String() } func main() { fmt.Println(\u0026#34;Scan with all attributes.\u0026#34;) allAttrRes := Scan() fmt.Println(ThreadsToJSONString(allAttrRes)) } まず Thread Struct をタグ付きで定義します。 dynamodbav タグは DynamoDB テーブルの属性名を指定します。 json タグは JSON で出力する際の属性名を指定します。 Scan() では引数に *dynamodb.ScanInput 型の変数を渡します。その変数ではテーブル名 TableName のみ指定しています。\nThreadsToJSONString() 関数では Thread オブジェクトを整形済みの JSON 文字列に変換しています。\nこのスクリプトを実行すると、次のような出力が得られます。\n$ go run ./src/dynamo/main.go Scan with all attributes. [ { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon CloudFront\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;CloudFront Thread 1\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;User A\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;2015-09-22T19:58:22.514Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;CloudFront thread 1 message\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: [ \u0026#34;index\u0026#34;, \u0026#34;primarykey\u0026#34;, \u0026#34;table\u0026#34; ], \u0026#34;views\u0026#34;: 10 }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon S3\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;S3 Thread 1\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;User A\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;2015-09-29T19:58:22.514Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;S3 thread 1 message\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: [ \u0026#34;largeobjects\u0026#34;, \u0026#34;multipart upload\u0026#34; ], \u0026#34;views\u0026#34;: 0 }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;DynamoDB Thread 1\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;User A\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;2015-09-22T19:58:22.514Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;DynamoDB thread 1 message\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: [ \u0026#34;index\u0026#34;, \u0026#34;primarykey\u0026#34;, \u0026#34;table\u0026#34; ], \u0026#34;views\u0026#34;: 0 }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;User A\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;2015-09-15T19:58:22.514Z\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;DynamoDB thread 2 message\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: [ \u0026#34;items\u0026#34;, \u0026#34;attributes\u0026#34;, \u0026#34;throughput\u0026#34; ], \u0026#34;views\u0026#34;: 0 } ] 特定の属性のみを取得するように Scan する 基本的には上の例と同じなので、異なる部分のみ抜き出しています。今回は Thread テーブルの属性のうち、 ForumName と Subject のみ取得してみます。\n// ThreadWithSomeAttr is a struct of a Thread item with some attributes. type ThreadWithSomeAttr struct { ForumName string `dynamodbav:\u0026#34;ForumName\u0026#34; json:\u0026#34;forum_name\u0026#34;` Subject string `dynamodbav:\u0026#34;Subject\u0026#34; json:\u0026#34;subject\u0026#34;` } // ScanSomeAttr is a function to scan table with some attributes. func ScanSomeAttr() []ThreadWithSomeAttr { var threads []ThreadWithSomeAttr = []ThreadWithSomeAttr{} scanOut, err := db.Scan(\u0026amp;dynamodb.ScanInput{ TableName: aws.String(tableName), ExpressionAttributeNames: map[string]*string{ \u0026#34;#FNAME\u0026#34;: aws.String(\u0026#34;ForumName\u0026#34;), \u0026#34;#SUBJ\u0026#34;: aws.String(\u0026#34;Subject\u0026#34;), }, ProjectionExpression: aws.String(\u0026#34;#FNAME, #SUBJ\u0026#34;), }) if err != nil { fmt.Println(err.Error()) return threads } for _, scanedThread := range scanOut.Items { var threadTmp ThreadWithSomeAttr _ = dynamodbattribute.UnmarshalMap(scanedThread, \u0026amp;threadTmp) threads = append(threads, threadTmp) } return threads } Scan した結果を受け取る Struct として ThreadWithSomeAttr を定義します。Scan() の引数としては、テーブル名に加えて、取得したい属性名を指定した *dynamodb.ScanInput を指定しています。\nこの状態で実行すると、次のような出力が得られます。\n$ go run ./src/dynamo/main.go Scan with some attributes. [ { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon CloudFront\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;CloudFront Thread 1\u0026#34; }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon S3\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;S3 Thread 1\u0026#34; }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;DynamoDB Thread 1\u0026#34; }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34; } ] ちなみに Scan した結果を ThreadWithSomeAttr Struct の代わりに、すべての属性を定義した Thread で受け取ると、次のような結果になります。\n$ go run ./src/dynamo/main.go Scan with some attributes. [ { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon CloudFront\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;CloudFront Thread 1\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: null, \u0026#34;views\u0026#34;: 0 }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon S3\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;S3 Thread 1\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: null, \u0026#34;views\u0026#34;: 0 }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;DynamoDB Thread 1\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: null, \u0026#34;views\u0026#34;: 0 }, { \u0026#34;forum_name\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34;, \u0026#34;answered\u0026#34;: 0, \u0026#34;last_posted_by\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;last_posted_date_time\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;replies\u0026#34;: 0, \u0026#34;tags\u0026#34;: null, \u0026#34;views\u0026#34;: 0 } ] ForumName と Subject の値は取得できますが、それ以外の属性についてはそれぞれの属性に定義された型の初期値になります。\nまとめ AWS SDK for Go を使って DynamoDB のテーブルから特定の属性 (Attributes) のみを取得してみた話でした。\nポインタ型については 1 年目のころに C 言語で扱って以来だったのでちょっと拒否反応がありましたが、慣れれば特に問題ないですね。\nAWS SDK に関しては公式ドキュメントもしっかり用意されているので、その点でも安心です。\ndynamodb - Amazon Web Services - Go SDK 今回のサンプルスクリプトは下記 Gist に置いています。\nSample script to scan DynamoDB table via AWS SDK for Go. ",
    "permalink": "https://michimani.net/post/aws-scan-dynamodb-table-with-go-sdk/",
    "title": "AWS SDK for Go で DynamoDB のテーブルから特定の属性だけ取得する"
  },
  {
    "contents": "AWS Lambda の関数を Python で実装する際に使用するレイヤーを作ったので、その時のメモです。\n目次 概要 Docker イメージの作成 イメージサイズの削減 Docker Hub へ Push レイヤー用 zip ファイルの作成 Lambda レイヤーの登録 まとめ 概要 Python 3.8 をランタイムとして、 AWS Lambda の関数を実装することを考えます。その際に、 pip でインストールしたモジュールを Lambda レイヤーとして構築します。\nAWS Lambda ではラインタイムごとに実行環境 (OS) が異なります。このエントリを書いている時点 (2020/05/16) では、 Python 3.8 の実行環境は Amazon Linux 2 となっています。\nAWS Lambda runtimes - AWS Lambda 今回は Lambda レイヤー用のモジュール群 (zip ファイル) を、 Docker を使って Amazon Linux 2 環境で作成します。\nDocker イメージの作成 まずは Python 3.8 をインストールした Amazon Linux 2 の Docker イメージを作成します。Amazon Linux 2 のイメージは Docker Hub で公開されているので、今回はその中から 2.0.20200406..0 を使います。\namazonlinux - Docker Hub イメージサイズの削減 とりあえず作成したとき、イメージのサイズが 1.17 GB になってしまいました。さすがにサイズが大きすぎると思ったので、削減方法について調べてみました。\nDockerfile の RUN の挙動 Dockerfile では RUN コマンドを使って Linux コマンドを記述して Python 3.8 をインストールしていきます。この RUN コマンドは実行するたびにレイヤを重ねていく形になります。なので、 Python 3.8 用のインストールパッケージなど不要な一時ファイルを削除する場合は、同じ RUN コマンド内で処理する必要があります。\nDockerfile reference | Docker Documentation - RUN イメージビルド時のサイズの変遷を調べる docker build でイメージをビルドしていく過程で、イメージのサイズがどのように変化していったかを確認することができます。\n$ docker history IMAGE docker history | Docker Documentation 実行結果は次のような形で出力されます。\ndocker history 6a20bde9c6da IMAGE CREATED CREATED BY SIZE COMMENT 6a20bde9c6da 10 hours ago /bin/sh -c pip3.8 -V 0B 49f06a13c22d 10 hours ago /bin/sh -c python3.8 -V 0B 669423823b31 10 hours ago /bin/sh -c rm -rf Python-3.8.3 0B 9e98a158748d 10 hours ago /bin/sh -c rm -rf Python-3.8.3.tgz 0B 2af6cd8cdd37 10 hours ago /bin/sh -c cd Python-3.8.3 \u0026amp;\u0026amp; ./configure --… 360MB 22c8ca1d8b4c 10 hours ago /bin/sh -c tar xzf Python-3.8.3.tgz 84.1MB b2acff82a0d2 10 hours ago /bin/sh -c wget https://www.python.org/ftp/p… 24.1MB 621e604d00a9 10 hours ago /bin/sh -c yum -y install gcc openssl-devel … 332MB 7f335821efb5 3 weeks ago /bin/sh -c #(nop) CMD [\u0026#34;/bin/bash\u0026#34;] 0B \u0026lt;missing\u0026gt; 3 weeks ago /bin/sh -c #(nop) ADD file:c64a7d77d1e9f4364… 163MB イメージ内のディレクトリごとのサイズを調べる ビルド後のイメージについて、ディレクトリごとのサイズを確認します。\n$ docker run -it --rm 6a20bde9c6da du / | sort -n | tail 179120\t/var/cache/yum/x86_64/2/amzn2-core 179128\t/var/cache/yum/x86_64/2 179132\t/var/cache/yum/x86_64 179136\t/var/cache/yum 179160\t/var/cache 190324\t/var 212184\t/usr/local/lib 230872\t/usr/local 544248\t/usr 736976\t/ これで不要なディレクトリがあれば削除するようなコマンドを Dockerfile に追記して再ビルド -\u0026gt; 確認 -\u0026gt; 再ビルド \u0026hellip; していきます。\n最終的に 550 MB くらいになりましたが、これでも大きいですよね。多分。\nDocker Hub へ Push 不要ファイルを削除してサイズ削減した Dockerfile は次のようになりました。\nFROM amazonlinux:2.0.20200406.0 RUN yum -y install \\ gcc \\ openssl-devel \\ bzip2-devel \\ libffi-devel \\ wget \\ tar \\ gzip \\ make \\ wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz \u0026amp;\u0026amp; \\ tar xzf Python-3.8.3.tgz \u0026amp;\u0026amp; \\ cd Python-3.8.3 \u0026amp;\u0026amp; \\ ./configure --enable-optimizations \u0026amp;\u0026amp; \\ make altinstall \u0026amp;\u0026amp; \\ cd ~ \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum/* \u0026amp;\u0026amp; \\ rm -rf /Python-3.8.3 RUN python3.8 -V RUN pip3.8 -V この Dockerfile を使ってイメージを作成します。このとき、 -t で指定するタグ名は {Docker Hub のアカウント ID}/{Docker Hub のリポジトリ名} にしておきます。\n$ docker build -t michimani/amzn2py38 . イメージが作成できたら、 Docker Hub にログインします。\n$ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don\u0026#39;t have a Docker ID, head over to https://hub.docker.com to create one. Username: michimani Password: Login Succeeded ログインできたら、先ほど作成したイメージを Docker Hub に Push します。\n$ docker push michimani/amzn2py38:latest Push されたイメージを Docker Hub で確認します。\nmichimani/amzn2py38 - Docker Hub レイヤー用 zip ファイルの作成 必要な Python モジュールを requirements.txt に記述しておきます。そして、下記のようなディレクトリ構成を作ります。\n. ├── Dockerfile ├── docker-compose.yml └── requirements.txt Dockerfile は次のように記述します。\nFROM michimani/amzn2py38:latest RUN yum -y install zip RUN mkdir /home/dist docker-compose.yml は次のように記述します。\nversion: \u0026#34;3\u0026#34; services: app: build: . volumes: - \u0026#34;.:/home\u0026#34; command: \u0026gt; bash -c \u0026#34; mkdir /home/dist \u0026amp;\u0026amp; mkdir -p /home/tmp/python \u0026amp;\u0026amp; pip3.8 install -r /home/requirements.txt -t /home/tmp/python --upgrade \u0026amp;\u0026amp; cd /home/tmp \u0026amp;\u0026amp; zip -r /home/dist/layer.zip ./python \u0026amp;\u0026amp; cd /home \u0026amp;\u0026amp; rm -rf /home/tmp\u0026#34; あとは次のコマンドを実行すれば、 dist/layer.zip が生成されます。\n$ docker-compose up Lambda レイヤーの登録 Lambda レイヤーの登録は、下記の AWS CLI コマンドで実行できます。\n$ aws lambda publish-layer-version \\ --layer-name my_layer \\ --zip-file fileb://dist/layer.zip まとめ Python 3.8 用の Lambda レイヤーを Docker を使って作成してみた話でした。\nDocker まだまだよくわからん状態なので、最初にイメージサイズが 1 GB 超えたときは焦りました。 RUN の挙動についてもちゃんとわかっていなかったので、調べる機会になってよかったです。\nLambda レイヤーについては、外部モジュールを Lambda 自体のデプロイパッケージサイズを小さくすることができ、さらに他の関数でも利用できるので、用途別にレイヤーを作成しておけば Lambda 関数の実装がしやすくなりそうです。\nAWS Lambda レイヤー - AWS Lambda ",
    "permalink": "https://michimani.net/post/aws-create-lambda-layers-with-docker/",
    "title": "Docker で Python 用の Lambda レイヤーを作る"
  },
  {
    "contents": "AWS Chatbot は連携先のクライアントとして、 Slack と Amazon Chime を選択することができます。 Slack に関しては以前に別のエントリで書いたので、今回は Amazon Chime をクライアントとして AWS Chatbot と連携してみます。\n目次 概要 1. Amazon Chime で Webhook URL を生成 チャットルームを作成する Webhook URL を生成する 2. AWS Chatbot でクライアントの設定 CloudWatch Alarm を Amazon Chime のチャットルームに通知する まとめ 概要 AWS Chatbot と Amazon Chime を連携させて、 Amazon Chime のチャットルームに CloudWatch Alarm の通知を飛ばしてみます。\nやることとしては次のとおりです。\nAmazon Chime で Webhook URL を生成 AWS Chatbot でクライアントの設定 より詳細な手順については下記の公式ドキュメントの Setting up AWS Chatbot with Amazon Chime パートを参照してください。\nGetting started with AWS Chatbot - AWS Chatbot 前提 Amazon Chime でチャットルームの作成をするには Amazon Chime のアカウントを作成している必要があります。アカウントの作成については省略するので、下記のリンクから作成しておきます。\nAmazon Chime Login 1. Amazon Chime で Webhook URL を生成 まずは Amazon Chime で Webhook URL を生成します。ここからの操作は Amazon Chime の Web クライアント、およびデスクトップクライアントで行います。今回は Web クライアントで操作してみますが、デスクトップクライアントでもほぼ同じです。\nチャットルームを作成する Webhook URL はチャットルームに対して作成するので、まずはチャットルームを作成します。\nHome の右側にある Quick actions の中から Create a new chat room をクリックします。\nチャットルームの名前を入力するダイアログが表示されるので、任意の名前を入力して Create ボタンを押します。\nこれでチャットルームが作成され、画面左の CHAT ROOMS に表示されます。\nWebhook URL を生成する 続いて Webhook URL を生成します。\n先ほど作成したチャットルームを開いて、画面上にある設定アイコンを押します。表示されるメニューから Manage webhooks and bots を選択します。\nすると次のようなダイアログが表示されるので、上部の Add webhook をクリックします。\n選択中のチャットルームに対する webhook を作成するダイアログが表示されるので、任意の名前を入力して Create ボタンを押します。\nこれで webhook が作成され、 Manage webhooks and bots のダイアログに一覧として表示されます。 Copy URL をクリックして、作成した webhook の URL をコピーしておきます。\n2. AWS Chatbot でクライアントの設定 続いて、 AWS Chatbot でクライアントの設定をします。 前回 はチャットクライアントとして Slack を選択しましたが、今回は Amazon Chime を選択します。\nすると、次のような Amazon Chime ウェブフックの設定フォームが表示されます。\nクライアントを Slack にした場合との違いは、一番上の 設定の詳細 の項目です。\n名前 設定する Amazno Chime Webhook の名前を入力します。 名前には a-z, A-Z, 0-9, -, _ を使用できます。\nウェブフック URL 先ほどコピーしておいた Amazon Chime の Webhook URL を入力します。\nウェブフックの説明 ここも必須になっているので、この設定に関する説明を入力します。説明のフォーマットとして チャットルーム名/ウェブフックの目的 を書かれていますが、特にこのフォーマットである必要はありません。\n以降の設定については Slack の場合と同様です。アクセス許可については、ポリシーテンプレートから 通知のアクセス許可 を選択しておきます。\n通知用の SNS トピックを指定して 設定 ボタンを押します。\nこれで AWS Chatbot のクライアント設定が完了しました。\nCloudWatch Alarm を Amazon Chime のチャットルームに通知する では、 AWS Chatbot 経由で CloudWatch Alarm の通知を Amazon Chime のチャットルームに飛ばしてみます。今回も 前回 と同様に CloudFront の Alarm 画面から、あるディストリビューションに対する 4xx Error Rate に関するアラームを設定しました。\n設定後、該当のディストリビューションに対して意図的に 4xx エラーを発生させて暫く待つと、次のような通知が Amazon Chime のチャットルームに届きました。\nちなみに、同時に Slack の方にも届いた通知は次のような内容でした。\n内容は全く同じですね。\nまとめ AWS Chatbot と Amazon Chime を連携させて、 Amazon Chime のチャットルームに CloudWatch Alarm の通知を飛ばしてみた話でした。\nクライアント設定の部分が違うだけで、 Slack の場合と同様に整形された通知を受け取ることができました。個人的には Amazon Chime はオンラインセミナーでの Instant meeting しか利用したことがなかったのですが、 Slack が利用できない環境の場合は Amazon Chime のチャットルームで通知を確認する方法もあるということがわかりました。\nただ、 Slack と違って AWS Chatbot をチャットルームに招待するわけではなく Webhook での連携なので、チャットルームから AWS CLI コマンドを実行することはできないようです。CLI コマンドの実行もしたい場合は Slack を利用したほうが良さそうです。\n",
    "permalink": "https://michimani.net/post/aws-setting-aws-chatbot-with-amazon-chime/",
    "title": "AWS Chatbot を使って Amazon Chime のチャットルームに CloudWatch Alarm の通知を飛ばしてみる"
  },
  {
    "contents": "AWS 認定のデジタルバッジが、 Acclaim というプラットフォームで管理されるようになりました。これにより、自分の所持しているデジタルバッジを SNS でシェアしたり、ウェブサイトへ埋め込んだり、また、デジタルバッジを付与したパブリックなプロフィールの作成と公開・共有が簡単になります。\n目次 概要 Acclaim とは Acclaim での操作 AWS 認定アカウントページに新メニューが登場 Acclaim のアカウント作成 マイプロフィールへの AWS 認定バッジの表示 デジタルバッジの個別ページではいろんな情報が確認できる 今までのバッジ管理はどうなる？ まとめ 概要 AWS 認定に合格した証として付与されるデジタルバッジが、これまでの AWS 認定アカウントページ (CertMetrics) での管理 から Acclaim での管理 に変更されます。今回は、新たな管理プラットフォームとなる Acclaim そのものについてと、実際に Acclaim で AWS 認定バッジをどのように管理・シェア出来るのか確認してみた話です。\nAWS Certification Digital Badges Acclaim とは Acclaim (アクレーム) とは、デジタルバッジを通じて個人の能力を証明するソリューション (ツール) で、多くの認定団体と提携しているサービスです。今回、 AWS もその団体の一つとして提携することになったようです。\n2018 年に、デジタル認証の発行・管理を行う Credly というソリューションと合併することになり、スキル評価や資格の証明などをまとめて提供するユニバーサルなプラットフォームとなりました。\nピアソンの ACCLAIM 事業は、CREDLYと合併へ | Acclaim :: ピアソン VUE Acclaim のサイトでは下記のように説明されています。\nCredly\u0026rsquo;s Acclaim platform is the world\u0026rsquo;s largest network of individuals and organizations using verified achievements to unlock opportunities. Join millions of professionals in sharing your achievements online.\nAcclaim - Acclaim ざっくり意訳すると、 Acclaim は 認定済みのデジタルバッジをパブリックに共有できる世界最大のプラットフォーム ということです。そこに AWS 認定のバッジも表示できるようになったという感じですね。\nAcclaim での操作 では実際に Acclaim で AWS 認定バッジを管理してみます。いきなり Acclaim のページに行くのではなく、まずはこれまで通り AWS 認定アカウントのページへログインし、そこから Acclaim のページへと遷移していきます。\nAWS 認定アカウントページに新メニューが登場 AWS 認定アカウントページにログインすると、 デジタルバッジ(廃止予定) と デジタルバッジ(新) というメニューが増えています。\nデジタルバッジ(新) ページへ遷移すると、上のキャプチャのように AWS 認定バッジが Acclaim 経由で提供される説明が記述されています。説明の右にある Credly の Acclaim プラットフォーム ボタンを押すと Acclaim のアカウント作成ページへ遷移します。\nAcclaim のアカウント作成 AWS 認定アカウントページから遷移してくると、次のようなアカウント作成フォームが表示されます。\nCountry の選択、 Acclaim ログイン用のパスワードを入力して、 Create Account ボタンを押します。\n規約の同意画面が表示されるので、確認して I Accept ボタンを押します。\nこれでアカウント作成は完了です。これにより、 Acclaim で作成した自分のプロフィールを、個別の URL で共有できるようになります。\nマイプロフィールへの AWS 認定バッジの表示 アカウント作成が完了すると、次のようなダッシュボードが表示されます。\n所持している AWS 認定バッジが表示されています。が、この状態では共有した自分のプロフィールには表示されません。表示するためには、表示させたいバッジを Accept する必要があります。\n試しに AWS Certified Developer - Associate のバッジを選択してみると、バッジに関する個別ページに遷移します。すると、画面トップに次のような表示があります。\nここで Accept Badge ボタンを押すと、次のようなモーダルが表示されます。\nここで Public にチェックを入れると、自分のプロフィールに表示されるようになります。 Auto Accept にチェックを入れると、 AWS 認定のデジタルバッジについては自動で Accept および Public になるように設定できます。\nデジタルバッジを Accept すると、自分のプロフィールページでは次のように表示されます。\nこのページは個別の URL が割り振られているので、その URL を共有することで簡単に自分のプロフィールを所持しているデジタルバッジとともに共有することができます。\nこの画面に表示する名前や bio 、ソーシャルアカウントについては設定画面から編集可能です。\nデジタルバッジの個別ページではいろんな情報が確認できる デジタルバッジの Accept の際にデジタルバッジの個別ページを表示するのですが、そのページでは様々な情報が確認できます。\nデジタルバッジの概要 まずはデジタルバッジの詳細についてです。\nバッジの画像と認定の名前、認定元、このバッジを所持していると証明できるスキルの説明が記載されています。\nおすすめの関連情報 認定バッジに関連するおすすめの情報へのリンクが表示されています。 AWS 認定の場合、継続した勉強のための情報、認定によって得られるメリット、上位資格の情報などへのリンク (AWS 公式ドキュメント) が表示されています。\nデジタルバッジ保持者の統計 同じデジタルバッジを所持している人の統計が表示されています。内容としては、バッジ保持者の 職種 (ロール) 、地域、年収、企業、関連する技術 の統計を確認するとができます。\n関連するデジタルバッジ (認定) 該当のデジタルバッジに関連するデジタルバッジ (認定) が表示されています。デジタルバッジ個別ページに関しては 所持/未所持 問わずに閲覧可能です。\n今までのバッジ管理はどうなる？ これまでの AWS 認定アカウントページ (CertMetrics) でのバッジ管理は 2020年 8月 1日 以降 使用できなくなります。もしこれまでに CertMetrics の共有機能を利用して外部にデジタルバッジを公開していた場合、 2020年 8月 1日 までに Acclaim からの共有に切り替える必要があります。\nまとめ AWS 認定に合格した証として付与されるデジタルバッジが、これまでの AWS 認定アカウントページ (CertMetrics) での管理 から Acclaim での管理 に変更されるという話でした。\nこれまでのバッジ管理では認定バッジついて個別の共有 URL しか発行できませんでしたが、 Acclaim 経由で共有できるようになったことで、個人のプロフィールとして所持しているバッジを簡単に共有することが出来るようになりました。\nちなみに AWS 認定以外では、 Oracle の各種認定や Adobe の各種認定にも対応しているようなので、 AWS 認定以外にもデジタルバッジを所持されている方はそれらもまとめて共有できるのは便利なのではないでしょうか。\nAWS Certification Digital Badges Acclaim - Acclaim ",
    "permalink": "https://michimani.net/post/aws-certification-degital-badges-provided-via-acclaim/",
    "title": "AWS 認定バッジが Acclaim で管理されるようになって SNS でシェアしやすくなった"
  },
  {
    "contents": "JAWS-UG 初心者支部のオンラインイベント 初心者支部＆千葉支部#26 新人さん歓迎！ハンズオン\u0026amp;LT(オンライン) に参加してきました。今回はなんと LT 枠で参加しました。人生初の LT がどんな感じだったのか、その感想をメインに書いていきます。\nとりあえず LT で使用した資料を見たい！という方は 完成したスライド までジャンプしてください。\nなお、当日の様子は #jawsug_bgnr および #jawsugchiba のハッシュタグでツイートされているので、そちらも見てみてください。\n目次 タイムテーブル LT 枠で参加してきました！ LT をしようと思ったきっかけ LT の内容 LT 用のスライド作成 フォントを考える 流れを考える 各パートの作成 練習と修正 完成したスライド Amazon Chime を使った LT Chime で画面共有 デバイスの切り替えはデスクトップ版のみ可 画面共有中は共有した画面しか見れない まとめ タイムテーブル タイムテーブルは下記の通りです。\n時間 内容 登壇者 19:20〜 受付開始 - 19:30-19:40 諸注意/初心者支部とは/テーマ説明 初心者支部運営 織田さん 19:40-19:45 千葉支部のご紹介 千葉支部運営 和田さん 19:45-21:15 サーバーレスクイックスタート: 手を動かしながら学ぶサーバーレスはじめの一歩 AWSJ 金澤さん 21:15-21:20 休憩 - 20:20-21:30 宿題の回答LT1 AWS CDK で文字起こし+翻訳パイプライン作ってみた 伊藤 (僕です) 20:32-21:42 宿題の回答LT2 batchiさん 20:44-21:49 サーバーレスLT-Lambdaって課金されるの？〜非同期処理でちょっとやらかした話 千葉支部運営 和田さん 21:51-22:00 締め \u0026amp; アンケート 運営 織田さん なお今回のハンズオンについては、初心者支部 #24 で実施されたものと同じ内容です。\nJAWS-UG 初心者支部#24 サーバレスハンズオン勉強会 - connpass その他、今回のイベント詳細については connpass のイベントページを参照してください。\nJAWS-UG 初心者支部＆千葉支部 #26 新人さん歓迎！ハンズオン\u0026amp;amp;LT(オンライン) - connpass LT 枠で参加してきました！ 今回のイベントは、ハンズオンがメインで LT が 3 本という内容でした。 connpass でのイベント公開の前に、 Facebook の初心者支部グループで運営の織田さんから LT してくれる方を募集しているという投稿がありました。LT の内容としては、前回のハンズオンで出ていた宿題の回答について、でした。\n前回のハンズオンの宿題についてはハンズオン後すぐに実施しており、ブログにも書いていました。なので、その内容であれば話せそうだなと思い、メッセージで LT をしたい旨を伝え、 LT 枠で参加することになりました。\nLT をしようと思ったきっかけ 僕自身、これまでにイベントでの登壇や LT の経験は無く、もちろんオンラインでの登壇経験もありません。\nただ、 2 年前くらいからこのブログで色々アウトプットするようになって、去年からはハンズオンやその他イベントにも参加するようになって、自分も何かしらのイベントで登壇という形でアウトプットしてみたいな、とは思っていました。\n前に参加した JAWS-UG 初心者支部のイベントで「登壇も初心者です」と言って登壇されている方がいて、ここのグループなら登壇デビューできるかもしれないなと感じていました。\nそんなところに今回のチャンスが巡ってきたわけです。気付いたら参加表明のメッセージを送っていました。まあ、なんとかなるやろ精神です。\nLT の内容 先述した通り LT の内容は、前回のハンズオンの宿題に対する回答なので、ブログにした内容をそのまま話せばいいかなと思っていました。\nただ、この内容だとちょっと味気ないなと思い、最近興味のある IaC に絡めて話してみることにしました。そして付けたタイトルが 「AWS CDK で文字起こし + 翻訳パイプラインを作ってみた」 です。このタイトルを考えた時点では CDK を使ってやってみたという事実は一切ありません。はい。タイトルを決めてから CDK のプロジェクトを作り始めました。\nGA から 10 ヶ月くらい経った AWS CDK ですが、いまだにアップデートのスピードは衰えること無く、非常に速いスピードで改善・改良が進んでいます。そんなホットな話題に少しでも触れられる LT ができれば面白いかなと思ってこのタイトルに決めました。\n幸い、 CDK 自体は少し触ったことがあったので、最低限動く実装自体はささっとできました。\nLT で紹介した CDK プロジェクトは GitHub で公開していますので、 clone して触ってみてください。\nmichimani/jaws-ug-bgnr-24-homework: JAWS-UG 初心者支部#24 サーバレスハンズオン勉強会 にて宿題となっていた 文字起こし \u0026#43; 翻訳 パイプライン を構築するための CDK プロジェクトです。 LT 用のスライド作成 今回の LT は 10 分ということで、 LT としては少し長めの設定でした。ただ、登壇経験がまったく無いので 10 分と言われてもボリューム感が全くわかりません。ということで、まずは時間を気にせずスライドを作ってみることにしました。\n以下、スライド作成時にやったことを順に振り返ってみます。\nフォントを考える まず最初にやったのが、使用するフォントの選定です。たぶんこれを最初にやるのは間違っている気はしてます。が、 1 枚目のタイトルページを作るときにフォントが気になってしまって、結果的にタイトルページ作るだけで 1 日終わってしまったのは良い思い出です。\n最終的に選んだフォントは次のとおりです。どちらも Adobe Fonts で使用できるフォントです。\nタイトル／見出し : A-OTF 見出しゴMB31 Pr6N 本文 : A-OTF UD新ゴ Pr6N なお、コード部分に関しては VS Code をスクショを使用したので、普段 VS Code で使用している Hackgen Console になっています。\nyuru7/HackGen: HackGen is Japanese programming font which is a composed of Hack and GenJyuu-Gothic. 流れを考える 次に全体の流れを考えました。\n今までに参加してきたイベントを思い出したり、登壇者のスライドを参考にしたりして、次のような流れにしようと考えました。\nタイトル 自己紹介 アジェンダ 話すこと 話さないこと メイン 宿題の内容をおさらい AWS CDK とは AWS CDK で宿題やってみた まとめ おしまい 特におもしろみもなく、至って普通な構成かと思います。ただ、先に全体の流れと構成を考えることで、あとはそれぞれ肉付けしていくだけになりました。\n各パートの作成 上の構成のうち、一番大事なのは メイン です。(当たり前) その中でも、今回の LT のメインは ハンズオンの宿題の回答 なので、 3. AWS CDK で宿題やってみた 部分が一番大事な部分です。\nとは言っても、実装内容を淡々と話してもつまらないだろうし、どうするかなーと悩んでいました。そこで、思いついたのが、 パラパラ漫画作戦 です。ダサいネーミングです。\n10 分という長いようで短い時間で、なんとなくでも内容を伝えようと思うと、やはり喋るよりもテンポよく動きを見せたほうが良いのかなという思いから考えた作戦です。といっても既にこのような方法を実践されている登壇資料はたくさんあり、僕自身がその登壇を拝聴していて凄くわかりやすかったという体験があったのも大きいです。\nなんとなくイメージは出来ると思いますが、具体的には、 実際に CDK コマンドを実行しているように見せる みたいなことをやりました。\nその結果、 LT 終了後にはその部分を良かったと言っていただけるツイートがあり、シンプルに嬉しかったです。\n伊藤さんによる宿題回答！AWS CDK で構築していただいたとのこと😍素晴らしいです！AWS CDK を実行する部分の資料とても分かりやすい。。。#jawsug_bgnr #jawsugchiba\n\u0026mdash; ketancho 🙂｜Kei Kanazawa (@ketancho) April 28, 2020 これ私も思った！！\nコンソール画面でコマンド打ってるかのようなスライド、わかりやすかった！まさかの初LTだったなんて！#jawsug_bgnr https://t.co/07OgR1Zugl\n\u0026mdash; たけだ (@taketakekaho) April 28, 2020 練習と修正 全体が完成したら、あとは実際に時間を計って練習してみます。幸いにも一発目の練習で 12 分くらいだったので、今回は少し話す内容を削るだけで対応できました。\nちなみにスライドは macOS に付属している Keynote で作成し、実際の発表時にも Keynote の再生機能を利用しました。マルチディスプレイ環境で Keynote の再生機能を利用すると、メインにはスライドがフルスクリーンで表示され、サブには現在表示中のスライドおよび次のスライド、スピーカーメモ、再生開始からの経過時間などが表示されて非常に便利です。\n完成したスライド Amazon Chime を使った LT 今回、イベントの配信には Amazon Chime を使用しました。\nAmazon Chime（従量課金制のオンライン会議、チャットサービス）| AWS 主催者側で色々設定があるようなのですが、今回のイベントでは次のような形でした。\n招待 URL から参加したユーザはマイク・ビデオ共にオフ 主催者に Presenter に指定されるとマイク・ビデオがオンにできる Presenter になると画面の共有もできる あとは基本的にその他のビデオ会議ツールと同様で、チャット機能があったりします。\nChime で画面共有 Chime で画面共有するには、画面上部にある Screen アイコンから開始します。\n選択肢としては Share Screen と Share Window があります。\nShare Screen ここで言う Screen とは、ディスプレイ (モニター) 全体のことです。マルチディスプレイ環境で使用している場合は、 Share Screen を選んだあとにどのディスプレイを共有するか選択します。\nShare Window ここで言う Window とは、各アプリ (ソフト) のひとつのウィンドウです。 macOS であれば、 Finder のウィンドウを一つ選択すると、他のユーザにはその Window が画面全体に表示される形になります。\nデスクトップ上に見せたくないものがあるなどの場合を除いては、 Share Screen を選択して、 Keynote や PowerPoint の再生・スライドショーを開始するのがスムーズかなと思います。\nデバイスの切り替えはデスクトップ版のみ可 自宅の環境によってはマイクやスピーカーに様々なオーディオデバイスを使用されている方もいると思います。 Chime では、デスクトップ版のアプリでないとオーディオデバイスの切り替えができないようです。\nデスクトップ版アプリでは画面上部の more からデバイス設定が可能ですが、ブラウザ版では見つけることができませんでした。\n今回は、先日買った TASCAM の DR-07X をマイクに使用してみたかったので、デスクトップ版の Chime クライアントを使用しました。 LT 聴いてくださった方、音声どうだったでしょうか？\n画面共有中は共有した画面しか見れない 登壇者は自分の画面を共有して喋ることになるのですが、画面を共有すると Chime の画面が見れません。どういうことかというと、自分含めビデオをオンにしているユーザの顔が見えないのはもちろん、チャットも見れません。これが辛いポイントとしては、 画面共有して喋っている自分の顔が自分で確認できない ということです。スライドはもちろん、自分がどんな風に見えているのかは気になりますよね。\nそんなときに試したいのが、 他の端末で別ユーザとしてミーティングに参加しておく ということです。もちろん、 参加人数に余裕があれば の話ですが、この方法を使うことで自分のスライドや顔がどう見えているかを確認することができます。オンラインだとどうしてもスライドのページ遷移にと喋りにラグが発生したりするので、ページ切り替えや喋りにタイミングを確認するのにも良いかなと思います。今回のように紙芝居風にしようとするとタイミングは非常に重要なので、可能な場合は試してみると良いでしょう。\nまとめ JAWS-UG 初心者支部のオンラインイベント 初心者支部＆千葉支部#26 新人さん歓迎！ハンズオン\u0026amp;LT(オンライン) に LT 枠で参加してきた話でした。\n人生初の LT 、しかもオンラインということでどうなるか不安でしたが、なんとか無事に終えることができたかなと思います。今回このような機会を用意していただいた初心者支部運営のみなさま、また、当日参加されて LT を聴いてくださったみなさま、本当にありがとうございました！とても良い経験になりました。\nオンラインでの登壇については、参加者の顔が見えない、内容が伝わっているかが判断できない、会場の雰囲気がわからない、など、オフラインに比べて辛いポイントがたくさんあるという話を事前に聞いていました。しかし、人前に出るのに慣れていない、且つ緊張しやすい自分には、オンラインでの LT は気軽にできてよかったなと言う印象です。緊張してなかったわけではないのですが。\nもし LT や登壇してみたいけど僕と同じように人前に出るのは苦手だなと感じている方がいれば、オンラインでの LT や登壇はオフラインに比べてハードルが低く、 LT・登壇デビューにはもってこいな環境だと伝えたいです。\nあらためてですが、今回の LT 参加はとても良い経験になりましたし、とても楽しかったです。運営のみなさま、参加者のみなさま、ありがとうございました！\nもし自分の LT に対するコメント (賛否問わず) あれば、 Twitter やはてブなどでご意見いただけると幸いです。\n自分が喋ったイベントのツイート掘るのはこんなに楽しいのか。また機会があればやってみたいな。\n\u0026mdash; よっしー Lv.854 (@michimani210) April 28, 2020 ",
    "permalink": "https://michimani.net/post/event-jaws-ug-beginner-26/",
    "title": "[レポート] JAWS-UG 初心者支部＆千葉支部#26 新人さん歓迎！ハンズオン\u0026LT(オンライン) に参加してきました #jawsug_bgnr #jawsugchiba"
  },
  {
    "contents": "AWS Chatbot では CloudWatch や Code シリーズなどの通知を整形してくれるという機能が注目されがちですが、 Slack から AWS CLI コマンドを実行することもできます。実行できる CLI コマンドにはいくつか制限もあるので、そのあたりも一緒に確認してみます。\n目次 前提 チャンネルの設定 読み取り専用コマンドのアクセス許可 Lambda 呼び出しコマンドのアクセス許可 AWS サポートコマンドのアクセス許可 実際にコマンドを実行してみる 読み取り専用コマンド Lambda 呼び出しコマンド AWS サポートコマンド まとめ 前提 AWS Chatbot と Slack のワークスペースの連携は済んでいるものとします。連携の手順については公式ドキュメントを参照するか、一つ前の記事で書いた内容を参考にしてください。\nGetting started with AWS Chatbot - AWS Chatbot AWS Chatbot を使って CloudWatch Alarm の通知を Slack に飛ばしてみた - michimani.net チャンネルの設定 AWS Chatbot では、 Slack ワークスペース の中に複数の チャンネル を設定します。設定については 前回の記事 と同様なので詳細は割愛します。\n今回、 Slack で AWS CLI コマンドを実行するためには、 AWS Chatbot のチャンネル設定で 適切にアクセス許可をする 必要があります。\nコンソールの設定項目でいうと、この部分です。\nポリシーテンプレート から必要な権限を選択してアクセス許可を設定するのですが、デフォルトでは 通知のアクセス許可 のみが選択されています。この権限を付与することで、 Amazon CloudWatch からメトリクスグラフを取得できるようになります。\nポリシーテンプレートには 通知のアクセス許可 の他に、つぎのテンプレートが用意されています。\n読み取り専用コマンドのアクセス許可 Lambda 呼び出しコマンドのアクセス許可 AWS サポートコマンドのアクセス許可 これらはすべて Slack から実行できる AWS CLI コマンドを制限するものです。それぞれの権限で実行できるコマンドについて、公式ドキュメントをもとに整理してみます。\n読み取り専用コマンドのアクセス許可 AWS CLI コマンドのうち、 読み取り専用 のコマンドのみ実行可能となります。基本的にはすべてのサービスが対象となりますが、以下のサービスについては読み取り専用であっても実行不可だったりするものがあります。\nサービス 制限内容 IAM すべて実行不可 AWS KMS すべて実行不可 AWS STS すべて実行不可 Amazon Cognito 読み取り専用のみ実行可 GetSigningCertificate は実行不可 Amazon EC2 読み取り専用のみ実行可 GetPasswordData は実行不可 Amazon ECR 読み取り専用のみ実行可 GetAuthorizationToken は実行不可 GameLift 読み取り専用のみ実行可 資格情報 (credentials) に関するコマンド及び GetInstanceAccess は実行不可 Amazon Lightsail List および Read は実行可 キーペアに対するコマンド及び GetInstanceAccess は実行不可 Amazon Redshift GetClusterCredentials は実行不可 Amazon S3 読み取り専用のみ実行可 GetBucketPolicy は実行不可 AWS Storage Gateway 読み取り専用のみ実行可 DescribeChapCredentials は実行不可 読み取り専用のコマンドの中でも、セキュリティや重要な情報に関するコマンドは実行不可となっています。また、 IAM や KMS 、 STS といった重要なサービスに関してはすべてのコマンドが実行不可となっています。\nIAM ポリシーとしては次のようになります。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:*\u0026#34;, \u0026#34;kms:*\u0026#34;, \u0026#34;sts:*\u0026#34;, \u0026#34;cognito-idp:GetSigningCertificate\u0026#34;, \u0026#34;ec2:GetPasswordData\u0026#34;, \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;gamelift:RequestUploadCredentials\u0026#34;, \u0026#34;gamelift:GetInstanceAccess\u0026#34;, \u0026#34;lightsail:DownloadDefaultKeyPair\u0026#34;, \u0026#34;lightsail:GetInstanceAccessDetail\u0026#34;, \u0026#34;lightsail:GetKeyPair\u0026#34;, \u0026#34;lightsail:GetKeyPairs\u0026#34;, \u0026#34;redshift:GetClusterCredentials\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;storagegateway:DescribeChapCredentials\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } ちなみに Amazon S3 に関するコマンドに関しては、 AWS CLI の s3api のサブコマンドを使用して、 s3 list-buckets のように呼び出します。 s3 ls は使用できません。\nLambda 呼び出しコマンドのアクセス許可 その名の通り、 Lambda 関数を実行するためのコマンドを実行可能となります。具体的には次のコマンドです。\nlambda invoke lambda invoke-async IAM ポリシーは次のようになります。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:invokeAsync\u0026#34;, \u0026#34;lambda:invokeFunction\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } AWS サポートコマンドのアクセス許可 AWS サポートに関するコマンドを実行可能となります。この権限が用意されている理由としては、例えばサポート担当の人がマネジメントコンソールにアクセスする必要なく、 Slack から AWS サポートに対してチケットを作成できるユースケースが考えられるからです。\nIAM ポリシーとしては次のようになります。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;support:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } これらの権限はチャンネルごとに設定できるため、チャンネルにジョインしているユーザーに合わせて権限を変えることで、可能な操作を制限しつつ便利に AWS リソースを操作できそうです。\nRunning AWS CLI commands from Slack channels - AWS Chatbot 実際にコマンドを実行してみる では実際にコマンドを実行してみます。今回は上で説明したポリシーテンプレートはすべて付与しているチャンネルで実行します。\nまずは全体のヘルプを確認してみます。Slack チャンネル内での AWS CLI コマンドの実行は、普段の aws の代わりに @aws として実行します。\nなので、ヘルプを確認する場合は @aws help とします。\nこんな感じで応答が返ってきます。これぞ chat bot っていう感じですね。\n読み取り専用コマンド 読み取り専用のコマンドの例として、 Lambda 関数の一覧を取得してみます。実行するのは @aws lambda list-functions です。\nLambda 関数のリストが返ってきました。\nでは、書き込みのコマンドを実行するとどうなるでしょうか。試しに s3 create-bucket コマンドを実行してみます。\nit isn\u0026rsquo;t enabled と言われて実行不可でした。ちなみに、実行不可なコマンドに関してはヘルプを見ることもできません。\nちなみに、チャンネルの IAM ロールにはポリシーテンプレートを使用せずに、既存の IAM ロールをアタッチすることも可能です。その際、たとえば S3 への FullAccess をもつ IAM ロールをアタッチしたとしても、 Slack からのコマンド実行でバケット作成などの書き込み操作ができるようになるわけではありません。\nLambda 呼び出しコマンド Lambda 呼び出しコマンド lambda invoke を実行してみます。実行する Lambda 関数としては、 context の一部を Slack にポストするだけの関数を用意しました。関数名は chatbot-function とします。\nimport json import urllib.parse from urllib.request import Request, urlopen from urllib.error import URLError, HTTPError SLACK_WEBHOOK_URL = \u0026#39;https://hooks.slack.com/************\u0026#39; def lambda_handler(event, context): print_context_attrs = [ \u0026#39;aws_request_id\u0026#39;, \u0026#39;function_name\u0026#39;, \u0026#39;function_version\u0026#39;, \u0026#39;memory_limit_in_mb\u0026#39;, \u0026#39;client_context\u0026#39;, ] context_data = {} for attr in print_context_attrs: if attr == \u0026#39;client_context\u0026#39;: client_context = context.client_context attr_value = {} if client_context is not None: for client_context_attr in dir(client_context): attr_value[client_context_attr] = getattr(client_context, client_context_attr) else: attr_value = getattr(context, attr) context_data[attr] = attr_value context_str = json.dumps(context_data, indent=2, ensure_ascii=False) message = f\u0026#39;```\\n{context_str}\\n```\u0026#39; post_to_slack(message) def post_to_slack(message): req = Request(SLACK_WEBHOOK_URL, json.dumps( {\u0026#39;text\u0026#39;: message}).encode(\u0026#39;utf-8\u0026#39;)) try: response = urlopen(req) response.read() except HTTPError as e: print(f\u0026#39;Request failed: {e.code} {e.reason}\u0026#39;) except URLError as e: print(f\u0026#39;Server connection failed: {e.reason}\u0026#39;) 本当に実行するかコマンドの確認があり、 Yes を押すと実行されます。\nチャンネルが一緒なので Lambda からのポストが間に入ってますが、実行結果も AWS Chatbot から返ってきています。\nここで、 Incoming WebHook のポストメッセージが AWS CLI のコマンドになっていたらどうなるのか？とふと思い、やってみました。同じ関数だとループしたとき怖いので、次のような別の Lambda 関数を作成しました。関数名は chatbot-pre-function とします。\nimport json import urllib.parse from urllib.request import Request, urlopen from urllib.error import URLError, HTTPError SLACK_WEBHOOK_URL = \u0026#39;https://hooks.slack.com/services/********\u0026#39; def lambda_handler(event, context): message = \u0026#39;@aws lambda invoke --function-name chatbot-function --region ap-northeast-1\u0026#39; post_to_slack(message) def post_to_slack(message): req = Request(SLACK_WEBHOOK_URL, json.dumps( {\u0026#39;text\u0026#39;: message}).encode(\u0026#39;utf-8\u0026#39;)) try: response = urlopen(req) response.read() except HTTPError as e: print(f\u0026#39;Request failed: {e.code} {e.reason}\u0026#39;) except URLError as e: print(f\u0026#39;Server connection failed: {e.reason}\u0026#39;) この関数を Slack から呼び出してみます。\nchatbot-function は呼び出されませんでした。\nAWS サポートコマンド 実際にサポートチケットを作成するわけにはいかないので、ヘルプを確認して実行可能なことを確認します。\nまとめ AWS Chatbot を使って Slack から AWS CLI コマンドを実行してみた話でした。\nサポートに関するコマンドだけを実行できる権限というのは、サポート担当の非エンジニアの方が Slack からサポートチケットを作成できるので、ハードルが少し低くなるかなと思いました。\nまた、 Lambda 関数を呼び出すことができるということは出来ることの幅がかなり広がるので、アイデア次第では面白い・便利な使い方ができるかもしれません。たとえば、特定の CodePipeline を実行する Lambda を作成しておけば、マネジメントコンソールにログインできなくても、 PC でターミナルを操作することができなくても、スマホに Slack アプリさえ入っていれば寝転びながらでもパイプラインを実行することができます。\n他にも使い方は色々ありそうなので、これからたくさんの情報が出てくることにワクワクしています。\n",
    "permalink": "https://michimani.net/post/aws-run-cli-commands-via-aws-chatbot/",
    "title": "AWS Chatbot を使って Slack から AWS CLI コマンドを実行する"
  },
  {
    "contents": "昨年 7 月に beta として登場した AWS Chatbot がついに一般利用可能 (GA) となりました！今回は、 AWS Chatbot を使って CloudWatch Alarm の通知を Slack に飛ばしてみました。CloudWatch Alarm の通知では、 AWS Chatbot がよしなに内容を整形してくれるので、これまで独自に Lambda で整形していた方にはそれらの処理が不要になります。\nAWS Chatbot Now Generally Available 目次 AWS Chatbot とは AWS Chatbot の設定 ワークスペースの作成 チャンネルの設定 Slack への招待 CloudWatch Alarm を Slack に通知する まとめ AWS Chatbot とは 詳細はサービスページや AWS ブログを見ていただければわかるのですが、簡単に言うと、 CloudWatch (など) からの通知をいい感じに整形して Slack または Amazon Chime Webhook に飛ばしてくれるサービスです。\nまた、 Slack から特定の AWS CLI コマンドを実行することもできます。\nAWS Chatbot - Amazon Web Services Introducing AWS Chatbot: ChatOps for AWS | AWS DevOps Blog AWS Chatbot – ChatOps for Slack and Chime | AWS News Blog AWS Chatbot 自体の利用には料金は発生せず、付随して利用する Amazon SNS などの料金のみを支払うことになります。\nAWS Chatbot の設定 では、実際に AWS Chatbot と Slack を連携してみます。\nワークスペースの作成 まずは ワークスペースの作成です。\nマネジメントコンソールで AWS Chatbot のダッシュボードを開きます。\nbeta が消えてますね。\nチャットクライントを設定 から Slack を選択して進みます。\nSlack の認証画面に遷移するので、内容を確認して Allow ボタンを押します。\nSlack との連携が完了し、 ワークスペース が作成されます。\nチャンネルの設定 続いて、作成したワークスペース内でチャンネルの設定をします。が、その際に SNS トピックを指定する箇所があるので、先に作成しておきます。もちろん既存の SNS トピックを使用することも可能です。\n今回は、 CloudFront のメトリクスに対するアラームを通知したいので、次のようなトピックをバージニア北部リージョンに作成しました。\nSNS トピックを作成したら、 AWS Chatbot のワークスペースに戻って 新しいチャンネルを設定 を押します。\n入力する項目は次のとおりです。\n設定の詳細 設定名 チャンネル設定の名前を入力します。\nログ記録 CloudWatch Logs にログを出力したい場合はチェックします。\nSlack チャンネル 連携した Slack ワークスペース内のチャンネルを指定します。パブリック または プライベートを選択し、チャンネルを指定します。\nアクセス許可 AWS Chatbot では CloudWatch のメトリクスを取得したり、特定の AWS CLI コマンドを実行することができるので、そのために IAM ロールを作成します。\n今回は CloudWatch Alarm の通知をしたいので、 ポリシーテンプレート で 通知のアクセス許可 を指定しました。\n通知 AWS Chatbot は SNS トピックを使用してCloudWatch Alarm を通知するので、任意の SNS トピックを指定します。指定方法は、まずリージョンを指定してから、そのリージョン内のトピックを指定します。\n今回は先ほどバージニア北部リージョンに作成した CloudWatchAlarm トピックを指定しました。\nこの設定をすることで、指定して SNS トピックのサブスクライブに AWS Chatbot エンドポイントが追加されます。\nSlack への招待 チャンネル設定で指定した Slack のチャンネルで、 /invite @aws を実行し、 AWS Chatbot を招待します。\nこれで準備は完了です。\nCloudWatch Alarm を Slack に通知する では、 AWS Chatbot 経由で CloudWAtch Alarm を Slack に通知してみます。\n今回は CloudFront の Alarm 画面から、次のようなアラームを設定しました。\nディストリビューションに対して、 5 分間の 4xx エラー率が 10 % を超えたときに通知するような設定です。\n設定後、意図的に 404 エラーを発生させ、暫く待つと、次のような通知が Slack に届きました。\nアラームの内容は良い感じに整形され、アラーム発生時点のメトリクスのグラフも画像として添付されて通知されます。めちゃくちゃ便利です。\nまとめ 昨年 7 月に beta として登場した AWS Chatbot がついに一般利用可能となりました！ということで、 CloudWatch Alarm の通知を Slack に飛ばしてみた話でした。\nこれまで、通知の内容を Lambda で整形して Slack へ通知していた方も多いと思います。それがなんと、AWS Chatbot を使うと簡単な設定のみで良い感じのメッセージにしてくれます。しかも無料。\nこれはなんというか、良いですね。(語彙力)\nまた、最近では Code シリーズからの通知も AWS Chatbot に対応しているので、それらの通知も良い感じに整形されると思うと凄く助かる気がしています。\nとりあえず今回は CloudWatch Alarm の通知をやってみたので、その他の通知についても随時試してみたいと思います。\n",
    "permalink": "https://michimani.net/post/aws/aws-post-slack-by-chatbot/",
    "title": "AWS Chatbot を使って CloudWatch Alarm の通知を Slack に飛ばしてみた"
  },
  {
    "contents": "Hugo でビルドしたブログやサイトを AWS 環境 (CloudFront + S3) で運用している方も多いと思います。今回は、Hugo のみならず静的サイトを AWS 環境で運用する際に利用したい Lambda@Edge についての話です。\n目次 前提 AWS 環境で静的サイトを運用する際の注意点 インデックスドキュメントの挙動 静的サイトの URL の正規化 Lambda@Edge とは Lambda@Edge で URL の正規化をする (ここでの) 正規化の定義 Hugo でアクセスが想定される URL Lambda の処理 まとめ 前提 まず、今回の話は下記のような構成で静的サイトを AWS 環境で運用していることを前提とします。\nS3 にコンテンツを配置し、前段に CloudFront を用意します。 CloudFront から S3 へのアクセスには Origin Access Identity を使用し、クライアントから S3 バケット内のオブジェクトへは直接アクセスできないようになっています。\nAWS 環境で静的サイトを運用する際の注意点 AWS 環境で静的サイトを運用する方法としては、上記のように S3 の前段に CloudFront を置く方法と、 S3 の Static website hosting 機能を使用する方法があり、この 2 つの方法では インデックスドキュメントの挙動が異なります 。\nインデックスドキュメントの挙動 例えば、次のような構造を持つサイトがあるとします。(仮にホスト名を http://example.com としておきます)\n├── error.html ├── index.html └── posts ├── sample_1 │ └── index.html └── sample_2 └── index.html このサイトを CloudFront + S3 と S3 の Static website hosting それぞれで運用した場合のインデックスドキュメントの挙動は次のようになります。\nCloudFront + S3 http://example.com または http://example.com/ へのアクセス\nindex.html の内容が表示される http://example.com/index.html へのアクセス\nindex.html の内容が表示される http://example.com/posts/sample_1 または http://example.com/posts/sample_1/ へのアクセス\n403 Access Denied http://example.com/posts/sample_1/index.html へのアクセス\nposts/sample_1/index.html の内容が表示される S3 の Static website hosting http://example.com または http://example.com/ へのアクセス\nindex.html の内容が表示される http://example.com/index.html へのアクセス\nindex.html の内容が表示される http://example.com/posts/sample_1 または http://example.com/posts/sample_1/ へのアクセス\nposts/sample_1/index.html の内容が表示される http://example.com/posts/sample_1/index.html へのアクセス\nposts/sample_1/index.html の内容が表示される つまり、 CloudFront + S3 で静的サイトを運用する場合、対象のバケットの直下にある index.html のみ省略が可能で、それ以降の階層にアクセスするためには明示的に index.html \u0008にアクセスする必要があるということです。\n静的サイトの URL の正規化 これが今回の一番のポイントです。\n上で説明したとおり、個別のページには /index.html を付与する必要があります。しかし、トップページには付与する必要はありませんし、付与してもアクセス可能です。つまり、トップページに対しては http://example.com と http://example.com/index.html という 2 つの URL を持つことになります。\nこれらの見た目は同一ページですが、 Google Analytics などのアクセス解析などを利用する際には別ページとして扱われます。なので、 index.html ありかなしかは統一されていたほうがよいです。\nじゃあどっちに統一するかという話ですが、トップページに関してはドメインのみでアクセスできたほうがスッキリするので index.html が無いほうがいいですよね。合わせて個別ページも / でアクセスできるようにしたいところですが、上で説明したように深い階層にアクセスする際には index.html を付ける必要があります。\nこれを解決するのが、 CloudFront の Lambda@Edge という機能です。\nLambda@Edge とは Lambda@Edge は、Amazon CloudFrontの機能で、アプリケーションのユーザーに近いロケーションでコードを実行できるため、パフォーマンスが向上し、待ち時間が短縮されます。\n(中略)\nLambda@Edge を使用すると、サーバー管理を何も行わなくても、ウェブアプリケーションをグローバルに分散させ、パフォーマンスを向上させることができます。Lambda@Edge は、Amazon CloudFront コンテンツ配信ネットワーク (CDN) によって生成されたイベントに対応してコードを実行します。\nLambda@Edge | AWS つまりどういうことかというと、 CloudFront に発生するイベント をトリガーに Lambda 関数を実行できるということです。 CloudFront に発生するイベント とは、次の 4 つです。\nViewer Request : クライアントから CloudFront へのリクエスト Viewer Response : CloudFront からクライアントへのレスポンス Origin Request : CloudFront から Origin へのリクエスト Origin Response : Origin から CloudFront へのレスポンス たとえば、 Basic 認証を設定したい場合、 Viewer Request をトリガーにして認証情報をチェックする Lambda 関数を実行し、正しければ Origin へのリクエストを継続し、正しくなければクライアントへ認証失敗のレスポンスを返す といったことが可能になります。\nLambda@Edge を使用することで、クライアントからのリクエストを Origin に到達するまでにチェックすることが可能になります。なので、上の例と同じく Viewer Request をトリガーにしてクライアントからのリクエストをよしなに処理する Lambda を実行することで、 URL の正規化をやろうというのが今回の話です。(ここまで長くなりました)\nLambda@Edge で URL の正規化をする Viewer Request をトリガーにすることで、クライアントからのリクエスト情報にアクセスすることができます。リクエストの URL とかヘッダーとかです。\n実行される Lambda ではリクエストの URL をチェックして、 index.html ありなのかなしなのか、 / で終わっているかどうか、などをチェックし、必要であればクライアントにリダイレクトさせるなどの処理を実行します。その処理をうまいことやって、 URL の正規化を実現します。\n(ここでの) 正規化の定義 一言で 正規化 と言っても、どういった形に落とし込むかは色々あると思います。なので、今回は次のような条件を満たすように正規化を考えることにします。\n各ページには / 終わりでのアクセスに統一 / 無し、および /index.html 終わりのリクエストは / に 302 リダイレクト これによって、次のリクエストはすべて https://michimani.net/post/development-lambda-edge-for-hugo-hosted-aws/ に統一され、アクセス解析でもこの URL へのアクセスとして計算されることになります。\nhttps://michimani.net/post/development-lambda-edge-for-hugo-hosted-aws/ https://michimani.net/post/development-lambda-edge-for-hugo-hosted-aws https://michimani.net/post/development-lambda-edge-for-hugo-hosted-aws/index.html Hugo でアクセスが想定される URL Hugo でビルドしたブログへのアクセスとして想定される URL を確認しておきます。\n/page/* : 記事一覧ページのページング /posts/* または /post/* : 各記事 /about/ : about ページ /categories/* : カテゴリ別ページ /tags/* : タグ別ページ /archives/* : アーカイブページ これらのページ対して正規化の処理を実行するようにします。\nLambda の処理 お待たせしました。やっと Lambda 本体にたどり着きました。\nやっていることは次の 2 つです。\n/ 無し、および index.html 終わりのリクエストは / 終わりの URL に 302 リダイレクト / 終わりのリクエストは /index.html へのリクエストとして改ざんして Origin へ まず一つ目ですが、正規表現で条件にマッチする場合は、クライアントにリダイレクトさせるような値を返却しています。\nconst redirectUrl = host + requestUri.replace(\u0026#39;/index.html\u0026#39;, \u0026#39;\u0026#39;) + \u0026#39;/\u0026#39;; const response = { status: \u0026#39;302\u0026#39;, statusDescription: \u0026#39;Found\u0026#39;, headers: { location: [{ key: \u0026#39;Location\u0026#39;, value: redirectUrl, }], }, } // return response to redirect (for viewer) callback(null, response); こうすることで、クライアントのリクエストは Origin まで到達せず、 CloudFront からレスポンスが返ることになります。\n二つ目に関しては、 / 終わりでリクエストされた URL を /index.html へのリクエストという風に request.uri を更新しています。\n// Replace the received URI with the URI that includes the index page request.uri = actualUri; // Return to CloudFront (for origin) callback(null, request); これにより、 / という Viewer Request が /index.html という Origin Request に変わり、 Origin は /index.html ならあるよ ということでレスポンスを返してくれます。クライアントからすれば、 / のリクエストで /index.html が返ってきた感じです。\nこの Lambda が実行されるのは CloudFront にキャッシュが存在しない場合のみなので、料金的にもあまり心配はいりません。\nまとめ AWS 上で静的サイトを運用するときに Lambda@Edge で URL を正規化する方法について書きました。\n最後に紹介した Lambda 関数は Hugo を想定したものになっていますが、他の静的サイトジェネレータでビルドしたサイトでも同様の方法で URL の正規化はできます。\nAWS で静的サイトを運用する場合は、インデックスドキュメントの挙動に注意して、 URL の正規化についても頭に入れておいたほうが良さそうです。Lambda@Edge のユースケースごとの実装サンプルについて公式ドキュメントにいくつか用意されているので、こちらもすごく参考になります。\nLambda@Edge Example Functions - Amazon CloudFront ",
    "permalink": "https://michimani.net/post/development-lambda-edge-for-hugo-hosted-aws/",
    "title": "Lambda@Edge で静的サイトの URL を正規化する"
  },
  {
    "contents": "ブログでよくある「最近読まれている記事」みたいなものを静的サイトでも実現したいと思い、Google Analytics Reporting API v4 を使って直近一週間のページごとの PV 数を取得してみました。取得したデータを JSON とかで保持しておけば、前日までのデータで PV ランキングが作成できそうです。\n目次 概要 前提 実装 1. 必要なライブラリのインストール 2. サンプルスクリプトの実行 3. PV 取得用に編集 まとめ 概要 Google Analytics Reporting API v4 (以下、Reporting API) を使用して、直近一週間の各ページごとの PV 数を取得します。取得したデータは、下記のような形で保存します。\n[ { \u0026#34;page_path\u0026#34;: \u0026#34;/path/to/post_1\u0026#34;, \u0026#34;page_view\u0026#34;: 100 }, { \u0026#34;page_path\u0026#34;: \u0026#34;/path/to/post_2\u0026#34;, \u0026#34;page_view\u0026#34;: 99 } ] 取得スクリプトは Python 3.x で実装します。\n(.venv) $ python -V Python 3.7.6 適宜、仮想環境を作るなりして作業してください。\n前提 Reporting API v4 が有効になっている サービスアカウントが作成されている サービスアカウントの認証情報 (client_secrets.json) が手元にある 上記の準備については下記の公式レイファレンスを参照してください。\nはじめてのアナリティクス Reporting API v4: サービス アカウント向け Python クイックスタート 実装 実装については公式のクイックスタートにあるサンプルがほぼそのまま使えるので、それをもとに下記の手順で進めていきます。\n必要なライブラリのインストール サンプルスクリプトの実行 PV 取得用に編集 1. 必要なライブラリのインストール Reporting API を Python で使用するために、まずはクライアントライブラリをインストールします。\n$ pip install --upgrade google-api-python-client これに加えて、 oauth2client もインストールします。\n$ pip install oauth2client 2. サンプルスクリプトの実行 クイックスタートのページにあるサンプルスクリプト HelloAnalytics.py をコピーまたはダウンロードしてきて、下記のように編集します。\n\u0026#34;\u0026#34;\u0026#34;Hello Analytics Reporting API V4.\u0026#34;\u0026#34;\u0026#34; from apiclient.discovery import build from oauth2client.service_account import ServiceAccountCredentials SCOPES = [\u0026#39;https://www.googleapis.com/auth/analytics.readonly\u0026#39;] - KEY_FILE_LOCATION = \u0026#39;\u0026lt;REPLACE_WITH_JSON_FILE\u0026gt;\u0026#39; - VIEW_ID = \u0026#39;\u0026lt;REPLACE_WITH_VIEW_ID\u0026gt;\u0026#39; + KEY_FILE_LOCATION = \u0026#39;./client_secrets.json\u0026#39; + VIEW_ID = \u0026#39;1234567890123\u0026#39; 認証情報は client_secrets.json という名前で、このスクリプトと同じディレクトリに保存されているとします。\nVIEW_ID に指定するのは、 Google Analytics のビューの ID です。確認するには、 Google Analytics のコンソールか、 Account Explorer を使用します。\nAccount Explorer — Google Analytics Demos \u0026amp;amp; Tools コンソールで確認する場合は、下のキャプチャで赤く塗られている部分の値を確認します。\nサンプルスクリプトを実行すると、7 日前から今日までの 各国 (地域) ごとのセッション数が出力されます。\n$ python HelloAnalytics.py ga:country: (not set) Date range: 0 ga:sessions: 5 ga:country: Australia Date range: 0 ga:sessions: 1 ga:country: Bahrain Date range: 0 ga:sessions: 1 ga:country: Brazil Date range: 0 ga:sessions: 1 ga:country: Canada Date range: 0 ... 3. PV 取得用に編集 各ページ (パス) ごとの PV 数を取得するために、サンプル内の get_report() 関数を次のように編集します。\nreturn analytics.reports().batchGet( body={ \u0026#39;reportRequests\u0026#39;: [ { \u0026#39;viewId\u0026#39;: VIEW_ID, - \u0026#39;dateRanges\u0026#39;: [{\u0026#39;startDate\u0026#39;: \u0026#39;7daysAgo\u0026#39;, \u0026#39;endDate\u0026#39;: \u0026#39;today\u0026#39;}], - \u0026#39;metrics\u0026#39;: [{\u0026#39;expression\u0026#39;: \u0026#39;ga:sessions\u0026#39;}], - \u0026#39;dimensions\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;ga:country\u0026#39;}] + \u0026#39;dateRanges\u0026#39;: [{\u0026#39;startDate\u0026#39;: \u0026#39;7daysAgo\u0026#39;, \u0026#39;endDate\u0026#39;: \u0026#39;yesterday\u0026#39;}], + \u0026#39;metrics\u0026#39;: [{\u0026#39;expression\u0026#39;: \u0026#39;ga:pageviews\u0026#39;}], + \u0026#39;dimensions\u0026#39;: [{\u0026#39;name\u0026#39;: \u0026#39;ga:pagePath\u0026#39;}] }] } ).execute() 上記のように metrics と dimensions を編集することで、次のようなレスポンスが取得できます。\n{ \u0026#34;reports\u0026#34;: [ { \u0026#34;columnHeader\u0026#34;: { \u0026#34;dimensions\u0026#34;: [ \u0026#34;ga:pagePath\u0026#34; ], \u0026#34;metricHeader\u0026#34;: { \u0026#34;metricHeaderEntries\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ga:pageviews\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;INTEGER\u0026#34; } ] } }, \u0026#34;data\u0026#34;: { \u0026#34;rows\u0026#34;: [ { \u0026#34;dimensions\u0026#34;: [ \u0026#34;/path/to_post_1\u0026#34; ], \u0026#34;metrics\u0026#34;: [ { \u0026#34;values\u0026#34;: [ \u0026#34;100\u0026#34; ] } ] }, { \u0026#34;dimensions\u0026#34;: [ \u0026#34;/path/to/post_2\u0026#34; ], \u0026#34;metrics\u0026#34;: [ { \u0026#34;values\u0026#34;: [ \u0026#34;99\u0026#34; ] } ] } ] } } ] } このレスポンスから各パスの PV 数を集計するための関数を作成します。合わせて、結果を出力する関数も作成します。この際、レスポンスに含まれるパスにはクエリパラメータも含まれているので、集計時には無視するようにしています。\ndef calc(response): # type: (dict) -\u0026gt; () \u0026#34;\u0026#34;\u0026#34;Calculate page views of each page path. Args: response: The Analytics Reporting API V4 response. \u0026#34;\u0026#34;\u0026#34; calc_res = dict() pv_summary = [] report = response.get(\u0026#39;reports\u0026#39;, [])[0] for report_data in report.get(\u0026#39;data\u0026#39;, {}).get(\u0026#39;rows\u0026#39;, []): # get page path page_path = report_data.get(\u0026#39;dimensions\u0026#39;, [])[0] # ignore query parameters page_path = re.sub(r\u0026#39;\\?.+$\u0026#39;, \u0026#39;\u0026#39;, page_path) # get page view page_view = int(report_data.get(\u0026#39;metrics\u0026#39;, [])[0].get(\u0026#39;values\u0026#39;)[0]) if page_path in calc_res: calc_res[page_path] += page_view else: calc_res[page_path] = page_view for path in calc_res: pv_summary.append({ \u0026#39;page_path\u0026#39;: path, \u0026#39;page_views\u0026#39;: calc_res[path] }) # sort by page views pv_summary.sort( key=lambda path_data: path_data[\u0026#39;page_views\u0026#39;], reverse=True) return pv_summary def save_as_json(data, file_path=\u0026#39;./res.json\u0026#39;): # type: (dict or list, str) -\u0026gt; () \u0026#34;\u0026#34;\u0026#34; Save dict pr list as JSON file. Args: data: dict or list object to save. file_path: file path for JSON file. (default is \u0026#39;./res.json\u0026#39;) \u0026#34;\u0026#34;\u0026#34; with open(file_path, \u0026#39;w\u0026#39;) as f: f.write(json.dumps(data, indent=2)) もともとあった print_response() 使用しないので削除して、最終的には下記のようになります。\n実行すると、下記のような JSON ファイルが生成されます。\n[ { \u0026#34;page_path\u0026#34;: \u0026#34;/path/to/post_1\u0026#34;, \u0026#34;page_views\u0026#34;: 100 }, { \u0026#34;page_path\u0026#34;: \u0026#34;/path/to/post_2\u0026#34;, \u0026#34;page_views\u0026#34;: 99 }, { \u0026#34;page_path\u0026#34;: \u0026#34;/path/to/post_4\u0026#34;, \u0026#34;page_views\u0026#34;: 10 }, { \u0026#34;page_path\u0026#34;: \u0026#34;/path/to/post_3\u0026#34;, \u0026#34;page_views\u0026#34;: 5 } ] まとめ Google Analytics Reporting API v4 を使って直近一週間のページごとの PV 数を取得してみた話でした。本当は記事のタイトルも取得できれば良いのですが、今回はパスごとの PV 数で一旦妥協してます。ただ、URL さえわかればはてなブログの埋め込みスクリプトを使用してリンクを作成することができるので、この情報だけでもなんとかなりそうです。\nあとはこのスクリプトを定期的に (1日1回) 実行して S3 にでも JSON を配置できれば、静的サイトでも記事の閲覧数ランキングが簡単に作成できそうです。\n",
    "permalink": "https://michimani.net/post/programming-get-access-count-from-google-analytics/",
    "title": "[Python] Google Analytics Reporting API v4 で直近一週間のページごとの PV 数を取得する"
  },
  {
    "contents": "今日 4 月 10 日はフォントの日です。普段生活していてフォントを意識することはあまりないとは思いますが、フォントが変わるだけで目に入っていくる情報の印象はかなり変わります。\nこのブログでもWebフォントをつかっているので、今回はその印象の変わり方について、ざっくりと簡単に書きたいと思います。\n目次 そもそもフォントとは どんな種類のフォントがあるか フォントって誰が作ってるのか 実際にどれだけ印象が変わるのか？ UD新ゴ UD黎ミン サン ラインG ブログでもフォントに拘る まとめ そもそもフォントとは 冒頭にも書きましたが、普段からフォントを意識している人は少ないと思います。とは言っても、仕事などで資料作成する際に、文字の形を変えるために無意識に色んなフォントを使っていると思います。\n私生活で言うと、新聞とか電車内の電子案内版とか道路標識とか、同じ文字でも違う形をしています。あれがフォントの違いです。\nどんな種類のフォントがあるか Windows ユーザの方なら、 MS ゴシック とか MS 明朝 とかは馴染みがあると思います。 Mac ユーザの方なら、 ヒラギノ とかです。\nWindows 派、 Mac 派 の間でしばしば論争になるのが、システムフォントの違いです。(主に Mac 派が MS ゴシック、 MS 明朝をディスる時に一方的に言ってる気もしますが)\nシステムフォントは、 PC の設定画面とかエクスプローラ (Finder) で表示される文字に使われるフォントで、 PC を使って作業するうえで一番目にするフォントです。実際、私も Mac 歴が長いので、久々に Windows の画面を見ると ウッ となることがあります。\nOS を選択する条件にもなり得るという意味では、フォントの持つ影響は大きいと言えます。\nフォントって誰が作ってるのか フォントは、 Microsoft や Google、 Adobe などの企業が独自に作っていたりする場合もありますが、フォントを専門に作っている会社(フォントベンダー)もあります。\n世界的に有名なのは Monotype 、日本では モリサワ 、 フォントワークス 、 視覚デザイン研究所 などがあります。\nまた、個人のデザイナーさんが独自に作っている場合もあります。\nフォントベンダーが扱うフォントについてはほとんどが有償で、 1 書体いくら とか、いくつもの書体を使える年間のサブスクリプションを買うという方法が一般的です。\n最近では書類だけではなく Web サイトにもフォントを適用できる (Web フォント) ので、各ベンダーは Web フォント用のサービスも展開しています。\n実際にどれだけ印象が変わるのか？ じゃあ、フォントによってどれくらい印象が変わるのか、いくつかのフォントで試してみたいと思います。\n検証に使うのは、皆さんご存知 枕草子の第 1 段です。\nUD新ゴ まずは、モリサワの A-OTF UD新ゴ の場合。\nゴ は、ゴシックを意味しているので、基本的にはゴシック体です。 UD とは、Universal Design (ユニバーサルデザイン) のことで、文化・言語・国籍の違い、老若男女といった差異、障害・能力の如何を問わずにどんな人にも読みやすい設計であるということです。\n最近では教育業界での注目度が高まっています。\n認知されにくい読み書き障害　ＵＤフォントを使うと　（1/3ページ） - 産経ニュース 革命的に読みやすい、ＵＤフォント　学力向上効果も期待：朝日新聞デジタル UD黎ミン 続いては、同じくモリサワの UD黎ミン の場合です。\n黎ミン は 「レイミン」 と読みます。これは明朝体をベースにしたフォントです。\n明朝体はゴシック体と違って、「大人っぽさ」「洗練された感じ」が出ると言われることが多いです。\nUD 新ゴと同様に、こちらも U D書体となっているので、非常に読みやすいです。\nサン 続いては、タイプバンクの サン の場合。\nサンは手書き感があるフォントで、読みやすさ というよりは デザイン性 に振っているフォントかなという印象です。\nブログの本文としてはちょっと使いにくいので、例えば引用箇所とか、一部分にだけ使うの分にはよさそうです。\nラインG 最後に、視覚デザイン研究所の ラインG の場合。\n視覚デザイン研究所は、普通のゴシック体や明朝体に加えて、非常にデザイン要素の強いフォントを扱っている印象です。\nこのラインGもデザイン性が強く、長い文章には向いていません。\nポスターやWeb広告など、掲示物に使用するとインパクトはありそうです。\nブログでもフォントに拘る フォントが違うだけでかなり印象が変わって、同じ文章でも、内容の入ってきやすさ、そもそも 読みたい/読みたくない という切り分けの要因にもなります。\nブログにおいてもフォントは重要だと思っていて、すごく良い文章が書かれていても、それがデザインに特化したフォントで書かれていると、多分途中で読むのを諦めてしまいます。\nそんなときは Web フォントを使ってみます。冒頭に書いたように各フォントベンダーが提供している Web フォントのサービスを使うという方法があります。\nモザイク | Monotype. TypeSquare - モリサワ しかし、各ベンダーのこういったサービスは、個人向けと言うよりは企業のポータルサイトやECサイトなどが主なターゲットになっています。\nなので、個人でWebフォントを利用する場合は、 Adobe の Adobe Fonts がおすすめです。\nAdobe Fonts | 無制限のフォントを検索 Adobe Fonts の良いところは、様々なフォントベンダーのフォントを使えるという点です。また、一部のフォントは PC にダウンロードすることができ、 Adobe CC アプリケーション (Photoshop, Illustrator など) 以外のエディタやその他のアプリケーション内でも利用が可能です。もちろん Word や PowerPoint でも。\nブログのデザインをガラッと変えることはなかなか難しいですが、フォントだけであれば簡単に変更できるにもかかわらず変わった感が大きいので、手軽にブログの雰囲気を変えたい場合にも Web フォントはおすすめです。\nまとめ フォントの日ということで、フォントの違いによる印象の違いについてざっくりと書きました。\n最近では、山手線の新駅 高輪ゲートウェイ駅のフォントが少し話題になりましたが、街なかにある看板やレストランのメニューなど、フォントに注目して見てみるとなかなか面白いです。\nぜひこの機会に普段仕事で作成している資料のフォントや、身近なも文字のフォントについて考えてみてはどうでしょうか。\n",
    "permalink": "https://michimani.net/post/other-font-day-2020/",
    "title": "4月10日はフォントの日！フォントによる印象の違いのざっくりとした話"
  },
  {
    "contents": "テレワークが本格的に始まって 1 ヶ月が過ぎたので、その間に試行錯誤しながら準備した作業環境が一旦それっぽい感じで落ち着いたのでご紹介します。なお、タイトルはプチバズを起こしているブログ記事のパロディです。クラスメソッドの社長さんの記事ですが、おもしろさと現実味と勢いがすごいです。トータルで、めちゃくちゃ羨ましいです。\n社長として最低限のテレワーク環境を整えてみた | Developers.IO 目次 試験的導入から本番導入へ 快適な広い作業環境 快適な手元の作業環境 快適な音声環境 快適な着座環境 快適な空気環境 広さは正義 おまけ 配線 雑念 まとめ 試験的導入から本番導入へ 私の所属している企業では、今年に入ってから試験的にテレワークと時差出勤が導入されていました。試験的というのは、今年開催される予定だった東京オリンピックに備えて、その期間に問題なくテレワークできるように色々試しておこうという目的からです。試験的なので週 2 日を上限にテレワークしてみる、という状態でした。それが COVID-19 の影響から、週 2 日の上限を撤廃 -\u0026gt; テレワーク推奨 -\u0026gt; 原則テレワーク と、試験的導入から一気に本番導入になりました。\nそこから 1 ヶ月が過ぎて、ある程度 自宅での作業環境が整ってきました。もともとあった物、テレワークが始まってから購入したもの、どちらもありますが、今回は現在の作業環境をご紹介したいと思います。\n快適な広い作業環境 会社では 27 インチのサブディスプレイが支給されていたので、そちらをメインにして作業をしていました。自宅でも快適に作業をするためにデュアルディスプレイで作業しています。使用しているのは、 BenQ EL2870U です。\nこれはもともとスマブラ SP を快適にプレイしたいと思って 1 年くらい前に購入していたもので、 BenQ のシリーズの中でも ゲーミングモニター に分類されるモニターです。28 インチ 4K HDR 対応となっていて、目にも優しく、解像度も高いので作業環境が広く、快適に作業をすることができます。\nBenQ ゲーミングモニター ディスプレイ EL2870U 27.9インチ/4K/HDR/TN/1ms/FreeSync対応/HDMI×2/DP1.4/スピーカー/アイケア機能B.I.+ 快適な手元の作業環境 続いては手元の作業環境、キーボードとマウスです。\n会社では Apple のワイヤレスキーボードを使用していましたが、いま自宅で使用しているのは REALFORCE for MAC です。\n前から気になっていましたが、決して安いものではないのでなかなか踏み切れずにいました。しかし、買うなら今でしょっていうくらいの状況になったので、重い腰をあげて購入しました。高い買い物でしたが、これは本当に買ってよかったです。もう Apple のキーボードには戻れないです。快適すぎます。詳しいレビューについては個別に書いているので、購入を考えている方は参考にしてみてください。\n東プレ R2TLSA-JP3M-BK REALFORCE TKL SA R2 for Mac 日本語 テンキーレスキーボード （91配列 APC機能 + 静音）：シルバー/黒 レーザー印刷 30g マウスなんですが、色々考えた結果やっぱりトラックパッドが便利だなという結論になりました。古い iMac についてきた Magic Trackpad が手元にありましたが、乾電池駆動というのがイケてないので、新たに Magic Trackpad 2 を購入しました。\n旧 Trackpad との違いは\n白い 広い 傾斜がゆるい 充電式 (Lightning ケーブル) 感圧タッチ対応 クリック音が静か です。基本的にはパワーアップしているんですが、傾斜に関しては旧型の方が良かったですね。\nマウスではなくトラックパッドにしたことによるメリットとしては、やはり Mac の優秀なジェスチャがそのまま使えるという点ですね。指の本数、ピンチ、上下の移動などで様々な動作をすることができるので、そこはトラックパッドが優秀な部分です。\nちなみに黒 (スペースグレイ) もありますが、白よりも値段が高いです。\n快適な音声環境 テレワークが始まり、当たり前ですが会議などは全てオンラインです。その際に、こちらの音声を良い感じに相手に伝えるためには外部マイクが必要になります。というこで使用しているのが、 TASCAM DR-07X です。\nマイクアームは準備できていないので、ゴリラポッドでディスプレの上部に引っ掛けてます。\nDR-07X は公式ページに\n簡単操作で高音質録音を提供するコンパクトなハンディレコーダーです。\nDR-07X | 製品トップ | TASCAM (日本) とあるように、いわゆるリニア PCM レコーダーなんです。ただし、 USB 接続できるオーディオインターフェースが搭載されているので、外部マイクとして使用することもできます。マイク部分は可動式の単一指向性マイクで、自分の声をしっかり届けたいときは閉じた状態で、周りの音も届けたい場合は開いた状態で使うことができます。まあ、会議中に周りの音を届けたい場面はあまりないと思いますが。意外と安価なのでおすすめです。\nDR-07X については昨日届いたばかりなので、またあらためて個別のレビュー記事を書きたいと思います。\nTASCAM タスカム - USB オーディオインターフェース搭載 ステレオ リニアPCMレコーダー DR-07X 快適な着座環境 長い時間座って作業するので、快適な着座環境が必要になります。自宅で使っているのは AKRacing Pro-X です。\nこちらもスマブラ SP を快適にプレイしたいという思いから準備したもので、使い始めてから半年くらい経ちます。ワンルームでお世辞にも広いとは言えない部屋に置くと存在感がすごいですが、快適さも半端ないです。ふつうに座って作業するもよし、座面にあぐらをかいて座るもよし、リクライニングをフルに倒して昼寝をするもよし。とにかく丸一日座っていてもお尻が痛くなったりすることはありません。\nゲーミングチェアってめちゃくちゃ高いものだと思っていたんですが、いわゆる高級オフィスチェアの半分以下くらいの値段で買えるのでおすすめです。\nこちらも個別記事を書いているので参考にしてみてください。\n快適な空気環境 テレワークが本格的に始まった 2 月後半から 3 月上旬あたりは、まだまだ寒い日が多く空気が乾燥していました。今年のはじめに乾燥から喉をやられて風邪をこじらせたことがあったので、同じ過ちを繰り返さないように加湿器を買いました。何度も給水するのが面倒なのと、価格も安いものが良かったので、 Amazon で適当に探して良さげだなと思った Dreamegg DG001 を使っています。\n容量は 2 L。蒸気の強さ (噴出量) は 2 段階で調節可能です。タイマーも 1,3,6 時間で設定可能です。\n音も静かなので、寝てる間に起動していても全く気になりません。起動中の青色 LED がちょっと眩しいくらいです。\n最近は空気が乾燥することもなくなってきたので起動回数は減ってますが、これがなければまた風邪をこじらせていたと思います。\nDreamegg 加湿器 卓上 超音波 2020革新版 大容量 2L しずく型 タイマー機能（1H/3H/6H） 最大30時間稼働 LEDライト付 静音 ミスト噴出調整可 空焚き防止 省エネ 乾燥対策 DG001 広さは正義 ということで色々紹介してきましたが、全体的にはこんな感じになっています。\n実はこれで机の半分のスペースなんです。使っている机は IKEA の LINNMON / ADILS テーブル です。奥行き 60 cm で、横幅はなんと 200 cm ！ めちゃくちゃ広いです。ただ、ここに写っていない右半分には古い iMac とか本がおいてあったり、ちょっとした物置状態になっています。\nちなみにこのテーブルは高さが調節できないんですが、別売りで OLOV という伸縮できる脚があるのでそれをつかって調節してます。\n幅も奥行きも大満足なこのテーブルですが、唯一の弱点が 軽さ です。椅子の肘置きがあたっただけでも結構揺れたり、長い辺の橋に極端に重たいものをのせると少し傾いたりすることがあるので、安定感に難ありです。本当はモニターアームを使って机上をより広く使いたいという妄想もあるんですが、果たして端っこに荷重をかけて大丈夫なのか、不安があります。\nそれでも、広さは正義です。\nLINNMON リンモン / ADILS オディリス テーブル - ホワイト - IKEA OLOV オーロヴ 脚 伸縮式 - ブラック - IKEA おまけ 作業とは直接関係の無い部分についても少し紹介します。\n配線 これまでは床にタップがあったので、あらゆるコードが床を這っていました。そのため、椅子の脚 (コロコロ) で踏んだり絡まったりすることがよくありました。割とありがちな対策ですが、電源タップを机の裏にくっつけることによって、それらの問題は解決しました。\n電源タップ自体にも様々な種類がありますが、 エレコム ECT-0103BK は 10 個口で各口の間隔も少しずつ違っているため、様々なアダプタの形状があってもタップの口を有効に使うことができます。壁などに取り付けるためのネジも付属しているのでおすすめです。\nエレコム 電源タップ 雷ガード 10個口 ほこりシャッター付 固定\u0026amp;吊下可能な回転パーツ付き 3m ブラック ECT-0103BK 雑念 ちなみに机の左端には\n気分をあげる BGM を再生してくれる Amazon Echo いやし効果のあるパックンフラワーの amiibo ここに置くしかない Nintendo Switch 置き場所を失ったキューブたち が置かれています。\n本来、雑念であるこれらのものは視界に入らない場所に置くほうが良いと思いますが、個人的には少し遊びがあるくらいのほうが集中できる気がしてます。\n余談ですが、この外出自粛期間にスピードキューブ始めてみませんか？室内でできる趣味として非常におすすめです。気付いたら数時間溶けてることもザラにあります。\nまとめ テレワークが始まって 1 ヶ月が過ぎ、なんとなく落ち着いた作業環境についての話でした。\n一通りはアイテムが整った感じですが、追加するとなると Web カメラですかね。現状は右側に置いている MacBook Pro のカメラを使用していますが、どうしても目線が違う方向を向くことになります。まあ、必ずしもカメラ目線になる必要はないと思うのでこの状態で過ごしていますが、次の課金ポイントとしてはそこかなと思います。\n色々紹介しましたが、一番大事なのは椅子かなと。触れている時間が一番長い、さらに特殊な場合1 を覗いて無いと作業できないくらい大事なアイテムです。様々な種類の椅子があり値段もピンキリですが、自分に合う一品を見つけて快適な着座環境をゲットしてもらいたいです。形状は様々ですが、アームレストはあったほうがいいです。椅子の昇降に加えて、アームレスト単体での昇降もあるとなお良いです。\n個人的に他人の作業環境を見るのが好きなので、この機会にこんな記事がたくさん出てくると楽しいなと思ってます。\n立った状態、または寝転んだ状態\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n",
    "permalink": "https://michimani.net/post/gadget-for-remote-work-tele-work/",
    "title": "一般的なエンジニアとしてそれっぽいテレワーク環境を整えてみた"
  },
  {
    "contents": "Amazon CloudFront は AWS が提供している CDN サービスで、 AWS 環境で Web アプリケーションを運用する際に利用することが多いと思います。このブログの運用にも使用しているのですが、ふと CloudFront のキャッシュ仕様について気になったので、あらためて調べてみました。\n気になったきっかけは先日書いたブログ記事です。\n目次 前提 触れること 各項目の概要 各項目の詳細 Cached HTTP Methods Cache Based on Selected Request Headers Object Caching Forward Cookies Query String Forwarding and Caching まとめ 前提 CloudFront の Web Distribution において、 オリジンに S3 を指定している場合 のキャッシュ仕様について自分なりにまとめたものになります。公式ドキュメント以上の内容はありません。\nまた、 RTMP Distribution または Web Distribution でカスタムオリジンを指定している場合は挙動が変わる部分もあるので、詳細は公式ドキュメントを参照してください。\nAmazon CloudFront とは - Amazon CloudFront ちなみに、RTMP Distribution は 2020/12/31 でサポートが終了するみたいです。\nAWS Developer Forums: RTMP Support Discontinuing on December 31, 2020 触れること 今回は、 Amazon CloudFront (以下、 CloudFront) のキャッシュ仕様について調べてみた内容をまとめます。具体的には、 Web Distribution の Behavior の設定画面で出てくる項目のうち、次の項目についてです。\nCached HTTP Methods Cache Based on Selected Request Headers Object Caching Forward Cookies Query String Forwarding and Caching 設定画面では下のキャプチャ内で赤枠で囲った部分です。\n各項目の概要 上であげた項目の詳細の前に、簡単にそれぞれの項目が何を設定する項目なのか、概要をおさらいします。\nCached HTTP Methods キャッシュ対象とする HTTP メソッドを指定します。\nCache Based on Selected Request Headers リクエストヘッダの情報をもとにキャッシュするかどうかを指定します。\nObject Caching キャッシュ時間を指定します。\nForward Cookies Cookie をもとにキャッシュするかどうかを指定します。\nQuery String Forwarding and Caching クエリパラメータをもとにキャッシュするかどうかを指定します。\n各項目の詳細 では、各項目の指定内容と仕様についてまとめていきます。\nCached HTTP Methods キャッシュ対象とする HTTP メソッドを指定します。デフォルトで GET と HEAD はキャッシュ対象になっており、対象から外すことはできません。\nAllowed HTTP Methods の指定方法によって、選択できる HTTP メソッドが下記のように変わります。\n「GET, HEAD」を指定した場合 デフォルトの GET と HEAD のみキャッシュ対象となります。\n「GET, HEAD, OPTIONS」 または「GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE」を指定した場合 デフォルトの GET と HEAD に加えて OPTIONS もキャッシュ対象にすることが可能です。\nCache Based on Selected Request Headers CloudFront は、デフォルトではオリジンのオブジェクトをキャッシュする際にリクエストヘッダは考慮されません。なので、例えば下記のように custom-header に別々の値を指定してリクエストしても、 どちらか一方のみがキャッシュされる ことになります。\nGET /get HTTP/1.1 Accept: */* Accept-Encoding: gzip, deflate Connection: keep-alive Host: httpbin.org User-Agent: HTTPie/2.0.0 custom-header: foo GET /get HTTP/1.1 Accept: */* Accept-Encoding: gzip, deflate Connection: keep-alive Host: httpbin.org User-Agent: HTTPie/2.0.0 custom-header: bar Cache Based on Selected Request Headers の項目では、リクエストヘッダをもとにキャッシュするかどうかを、次の 3 つの方法でしていします。\nNone (Improves Cacheing) オブジェクトをキャッシュする際に、リクエストヘッダは考慮されません。上であげた例の通り、特定のヘッダの値が異なっていても、どちらか (複数のパターンが有る場合はどれかひとつ) のオブジェクトをキャッシュします。\nデフォルトではこの方法が指定されています。\nWhitelist 明示的に指定したリクエストヘッダの値に基づいてオブジェクトをキャッシュします。たとえば、上の例で使用した custom-header というヘッダをホワイトリストで指定した場合、 custom-header: foo と custom-header: bar は別々にキャッシュされます。\nBehavior の設定画面ではこの方法を指定するとテキストエリアが表示されるので、キャッシュ対象としたいリクエストヘッダ名を改行で区切って指定します。\nCloudFront はデフォルトでいくつかのリクエストヘッダをオリジンに転送しますが、キャッシュ対象となるのはホワイトリストで指定したヘッダのみです。また、ホワイトリストに指定できるヘッダの上限は、 10 となっています。この数を超えて指定したい場合は、クォータの引き上げリクエストをする必要があります。\nクォータ - Amazon CloudFront | カスタムヘッダーのクォータ (ウェブディストリビューションのみ) All すべてのリクエストヘッダをオリジンに転送します。その代わり、この設定をした場合 CloudFront は、 この Behavior で対象となるオブジェクトをキャッシュしない ようになります。\nリクエストヘッダーに基づくコンテンツのキャッシュ - Amazon CloudFront Object Caching CloudFront がオリジンのオブジェクトをキャッシュし、再度オリジンにリクエストを転送するまでの時間 (キャッシュに保持する時間) を指定します。ここで指定した時間、キャッシュは CloudFront のエッジロケーションに保持され、オブジェクトはそこからビューアへ返却されます。\nただし頻繁にアクセスが無いオブジェクトに関しては、キャッシュ保持時間に達する前にエッジロケーションから削除される場合もあります。\nデフォルトでは 24 時間 (86400 秒) 後にキャッシュの有効期限が切れるようになっていますが、次の 2 つの方法でキャッシュ時間を制御します。\nUse Origin Cache Headers オリジンのオブジェクトに指定されている Cache-Control: max-age または Cache-Control: s-maxage または Expires ヘッダの値をもとにキャッシュ時間を制御します。これらのヘッダがオリジンのオブジェクトに存在しない場合は、デフォルトの 24 時間 が適用されます。\n各ヘッダの指定方法の例は下記のようになります。\nCache-Control: max-age=3600 Cache-Control: s-maxage=3600 Expires: Thu, 30 Apr 2020 23:59:59 GMT CloudFront では Cache-Control: max-age と Cache-Control: s-maxage を併用することができます。また、 Cache-Control: max-age と Expires を両方指定した場合、 CloudFront は Cache-Control: max-age の値を使用します。 挙動については後述します。\nコンテンツがエッジキャッシュに保持される期間の管理 (有効期限) - Amazon CloudFront | ヘッダーを使用した個々のオブジェクトのキャッシュ保持期間の制御 Customize デフォルトでは 24 時間となっているキャッシュ時間を、 最小 (Minimum TTL) 、 最大 (Maximum TTL) 、 デフォルト (Default TTL) の 3 つの値で指定します。各値が適用される条件については後述します。\nキャッシュの保持時間の挙動 実際に CloudFront のエッジロケーションでキャッシュが保持される時間、およびブラウザでのキャッシュ保持時間は、オリジンのオブジェクトのヘッダ情報と、Minimum/Maximum/Default および CloudFront のデフォルト (24h = 86400sec) の条件によって変わります。具体的には下記の条件の組み合わせになります。\nオリジン Cache-Control: max-age が存在する Cache-Control: max-age が存在しない Cache-Control: max-age および Cache-Control: s-maxage が存在する Expires が存在する Cache-Control: no-cache、 no-store 、 private のいずれかが存在する CloludFront Minimum TTL = 0 秒 Minimum TTL \u0026gt; 0 秒 ※表内では表記の都合上、 Minimum TTL、 Maximum TTL をそれぞれ MIN と MAX と書いてます\nオリジン Minimum TTL = 0 Minimum TTL \u0026gt; 0 Cache-Control: max-age あり CloudFront max-age と Maximum TTL の小さい方の値 ブラウザ max-age の値 CloudFront ・MIN \u0026lt; max-age \u0026lt; MAX の場合 max-age の値 ・ max-age \u0026lt; MIN の場合 Minimum TTL の値 ・max-age \u0026gt; MAX の場合 Maximum TTL の値 ブラウザ max-age の値 Cache-Control: max-age なし CloudFront Default TTL の値 ブラウザ ブラウザによる CloudFront Default TTL または Min TTL のうち大きい方の値 ブラウザ ブラウザによる Cache-Control: max-age および Cache-Control: s-maxage あり CloudFront s-maxage と Maximum TTL の小さい方の値 ブラウザ max-age の値 CloudFront ・MIN \u0026lt; s-maxage \u0026lt; MAX の場合 s-maxage の値 ・ s-maxage \u0026lt; MIN の場合 Minimum TTL の値 ・s-maxage \u0026gt; MAX の場合 Maximum TTL の値 ブラウザ max-age の値 Expires あり CloudFront Expires の日付と Maximum TTL に対応する日付の早い方の日付まで ブラウザ Expires の日付まで CloudFront ・MIN \u0026lt; Expires \u0026lt; MAX の場合 Expires の日付まで ・ Expires \u0026lt; MIN の場合 Minimum TTL に対応する日付まで ・Expires \u0026gt; MAX の場合 Maximum TTL の値に対応する日付まで ブラウザ Expires の日付まで Cache-Control: no-cache、 no-store 、 private のいずれかあり CloudFront ヘッダを優先 ブラウザ ヘッダを優先 CloudFront Minimum TTL の値 ブラウザ ヘッダを優先 より詳細な表については下記公式ドキュメントを参照してください。\nコンテンツがエッジキャッシュに保持される期間の管理 (有効期限) - Amazon CloudFront | ウェブディストリビューションで CloudFront がキャッシュにオブジェクトを保持する期間の指定 Forward Cookies CLoudFront では、デフォルトでキャッシュに Cookie が考慮されません。この項目ではオリジンに Cookie を転送し、 Cookie の値によってキャッシュをするかどうかを次の 3 つの方法で指定します。\nNone (Improves Caching) Cookie をオリジンに転送せず、キャッシュに考慮しません。また、リクエストヘッダから Cookie ヘッダを削除し、レスポンスヘッダから Set-Cookie ヘッダを削除します。\nWeb Distribution ではオリジンによって Cookie を処理しないパターンがあり、その場合はこの方法を指定するべきです。今回の前提としている S3 は Cookie を処理しないため、この項目では None を指定します。\nS3 の他、 Cookie を処理しないオリジンの場合に None 以外を指定するとパフォーマンスの低下に繋がります。\nWhitelist ホワイトリストで指定した Cookie のみオリジンに転送し、その値をもとにキャッシュを保持します。指定した Cookie 以外はオリジンへの転送時に削除されます。一方、指定していない Cookie がオリジンからのレスポンスに含まれていた場合は、その値も含めてビューアに返却します。\n指定方法はリクエストヘッダの場合と同様で、改行区切りで指定します。\nAll すべての Cookie をオリジンに転送し、その値をもとにキャッシュを保持します。キャッシュに考慮されるのはリクエスト時に含まれていた Cookie のみで、リクエストには含まれずオリジンからのレスポンスには含まれるような Cookie はキャッシュに考慮されません。\nビューアへのレスポンスには、リクエストとオリジンからのレスポンスの両方の Cookie がすべて返却されます。\nCookie に基づくコンテンツのキャッシュ - Amazon CloudFront Query String Forwarding and Caching リクエスト URL に含まれる ? 以降の文字列で指定されたクエリパラメータをもとにキャッシュを保持するかを指定します。指定方法は次の 3 です。\nNone (Improves Caching) クエリパラメータをオリジンに転送せず、キャッシュに考慮しません。つまり、下記のリクエストではすべてキャッシュされるのはどれか一つのみとなります。\n- https://michimani.net/js/bundle.js - https://michimani.net/js/bundle.js?query_param=value1 - https://michimani.net/js/bundle.js?query_param=value2 Foward all, cache based on whitelist ホワイトリストで指定したパラメータのみオリジンに転送し、その値をもとにキャッシュを保持します。例えば上の例で使用した query_param というパラメータをホワイトリストで指定した場合、上の 3 つは別々にキャッシュされます。一方で、指定していないパラメータはキャッシュに考慮されないため、下記のリクエストはすべて同一とみなされます。\n- https://michimani.net/js/bundle.js - https://michimani.net/js/bundle.js?query_param_2=value1 - https://michimani.net/js/bundle.js?query_param_2=value2 下記も同一となります。\n- https://michimani.net/js/bundle.js?query_param=value1\u0026amp;query_param_2=value2_1 - https://michimani.net/js/bundle.js?query_param=value1\u0026amp;query_param_2=value2_2 Foward all, cache based on all すべてのクエリパラメータをオリジンに転送し、その値をキャッシュに考慮します。\nクエリパラメータによるキャッシュの注意点 クエリパラメータによるキャッシュを利用する場合に注意したいのが、 パラメータの順序 と パラメータの値の大文字/小文字 です。これらに注意しないと、オリジンへの不要なリクエストが発生し、パフォーマンスが低下する可能性があります。\nパラメータの順序 例えば下記の 2 つのリクエストがあるとします。\n- https://michimani.net/images/2020040220492101.jpg?width=1200\u0026amp;height=900\u0026amp;color=brack - https://michimani.net/images/2020040220492101.jpg?height=900\u0026amp;color=black\u0026amp;width=1200 オリジンからすればどちらも width: 1200, height: 900, color: black のパラメータですが、CloudFront ではこれらは別々のキャッシュとして保持されます。\nパラメータの値の大文字/小文字 パラメータの順序と同様に、パラメータ名および値の大文字/小文字について注意が必要です。\n- https://michimani.net/images/2020040220492101.jpg?color=brack - https://michimani.net/images/2020040220492101.jpg?Color=brack - https://michimani.net/images/2020040220492101.jpg?color=Brack - https://michimani.net/images/2020040220492101.jpg?Color=Brack もしオリジン側では大文字/小文字を区別していなかったとしても、 CloudFront では上の 4 つはすべて別々のキャッシュとして保存されます。\nクエリ文字列パラメータに基づくコンテンツのキャッシュ - Amazon CloudFront まとめ Amazon CloudFront において、 Web Distribution で S3 をオリジンに指定した際のキャッシュの仕様についてあらためて調べてみた話でした。\nこれまでなんとなく指定していた項目 (特にキャッシュ保持時間) もあって、やっぱり公式ドキュメントをしっかり読むのは大事だなと思いました。逆に言えば、 AWS は公式ドキュメントが非常に充実していて読みやすいということが言えると思います。\n今回これだけの内容をまとめるのに結構時間がかかりましたが、まだまだ触れられていない内容もたくさんあります。もう疲れたのでこれ以上は書きませんが、今後も公式ドキュメントをしっかり読む癖は続けていきたいと思います。\n",
    "permalink": "https://michimani.net/post/aws-about-cache-spec-of-cloudfront/",
    "title": "Amazon CloudFront のキャッシュ仕様についてあらためて調べてみた"
  },
  {
    "contents": "実装した API をコマンドラインから試す際に cURL を使用している方は多いと思います。 ただ、オプションが複雑だったりレスポンスが見づらかったり、不便さを感じている方もいると思います。まあ、僕なんですが。今回紹介する HTTPie は、直感的でわかりやすいコマンドでコマンドラインから http リクエストを送信できるツールです。\n目次 HTTPie とは 実際に使ってみる インストール GET リクエスト GET 以外のリクエスト パラメータ付きリクエスト ヘッダ情報の付与 https:// の省略 出力オプション その他のオプション まとめ HTTPie とは cURL は HTTP 以外にも FTP や TELNET, LDAP など、様々なプロトコルに対応しているツールです。一方、今回紹介する HTTPie (読み方は エイチティーティーパイ) は、 コマンドラインでの HTTP および HTTPS 通信に特化した クライアントです。\n最近ではサーバレスアーキテクチャで API を実装して、ちょっと動きを確認する みたいなことも増えてきていると思います。 HTTPie を使用することで、 API の動作確認が非常に直感的なコマンドで実施することができ、またレスポンスについても確認しやすい形で取得することができます。\nHTTPie – command line HTTP client ソースコードは GitHub で公開されています。\njakubroztocil/httpie: As easy as HTTPie /aitch-tee-tee-pie/ 🥧 Modern command line HTTP client – user-friendly curl alternative with intuitive UI, JSON support, syntax highlighting, wget-like downloads, extensions, etc. https://twitter.com/clihttp 実際に使ってみる では、実際に HTTPie を使ってみます。\nインストール HTTPie は macOS、 Linux、Windows およびその他の OS で使用することができます。\nmacOS の場合 macOS では Homebrew でインストール可能で、この方法が推奨されています。\n$ brew install httpie Linux の場合 Linux の場合は、各 OS ごとのパッケージマネージャを使用してインストールすることができます。例えば、 Debian または Ubuntu の場合は下記のコマンドでインストールできます。\n$ apt-get install httpie Windows およびその他の OS の場合 Windows およびその他の OS (macOS、Linux を含む) でのインストール、または安定していない最新版をインストールする場合は、 pip でインストールします。この際、 Python のバージョンは 3.6 以上である必要があります。\n$ pip install --upgrade pip setuptools $ pip install --upgrade httpie 安定していない最新版をインストールする場合は、下記のようにします。\n$ pip install --upgrade https://github.com/jakubroztocil/httpie/archive/master.tar.gz インストール方法の詳細については下記の公式ドキュメントを参照してください。\nHTTPie 2.0.0 (latest) documentation | Installation それぞれの方法でインストールができたら、バージョンを確認しておきます。\n$ http --version 1.0.3 HTTPie では http コマンドで諸々の操作を行います。\nでは、次からは実際に http コマンド使用方法やレスポンスについて見ていきます。\nなお、このブログ内ではアクセス先に https://httpbin.org のエンドポイントを利用します。これは HTTPie 公式のサンドボックスのようなものです。 HTTPie ではブラウザ上で動作を確認できるツールも公開しているので、インストールせずに試したい方は下記のツールを利用してみてください。\nHTTPie demo GET リクエスト ではまずは GET でのリクエストを試してみます。\n$ http https://httpbin.org/get これだけです。簡単ですね。\nちなみに、実行結果は下記のようになります。特にオプションを付けたりしなくても、カラーリング、レスポンスが JSON の場合は整形もされます。\nGET 以外のリクエスト GET 以外のリクエストをする場合は、 http コマンドに続いて使用したいメソッドを指定します。例えば POST であれば下記のようになります。\n$ http POST https://httpbin.org/post 同じ要領で、 PUT , PATCH , DELETE を指定することで、それぞれのメソッドでのリクエストを行うことができます。もちろん GET を指定することも可能です。\nパラメータ付きリクエスト 続いてはパラメータを渡す場合のコマンドです。\nクエリパラメータ クエリパラメータとしてパラメータを渡す場合は、下記のようにします。\n$ http https://httpbin.org/get qp1==1 qp2==2 パラメータ名と値を == で繋いで記述します。上記のコマンドは下記のコマンドと同じ意味になります。\n$ http \u0026#39;https://httpbin.org/get?qp1=1\u0026amp;qp2=2\u0026#39; データフィールド POST 時など body にパラメータを含めたい場合は、下記のようにします。\n$ http POST https://httpbin.org/post data1=value1 data2=value2 フィールド名と値を = で繋いで記述します。この場合、下記のようなデータを送信したことになります。\n{ \u0026#34;data1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;data2\u0026#34;: \u0026#34;value2\u0026#34; } 単純な文字列であれば上記の方法で問題ないですが、 文字列 (String) ではなく 数値 (Number) として渡したい場合や 真偽値 (Boolean) または、 オブジェクト (Object) や 配列 (Array) を値として指定したい画面があると思います。例えば、下記のようなデータを送信したい場合を考えます。\n{ \u0026#34;data1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;data2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;data3\u0026#34;: 3, \u0026#34;data4\u0026#34;: { \u0026#34;data4_1\u0026#34;: \u0026#34;value4_1\u0026#34;, \u0026#34;data4_2\u0026#34;: 42 }, \u0026#34;data5\u0026#34;: [ \u0026#34;data5_1\u0026#34;, \u0026#34;data5_2\u0026#34;, \u0026#34;data5_3\u0026#34; ] } この場合は、 = の代わりに := を使用して次のような形で実行します。 (適宜改行しています)\n$ http POST https://httpbin.org/post \\ data1=value1 \\ data2=value2 \\ data3:=3 \\ data4:=\u0026#39;{\u0026#34;data4_1\u0026#34;: \u0026#34;value4_1\u0026#34;,\u0026#34;data4_2\u0026#34;: 42}\u0026#39; \\ data5:=\u0026#39;[\u0026#34;data5_1\u0026#34;,\u0026#34;data5_2\u0026#34;,\u0026#34;data5_3\u0026#34;]\u0026#39; また、すでに上記のようなデータを JSON ファイル post_data.json として保存してる場合、そのファイルを指定することも可能です。\n$ http POST https://httpbin.org/post @post_data.json データ全体ではなく、フィールド名を指定して、値にファイルを指定することも可能です。\n$ http POST https://httpbin.org/post data1:=@post_data_1.json ヘッダ情報の付与 リクエストヘッダの付与は下記のような形で実行します。\n$ http https://httpbin.org/get any-header:value-for-header パラメータの指定と同じような形ですが、ヘッダ情報を指定する場合は : でヘッダ名と値を繋いで記述します。もちろん、パラメータの指定と一緒に使用することも可能です。\n$ http POST https://httpbin.org/post data1=value1 data2=value2 any-header:value-for-header AWS の API Gateway で API キーを使用した API を実行する場合は X-Api-Key ヘッダで API キーを指定するので、下記のようになります。\n$ http https://httpbin.org/get x-api-key:api-key-value https:// の省略 これまでアクセス先の URL は https:// を付けていましたが、下記のようにドメイン名からの指定も可能です。\n$ http httpbin.org/get プロトコル部分の省略は cURL でも可能ですが、 HTTPie では localhost へのアクセスを下記のように短縮することができます。\n$ http : 例えば localhost:1313/get にアクセスする場合は次のようになります。\n$ http :1313/get 出力オプション デフォルトではレスポンスヘッダとレスポンスボディが出力されますが、下記のオプションを指定することで出力内容を変更することができます。\nオプション 出力内容 --headers, -h レスポンスヘッダのみ --body , b レスポンスボディのみ --verbose, -v リクエストとレスポンスの情報両方 --print , -p 指定した情報のみ (下記参照) -v を指定することで、レスポンスだけでなくリクエストの情報も合わせて出力されるので、 API のデバッグ時には役に立ちそうです。\n--print または -p オプションを指定する場合は、合わせて下記の値を指定することで、必要な情報を明示的に指定して出力することができます。\n指定する値 出力内容 H リクエストヘッダ B リクエストボディ h レスポンスヘッダ b レスポンスボディ 例えば、リクエストヘッダとレスポンスヘッダのみ出力したい場合は、下記のようにします。\n$ http -p Hh https://httpbin.org/get その他のオプション 上記の内容で基本的な HTTP アクセスは可能かと思いますが、他にも HTTPie には下記のような様々なオプションが用意されています。\nAuthentication Basic 認証 Digest 認証 パスワードプロンプト プロキシ カスタム SSL 証明書 セッションの利用 詳細については公式ドキュメント及び --help オプションの出力を参照してください。\nHTTPie 2.0.0 (latest) documentation まとめ 直感的で使いやすい http クライアント HTTPie の紹介でした。\ncURL と比べて、メソッドやパラメータの指定がシンプルでわかりやすく、レスポンスの出力もカラーリング・整形されていて見やすいという、コマンドラインでの http 通信がかなりやりやすくなるツールです。特に API のテスト時には、 localhost 用の短縮コマンドなどもあり、非常に使いやすいツールです。\n単純な http 通信試したいけど cURL のオプションとかよくわからん という方は使ってみてはどうでしょうか。\n",
    "permalink": "https://michimani.net/post/development-about-httpie-cli-of-http-client/",
    "title": "CLI で http リクエストするなら HTTPie が便利"
  },
  {
    "contents": "このブログは静的サイトジェネレータの Hugo を使ってビルドしています。静的サイトの魅力といえば、表示の速さです。また、CloudFront などの CDN でコンテンツをキャッシュすることで、より表示スピードを上げることができます。今回は静的サイトにおけるキャッシュ戦略について、実際にこのブログで運用している方法を例に紹介したいと思います。\n目次 前提 キャッシュの種類 コンテンツ・データの種類 静的サイトのキャッシュ戦略 CDN (CloudFront) のキャッシュ戦略 ブラウザのローカルキャッシュ戦略 まとめ 前提 このブログは下記のサービスを用いて AWS 環境上で運用しています。\nRoute 53, CloudFront (ネットワーク) S3 (コンテンツデータ) CodePipeline, CodeBuild (自動デプロイ) デプロイの詳細については下記の記事を参照してください。\nキャッシュの種類 静的サイトだけでなく Web サイトに対するキャッシュとしては、大きく分けて下記の二種類のキャッシュがあります。\nサーバサイド クライアントサイド サーバサイドキャッシュには、 CDN のキャッシュの他、データベースキャッシュなどが該当します。クライアントサイドのキャッシュとしては、 cookie、ブラウザのローカルキャッシュなどが該当します。\n今回は上記の大きな分類それぞれから CDN のキャッシュ と ブラウザのローカルキャッシュ についての戦略を考えてみます。\nコンテンツ・データの種類 キャッシュするコンテンツ・データにも下記のような種類があります。\nhtml css, js 画像データ それぞれキャッシュが残っていることで、古い情報が表示され続けていたり、表示が崩れていたり、ボタンなどの動作が正しくなかったりといったことが起こります。場合によってはコンテンツ・データの種類によってキャッシュ戦略を変える必要も出てきます。\n静的サイトのキャッシュ戦略 静的サイトでは、その名の通りデプロイ時以外にコンテンツ情報が更新されることはないため、できる限りキャッシュの恩恵を受けることが望ましいです。そのため、基本戦略として CDN 、 ブラウザともにキャッシュ時間は長くしておくのがよいでしょう。\nただし注意したいのは、 CDN のキャッシュは管理者 (サイト運営者) 側で削除することができますが、ブラウザのローカルキャッシュは削除できないということです。ブラウザのローカルキャッシュはサイトを閲覧しているユーザの端末に保存されるキャッシュなので、ブラウザのスーパーリロードを実施してもらったり、その他の方法でキャッシュを削除してもらうようユーザに依頼する必要があります。\nこれらを踏まえて、実際にこのブログで運用しているキャッシュ戦略について紹介します。\nCDN (CloudFront) のキャッシュ戦略 CloudFront では、 Behavior はデフォルト (*) のみで、キャッシュ時間は下記のように設定しています。\nMinimun TTL: 86400 (1 日) Maximum TTL: 2592000 (30 日) Default TTL: 1209600 (14 日) 各キャッシュ時間の適用については下記の公式ドキュメントを参照してください。簡単に書いておくと、オリジンの Cache-Control 、 Expires ヘッダ の値との関係で、 Minimum, Maximum, Default の値がキャッシュ時間として適用されます。\nコンテンツがエッジキャッシュに保持される期間の管理 (有効期限) - Amazon CloudFront また、 CloudFront では クエリ文字列も含めてキャッシュするかを設定することができます。このブログでは c というクエリパラメータのみキャッシュ対象とするように設定しています。具体的には Query String Forwarding and Caching の項目で Forward all, cache based on whitelist を選択して、 Query String Whitelist に c を入力しています。\nこうすることで、下記のコンテンツはそれぞれ別々にキャッシュされることになります。\nhttps://michimani.net/js/bundle.js https://michimani.net/js/bundle.js?c=12345 https://michimani.net/js/bundle.js?c=abcde 一方で、他のクエリ文字列では同一のコンテンツとして扱われます。\nhttps://michimani.net/js/bundle.js?other_query=12345 https://michimani.net/js/bundle.js?other_query=abcde また、静的サイトということもありリクエスト時のヘッダ情報や Cookie によるリクエストの違いでキャッシュする必要がないので、リクエストヘッダでのキャッシュ (Cache Based on Selected Request Headers) と Cookie でのキャッシュ (Forward Cookies) は None にしています。\nブラウザのローカルキャッシュ戦略 ブラウザのローカルキャッシュ戦略として、基本的には キャッシュさせない ようにしています。ブラウザのローカルキャッシュを保存させないようにするためには、レスポンスヘッダに Cache-Control でキャッシュの動作を指定する必要があります。\nキャッシュさせたくない場合は Cache-Control: no-store を指定し、時間を指定してキャッシュさせる場合は Cache-Control: public, max-age=31536000 のように指定します。\nCache-Control - HTTP | MDN このブログでは、 html と その他のアセット (画像、css、js) でそれぞれ下記のように設定しています。\nhtml: Cache-Control: no-store その他: Cache-Control: public, max-age=1209600 オリジンに対して max-age を指定する場合、 前述した CloudFront の Minimum, Maximum, Default の値との関係に注意する必要があります。詳しくは下記の公式ドキュメントを参照してください。\nCloudFront キャッシュ時間に関する問題のトラブルシューティング データの種類でキャッシュ方法を分ける理由 html とその他アセットでキャッシュ方法を分けているのは、下記の理由からです。\nhtml (ブログ記事本文) の更新はすぐに反映されてほしい その他アセットは頻繁に更新されることがない その他アセットのデータサイズが大きくなりがち html (ブログ記事本文) については、誤字などがあった際にすぐに変更が反映されてほしいので、ブラウザのローカルキャッシュはしないようにしています。\n一方でその他のアセット類については、ブログ記事本文ほど頻繁に更新が発生するものでもないこと、また、特に画像ファイルなどはサイズも大きくなりがりなので、表示の高速化のためにもブラウザのローカルキャッシュをするようにしています。\nただしこれらアセット類に更新があった際には、サイトのスタイルが崩れたり動作が正しくなかったりと、閲覧に支障をきたします。その対策として、各アセットデータには先述したクエリパラメータを付与しています。アセット類を更新した際にクエリパラメータの値も一緒に更新することで、ブラウザは更新後のデータを取得するようになり、新しいデータをローカルキャッシュに保存するようになります。\nオリジンの Cache-Control ヘッダ設定 オリジンの Cache-Control について書きましたが、 S3 バケットをオリジンとして指定している場合はどのように設定すればよいのでしょうか。\nこのブログでは Hugo のビルドと S3 バケットのデプロイを CodeBuild 内で完結しています。実際には下記のような buildspec.yml を使用しています。(一部マスクしています)\nversion: 0.2 phases: install: commands: - curl -Ls https://github.com/gohugoio/hugo/releases/download/v0.68.3/hugo_0.68.3_Linux-64bit.tar.gz -o /tmp/hugo.tar.gz - tar xf /tmp/hugo.tar.gz -C /tmp - mv /tmp/hugo /usr/bin/hugo - rm -rf /tmp/hugo* build: commands: - hugo post_build: commands: - aws s3 sync \u0026#34;public/\u0026#34; \u0026#34;s3://\u0026lt;target-bucket-name\u0026gt;\u0026#34; --delete --metadata-directive \u0026#34;REPLACE\u0026#34; --cache-control \u0026#34;public, max-age=1209600\u0026#34; --exclude \u0026#34;index.html\u0026#34; --exclude \u0026#34;post/*\u0026#34; --exclude \u0026#34;tags/*\u0026#34; --exclude \u0026#34;archives/*\u0026#34; --exclude \u0026#34;categories/*\u0026#34; --exclude \u0026#34;about/*\u0026#34; - aws s3 sync \u0026#34;public/\u0026#34; \u0026#34;s3://\u0026lt;target-bucket-name\u0026gt;\u0026#34; --delete --metadata-directive \u0026#34;REPLACE\u0026#34; --cache-control \u0026#34;no-store\u0026#34; --exclude \u0026#34;*\u0026#34; --include \u0026#34;index.html\u0026#34; --include \u0026#34;post/*\u0026#34; --include \u0026#34;tags/*\u0026#34; --include \u0026#34;archives/*\u0026#34; --include \u0026#34;categories/*\u0026#34; --include \u0026#34;about/*\u0026#34; - aws cloudfront create-invalidation --distribution-id YOUR-DISTRIBUTION-ID --paths \u0026#34;/*\u0026#34; post_build 内で 3 つのコマンドを実行しています。最初の 2 つは S3 バケットへのデプロイで、 3 つ目は CloudFront の Invalidation を作成しています。\nS3 バケット内のオブジェクトに対して Cache-Control を設定するには s3 sync コマンドの --metadata-directive オプションを使用します。 Cache-Control だけでなく、その他のレスポンスヘッダを付与したい場合にも使用します。\n--exclude および --include オプションを使用することで、 S3 バケットへのデプロイを html とその他アセット類を別々にデプロイし、それぞれ先述した Cache-Control を設定するようにしています。\nまとめ 静的サイトのおけるキャッシュ戦略について、 Hugo + AWS で運用しているこのブログを例にして書いてみました。\n静的サイトだとシンプルですが、動的サイトになるとデータベースのキャッシュなども考える必要があるので、あらためてキャッシュって怖いなというのが率直な感想です。\nもし Hugo やその他の静的サイトでキャッシュどうしようか考えている方がいれば参考にしてみてください。それおかしいやろ等のご意見もいただけると幸いです。\n",
    "permalink": "https://michimani.net/post/development-cache-strategies-for-hugo/",
    "title": "[Hugo] 静的サイトのキャッシュ戦略について"
  },
  {
    "contents": "AWS Resource Groups って知ってますか？私は今朝知りました。調べてみると、どうやらマネジメントコンソール上での AWS リソースの管理を快適にできるサービスのようなので、実際に試してみました。\n目次 AWS Resource Groups とは やってみる 前提 リソースグループを作成する リソースグループへのアクセス まとめ AWS Resource Groups とは AWS リソースを整理するには、リソースグループを使用します。リソースグループを使用すると、多数のリソース上のタスクを一度に管理および自動化しやすくなります。\nAWS リソースグループ とは? - AWS リソースグループ 通常 マネジメントコンソール上で各リソースを操作 (作成・編集・閲覧) する場合、各サービスのコンソールに移動する必要があります。各サービスのダッシュボードで操作したいリソースを探して操作します。AWS Resource Groups を使用すると、 各リソースに付与されたタグによる タグベース でのグループ化、または AWS CloudFormation スタックベース でのグループ化ができます。その結果、一つの画面でグループ化されたリソースを管理できるというわけです。\nやってみる では実際にやってみます。\nマネジメントコンソールにアクセスすると、次のような画面になっていると思います。通常はここから各サービスのダッシュボードの移動して操作をします。\n前提 今回は例として タグベース でのグループ化を試してみます。そのため、予めいくつかのリソースに同じタグで管理されていることを前提とします。\n今回であれば、 Product というタグ名で michimani.net という値を持っているリソースでリソースグループを作成してみます。\nリソースグループを作成する まずはリソースグループを作成します。\nマネジメントコンソールの上部にある リソースグループ をクリックするとメニューが出てくるので、 グループを作成します をクリックします。\nすると、次のようなフォームが表示されるので、各項目を埋めていきます。\nGroup type リソースグループのタイプです。タグベース と CloudFkrmation スタックベース を選択できます。今回は タグベースを選択します。\nグループ分けの条件 リソースグループの条件を設定します。\nリソースタイプ リソースグループとして管理するリソースを選択します。特定のサービスのリソースのみを管理したい場合は、必要なリソースタイプを個別に指定します。特にこだわりなくすべてのサービスのリソースを管理したい場合は サポートされるすべてのリソースタイプ を選択します。\nタグ グループ化するための対象となる タグキー と タグ値 を指定します。タグキーは必須ですが、タグ値はオプションとなっています。今回は タグキー: Product, タグ名: michimani.net を指定します。\nグループリソース 作成時点では何も表示されていません。\nグループの詳細 作成するリソースグループの名前と説明 (オプション) を入力します。\nグループタグ - オプション 作成するリソースグループ自体に付与するタグです。ここで指定したタグはリソースグループのメンバー (リソース) には適用されません。適用されたら相互にグループ化されてしまいますよね。\n各項目の入力が終わったら グループの作成 ボタンを押します。\nすると、次のような形で設定したリソースグループの条件に該当するリソースが一覧で表示されるようになります。\nリソースグループへのアクセス 既に作成されたリソースグループへアクセスするには、作成時と同様にマネジメントコンソールの上部からメニューを表示して、 グループを保存しました をクリックします。日本語だと微妙な文言になっていますが、英語だと Saved groups となっています。\nリソースグループの一覧が表示されるので、対象のグループ名のリンクをクリックすると、先程のリソース一覧画面が表示されます。\nまとめ マネジメントコンソール上でリソースの管理が快適になる AWS Resource Groups を使ってみた話でした。\nプロダクトや運用しているサービスごとに利用しているリソースを操作したくなることはよくあると思うので、いちいち AWS のサービスごとのダッシュボードを行き来するのが面倒だなと感じている方は試してみてはどうでしょうか。\n",
    "permalink": "https://michimani.net/post/aws-about-resource-groups/",
    "title": "AWS Resource Groups を使ってマネジメントコンソールでのリソース管理を快適にする"
  },
  {
    "contents": "CloudFront と S3 で公開する静的サイトに IP 制限をかけるために AWS WAF で WebACL を CloudFormation で作成しようとしたところ、 WAF の v1 と v2 で挙動が異なる部分がありました。がっつりハマって解決までに苦戦したので、今回はその話です。\n目次 概要 CloudFormation テンプレート作成 WAF v1 の場合 WAF v2 の場合 スタックの作成 WAF v1 の場合 WAF v2 の場合 WAF v2 で Scope: CLOUDFRONT を指定できるのは us-east-1 リージョンのみ S3 バケットは東京、 WAF WebACL はバージニア北部 をどうにかする WAF 用のテンプレートからエクスポートして、S3 \u0026#43; CloudFront 用のテンプレートでインポートすればよいのでは？ WAF 用のテンプレート内で SSM パラメータストアに登録すればよいのでは？ WebACL の ARN をなんとかして取得して、 S3 \u0026#43; CloudFront 用のテンプレート デプロイ時のパラメータとして渡せばよいのでは？ まとめ 概要 下図のような構成を構築する CloudFormation テンプレートを作成したいと思います。\nとてもシンプルでありがちな構成ですが、一点今回のポイントともなるのが、S3 バケットは東京 (ap-northeast-1) リージョンに作成するという点です。 IP 制限については、 WAF の IPSet で指定した IP のみアクセス可能なホワイトリスト形式での制限を考えます。\nこのアーキテクチャで、 WAF の WebACL を v1 と v2 のそれぞれで作成してみようとしたところ、 v1 と v2 で挙動が異なる部分がありました。その内容と、解決方法について書いていきます。\nCloudFormation テンプレート作成 では、上図の構成を構築する CloudFormation テンプレートを作成していきます。 CloudFront と S3 については WAF のバージョンがどちらでも大きく変わらないので、 WAF のリソースを作成する部分を見ていきます。S3 と CloudFront も含めたテンプレート全体については GitHub に置いているので、そちらを参照してください。\nmichimani/cfn-template-samples: Samples of CloudFormation template. WAF v1 の場合 # AWS WAF v1 WAFv1IPSet: Type: \u0026#34;AWS::WAF::IPSet\u0026#34; Properties: Name: \u0026#34;MyWAFv1IPSet\u0026#34; IPSetDescriptors: - Type: \u0026#34;IPV4\u0026#34; Value: 192.0.2.44/32 - Type: \u0026#34;IPV6\u0026#34; Value: 1111:0000:0000:0000:0000:0000:0000:0111/128 WAFv1Rule: Type: \u0026#34;AWS::WAF::Rule\u0026#34; Properties: Name: \u0026#34;MyWAFv1Rule\u0026#34; MetricName: \u0026#34;MyWAFv1RuleMetric\u0026#34; Predicates: - DataId: !Ref WAFv1IPSet Negated: false Type: \u0026#34;IPMatch\u0026#34; WAFv1WebACL: Type: \u0026#34;AWS::WAF::WebACL\u0026#34; Properties: Name: \u0026#34;MyWAFv1WebACL\u0026#34; MetricName: \u0026#34;MyWAFv1WebACLMetric\u0026#34; DefaultAction: Type: \u0026#34;BLOCK\u0026#34; Rules: - RuleId: !Ref WAFv1Rule Priority: 0 Action: Type: \u0026#34;ALLOW\u0026#34; WAF v1 では IPSet 、 Rule 、 WebACL の 3 つのリソースを定義する必要があります。IPv4 と IPv6 は同じリソース内で定義できます。\nWAF v2 の場合 # AWS WAF v2 WAFv2IPSet: Type: \u0026#34;AWS::WAFv2::IPSet\u0026#34; Properties: Addresses: - 192.0.2.44/32 IPAddressVersion: IPV4 Name: \u0026#34;MyWAFv2IPSet\u0026#34; Scope: \u0026#34;CLOUDFRONT\u0026#34; WAFv2IPSetV6: Type: \u0026#34;AWS::WAFv2::IPSet\u0026#34; Properties: Addresses: - 1111:0000:0000:0000:0000:0000:0000:0111/128 IPAddressVersion: IPV6 Name: \u0026#34;MyWAFv2IPSetV6\u0026#34; Scope: \u0026#34;CLOUDFRONT\u0026#34; WAFv2WebACL: Type: \u0026#34;AWS::WAFv2::WebACL\u0026#34; Properties: DefaultAction: Block: {} Name: MyWAFv2WebACLI Rules: - Name: \u0026#34;MyWAFv2WebACLRuleIPSet\u0026#34; Action: Allow: {} Priority: 0 Statement: IPSetReferenceStatement: Arn: !GetAtt WAFv2IPSet.Arn VisibilityConfig: CloudWatchMetricsEnabled: true MetricName: \u0026#34;MyWAFv2WebACLRuleIPSetMetric\u0026#34; SampledRequestsEnabled: false - Name: \u0026#34;MyWAFv2WebACLRuleIPSetV6\u0026#34; Action: Allow: {} Priority: 1 Statement: IPSetReferenceStatement: Arn: !GetAtt WAFv2IPSetV6.Arn VisibilityConfig: CloudWatchMetricsEnabled: true MetricName: \u0026#34;MyWAFv2WebACLRuleIPSetV6Metric\u0026#34; SampledRequestsEnabled: false Scope: \u0026#34;CLOUDFRONT\u0026#34; VisibilityConfig: CloudWatchMetricsEnabled: true MetricName: \u0026#34;MyWAFWebACLMetrics\u0026#34; SampledRequestsEnabled: false 一方で v2 の場合は IPSet と WebACL の 2 種類 のリソースを定義します。ただし、 IPv4 と IPv6 は別のリソースとして定義する必要があります。\n特定の IP からのアクセスを許可するという同様の動作をさせる場合でも v1 と v2 では CloudFormation テンプレートの記述方法が大きく異なります。詳しくはそれぞれの公式ドキュメントを参照してください。\nWAF Resource Type Reference - AWS CloudFormation WAFv2 Resource Type Reference - AWS CloudFormation スタックの作成 冒頭に書いたとおり、 S3 バケットは東京リージョンに作成したいので、東京リージョンの CloudFormation でスタックを作成していきます。\nWAF v1 の場合 WAFv1Sample という名前でスタックを作成します。しばらくすると、無事にリソースが作成されました。\nWAF v2 の場合 WAFv2Sample という名前でスタックを作成します。しばらくすると、 IPSet の作成時に下記のエラーとなり、リソースの作成に失敗しています。\nError reason: The scope is not valid., field: SCOPE_VALUE, parameter: CLOUDFRONT (Service: Wafv2, Status Code: 400, Request ID: XXXXXXXXXXXXXXXXXXXXXX)\nWAF v2 で Scope: CLOUDFRONT を指定できるのは us-east-1 リージョンのみ 調べてみると、どうやら WAF v2 の IPSet や WebACL で必須となっている Scope パラメータに CLOUDFRONT を指定する場合、スタックを作成するリージョンは バージニア北部 (us-east-1) である必要があるようです。AWS の公式ドキュメント内にはこれに関する記述を見つける事ができなかったのですが、 AWS の Discussion Forums ではその話題に関するスレッドがありました。\nAs the scope has been set to \u0026ldquo;CLOUDFRONT\u0026rdquo;, you would need to deploy the stack using the \u0026ldquo;us-east-1\u0026rdquo; region. Otherwise, you will face the same error in any other region except the \u0026ldquo;us-east-1\u0026rdquo; region. I have tested the template you have attached, in the \u0026ldquo;us-east-1\u0026rdquo; region and it worked correctly.\nAWS Developer Forums: WAFv2 CLOUDFRONT scope in CloudFormation \u0026amp;hellip; ということで、おとなしくバージニア北部リージョンでスタックを作成したところ、無事にリソースが作成されました。\nS3 バケットは東京、 WAF WebACL はバージニア北部 をどうにかする では、 WAF v2 を利用する場合は今回の当初の目的であった S3 バケットは東京リージョンに作成する を諦めるしかないのでしょうか。\nWAF v2 の WebACL に関してはバージニア北部リージョンで作成する必要があり、また CloudFormation で作成されるリソースは CloudFormation のリージョンに依存することも考えると、同じテンプレート内で定義することはできません。なので、 WAF 用のテンプレートと、 S3 および CloudFront 用のテンプレートを分けて作成します。\nCloudFront を定義する際には WebACL の ARN が必要になるため、テンプレートを分けるとなるとその値をどこから参照するかが問題になります。そこでいくつか対応方法を考えたので、それらについて解説、というか調べた内容を書いておきます。\nWAF 用のテンプレートからエクスポートして、S3 + CloudFront 用のテンプレートでインポートすればよいのでは？ いわゆるクロススタック参照ですね。\nただ、残念ながら CloudFormation のクロススタック参照は別リージョンのスタックの出力値を参照することができないため、この方法は使えません。\nリージョンにわたるクロス スタックの参照を作成することはできません。Fn::ImportValue 組み込み関数は、同じリージョン内でエクスポートされた値のみインポートできます。\nチュートリアル: 別の AWS CloudFormation スタックのリソース出力を参照する - AWS CloudFormation WAF 用のテンプレート内で SSM パラメータストアに登録すればよいのでは？ では WAF 用テンプレート内で、SSM パラメータストアに WebACL の ARN を登録する方法はどうでしょうか。CLoudFormation には SSM パラメータストアを参照して値を取得することができるというのをどこかで見たので、いけそうな気がします。\nしかし、残念ながら SSM パラメータストアはリージョンごとにパラメータを持つ仕様で、さらに CloudFormation テンプレートで参照できるのは CloudFormation と同じリージョンのものだけです。なので、この方法も使えません。\nWebACL の ARN をなんとかして取得して、 S3 + CloudFront 用のテンプレート デプロイ時のパラメータとして渡せばよいのでは？ じゃあもうなんとかして ARN を取得して、デプロイ時のパラメータに渡すしかないという結論に至りました。結論から言うと、この方法に落ち着きました。\nただ、この なんとかして の部分をできるだけ簡単に、事故がないようにするために工夫してみました。\nWAF 用のテンプレートで ARN の値をエクスポートする まず、WAF 用のテンプレートで ARN の値をエクスポートするようにします。上で紹介した WAF v2 のテンプレートに下記の定義を追加します。\nOutputs: WAFWebACLArn: Value: !GetAtt WAFv2WebACL.Arn Export: Name: My-WAF-v2-WebACL-Arn これで、バージニア北部リージョンの CloudFormation のエクスポートリストから WebACL の ARN を取得できるようになりました。\nAWS CLI で ARN の文字列のみを取得する AWS CLI では、 CloudFormation のエクスポートリストを取得する cloudformation list-exports というコマンドがあります。このコマンドを使用すると、下記のようにスタックからエクスポートされた値のリストが取得できます。\n$ aws cloudformation list-exports --region us-east-1 { \u0026#34;Exports\u0026#34;: [ { \u0026#34;ExportingStackId\u0026#34;: \u0026#34;arn:aws:cloudformation:us-east-1:123456789012:stack/WAFv2Sample/2e4c7420-6856-11ea-b666-XXXXXXXXXXXX\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;My-WAF-v2-WebACL-Arn\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;arn:aws:wafv2:us-east-1:123456789012:global/webacl/MyWAFv2WebACLI/dca677c6-a3ae-4087-9397-XXXXXXXXXXXX\u0026#34; } ] } このレスポンスから、 WebACL の値だけ抜き出します。今回であれば対象のエクスポート値の名前は My-WAF-v2-WebACL-Arn なので、下記のコマンドで ARN のみ抜き出すことができます。\n$ aws cloudformation list-exports --region us-east-1 \\ | jq -r \u0026#39;.Exports[] | select (.Name==\u0026#34;My-WAF-v2-WebACL-Arn\u0026#34;) | .Value\u0026#39; arn:aws:wafv2:us-east-1:123456789012:global/webacl/MyWAFv2WebACLI/dca677c6-a3ae-4087-9397-XXXXXXXXXXXX この値を S3 + CloudFront 用のテンプレートでパラメータとして指定します。下記のような感じ。\nParameters: WAFv2WebACLARN: Description: \u0026#34;ARN string of WAF v2 WebACL\u0026#34; Type: \u0026#34;String\u0026#34; Resources: # S3 、 OriginAccessIdentity 部分は省略 # CloudFront CloudFrontDistribution: Type: \u0026#34;AWS::CloudFront::Distribution\u0026#34; Properties: DistributionConfig: Origins: - Id: \u0026#34;s3-for-waf-v2-origin\u0026#34; DomainName: !GetAtt \u0026#34;S3Bucket.DomainName\u0026#34; S3OriginConfig: OriginAccessIdentity: !Sub \u0026#34;origin-access-identity/cloudfront/${OriginAccessIdentiry}\u0026#34; Enabled: true DefaultCacheBehavior: TargetOriginId: \u0026#34;s3-for-waf-v2-origin\u0026#34; ViewerProtocolPolicy: \u0026#34;redirect-to-https\u0026#34; AllowedMethods: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;] ForwardedValues: QueryString: false WebACLId: !Ref WAFv2WebACLARN DefaultRootObject: \u0026#34;index.html\u0026#34; テンプレートのデプロイをコンソールではなく AWS CLI で実行する場合には、下記のようなコマンドを使用して操作を簡略化することもできます。\n$ aws cloudformation deploy \\ --stack-name WAFv2SampleS3AndCF \\ --template-file /path/to/s3_cf_template.yml \\ --parameter-overrides WAFv2WebACLARN=$(aws cloudformation list-exports --region us-east-1 | jq -r \u0026#39;.Exports[] | select (.Name==\u0026#34;My-WAF-v2-WebACL-Arn\u0026#34;) | .Value\u0026#39;) --region ap-northeast-1 まとめ WAF で IP 制限を実現する WebACL を v1 と v2 それぞれで作成したら、 v2 の挙動の違いにハマった話でした。\nv1 だと CloudFront 用のグローバルな WebACL を作成する場合にバージニア北部 (us-east-1) リージョンである必要がないのに、 v2 だとそこが厳しいのが辛いですね。何かしら理由があるのか、ただ単に対応されてないだけなのかはわかりませんが、ちょっと不便さを感じたのでできれば v1 と同じように Scope: CLOUDFRONT もバージニア北部リージョン以外の各リージョンで指定できるようにしてほしいなと思います。\n今回と同じような状況で別の解決方法があればぜひ教えていただきたいです。\n",
    "permalink": "https://michimani.net/post/aws-create-web-acl-at-waf-v1-v2/",
    "title": "AWS WAF v1 と v2 それぞれで WebACL を CloudFormation で作成したときにハマった話"
  },
  {
    "contents": "以前にフロントエンドの画面設計をする機会があったのですが、優れた UI/UX ってなんや？ という疑問が出てきました。これまでにも管理画面の機能追加やエンドユーザーが使用する画面の改修をした経験はありますが、 UI/UX について深く考えたことはありませんでした。そんなときに 「インタフェースデザインのお約束 - 優れた UX を実現するための 101 のルール」 という書籍を見つけたので、今回ご紹介させていただきます。\n目次 書籍の概要 著者・翻訳者 書籍の目次 各章の概要 1 章 プロローグ 2 章 文字と言葉 3 章 アイコンやボタン 4 章 UI 部品 5 章 フォーム 6 章 ナビゲーションとユーザージャーニー 7 章 ユーザーへの情報提示 8 章 アクセシビリティ 9 章 エピローグ まとめ 書籍の概要 インタフェースデザインのお約束 - 優れた UX を実現するための 101 のルール\nWeb アプリケーションだけでなく、デジタル製品のデザインに役立つ指針がまとめられています。その数は 101 項目ありますが、どれも基本的な内容で、優れた UX を実現するためにはぜひ抑えておきたいポイントとなっています。 101 のお約束は後述する 7 つの大項目 (+ エピローグ) でわけられており、興味のある部分だけを選んで読むこともできます。各お約束は 1 〜 2 ページで完結するように書かれており、それぞれの説明のあとに箇条書きでポイントをいくつか書かれているという構成です。なので、気になった項目に関してリファレンス的にも使うことが可能です。\n著者の Will Grant 氏はまえがきにて 「『これには賛成できない』と思えるルールもあるかもしれないが、それはそれでかまわない。」 としており、必ずすべてを取り込む必要はないことをコメントしています。\n著者・翻訳者 Will Grant 著 武舎広幸、武舎るみ 訳 (敬称略)\n書籍の目次 1 章 プロローグ 2 章 文字と言葉 3 章 アイコンやボタン 4 章 UI 部品 5 章 フォーム 6 章 ナビゲーションとユーザージャーニー 7 章 ユーザーへの情報提示 8 章 アクセシビリティ 9 章 エピローグ 各章の概要 それでは各章の概要と、どのような お約束 が書かれているのか、少しだけ紹介していきます。\n1 章 プロローグ この先の各章を読み進めていく前に、UX デザインにおいて大切なのは「共感性」と「客観性」であること、「学習と実践だけでは不十分であること」を強調しています。また、 UX デザインで重要なのは生まれ持った才能ではなく、適切なコツとルールを覚えることであることだと述べられています。\n2 章 文字と言葉 文字のフォント、大きさ、色、文章の言葉遣いについてのお約束が書かれています。文字色に関しては背景色との適切なコントラスト比について書かれていたり、用語の統一に関するお約束も書かれています。\n書体は最多でも 2 種類に 英語ページでは log in ではなく sign in を使え など\n3 章 アイコンやボタン アイコン画像の統一、ボタンの適切な大きさについて書かれています。たまにボタンとして使用されているアイコンやマークと実際の挙動が全然違うという場面に遭遇することがありますが、そのようなことが起こらないようなお約束も書かれています。\n古くさくなった機器のアイコンなど使うな 絵文字は世界公認のアイコンセット など\n4 章 UI 部品 ドロップダウンメニューやスライダー、リンクの形式といった UI 部品に関するお約束が書かれています。各お約束内では UI 部品のあるべき姿について、わかりやすく解説されています。\nデバイスにもともと備わっている入力方法を利用せよ 年月日の選択用のコントロールは？ など\n5 章 フォーム 検索、ユーザー情報、パスワード、画像のアップロードなど、フォームに関するお約束が書かれています。ユーザー情報に関する内容では、電話番号やクレジットカード情報など入力する内容ごとにポイントが紹介されています。\nユーザーの入力データの形式に関しては「太っ腹」で メールアドレスの細かな検証は不要 など\n6 章 ナビゲーションとユーザージャーニー スクロールの方法、ページ遷移、リフレッシュの方法などに関するお約束が書かれています。この章で紹介されているのは、ユーザーの実際の操作手順・体験 (ユーザージャーニー) をに重きをおいたお約束となっています。\n初期ページはユーザーへの説明の好機 どのジャーニーでも常に現在位置をユーザーに明示せよ など\n7 章 ユーザーへの情報提示 検索結果、設定内容、通知など、ユーザーに対するあらゆる情報の提示に関するお約束が書かれています。対象のユーザーが、利用するサービスの初心者なのかヘビーユーザーなのか、といったユーザー属性ごとの提示方法についても触れられています。\n隠れた部分もチラッと見せよ 「弊社のビジョン」に関心のあるユーザーなんていない など\n8 章 アクセシビリティ 本文の読み上げ、画面の拡大・縮小、色覚障害者に対する配慮などに関するお約束が書かれています。特に Tab キーでの操作やコントロールラベルに関するお約束は、あーなるほどな、と思いました。\nクリック可能なリンクのテキストは「読み上げ」機能に配慮して Tab キーでの移動の順序は支援技術の利用者を念頭に置いて など\n9 章 エピローグ エピローグでは、これまでの章には該当しない、でも実際に優れた UX を実現するためには考慮しなければならないような内容に関するお約束が書かれています。各章に該当しない内容とはいってもざっくりとした浅い内容になっているわけではなく、ここでも詳細な説明とポイントがわかりやすくまとめられています。\nデフォルト設定を過小評価するな ユーザーテストでは本物のユーザーを対象にせよ など\nまとめ 優れた UX を実現するための tips がまとめられた 「インタフェースデザインのお約束」を読んだので、その紹介でした。\n内容としてはこれまでにどこかで聞いたことがあったり、感覚的にそう思っていたりすることもありましたが、どれもふんわりとしたレベルでの認識でした。本書では、そういったなんとなくわかっている内容に関してもわかりやすく数値や図を用いて解説されているので、既に自分の中にある UX に対する考え方の地盤固めにも役立つと思います。\nただしまえがきで筆者がコメントしているように、本書で紹介されているお約束をすべて鵜呑みにして UX デザイン、画面設計をするのではなく、しっかりと対象のユーザー像をイメージすることが大事だと思いました。特にエピローグの章で紹介されている「ユーザーテストでは本物のユーザーを対象にせよ」というお約束がまさにその通りで、実際に使用するユーザーにテストをしてもらって、フィードバックをもらい、改善をしていくという流れが、優れた UX を実現するためには一番大事かなと思いました。\nこれから UX デザインをする方、 UX デザインについてなんとなくやるべきことはわかっている方、 そんな方たちにおすすめの一冊です。\n",
    "permalink": "https://michimani.net/post/bookreview-101-ux-principles/",
    "title": "[書評] 優れた UX を実現するための tips が詰まった「インタフェースデザインのお約束 - 優れた UX を実現するための 101 のルール」を読んでみた"
  },
  {
    "contents": "今月に入ってリモートワークが本格化し、これまで以上にセルフマネジメントが大事な状況になってきました。今回は、そのタイトルの通り 「ヤバい集中力」 を得るためのライフハックが詰まった書籍 「ヤバい集中力 - 1 日ブッ通しでアタマが冴えわたる神ライフハック 45」 を読んだので、そのレビューです。\n目次 書籍の概要 著者 書籍の目次 各章の概要と感想 序章 獣と調教師 〜ポテンシャルを 400 ％ 引き出すフレームワーク〜 第 1 章 餌を与える 〜脳の馬力を高めるサプリと食事法〜 第 2 章 報酬の予感 〜脳内ホルモンを操る目標設定の奥義〜 第 3 章 儀式を行う 〜毎回のルーティンで超集中モード〜 第 4 章 物語を編む 〜セルフイメージを書き換えて「やる」人間になる〜 第 5 章 自己を観る 〜マインドフルネスで静かな集中を取り戻す〜 第 6 章 諦めて、休む 〜疲労とストレスを癒すリセット法〜 おわりに まとめ 書籍の概要 書籍のタイトルの通り、「ヤバい集中力」を得るためのライフハックが詰まっています。\n「やらなければいけないタスクがあるのに、ついついスマホを手にとってゲームや Twitter を見てしまう」\n「期限が迫っているのに別のことばかりやってしまう」\nおそらくほとんどの人がこのような経験をしたことがあると思います。本書では、そもそも「集中力」とは何なのか、どのようなメカニズムで人間の心は動くのかを解説し、効率的に集中力を高める方法について、数々の論文や研究データに基づいて書かれています。\nそのメカニズムを知ることで、誰にでも眠っている「ヤバい集中力」を目覚めさせ、集中力を高いレベルでキープできるようになるわけです。凄そう。\n著者 ライター／編集者 鈴木裕 さん\nパレオな男 書籍の目次 本書の目次は下記のようになっています。\n序章 獣と調教師 〜ポテンシャルを 400 % 引き出すフレームワーク〜\n一般人の 4 倍の生産性を持つはいパフォーマーは何が違うのか？ 集中力の問題を一挙に解決するフレームワーク 「獣」は単純で過敏だが、超絶パワーを発揮する 「調教師」は論理的。大飯くらいの割に、力がショボい\u0026hellip;\u0026hellip; 集中力向上のための 3 つの教訓 第 1 章 餌を与える 〜脳の馬力を高めるサプリと食事法〜\nお手軽に覚醒作用を倍増させるカフェインの摂り方 食べるだけで脳機能が上がる魔法の食事法 脳を変えたいなら「食事日記」が最強のソリューションである 第 2 章 報酬の予感 〜脳内ホルモンを操る目標設定の奥義〜\n死んでしまうほど熱中する「ゲーム」の力をハックせよ あなたの仕事を「クソゲー」にしてしまう 2 つの要素 達成感がやみつきになるタスク管理法 第 3 章 儀式を行う 〜毎回のルーティンで超集中モード〜\n一見無駄な「マイ儀式」に隠された効果が次々に明らかに ドーパミンを出す儀式で一日中捗りまくり 「小さな不快」で獣を刺激する 儀式スタッキングで獣を導き良い習慣を連鎖させよう 儀式は週 4 回 2 ヶ月続けると完全に身につく 第 4 章 物語を編む 〜セルフイメージを書き換えて「やる」人間になる〜\n「なりたい自分になる」ためには物語が効く セルフイメージを上書きする 5 つの方法 第 5 章 自己を観る 〜マインドフルネスで静かな集中を取り戻す〜\n第ブームになった「意志力」、 2 つのアップデート 「自制」するためには「自省」が欠かせない 平静な自分を取り戻すデタッチド・マインドフルネス 第 6 章 諦めて、休む 〜疲労とストレスを癒すリセット法〜\n集中力は「あまのじゃく」 ネガティブな思考をカチッと切り替えるセルフ・アクセプタンス 疲労とストレスを科学的にリセットする方法 おわりに\n各章の概要と感想 ここからは各章の概要を少しだけご紹介いたします。\n序章 獣と調教師 〜ポテンシャルを 400 ％ 引き出すフレームワーク〜 本書を読み進めていくうえでの前提となるフレームワーク「獣と調教師」について書かれています。難しいものを嫌い、あらゆる刺激に反応する、パワーが強いという特徴をもつ獣。それに対して、論理性が武器、エネルギー消費が大きい、パワーが小さいという特徴をもつ調教師。自分の中にあるそれぞれのキャラクターをうまく扱うことが、ヤバい集中力を得るための筋道になるというわけです。\n第 1 章 餌を与える 〜脳の馬力を高めるサプリと食事法〜 「獣をうまく扱うにはまずは腹ごしらえが大事」 ということで、食事にフォーカスした集中力アップ方法について書かれています。何を、どれだけの量、どのような間隔で摂取するのが最も効果的である、といったような凄く具体的な数値を用いて紹介されています。途中で「脳に良い 10 のフードカテゴリー」として、各カテゴリに該当する具体的な食べ物、素性される摂取量を示した表も掲載されており、実際の行動にも繋げやすいように書かれています。\n第 2 章 報酬の予感 〜脳内ホルモンを操る目標設定の奥義〜 人がゲームに熱中してしまうのは程よいタイミングで報酬が継続的に与えられるからである、という話から始まり、それを仕事にも生かせないか？というのがこの章の内容です。仕事において、簡単過ぎるタスク、逆に難しすぎるタスクはその仕事を「クソゲー」にしてしまい、集中力が続きません。その問題点を「報酬感覚プランニング」という方法で解決していきます。\n第 3 章 儀式を行う 〜毎回のルーティンで超集中モード〜 集中する前に毎回何かしら同じことをやることで、やらない場合と比較して諸々の効果が良い方向に出たという研究結果をもとに、簡単なルーティンを使って獣をうまく扱うという話です。儀式というと大げさですが、例えば試験前に指を 10 回鳴らすとか、そういった簡単な内容でも効果があるようです。ただし、どうせルーティンとして組み込むのであれば、意味のある儀式のほうが効率的です。例えば、「一日の最初にその日にやるべきタスクをすべて書き出す」であったり、「タスクを完了したらすぐにチェックを入れる」といった儀式は現実にも役に立つ儀式として紹介されています。\n第 4 章 物語を編む 〜セルフイメージを書き換えて「やる」人間になる〜 パワーの小さい調教師ですが、その中でも最大の武器である「物語」を上手く扱おうという内容です。例えば、読書中に集中力が途切れてしまうような人は、そもそも自分が読書家であるということを自分の中で定義することが、物語の設定になります。そうすることで、仮に読書中に集中力が途切れそうになっても、読書家であるという自分が崩れてしまうのを恐れ、集中力をキープできるというイメージです。章の後半では、物語を自分にしっかりと定着させるための方法についても書かれています。\n第 5 章 自己を観る 〜マインドフルネスで静かな集中を取り戻す〜 自分の思想や感情から距離をおいてひたすら観察に徹し、獣の暴走から距離をおいて対応しようという内容です。そういった気持ちのコントロール方法として、メタファーを使用した方法、この場所ではこれしかやらないという \u0026ldquo;聖域\u0026rdquo; を用いた方法、調教師を切りな話す方法が紹介されています。聖域については物理的な場所だけでなく、仕事中に使用するスマホとプライベートのスマホを分けたりといったデジタル要素の管理についても触れられています。\n第 6 章 諦めて、休む 〜疲労とストレスを癒すリセット法〜 集中することを諦めて休むことの大切さについて書かれています。集中力を維持できない人は、「集中力を追い求めすぎてしまう」「集中力がない自分を責め過ぎてしまう」という特徴をもっていることが多いようです。特に自責の念はストレスの中でもたちが悪いもので、十分にパフォーマンスを発揮することができません。しっかりと休んで気持ちをリセットする方法として、米軍式快眠エクササイズと呼ばれる手法も紹介されています。\nおわりに 全 6 章にわたって様々な集中力アップ方法が紹介されていました、このまとめでは実際にどこから始めればよいのか、7 つのレベルに分類して紹介されています。\nまとめ ヤバい集中力を得るためのライフハックが詰まった書籍 「ヤバい集中力 - 1 日ブッ通しでアタマが冴えわたる神ライフハック 45」 のレビューでした。\n各章でさまざまなライフハックが紹介されており、全てを実現できれば文字通り「ヤバい集中力」を得られそうな気がします。ただ、最初からすべてを実行することは無理だと思うので、 おわりに の章で書かれているレベル別の方法に従って、まずは簡単な部分から取り入れていければと思います。\n一旦読んだあと、あらためて読むことでより集中力アップへの意識を高めることができるので、ヤバい集中力を手に入れたい方にはぜひ手元においておきたい一冊です。\n",
    "permalink": "https://michimani.net/post/bookreview-awesome-focus/",
    "title": "[書評] これであなたも調教師！「ヤバい集中力 - 1 日ブッ通しでアタマが冴えわたる神ライフハック 45」を読んでみた"
  },
  {
    "contents": "Serverless Framework を使用すると API Gateway の API キーも簡単に生成することができます。今回は、生成した API キーを デプロイ時に SSM のパラメータストアに登録するようにしてみた話です。\n目次 概要 前提 やること 1. sls info から API キーの値を取得する 2. 取得した API キーを SSM パラメータストアに登録する 最終的な buildspec.yml まとめ 概要 Serverless Framework で API Gateway の API キーを生成し、その値を AWS Systems Manager (SSM) のパラメータストアに登録します。 パラメータストアに登録しておけば、別のプロジェクトとして管理しているフロントエンドやその他のクライアントのコードに埋め込む際にも扱いやすくなります。\n前提 生成される API キーが各ステージに ひとつだけ であること\nserverless.yml では次のような設定になっていることを前提とします。\nprovider: name: aws runtime: python3.7 apiKeys: - ${self:provider.stage}-defaultApiKey usagePlan: quota: limit: 10000 period: DAY throttle: burstLimit: 500 rateLimit: 250 stage: ${opt:stage, self:custom.defaultStage} custom: defaultStage: dev CodeBuild でデプロイしている\nGitHub や CodeCommit へのイベントをトリガーとして、 CodeBuild デプロイすることを前提とします。 buildspec.yml は下記のようになっています。 $STAGE 変数にはデプロイ先のステージ (dev, stg など) が入る想定です。\nversion: 0.2 phases: install: commands: - npm install -g serverless - npm install build: commands: - sls deploy --stage $STAGE やること sls info から API キーの値を取得する 取得した API キーを SSM パラメータストアに登録する これらを実現するためのコマンドを buildspec.yml に追記する形で対応します。\n1. sls info から API キーの値を取得する Serverless Framework では sls info コマンドによって生成された API のエンドポイントや Lambda 関数の情報を取得することができ、その情報の中に API キーの値も含まれています。 sls info コマンドでは sls deploy と同様に --stage オプションで情報取得対象のステージを指定することができます。\n$ sls info --stage dev Service Information service: sls-sample stage: dev region: us-east-1 stack: sls-sample-dev resources: 14 api keys: dev-defaultApiKey: VaO3S4kmbvajSscuozbXSaKLO06YwzV79OxjaWgU endpoints: GET - https://XXXXXXXXXX.execute-api.us-east-1.amazonaws.com/dev/handler functions: hello: sls-sample-dev-hello layers: None sls info の出力結果から API キーを抜き出すために、下記の sed コマンドを使用します。\nsls info --stage dev | sed -n -r \u0026#39;s/^.*defaultApiKey: (.+)$/\\1/p\u0026#39; macOS に標準で搭載されている sed で確認する場合は -r オプションの代わりに -E オプションを使用して確認してください。\n$ sls info --stage dev | sed -n -E \u0026#39;s/^.*defaultApiKey: (.+)$/\\1/p\u0026#39; VaO3S4kmbvajSscuozbXSaKLO06YwzV79OxjaWgU 2. 取得した API キーを SSM パラメータストアに登録する SSM パラメータストアへ登録には AWS CLI の ssm put-patrameter を使用します。\n1 の出力結果を適当な変数 (ここでは API_KEY) に格納し、その値を登録します。\n$ API_KEY=$(sls info --stage dev | sed -n -r \u0026#39;s/^.*defaultApiKey: (.+)$/\\1/p\u0026#39;) $ aws ssm put-parameter \\ --name \u0026#34;/sampleapp/dev/api_key\u0026#34; \\ --value \u0026#34;${API_KEY}\u0026#34; \\ --type SecureString \\ --overwrite 最終的な buildspec.yml ここまでの手順を踏まえて、最終的に buildspec.yml は下記のようになります。\nversion: 0.2 phases: install: commands: - npm install -g serverless - npm install build: commands: - sls deploy --stage $STAGE - \u0026#34;API_KEY=$(sls info --stage $STAGE | sed -n -r \u0026#39;s/^.+defaultApiKey: (.+)$/\\\\1/p\u0026#39;)\u0026#34; - echo $API_KEY - aws ssm put-parameter --name \u0026#34;/sampleapp/${STAGE}/api_key\u0026#34; --value \u0026#34;${API_KEY}\u0026#34; --type SecureString --description \u0026#34;API key for Sample API - ${STAGE}\u0026#34; --overwrite 上記の説明と変わっているのは sed コマンド内の / をエスケープしている ことと、 ステージごとに可変にする部分を変数 STAGE で置き換えている ことです。\nbuild フェーズでデプロイしてるやんっていうツッコミもありますが、節約も兼ねて CodeBuild 内で完結するようにしているので見逃してください。やっている内容としては post_build フェーズのほうがしっくりくる気もしています。\nまとめ Serverless Framework で API Gateway の API キーを生成し、デプロイ時にその値を AWS Systems Manager (SSM) のパラメータストアに登録してみた話でした。\nSSM パラメータストアに登録してしまえば他のサービスからも利用しやすくなるので、 API キーの外出しに悩んでいる方は参考にしてみてください。\n",
    "permalink": "https://michimani.net/post/aws-put-api-key-to-ssm--parameter-with-serverless-framework/",
    "title": "Serverless Framework で生成した API キーをデプロイ時に SSM パラメータストアに登録する"
  },
  {
    "contents": "AWS CodeCommit のリポジトリに Git の認証情報を設定せずに IAM ユーザとして接続できる HTTPS with git-remote-codecommit という方法がサポートされました。これは git-remote-codecommit という Python のパッケージを使用した方法で、 SSH, HTTPS に続いて新たに追加された接続方法となります。\nCodeCommit に対する Git の認証が不要になり、 IAM ユーザとして接続できるということで、 CodeCommit リポジトリへの接続がとても楽になりそうです。\n目次 HTTPS with git-remote-codecommit これまでの接続方法 HTTPS with git-remote-codecommit で接続してみる 0. git-remote-codecommit を使用するための前提条件の確認・準備 1. CodeCommit に対する初期設定 2. git-remote-codecommit のインストール 3. CodeCommit リポジトリへの接続 まとめ HTTPS with git-remote-codecommit CodeCommit supports connections to CodeCommit repositories over HTTPS with git-remote-codecommit, a utility that modifies Git. This is the recommended approach for federated or temporary access connections to CodeCommit repositories. You can also use git-remote-codecommit with an IAM user. git-remote-codecommit does not require you to set up Git credentials for the user.\nAWS CodeCommit User Guide Document History - AWS CodeCommit AWS CodeCommit リポジトリに対する接続方法として、 git-remote-codecommit を使用した HTTPS での接続がサポートされました。この方法は、既に認証されたユーザ、または一時的な接続を行う際に推奨される方法です。 git-remote-codecommit を使用した HTTPS での接続では、 Git の認証情報の設定は不要です。\ngit-remote-codecommit は、もともと OSS で公開されていたツールでしたが、今回 AWS が公式にサポートするようになった、という経緯があるようです。\nAWS CodeCommit Introduces an Open Source Remote Helper もともとは GitHub の awslabs/git-remote-codecommit で管理されていましたが、現在は aws/git-remote-codecommit で管理されています。\naws/git-remote-codecommit: An implementation of Git Remote Helper that makes it easier to interact with AWS CodeCommit これまでの接続方法 CodeCommit リポジトリへの接続方法として、 SSH と HTTPS の 2 つの方法がありました。 これらの方法で接続する場合、それぞれ IAM ユーザの認証情報を設定する必要がありました。\nSSH SSH 接続用の公開鍵の登録 HTTPS 接続用のユーザ、パスワードの作成 SSH 接続は仕方ないとして、 HTTPS での接続時にも IAM の認証情報設定で CodeCommit の認証情報を作成する必要があり、面倒だなと思っていました。それが、今回新たにサポートされた HTTPS with git-remote-codecommit という接続方法により解消されることになります。\nHTTPS with git-remote-codecommit で接続してみる では実際に新しい接続方法 HTTPS with git-remote-codecommit で接続してみます。公式ドキュメントには下記の手順が記載されていますので、同じ流れで確認していきます。\n前提条件の確認・準備 CodeCommit に対する初期設定 git-remote-codecommit のインストール CodeCommit リポジトリへの接続 Setup Steps for HTTPS Connections to AWS CodeCommit with git-remote-codecommit - AWS CodeCommit 0. git-remote-codecommit を使用するための前提条件の確認・準備 git-remote-codecommit を使用するためにはローカルのマシンに以下のものがインストールされている必要があります。\nPython 3.x pip 9.0.3 以上 Git 1.7.9 以上 念のため確認しておきます。\n$ python3 -V Python 3.7.5 $ pip3 --version pip 19.3.1 from /usr/local/lib/python3.7/site-packages/pip (python 3.7) $ git --version git version 2.17.2 (Apple Git-113) もし pip のバージョンが低い場合は、下記のコマンドでアップデートします。\n$ curl -O https://bootstrap.pypa.io/get-pip.py $ python3 get-pip.py --user 1. CodeCommit に対する初期設定 CodeCommit に対する初期設定といっても、ここでやるのは下記の内容です。\nCodeCommite にアクセスできる IAM ユーザの作成 AWS CLI のインストールと認証情報の設定 ここは今回新たに出てくる内容でもないので割愛します。既存の IAM ユーザがあればそれを使用して、もし CodeCommit へのアクセス権限がない場合は AWSCodeCommitPowerUser またはその他のポリシーで CodeCommit の操作ができるようにしておきます。\n2. git-remote-codecommit のインストール 今回の肝となる git-remote-codecommit をインストールします。\n$ pip3 install git-remote-codecommit ... ... Successfully built git-remote-codecommit Installing collected packages: git-remote-codecommit Successfully installed git-remote-codecommit-1.11 3. CodeCommit リポジトリへの接続 では、実際に HTTPS with git-remote-codecommit で CodeCommit リポジトリへ接続してみます。\nマネジメントコンソールで CodeCommit のコンソールへアクセスしてリポジトリ一覧を見てみると、 URL のクローン の列に HTTPS (git-remote-codecommit) が新たに追加されています。\nここをクリックすると、下記のようなリンクがコピーされます。\ncodecommit::ap-northeast-1://your-repo-name このリンクを使って clone します。\n$ git clone codecommit::ap-northeast-1://your-repo-name 以上です。非常に簡単です。\nあとは普通に git push とか git pull とか、普通に Git コマンドを実行することができます。\nAWS CLI のプロファイルを使用する場合 IAM ユーザとして接続するので、AWS CLI のプロファイルを指定して接続することも可能です。\n例えば cc-user というプロファイル名を付けた IAM ユーザで接続したい場合は、下記のような形で clone します。\n$ git clone codecommit::ap-northeast-1://cc-user@your-repo-name リポジトリ名の前に プロファイル名@ を付ける形です。\nまとめ AWS CodeCommit のリポジトリに Git の認証情報を設定せずに IAM ユーザとして接続できる HTTPS with git-remote-codecommit という方法がサポートされたので、実際に試してみたという話でした。\nCodeCommit リポジトリへの接続には公開鍵の登録や HTTPS 接続用のユーザ情報を作成したりする必要がありましたが、今回サポートされた方法により、単純に IAM ユーザで接続できるようになりました。 SSH はともかく、 HTTPS でも接続用ユーザを作成する必要があったのは不便だったので、とてもありがたいアップデートです。\n同じように不便さを感じていた方は試してみましょう。\n",
    "permalink": "https://michimani.net/post/aws-codecommit-access-repos-with-iam-user/",
    "title": "Git の認証情報不要！CodeCommit に IAM ユーザで接続できる HTTPS with git-remote-codecommit がサポートされたみたいなので試してみました"
  },
  {
    "contents": "3月からリモートワークが本格的に導入されることになったので、前から気になっていた外付けキーボード「REALFORCE for Mac」を購入しました。for Mac にも様々なモデルがありますが、今回購入したのは テンキーレス/APC 対応/日本語配列/ブラック+シルバー モデルです。今回はそのレビューです。\nMac のキー配列に対応した外付けキーボードを探している方の参考に慣れば幸いです。\n目次 REALFORCE for Mac for Mac のラインナップ APC 機能の有無 もともと使っていたキーボード そしてフォースの力を手に入れた なにこれかっこいい 大きさ・重さについて 肝心の使用感は その他の特徴 まとめ REALFORCE for Mac そもそも REALFORCE とは、東プレという日本の企業が販売しているキーボードです。キーボードに関してはド素人なのであまり詳しくは書きません (書けません) が、打鍵感が気持ちよく、静音で、キーの構造からも誤入力が発生しづらくなっていたり、独自の APC という機能によってキーの反応速度を変更できたり、とにかく高機能で評価の高いキーボードです。\nこれまでは Windows のキー配列のものしか販売されていませんでしたが、昨年に for Mac として Mac 版が発売されました。\nfor Mac のラインナップ for Mac としては下記のモデルが発売されています。\nテンキーレスキーボード\nAPC あり (スイッチ音:静音、キー荷重:30g) US 配列 R2TLSA-US3M-WH R2TLSA-US3M-BK JIS 配列 R2TLSA-JP3M-WH R2TLSA-JP3M-BK APC なし (スイッチ音:標準、キー荷重:変荷重) US 配列 R2TL-USVM-WH R2TL-USVM-BK JIS 配列 R2TL-JPVM-WH R2TL-JPVM-BK フルキーボード\nAPC あり (スイッチ音:静音、キー荷重:30g) JIS 配列 R2SA-JP3M-WH R2SA-JP3M-BL APC なし (スイッチ音:標準、キー荷重:変荷重) JIS 配列 R2-JPVM-WH R2-JPVM-BK 製品 - Home \u0026amp;amp; Office - Mac | REALFORCE | 日本製プレミアムキーボードの最高峰 テンキーレスかフルサイズか、またそれぞれ APC 機能の有無が大きな違いとなります。テンキーレスモデルには US と JIS 両方の配列のモデルが用意されていますが、フルサイズに用意されているのは JIS 配列のみです。\nまた、全モデル共通で有線 (USB type A) のみです。\nAPC 機能の有無 APC 機能の有無が大きな違いだと書きましたが、この機能の有無によってスイッチ音とキー荷重についても違いが生まれます。\n上のモデル一覧にも書いていますが、 APC あり モデルだとスイッチ音は静音、キー荷重は 30g となります。一方で APC なし モデルでは、スイッチ音が標準、キー荷重は変荷重となっています。\nそもそも APC 機能とは APC (アクチュエーションポイントチェンジャー) 機能とは、キースイッチのオン位置を調整することができる機能です。APC 機能搭載モデルではキースイッチのオン位置を 1.5mm 、 2.2mm 、 3.0mm の3 段階で調整することができます。この調整はキー全体で一括して設定することはもちろんですが、キーごとに設定を変更することもできます。この値が小さいほど少ない押し込み量でキーオンとなるため、少しの力でタイピングすることが可能です。\n製品 : REALFORCE TKL SA for Mac / R2TLSA-JP3M-BK | REALFORCE | 日本製プレミアムキーボードの最高峰 この機能を搭載することにより通常のメカニカルキースイッチよりも最大で 25 % 高速に入力ができるよになるようです。(東プレ調べ)\nAPC ありモデルではキー荷重が 30g で固定ですが、キーオンの位置を調整することで反応速度を変更することができるようになっています。\nAPC なしモデルの変荷重とは あらかじめキーによって荷重が (物理的に) 設定されています。REALFORCE for Mac の APC なしモデルでは、キーによって荷重が 30g 、45g 、 55g に設定されています。ホームポジションに置いた際に、主要部分は荷重が重く、小指で入力するようなキーは荷重が軽くなっている、という感じです。\n特徴 | REALFORCE | 日本製プレミアムキーボードの最高峰 もともと使っていたキーボード 普段使っているのは MacBook Pro 2018 (仕事では別の MacBook Pro 2018) ですが、デュアルディスプレイで使用する際にも外付けキーボードは使用せず、MacBook Pro 本体のキーボードを使用していました。\nMacBook Pro のキーボードといえば何かと問題が多いのですが、諸々が改善されたと言われていた 2018 年モデルでさえも購入から数ヶ月後にはチャタリングが発生し、 Apple Store 駆け込んだという思い出があります。\nその後は特に不具合なく動作していました。この問題さえなければ MacBook Pro のキーボードはすごく良いキーボードだと思っています。\nただ、やっぱりデュアルで使用するとなると大きい画面をメインで使いたいので外付けのキーボードが欲しい、でも Mac の配列に特化しているキーボードって Apple 純正のキーボードくらいしかないよなーと思っていました。\nちなみに Apple 純正の外付けキーボードについては、 iMac late 2013 に付属してきたものをそのまま iMac 使っていますので、一応外付けキーボードがあるといえばあります。\nそしてフォースの力を手に入れた 昨今の諸々の情勢からリモートワークが主になるということで、これまで以上に自宅でデュアルディスプレイで作業することが増えるので、この際に外付けキーボードデビューしてみようということで、前から少し気になっていた REALFORCE for Mac を購入しました。\n今回購入したモデルは、 テンキーレス / APC あり / JIS 配列 モデル / ブラック+シルバー (R2TLSA-JP3M-BK) です。\n他の選択肢として HHKB もありましたが、いろんなレビュー記事を見て REALFORCE for Mac にしました。まあ、キーボード初心者なので見た目がかっこいいという理由が一番大きいです。\nなにこれかっこいい では、まずは写真多めで REALFORCE のカッコよさをお見せしていきます。\n白い箱にキーボードの図が描かれています。\nその中には黒い箱。このあたりからカッコよさが溢れてます。\nめちゃくちゃカッコいいですね。\n何がカッコいいかって、 JIS 配列ではあるものの、かな表示がなくてスッキリしてるところですね。写真では少し薄い色に写っていますが、実際は若干灰色っぽくなった墨色っていう感じで、そこに輝度の低い黄土色で刻印がされており、刻印の主張がほとんどありません。\n真上から見た全体像はこんな感じです。\n裏面はこんな感じ。\n全モデル共通で有線のみですが、配線は中央と左右の 3 箇所から出すことができるので、置く場所には融通が利きます。\n高さ調整の脚と 4 隅に滑り止めのゴムが装着されています。なお、高さ調整の脚を出した時は、滑り止めは手前側の 2 箇所のみ接地する形になります。\n大きさ・重さについて 箱から取り出したときの感想としては、 重いな というのが一番でした。\n購入時にキーボード本体の重量までみていなかったのですが、今回購入したテンキーレスモデルの重量は 1.1 kg と、なかなか重たいです。フルサイズモデルでは 1.4 kg となっています。\nただし、この重さが安定感につながっていて、裏面の滑り止めと合わさることによって置いたとき、タイピングしてるときの安定感が凄いです。\n大きさに関しては、比較として Apple 純正のテンキー付きキーボードとの画像を載せておきます。\n横幅は、当たり前ですがテンキーがない分狭くなってます。特に狭いとか広いとか感じるような幅のサイズ館ではありません。\n一方、高さに関してはかなり高くなっています。Apple のキーボードと MacBook Pro のキーボード面の高さはほぼ一緒なので、それと比べても 3 倍くらいの高さがあります。これは高さ調整の脚を出していない状態での比較なので、脚を出すとかなり高くなります。\n届いてからまだ数時間しか使っていませんが、個人的にはこの高さがちょっと辛いかなーという印象です。これまで平たいキーボードでずっと生活してきたので慣れていないというのは大きいですが、今後は手首を置く台みたいなのを導入するかも知れません。\n肝心の使用感は 届いてから数時間、 e-typing で数時間とこのブログ記事を書いているなかで感じた使用感についての感想です。\nインターネットでタイピング練習 イータイピング | e-typing ローマ字タイピング 打鍵音が気持ちいい いろんなレビューをみていて、よく書かれているのがこの 音が気持ちいい という感想です。なに言ってんだろうなって思ってたんですが、その意味がようやくわかりました。タイピングしていて気持ちいいと思うことなんてなかったんですが、なんというか、どんどん文字を打ちたくなるような、そんな音ですね。\n普段はなにかしら音楽を聴きながらブログを書くことが多いのですが、外部の音を無にして、キーボードの打鍵音を聴きたくなるような、そんな音です。とても気持ちいいです。\n打鍵音については、 e-typing を試している場面を iPhone 11 Pro で簡易的に撮影したものがあるので、参考にしてみてください。\nREALFORTH for Mac の打鍵音 pic.twitter.com/brb5BJwpGV\n\u0026mdash; よっしー Lv.854 (@michimani210) March 23, 2020 YouTube に 11 分くらいの動画もアップしています。\n快適なタイピング体験 届いてすぐに使用した時点では、めちゃくちゃ軽い押し込みで反応するなーと思っていて、むしろ軽すぎるなと思っていたんですが、 APC 機能でキーオンの位置を変更すると、めちゃくちゃいい感じになりました。\nデフォルトでは一括で 2.2mm に設定されているのですが、 3.0mm に変更することで最高のタイピング体験を得ることができました。\nAPC やその他諸々の設定については REALFORCE の専用ソフトウェアで行います。\nこのように一括で設定することもできますし、各機キーごとに設定値を変更することもできます。\nキーオンの位置を変更することで反応速度を変えられるというのも大きいですが、もひとつ大きな要因としては、REALFORCE の特徴である Nキーロールオーバー の恩恵が大きいです。\n複数のキーが同時に押された場合に押された順序に従ってすべて認識される『Ｎキーロールオーバー』対応なので、高速入力時に同時押しの様な入力をしても全て正確に入力されます。プロのオペレーターにも安心してご使用いただけます。\n東プレのサイトに書かれている通り、同時押しっぽい高速入力でもしっかりと反応して順番に入力されるので、タイピング中に抜けがあったりすることがなく、スムーズに入力を行うことができます。\nただしこの Nキーロールオーバーにはデメリットもあると思っていて、複数のキーをタイピングミスで同時に押してしまうとそれも反応してしまうんですよね。まあ、これに関してはキーの配置に慣れて確実に１つのキーを押すことができるように慣れば無視できるデメリットだと思いますが、慣れるまでは逆に誤入力が多くなるかもしれません。\nキーボード自体の安定感が抜群 外観のレビューのところでも書きましたが、キーボード本体が 1.1 kg という重さで、さらに裏面の 4 箇所に滑り止めがついていることもあり、キーボードの安定感が抜群です。これはタイピング時にも効果を発揮していて、多少荒いタイピングをしてもキーボードがずれたりすることがないので、いわゆる「カタカタカタカタカタカタ、ッターン！」みたいなことをやっても全然びくともしません。\nそんなに荒いタイピングをしないという場合でも、キーボード自体がズレる・動く心配がほとんどないというのは、タイピングをする上で実は重要なことなのかもしれないなと思いました。\nその他の特徴 ここまで紹介してきた内容以外に下記のような特徴もあるので、簡単に紹介しておきます。\nキースペーサーによるチューニング 専用ソフトウェアによるカスタマイズ キースペーサーによるチューニング REALFORCE for Mac には付属品として次のようなものが入っています。\nこの黒いのが キースペーサー と呼ばれるもので、素材としては柔らかいクッションのようなものです。キートップを外してこれを挟むことによって、キーストロークの戻りを調整することができます。キースペーサーは暑さが 2mm と 3mm のものが付属しているので、 APC の設定値と合わせて使用することでより素早いキー入力を実現できるようになるようです。\nちなみにキートップを外すための工具 (キートッププラー) も付属しています。\n専用ソフトウェアによるカスタマイズ APC 設定のところで少し紹介しましたが、専用ソフトウェアを使用することで様々なカスタマイズをすることができます。\n例えば キーロック 機能。\nこの機能を使用することで、特定のキーの反応を無効化することができます。\nこの画像で無効化しているのは 1 キーの左にある E/J というキーで、バッククウォートを入力できる便利なキーです。ただ、 1 を打とうとするときにどうしても誤タイプしてしまうので無効化しています。\nその他にも下記のようなカスタマイズが可能です。\nインジケータの色・輝度変更 CapsLock と Ctrl キーの入れ替え ファームウェアアップデート まとめ リモートワークに備えて東プレの最高峰キーボード REALFORCE for Mac を購入してみたので、そのレビューでした。\nほぼ初めての外付けキーボードで、なかなかのお値段がするものを買ったので合わなかったらどうしようと思っていましたが、杞憂でした。使用感は打鍵音、反応スピード共に最高で、大満足です。キーボード自体の性能は完璧なので、あとは人間のタイピング力を上げていきたいと思います。\n外観のところでも触れましたが、はりキーボードの高さについては気になるポイントです。こればっかりはこれまでの平たいキーボードばかり使ってきたという経緯からも、慣れででどうにかならない気がしているので、早い段階で手首を載せる台を購入したいと思います。\n逆に言えばそれ以外に関しては良いところしかないので、そこに対する解決策さえ用意できれば Mac 用の外付けキーボードとしてはとても良いアイテムだと思います。\n",
    "permalink": "https://michimani.net/post/gadget-review-realforce-for-mac/",
    "title": "[レビュー] キーボードの最高峰 REALFORCE for Mac の打鍵感が気持ちよすぎた話"
  },
  {
    "contents": "先日 JAWS-UG 初心者支部のハンズオンで初めて触った Amazon Transcribe ですが、文字起こしの際に個人を特定できるような文言 (電話番号や口座番号など) を自動で削除してくれるというアップデートがありました。最近触ったサービスのアップデートということで、実際に試してみました。\n目次 アップデートの概要 前提 やってみる 音声ファイルの準備 普通に文字起こししてみる ContentRedaction オプションを指定して文字起こししてみる まとめ アップデートの概要 Amazon Transcribe to automatically remove sensitive personally identifiable information (PII) from your transcription results. Amazon Transcribe uses state-of-the-art machine learning technology to accurately identify and remove PII. PII removed includes social security number, credit card/bank account information and contact information such as name, email address, phone number and mailing address.\n(公式アナウンスから抜粋)\nAWS の文字起こしサービス Amazon Transcribe に 個人を特定できる機密情報 (Personally Identifiable Information (PII)) を自動的に削除するオプションが追加されました。 PII として認識されるのは、社会保障番号、クレジットカード番号、銀行口座番号、名前、メールアドレス、電話番号、住所などです。\nアップデートが利用できるのは、 Amazon Transcribe が利用できるリージョン全てですが、サポートしている言語は現時点で en-US のみです。\n詳細については公式のアナウンスを参照してください。\nAmazon Transcribe now supports automatic content redaction 前提 今回は Transcribe の API を実行するスクリプトは、 Lambda 関数としてではなくローカル環境で実行します。\nというのも、一旦 Lambda で実行しようとしたのですが、どうやら Lmabda の boto3 が最新になっていないため下記のようなエラーが出たからです。\nUnknown parameter in input: \u0026#34;ContentRedaction\u0026#34;, must be one of: TranscriptionJobName, LanguageCode, MediaSampleRateHertz, MediaFormat, Media, OutputBucketName, OutputEncryptionKMSKeyId, Settings アップデートで追加された Content Redaction 機能を使うには、 start_transcription_job のパラメータに ContentRedaction を指定するのですが、 boto3 が最新でないために対応していないと言われてしまいます。\nこの問題については公式にも書かれており、対応方法としては最新の boto3 を pip でインストールして Lambda Layer を作成し、その Layer を使うという方法です。\nPython (Boto 3) Lambda 関数のランタイムエラーをトラブルシューティングする 少し手間がかかりそうだったので、今回はローカルで最新の boto3 をインストールして試してみることにします。\nやってみる では早速やってみます。\n音声ファイルの準備 スクリプトの実装の前に、もととなる音声ファイルが必要になります。\n自分で英語を喋って録音できればよいのですが、 Speaking スキルが低いので断念。ブラウザ上の Google 翻訳でそれっぽい文言を音読させて、それを録音して使うことにします。\n詳しい手順は割愛しますが、今回は次のような文言を音声ファイルとして利用します。\n普通に文字起こししてみる まず最初は ContentRedaction オプション無しで文字起こししてみます。\nimport boto3 import datetime def transcribe(): s3 = boto3.client(\u0026#39;s3\u0026#39;) transcribe = boto3.client(\u0026#39;transcribe\u0026#39;) bucket = \u0026#39;jugbgnr24-transcribe-input-michimani\u0026#39; key = \u0026#39;Jennifer_PII.mp3\u0026#39; try: transcribe.start_transcription_job( TranscriptionJobName= datetime.datetime.now().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) + \u0026#39;_Transcription\u0026#39;, LanguageCode=\u0026#39;en-US\u0026#39;, Media={ \u0026#39;MediaFileUri\u0026#39;: \u0026#39;https://s3.ap-northeast-1.amazonaws.com/\u0026#39; + bucket + \u0026#39;/\u0026#39; + key }, OutputBucketName=\u0026#39;jugbgnr24-transcribe-output-michimani\u0026#39; ) except Exception as e: print(e) print(\u0026#39;Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.\u0026#39;.format(key, bucket)) raise e if __name__ == \u0026#39;__main__\u0026#39;: transcribe() スクリプトの内容は JAWS-UG 初心者支部#24 のものをほぼそのまま持ってきて、S3 のバケット名および対象の音声ファイル名は固定で書いています。\njugbgnr24-transcribe-input-michimani バケットに音声ファイル Jennifer_PII.mp3 を置いて、スクリプトを実行。しばらくすると jugbgnr24-transcribe-output-michimani に文字起こし結果の JSON が生成されるので取得して中身を確認してみます。(長いので results.transcripts 部分のみ抜粋)\n{ \u0026#34;results\u0026#34;: { \u0026#34;transcripts\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;Hello. My name is Jennifer. My phone number is 98765432198 And my main bank account number is 00112233\u0026#34; } ] } } ほぼ最初の文言通り、文字起こしできています。\nContentRedaction オプションを指定して文字起こししてみる では今度は ContentRedaction オプションを指定して文字起こししてみます。\nimport boto3 import datetime def transcribe(): s3 = boto3.client(\u0026#39;s3\u0026#39;) transcribe = boto3.client(\u0026#39;transcribe\u0026#39;) bucket = \u0026#39;jugbgnr24-transcribe-input-michimani\u0026#39; key = \u0026#39;Jennifer_PII.mp3\u0026#39; try: transcribe.start_transcription_job( TranscriptionJobName= datetime.datetime.now().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) + \u0026#39;_Transcription\u0026#39;, LanguageCode=\u0026#39;en-US\u0026#39;, Media={ \u0026#39;MediaFileUri\u0026#39;: \u0026#39;https://s3.ap-northeast-1.amazonaws.com/\u0026#39; + bucket + \u0026#39;/\u0026#39; + key }, OutputBucketName=\u0026#39;jugbgnr24-transcribe-output-michimani\u0026#39; ContentRedaction={ \u0026#39;RedactionType\u0026#39;: \u0026#39;PII\u0026#39;, \u0026#39;RedactionOutput\u0026#39;: \u0026#39;redacted\u0026#39; } ) except Exception as e: print(e) print(\u0026#39;Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.\u0026#39;.format(key, bucket)) raise e if __name__ == \u0026#39;__main__\u0026#39;: transcribe() 色がついている部分が、 ContentRedaction オプションです。\nRedactionType の値としては PLL を指定します。\nRedactionOutput の値としては redacted または redacted_and_unredacted を指定します。redacted を指定したときは、 ContentRedaction が適用された結果のみ、 一方で redacted_and_unredacted を指定したときは ContentRedaction が適用された場合と適用しなかった場合の両方の結果が出力されます。\nまた、 ContentRedaction が適用された結果の JSON ファイルには redacted- というプリフィックスが付与されます。\nTranscribeService — Boto 3 Docs 1.12.9 documentation | Client.start_transcription_job これで実行してみると、結果の JSON は次のようになりました。(長いので results.transcripts 部分のみ抜粋)\n{ \u0026#34;results\u0026#34;: { \u0026#34;transcripts\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;Hello. My name is [PII]. My phone number is [PII] And my main bank account number is [PII]\u0026#34; } ] } } 名前、電話番号、口座番号に該当する部分が [PII] という文字列で置換されています。すごい。\nまとめ Amazon Transcribe で機密情報を自動で削除できるようになったみたいなので試してみた話でした。\n個人的にタイムリーなサービスのアップデートだったので試してみましたが、こんなことが簡単にできてしまうのは本当にすごいなと思いました。(小並感)\nAWS の公式アナウンスにもあるように、オペレーターとかサポートセンターの会話内容を文字起こしして残している際には有効なオプションになりそうです。ただし、現時点で ContentRedaction オプションがサポートしているのは en-US のみなので、日本で恩恵を受けることはあまり無いような気がします\u0026hellip;。\nJAWS-UG 初心者支部#24 サーバレスハンズオン勉強会の宿題をやってみた #jawsug_bgnr #jawsug - michimani.net ",
    "permalink": "https://michimani.net/post/aws-transcribe-automatic-content-redaction/",
    "title": "Amazon Transcribe で機密情報を自動で削除できるようになったみたいなので試してみた"
  },
  {
    "contents": "前に同一カテゴリ記事の一覧を記事下に表示する方法を書きましたが、今回はそのタグ版です。複数のタグが付けられている場合は、それぞれのタグに紐づく記事の一覧を表示するようにしてみます。\n目次 前提 やること 現在記事のタグ名を取得する 対象のタグが付けられた記事一覧を取得する 記事の一覧を表示させる 最終的にどうなったのか まとめ 前提 Hugo 0.65.3 Taxonomies を使ってタグ管理している やること 基本的には同一カテゴリの記事一覧を表示したときと同じです。\n各テーマディレクトリ内にある layouts/_default/single.html に修正を加えます。\n今回も前回と同様に indigo のテーマで実装するので、対象のファイルは themes/indigo/layouts/_default/single.html となります。\n現在記事のタグ名を取得する 同一タグの記事一覧を取得するために、現在の記事のタグ名を取得する必要があります。カテゴリの時と同様に、次のような記述でタグの リスト を取得できます。\n{{ .Params.tags }} 前回は、 一つの記事にカテゴリは一つしか設定しない という暗黙のマイルールに基づいて、カテゴリ名は次のような記述で取得していました。\n{{ $c := index .Params.categories 0 }} ただし、今回は複数のタグが付けられていることを想定して、すべてのタグに関して関連する記事一覧を表示するので、次のような記述でタグのリストからタグ名を再帰的に取得します。\n{{ range .Params.tags }} {{ $t := . }} {{ end }} この range の中で、各タグに関連する記事一覧を取得し、表示していきます。\n対象のタグが付けられた記事一覧を取得する $t にはタグ名が格納されるので、その値を用いて記事の一覧を取得します。具体的には次のような形で取得します。\n{{ $tposts := index $site_obj.Taxonomies.tags (replace (lower $t) \u0026#34; \u0026#34; \u0026#34;-\u0026#34;) }} これで $tposts 変数には、タグ名 $t が付けられた記事の一覧が格納されます。\nとは言っても急に出てきた部分がいくつかあるので、それぞれ説明していきます。\n変数 $site_obj まずは $site_obj という変数です。\nこれは range の外で次のように定義しておきます。\n{{ $site_obj := .Site }} range の中では .Site を新たに定義した変数として使用しています。理由としては、 range の中では . が示すのは個別のタグ名なので、 .Site を参照することができないからです。\n試しに .Site のまま使用しようとすると、下記のようなエラーとなります。 (適宜 改行しています)\nrender of \u0026#34;page\u0026#34; failed: \u0026#34;/path/to/michimani.net/themes/indigo/layouts/_default/single.html:106:33\u0026#34;: execute of template failed: template: _default/single.html:106:33: executing \u0026#34;_default/single.html\u0026#34; at \u0026lt;.Site.Taxonomies.tags\u0026gt;: can\u0026#39;t evaluate field Site in type string Rebuild failed: Failed to render pages: render of \u0026#34;page\u0026#34; failed: \u0026#34;/path/to/michimani.net/themes/indigo/layouts/_default/single.html:106:33\u0026#34;: execute of template failed: template: _default/single.html:106:33: executing \u0026#34;_default/single.html\u0026#34; at \u0026lt;.Site.Taxonomies.tags\u0026gt;: can\u0026#39;t evaluate field Site in type string なので、 range の外で .Site を変数に代入しておいて、 range の中では変数を介して .Site の情報を使用します。\n半角スペースの置換 タグ名には半角スペースを含めることができますが、 .Site.Taxonomies.tags オブジェクトのキーとなっているタグ名では英数字が小文字に、半角スペースがハイフン - に、それぞれ置換されています。\n例えば Amazon S3 というタグ名であれば、キー名としては amazon-s3 となっています。\n一方で range で回している .Params.tags のタグ名は Amazon S3 といった本来のタグ名なので、次の記述で英数字の小文字化と半角スペースの置換を行っています。\n{{ (replace (lower $t) \u0026#34; \u0026#34; \u0026#34;-\u0026#34;) }} replace | Hugo 記事の一覧を表示させる 記事の一覧が取得できたので、あとは表示するだけです。\nこの部分はカテゴリの場合と同じなので、特に新しいことはありません。前回と同様に、該当する記事一覧の最初の 5 件のみを表示します。\n{{ range first 5 $tposts }} {{ .Render \u0026#34;li\u0026#34; }} {{ end }} 最終的にどうなったのか ここまでやったことをまとめると、各タグの記事一覧を表示する部分は次のような形になりました。\n{{ if and (.Params.tags) }} {{ $site_obj := .Site }} {{ range .Params.tags }} {{ $t := . }} {{ $tposts := index $site_obj.Taxonomies.tags (replace (lower $t) \u0026#34; \u0026#34; \u0026#34;-\u0026#34;) }} {{ if gt (len $tposts) 1 }} \u0026lt;div class=\u0026#34;same-category-tags-posts-area\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Other posts tagged by \u0026#34;{{ $t }}\u0026#34;\u0026lt;/h2\u0026gt; {{ range first 5 $tposts }} {{ .Render \u0026#34;li\u0026#34; }} {{ end }} {{ if gt (len $tposts) 5 }} \u0026lt;a class=\u0026#34;more-btn\u0026#34; href=\u0026#34;/tags/{{ lower $t }}/\u0026#34;\u0026gt;more ...\u0026lt;/a\u0026gt; {{ else }} \u0026lt;a class=\u0026#34;more-btn\u0026#34; href=\u0026#34;/tags/\u0026#34;\u0026gt;other tags\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} {{ end }} {{ end }} リストの細かい表示条件はカテゴリの場合と同じで、同一タグの記事が 5 件以上あればタグ別記事一覧ページへのリンクを more 、 5 件より少なければタグ一覧ページへのリンクを othre tags で、それぞれ表示するようにしています。\n実際には次のような表示になります。\nまとめ Hugo で作ったブログの記事下に、その記事と同じタグが付けられている記事の一覧を表示してみた話でした。\n今までは各記事のタイトル部分にタグ別一覧へのリンクを置いていましたが、今回 記事の一覧を表示することでタグ別一覧ページへ遷移するというステップがなくなり、回遊がしやすくなりました。\nこのブログに関しては自分でも見返すことが多いので、関連する情報が追いかけやすくなったなーという印象です。\n同じようなことを実現したいと考えている方は、是非参考にしてみてください。\n[Hugo] 記事下に同じカテゴリの記事一覧を表示する - michimani.net ",
    "permalink": "https://michimani.net/post/development-show-other-posts-tagged-by-the-same-tag-on-hugo/",
    "title": "[Hugo] 記事下に同じタグが付けられている記事一覧を表示する"
  },
  {
    "contents": "AWS Chalice でサーバレスアプリケーションを実装しているときに、自動で作成されるはずの IAM ポリシーが作成されなかった場面があったので、そのときの話です。\n目次 AWS Chalice とは ソースコードを解析して必要なポリシーを自動的に付与 ソースコードからよしなに自動作成される場合 ソースコードからよしなに自動作成されない場合 AWC Chalice のソースコード解析は Client class での実装時のみ有効 Client と Resource のレスポンスの違い AWS Chalice で作成される IAM ロールにを手動でポリシーをアタッチする まとめ AWS Chalice とは AWS Chalice とは、 Python でサーバレスアプリケーションを作成するためのマイクロフレームワークです。AWS 環境上でサーバレスアプリケーションを構築するフレームワークには AWS SAM や Serverless Framework などがありますが、 Chalice はそれらに比べて非常にシンプルで、 AWS でのサーバレスアプリケーション開発を始めたいという人にとっては導入にもってこいのフレームワークです。\nGetting Started — Python Serverless Microframework for AWS 1.12.0 documentation aws/chalice: Python Serverless Microframework for AWS Chalice で自動的に作成される AWS リソースは API Gateway の API と Lambda の関数、 IAM ロールとポリシーのみです。他のフレームワークではその他のリソースも作成できたりしますが、 Chalice では必要最低限のリソースのみが作成されます。また、開発言語も Python 固定なので、それも含めてシンプルでとっつきやすいフレームワークになっています。\n今回はいわゆるチュートリアル的な話ではなくて、 Chalice で自動作成される IAM ポリシーに関する話です。チュートリアル的な話はまた別で書きたいと思います。\nソースコードを解析して必要なポリシーを自動的に付与 では、簡単な REST API を実装して、 Chalice によって作成される IAM ポリシーを確認してみます。\nREST API の内容としては、 DynamoDB に存在する Thread テーブルからデータを取得するというものです。データに関しては、下記の DynamoDB サンプルデータを使用します。\nサンプルテーブルとデータ - Amazon DynamoDB AWS Chalice では、基本的に app.py に API の処理を実装していきます。そして chalice deploy コマンドで AWS 環境にデプロイします。デプロイ時には、 API Gateway の API 、 Lambda 関数、 IAM ロール を自動で作成してくれます。この際に作成される IAM ロールには、ソースコードを解析して、必要なポリシーをよしなに付与してくれます。\n今回ハマったポイントとしては、実装方法によっては、 本来ソースコードを解析して必要なポリシーを持つ IAM ロールを作成してくれるはずが、権限が足りない場合が起こりうる ということです。なので、正しく作成される場合とそうでない場合の実装方法の違いを比較していきます。\nソースコードからよしなに自動作成される場合 まずは正しく必要なポリシーが付与される場合の実装です。\napp.py を下記のような内容で実装します。\nfrom chalice import Chalice, Response import boto3 import urllib.parse app = Chalice(app_name=\u0026#39;chalice-sample\u0026#39;) dynamo_client = boto3.client(\u0026#39;dynamodb\u0026#39;) TABLE_NAME = \u0026#39;Thread\u0026#39; @app.route(\u0026#39;/threads\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_threads(): items = dynamo_client.scan(TableName=TABLE_NAME) return Response(body=items[\u0026#39;Items\u0026#39;], status_code=200, headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}) @app.route(\u0026#39;/threads/{forum_name_url}\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_forum_threads(forum_name_url): forum_name = urllib.parse.unquote(forum_name_url) items = dynamo_client.query( TableName=TABLE_NAME, KeyConditions={ \u0026#39;ForumName\u0026#39;: { \u0026#39;AttributeValueList\u0026#39;: [ {\u0026#39;S\u0026#39;: forum_name} ], \u0026#39;ComparisonOperator\u0026#39;: \u0026#39;EQ\u0026#39; } }) return Response(body=items[\u0026#39;Items\u0026#39;], status_code=200, headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}) API のエンドポイントとしては GET /threads と GET /threads/{forum_name} を用意していて、前者はテーブル内のデータ前取得、後者はパーティションキーである ForumName を指定したデータの取得を行います。\nそれぞれの処理の中で boto3 を用いて DynamoDB に対する Scan と Query の API を呼んでいます。\nこの実装状態で chalice deploy を実行すると、下記のようなポリシーをもつ IAM ロールが作成されます。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Sid\u0026#34;: \u0026#34;ae5b14e8a77347428XXXXXXXXXXXXX\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } 実装通り、 dynamodb:Query と dynamodb:Scan の権限が付与されています。\nソースコードからよしなに自動作成されない場合 続いては、必要なポリシーが正しく作成されない場合の実装です。\napp.py は下記のように実装します。\nfrom chalice import Chalice, Response import boto3 import urllib.parse app = Chalice(app_name=\u0026#39;chalice-sample\u0026#39;) dynamo_resource = boto3.resource(\u0026#39;dynamodb\u0026#39;) TABLE_NAME = \u0026#39;Thread\u0026#39; @app.route(\u0026#39;/threads\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_threads(): items = dynamo_resource.Table(TABLE_NAME).scan() return Response(body=items[\u0026#39;Items\u0026#39;], status_code=200, headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}) @app.route(\u0026#39;/threads/{forum_name_url}\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_forum_threads(forum_name_url): forum_name = urllib.parse.unquote(forum_name_url) items = dynamo_resource.Table(TABLE_NAME).query(KeyConditions={ \u0026#39;ForumName\u0026#39;: { \u0026#39;AttributeValueList\u0026#39;: [ forum_name ], \u0026#39;ComparisonOperator\u0026#39;: \u0026#39;EQ\u0026#39; } }) return Response(body=items[\u0026#39;Items\u0026#39;], status_code=200, headers={\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}) API のエンドポイントとしては先ほどと同様に、 GET /threads と GET /threads/{forum_name} を用意していて、前者はテーブル内のデータ前取得、後者はパーティションキーである ForumName を指定したデータの取得を行います。\n実装方法で先ほどと異なる点は、 boto3 の Client class ではなく Resource class を使用した実装になっているという点です。\nこの場合、 chalice deploy を実行すると、下記のようなポリシーをもつ IAM ロールが作成されます。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } はい。CloudWatch Logs に対するポリシーしか付与されていません。\nこれではデプロイされた API を実行しても、権限が足りずにエラーになってしまいます。\nbotocore.exceptions.ClientError: An error occurred (AccessDeniedException) when calling the Query operation: User: arn:aws:sts::123456789012:assumed-role/chalice-sample-dev/chalice-sample-dev is not authorized to perform: dynamodb:Query on resource: arn:aws:dynamodb:ap-northeast-1:123456789012:table/Thread AWC Chalice のソースコード解析は Client class での実装時のみ有効 上記のように作成されるポリシーに差異が生じるのは、実装を boto3 の Client class を使うか、 Resource class を使うかが原因になっています。\nAWS Chalice ではデプロイ時のソースコード解析に、 boto3 の Client class の API を呼んでいるかどうかを見ています。\nChalice のソースコードではこの辺りです。\nchalice/analyzer.py at 3f89da242629772d6561cc7be19ff8039489bcae · aws/chalice Client class と Resource class の性質の違いを考えるとこうなっていることは理解できます。\nClient class は AWS の各サービスに対する API をほぼ網羅しているので、必要なポリシーが判定しやすいです。ただし、低レベルな API のためコール時には細かいパラメータを指定したり、レスポンスの加工が必要になったりと、実装時には手間がかかることが多いです。\n一方で Resource class は Client class を高レベルで抽象化した class で、同じ処理をするにしてもコール時のパラメータやレスポンスが扱いやすい形になっているため、実装フレンドリーだと言えます。\nClient と Resource のレスポンスの違い では実際にそれぞれの実装で正しくデータが取得できた場合のレスポンスを比較してみます。対象のエンドポイントは、 GET /threads/Amazon%20S3 として比較します。\nまずは Client class を用いた実装の場合のレスポンス。\n[ { \u0026#34;Answered\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon S3\u0026#34; }, \u0026#34;LastPostedBy\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34; }, \u0026#34;LastPostedDateTime\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2015-09-29T19:58:22.514Z\u0026#34; }, \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;S3 thread 1 message\u0026#34; }, \u0026#34;Replies\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;S3 Thread 1\u0026#34; }, \u0026#34;Tags\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;largeobjects\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;multipart upload\u0026#34; } ] }, \u0026#34;Views\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; } } ] 各項目の値に対して、型を示す情報も入った状態でレスポンスが返ってきます。\n続いて Resource class を用いた場合のレスポンス。\n[ { \u0026#34;Answered\u0026#34;: 0.0, \u0026#34;ForumName\u0026#34;: \u0026#34;Amazon S3\u0026#34;, \u0026#34;LastPostedBy\u0026#34;: \u0026#34;User A\u0026#34;, \u0026#34;LastPostedDateTime\u0026#34;: \u0026#34;2015-09-29T19:58:22.514Z\u0026#34;, \u0026#34;Message\u0026#34;: \u0026#34;S3 thread 1 message\u0026#34;, \u0026#34;Replies\u0026#34;: 0.0, \u0026#34;Subject\u0026#34;: \u0026#34;S3 Thread 1\u0026#34;, \u0026#34;Tags\u0026#34;: [ \u0026#34;largeobjects\u0026#34;, \u0026#34;multipart upload\u0026#34; ], \u0026#34;Views\u0026#34;: 0.0 } ] 自然なデータ構造で返ってきます。実装時では、間違いなくこちらの形の方が扱いやすいです。\n今回は DynamoDB に対する操作ですが、他のリソースに対する操作でも Resouce class を用いたほうが実装は楽になるでしょう。\nAWS SDK for Python(Boto3)ではClient APIよりResource APIを使おう - Qiita AWS Chalice で作成される IAM ロールにを手動でポリシーをアタッチする 自動で作成される IAM ロールに必要なポリシーが付与されないのは困るけど、実装は楽な方がいいからな\u0026hellip; という悩みが出てくると思いますが、 AWS Chalice では、自動で作成される IAM ロールに手動で作成したポリシーをアタッチすることができます。\n方法は簡単で、 .chalice/config.json で下記のような設定をします。\n{ \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;app_name\u0026#34;: \u0026#34;chalice-sample\u0026#34;, \u0026#34;stages\u0026#34;: { \u0026#34;dev\u0026#34;: { \u0026#34;api_gateway_stage\u0026#34;: \u0026#34;api\u0026#34;, \u0026#34;autogen_policy\u0026#34;: false, \u0026#34;iam_policy_file\u0026#34;: \u0026#34;custom-policy.json\u0026#34; } } } \u0026quot;autogen_policy\u0026quot;: false でポリシーの自動生成をオフにして、 \u0026quot;iam_policy_file\u0026quot;: \u0026quot;custom-policy.json\u0026quot; で手動で作成したポリシーの JSON ファイルを指定します。今回であれば下記のような JSON ファイルを作成します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } この状態で chalice deploy を実行すると、自動で作成される IAM ロールには上記のポリシーがアタッチされます。\n自分で必要な権限を考える必要はありますが、 Resource class を用いた実装をする場合にはこのような形でポリシーを指定する形になります。\nまとめ AWS Chalice でサーバレスアプリケーションを実装する際に、必要な IAM ポリシーが自動で作成されなかった場面があったという話でした。\n最初は なんで Resouce class だとポリシー作成されへんねん って思ってたんですが、 Client class との抽象度の違いとかを考えると納得できました。必要なポリシーを考えるのって AWS 触り始めたばかりだとよくわからんっていう感じですけど、自分で必要なポリシーを調べて都度足していくっていう作業は実は AWS 触る上では重要なことなんじゃないかなと思ってます。最初のうちは とりあえず Full Access で ってやりがちですけど、そうやって必要な権限を知って絞っていく作業が地味に大事だなと、最近感じてます。\n冒頭にも書いたように、 AWS Chalice はとてもシンプルなサーバレスフレームワークなので、とりあえず AWS でサーバレスやりたい とか、 Python でサーバレスやりたい という方にはおすすめのフレームワークです。\n参考 AWS SDK for Python(Boto3)ではClient APIよりResource APIを使おう - Qiita ",
    "permalink": "https://michimani.net/post/aws-about-auto-generate-iam-policy-in-chalice/",
    "title": "AWS Chalice で必要な IAM ポリシーが正しく作成されなかったときの話"
  },
  {
    "contents": "先日開催された JAWS-UG 初心者支部#24 サーバレスハンズオン勉強会で宿題になっていた課題をやってみました。内容としては、文字起こし \u0026amp; 翻訳のパイプライン処理です。\n前半部分は当日の様子と、ハンズオンの内容について書いています。宿題部分については後半に書いています。\n目次 当日の様子 ハンズオンの概要 宿題の内容 S3 バケットの作成 文字起こし用 Lambda 関数の作成 翻訳用 Lambda 関数の作成 文字起こしから通して確認してみる まとめ 当日の様子 当初は現地での参加を予定していましたが、昨今の諸々の情勢からリモート枠に変更し、自宅からリモートで参加させていただきました。\n現地開催についても様々な検討事項や懸念等あったかと思いますが、リモート含め開催されたことに感謝いたします。ありがとうございました。\nイベントの詳細については connpass のページを参照してください。今回使用した資料等もアップされています。\nJAWS-UG 初心者支部#24 サーバレスハンズオン勉強会 - connpass また、当日の様子については、ハッシュタグ #jawsug_bgnr および #jawsug でツイートされており、 Togetter にもまとめられていました。\nJAWS-UG 初心者支部#24 サーバレスハンズオン勉強会 まとめ - Togetter ハンズオンの概要 当時のハンズオンは以下の内容でした。\nAWS Lambda で 日 -\u0026gt; 英 翻訳する 翻訳 Web API を作る 文字起こし + 翻訳パイプラインを作る 簡単に概要を書いておきます。\nなお、ハンズオンの進行に使用されたメインのスライド、およびソースコードについては公開されています。\n[JAWS−UG 初心者支部 #24] サーバーレスクイックスタート： 手を動かしながら学ぶサーバーレスはじめの一歩 #jawsug #jawsug_bgnr / JAWS−UG-bgnr 24 - Serverless Quick Start hands-on - Speaker Deck ketancho/aws-serverless-quick-start-hands-on: \u0026amp;ldquo;サーバーレスクイックスタート： 手を動かしながら学ぶサーバーレスはじめの一歩\u0026amp;rdquo; のサンプルコードです 1. AWS Lambda で 日 -\u0026gt; 英 翻訳する AWS の翻訳サービス Amazon Translate の API を実行する Lambda 関数を作成して、日本語から英語への翻訳します。\n実行は Lambda のマネジメントコンソール上からテストする形で、テストに使用するイベントには Amazon API Gateway AWS Proxy テンプレートを使用しました。\nただし、翻訳する本文はこの時点ではハードコーディングされているもので、とりあえず Amazon Translate で翻訳できるよね、とういことを確認しました。\n下記のような Lambda 関数を使用して実行しました。\nimport json import boto3 def lambda_handler(event, context): translate = boto3.client(\u0026#39;translate\u0026#39;) input_text = \u0026#39;順調ですか？\u0026#39; response = translate.translate_text( Text=input_text, SourceLanguageCode=\u0026#39;ja\u0026#39;, TargetLanguageCode=\u0026#39;en\u0026#39; ) output_text = response.get(\u0026#39;TranslatedText\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;output_text\u0026#39;: output_text }) } 2. 翻訳 Web API を作る 先ほどの Lambda 関数を下記のように変更し、 API Gateway のイベントから翻訳対象の文字列を取得できるように変更しました。\n- input_text = \u0026#39;順調ですか？\u0026#39; + input_text = event[\u0026#39;queryStringParamaters][\u0026#39;input_text] マネジメントコンソール上で API Gateway と Lambda の繋ぎ込みをして、実際に URL に input_text パラメータを付与して翻訳結果が得られることを確認しました。\n3. 文字起こし + 翻訳パイプラインを作る 音声ファイルを S3 にアップロードし、それをトリガーにして Lambda 関数を実行するというパイプラインを作成しました。\n実行される Lambda 関数内では、AWS の文字起こしサービス Amazon Transcribe を使ってアップロードされた音声ファイルから文字起こしをして、その結果を S3 に出力するというパイプラインの作成です。ハンズオンで実施したのは文字起こしまでで、翻訳を含めたパイプラインについては宿題となっていました。後ほどこの部分はやってみます。\n宿題の内容 ここからは宿題となっていた 文字起こし + 翻訳 のパイプラインを作成してみます。\n作成するパイプラインは次のような構成です。\nS3 バケットの作成 ハンズオンの手順では S3 バケットを既に作っていましたが、あらためてその部分からやってみます。\n作成する S3 バケットは下記の 2 種類です。\n音声ファイルアップロード用 jugbgnr24-transcribe-input-michimani 文字起こし結果アウトプット用: jugbgnr24-transcribe-output-michimani 翻訳結果アウトプット用: jugbgnr24-translate-output-michimani 上の図では 文字起こし結果アウトプット用 と 翻訳結果アウトプット用 が同じバケットを指しているような形になっていますが、別のバケットとして作成します。\n$ aws s3 mb s3://jugbgnr24-transcribe-input-michimani make_bucket: jugbgnr24-transcribe-input-michimani $ aws s3 mb s3://jugbgnr24-transcribe-output-michimani make_bucket: jugbgnr24-transcribe-output-michimani $ aws s3 mb s3://jugbgnr24-translate-output-michimani make_bucket: jugbgnr24-translate-output-michimani $ aws s3 ls | grep jugbgnr24 2020-02-22 13:05:30 jugbgnr24-transcribe-input-michimani 2020-02-22 13:05:43 jugbgnr24-transcribe-output-michimani 2020-02-22 13:05:51 jugbgnr24-translate-output-michimani 文字起こし用 Lambda 関数の作成 続いて、文字起こし用の Lambda 関数を作成します。といってもこの部分に関してはハンズオン内で使用した Lambda 関数をほぼそのまま使えます。\nimport json import urllib.parse import boto3 import datetime s3 = boto3.client(\u0026#39;s3\u0026#39;) transcribe = boto3.client(\u0026#39;transcribe\u0026#39;) def lambda_handler(event, context): bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = urllib.parse.unquote_plus(event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;], encoding=\u0026#39;utf-8\u0026#39;) try: transcribe.start_transcription_job( TranscriptionJobName= datetime.datetime.now().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) + \u0026#39;_Transcription\u0026#39;, LanguageCode=\u0026#39;en-US\u0026#39;, Media={ \u0026#39;MediaFileUri\u0026#39;: \u0026#39;https://s3.ap-northeast-1.amazonaws.com/\u0026#39; + bucket + \u0026#39;/\u0026#39; + key }, OutputBucketName=\u0026#39;jugbgnr24-transcribe-output-michimani\u0026#39; ) except Exception as e: print(e) print(\u0026#39;Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.\u0026#39;.format(key, bucket)) raise e Lambda の実行ロールには S3 と Transcribe を操作できるポリシーをアタッチしておきます。\nハンズオンでは Lambda 関数作成時に設計図から作成する方法が紹介されていました、今回は 一から作成 を選択してしまったので、トリガーの設定をしておきます。\nここまではハンズオンの内容ですが、いったん処理を確認してみます。 音声ファイルは Amazon Polly のサンプルファイルを使用します。\nAmazon Polly（深層学習を使用したテキスト読み上げサービス）| AWS 今回使用する音声ファイルは、英語の音声で女性が話しているファイルで、長さは 4 秒ほどの音声です。\n$ aws s3 cp HelloEnglish-Joanna.mp3 s3://jugbgnr24-transcribe-input-michimani upload: ./HelloEnglish-Joanna.mp3 to s3://jugbgnr24-transcribe-input-michimani/HelloEnglish-Joanna.mp3 しばらく (1 分くらい) してから文字起こし結果アウトプット用のバケット jugbgnr24-transcribe-output-michimani を確認してみます。\n$ aws s3 ls s3://jugbgnr24-transcribe-output-michimani/ 2020-02-22 13:21:03 2 .write_access_check_file.temp 2020-02-22 13:21:55 1879 20200222042102_Transcription.json 20200222042102_Transcription.json というファイルが作成されています。 (.write_access_check_file.temp も作成されていますが、ここでは無視します)\n取得して中身を確認してみます。\n{ \u0026#34;jobName\u0026#34;: \u0026#34;20200222042102_Transcription\u0026#34;, \u0026#34;accountId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;results\u0026#34;: { \u0026#34;transcripts\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;Hello. Do you speak a foreign language? One language is never enough.\u0026#34; } ], \u0026#34;items\u0026#34;: [ { \u0026#34;start_time\u0026#34;: \u0026#34;0.04\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;0.65\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.9139\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;.\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;punctuation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;1.04\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;1.14\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Do\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;1.14\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;1.27\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;you\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;1.27\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;1.59\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;speak\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;1.59\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;1.65\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.9991\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;a\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;1.65\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;1.99\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;foreign\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;1.99\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;2.59\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;language\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;?\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;punctuation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;2.88\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;3.19\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.9944\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;One\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;3.19\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;3.61\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.991\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;language\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;3.61\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;3.75\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.991\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;is\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;3.75\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;4.03\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;never\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;start_time\u0026#34;: \u0026#34;4.03\u0026#34;, \u0026#34;end_time\u0026#34;: \u0026#34;4.48\u0026#34;, \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.9079\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;enough\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;pronunciation\u0026#34; }, { \u0026#34;alternatives\u0026#34;: [ { \u0026#34;confidence\u0026#34;: \u0026#34;0.0\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;.\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;punctuation\u0026#34; } ] }, \u0026#34;status\u0026#34;: \u0026#34;COMPLETED\u0026#34; } 色々とデータが入っていますが、 ['results']['transcripts'][0]['transcript'] の値として文字起こし結果の文字列が含まれていることがわかります。\n{ \u0026#34;results\u0026#34;: { \u0026#34;transcripts\u0026#34;: [ { \u0026#34;transcript\u0026#34;: \u0026#34;Hello. Do you speak a foreign language? One language is never enough.\u0026#34; } ], } } 実際の音声ファイルでもこのように話しているので、正常に文字起こしできているようです。\n翻訳用 Lambda 関数の作成 続いては、文字起こしした結果を翻訳する Lambda 関数を作成します。この関数は、文字起こし結果アウトプット用バケット jugbgnr24-transcribe-output-michimani にファイルが作成されたことをトリガーに実行され、作成されたファイル内の文字起こし結果を翻訳、そしてその結果を翻訳結果アウトプット用バケット jugbgnr24-translate-output-michimani に出力します。\nそれらを実現するために次のような Lambda 関数を作成します。\nimport json import urllib.parse import boto3 import datetime s3 = boto3.client(\u0026#39;s3\u0026#39;) translate = boto3.client(\u0026#39;translate\u0026#39;) def lambda_handler(event, context): bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = urllib.parse.unquote_plus(event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;], encoding=\u0026#39;utf-8\u0026#39;) try: # 文字起こし結果オブジェクト (json ファイル) を取得 transcribe_result_obj = s3.get_object(Bucket=bucket, Key=key) # json 内から文字起こし結果の文字列を取得 transcribe_result_json = json.loads(transcribe_result_obj[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)) input_text = transcribe_result_json[\u0026#39;results\u0026#39;][\u0026#39;transcripts\u0026#39;][0][\u0026#39;transcript\u0026#39;] # 英語から日本語に翻訳 response = translate.translate_text( Text=input_text, SourceLanguageCode=\u0026#39;en\u0026#39;, TargetLanguageCode=\u0026#39;ja\u0026#39; ) # 翻訳結果を S3 バケットに出力 out_bucket = \u0026#39;jugbgnr24-translate-output-michimani\u0026#39; out_key = datetime.datetime.now().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) + \u0026#39;_Translate.txt\u0026#39; out_body = response.get(\u0026#39;TranslatedText\u0026#39;) s3.put_object( Bucket=out_bucket, Key=out_key, Body=out_body ) except Exception as e: print(e) raise e Lambda の実行ロールには S3 と Translate を操作できるポリシーをアタッチしておきます。\nまた、先ほどと同様にトリガーの設定もしておきます。\n今回は簡易的に、翻訳結果をテキストファイルとして S3 バケットに出力するようにしています。\nでは、マネジメントコンソール上でテストイベントを作成し、テストしてみます。テンプレートは S3 の PUT イベントのテンプレートを使用します。\n{ \u0026#34;Records\u0026#34;: [ { \u0026#34;eventVersion\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;eventSource\u0026#34;: \u0026#34;aws:s3\u0026#34;, \u0026#34;awsRegion\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;eventTime\u0026#34;: \u0026#34;1970-01-01T00:00:00.000Z\u0026#34;, \u0026#34;eventName\u0026#34;: \u0026#34;ObjectCreated:Put\u0026#34;, \u0026#34;userIdentity\u0026#34;: { \u0026#34;principalId\u0026#34;: \u0026#34;EXAMPLE\u0026#34; }, \u0026#34;requestParameters\u0026#34;: { \u0026#34;sourceIPAddress\u0026#34;: \u0026#34;127.0.0.1\u0026#34; }, \u0026#34;responseElements\u0026#34;: { \u0026#34;x-amz-request-id\u0026#34;: \u0026#34;EXAMPLE123456789\u0026#34;, \u0026#34;x-amz-id-2\u0026#34;: \u0026#34;EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\u0026#34; }, \u0026#34;s3\u0026#34;: { \u0026#34;s3SchemaVersion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;configurationId\u0026#34;: \u0026#34;testConfigRule\u0026#34;, \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;jugbgnr24-transcribe-output-michimani\u0026#34;, \u0026#34;ownerIdentity\u0026#34;: { \u0026#34;principalId\u0026#34;: \u0026#34;EXAMPLE\u0026#34; }, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:s3:::example-bucket\u0026#34; }, \u0026#34;object\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;20200222042102_Transcription.json\u0026#34;, \u0026#34;size\u0026#34;: 1024, \u0026#34;eTag\u0026#34;: \u0026#34;0123456789abcdef0123456789abcdef\u0026#34;, \u0026#34;sequencer\u0026#34;: \u0026#34;0A1B2C3D4E5F678901\u0026#34; } } } ] } 対象の S3 バケットとキーを、先ほど作成された文字起こし結果のものに変更して、テストしてみます。エラーなく正常に実行が終わったら、翻訳結果アウトプット用バケット jugbgnr24-translate-output-michimani の中を確認してみます。\n$ aws s3 ls s3://jugbgnr24-translate-output-michimani 2020-02-22 13:55:35 101 20200222045534_Translate.txt テキストファイルが生成されているので、取得して中身を確認してみます。\n$ cat 20200222045534_Translate.txt こんにちは。 外国語を話せますか。 一つの言語では十分ではありません。 Hello. Do you speak a foreign language? One language is never enough. の翻訳結果が出力されました。\n文字起こしから通して確認してみる これでそれぞれのパートの処理が確認できたので、最後に音声ファイルのアップロードから翻訳結果のテキストファイル出力までの流れを確認してみます。\nその前に、対象の 3 つのバケットは空にしておきます。ローカルで適当に空のディレクトリを作って、そのディレクトリ内で下記のコマンドを実行しました。\n$ aws s3 sync . s3://jugbgnr24-transcribe-input-michimani --delete delete: s3://jugbgnr24-transcribe-input-michimani/HelloEnglish-Joanna.mp3 $ aws s3 sync . s3://jugbgnr24-transcribe-output-michimani --delete delete: s3://jugbgnr24-transcribe-output-michimani/.write_access_check_file.temp delete: s3://jugbgnr24-transcribe-output-michimani/20200222042102_Transcription.json $ aws s3 sync . s3://jugbgnr24-translate-output-michimani --delete delete: s3://jugbgnr24-translate-output-michimani/20200222045534_Translate.txt これで準備ができたので、音声ファイルを PUT してみます。\n$ aws s3 cp HelloEnglish-Joanna.mp3 s3://jugbgnr24-transcribe-input-michimani upload: ./HelloEnglish-Joanna.mp3 to s3://jugbgnr24-transcribe-input-michimani/HelloEnglish-Joanna.mp3 しばらく (1 分くらい) してから翻訳結果アウトプット用バケット jugbgnr24-translate-output-michimani を確認してみます。\naws s3 ls s3://jugbgnr24-translate-output-michimani 2020-02-22 14:09:24 101 20200222050923_Translate.txt テキストファイルが生成されています。取得して中身を確認してみます。\n$ cat 20200222050923_Translate.txt こんにちは。 外国語を話せますか。 一つの言語では十分ではありません。 成功してますね。\n念のため別の音声ファイルでも試してみたいと思います。今度は少し長い 33 秒の音声ファイルです。同じく Amazon Polly のサンプルページから取得します。\n$ aws s3 cp overview_joanna_news_2.mp3 s3://jugbgnr24-transcribe-input-michimani upload: ./overview_joanna_news_2.mp3 to s3://jugbgnr24-transcribe-input-michimani/overview_joanna_news_2.mp3 今回は 2 分ほど経ってからテキストファイルが出力されました。\n$ aws s3 ls s3://jugbgnr24-translate-output-michimani 2020-02-22 14:09:24 101 20200222050923_Translate.txt 2020-02-22 14:14:38 796 20200222051437_Translate.txt 取得して確認してみます。\n$ cat 20200222051437_Translate.txt アマゾン Polly は、テキストを音声のような生活に変えるサービスです。音声対応製品の全く新しいカテゴリを話して構築するアプリケーションを作成できます。 Amazon Polly は、標準的な TTS ボイスに加えて、ニューラルテキストからスピーチと TTS ボイスを使用可能にし、新しい機械学習アプローチを通じて画期的なスピーチ品質を向上させます。これにより、最も自然で人間のようなテキストからスピーチを顧客に提供します。Voiceが市場に出回っています。 ニューラルTTS技術は、ニュースナレーション、ユースケースに合わせたニュースキャスターの読書スタイルもサポートしています。 改行がないので見辛いですが、内容としは下記の内容になっていました。\nアマゾン Polly は、テキストを音声のような生活に変えるサービスです。音声対応製品の全く新しいカテゴリを話して構築するアプリケーションを作成できます。 Amazon Polly は、標準的な TTS ボイスに加えて、ニューラルテキストからスピーチと TTS ボイスを使用可能にし、新しい機械学習アプローチを通じて画期的なスピーチ品質を向上させます。これにより、最も自然で人間のようなテキストからスピーチを顧客に提供します。Voiceが市場に出回っています。 ニューラルTTS技術は、ニュースナレーション、ユースケースに合わせたニュースキャスターの読書スタイルもサポートしています。\nちゃんと翻訳されていますね。\nまとめ JAWS-UG 初心者支部#24 サーバレスハンズオン勉強会で宿題になっていた課題 文字起こし \u0026amp; 翻訳のパイプライン処理 を作成してみた話でした。\nサーバレス と聞くとどうしても API Gateway + Lambda のイメージが強く、 S3 をサーバレスの構成要素として考えることはほとんどありませんでした。ただ、今回のようにバケットへのオブジェクト作成をトリガーにした処理を実際に実装してみると、 S3 もサーバレスの処理の一部として普通に使えそうというか、ただのストレージとしてだけ使うのは勿体ないなという印象を持ちました。\n初心者支部のハンズオンということもあり、各マネージドサービスの基本的な部分を組み合わせた形で非常に作業もしやすく、また新たにできることが増えたなと感じることができました。\n去年からいろんなハンズオンに参加するようになりましたが、やはり実際に手を動かすのは大事だなと改めて感じたので、今後も積極的に現地でも、リモートでも参加していきたいと思います。\n",
    "permalink": "https://michimani.net/post/aws-jaws-ug-bgnr-24-serverless-quikc-hands-on/",
    "title": "JAWS-UG 初心者支部#24 サーバレスハンズオン勉強会の宿題をやってみた #jawsug_bgnr #jawsug"
  },
  {
    "contents": "CodeCommit リポジトリへのプルリクエストに対する承認について通知を受け取ることができるようになったので試してみました。\n後で気づきましたが、実際にやってみた中で approve のスペルを完全に間違っていました。恥ずかしい。\n目次 概要 やってみる 通知の設定 承認テンプレートの作成 プルリクエストの作成 プルリクエストの承認 プルリクエストのマージ まとめ 概要 以前に AWS の Code シリーズ (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) の各サービスから Amazon SNS (と AWS Chatbot(beta) ) に対して通知を送ることができるようになりましたが、今回は CodeCommit リポジトリへのプルリクエストに対する承認に関する通知にも対応したようです。\n詳細については下記の公式アナウンスを参照してください。\nYou can now receive notifications about pull request approvals in AWS CodeCommit 前回 CodeBuild で通知を試したときの話はこちらです。\n追記 2020/04/23 AWS Chatbot が一般利用可能 (GA) になりました。CLoudWatch Aram を Slack に通知してみた記事を書いたのでこちらも参考にしてみてください。\nやってみる さっそくやってみます。\nが、承認の通知の前に、プルリクエストの作成そのものに対する通知も設定してみます。\n通知の設定 通知を設定したいリポジトリの上部に表示されている 通知 から設定します。\n(いつ作成したかも忘れているリポジトリがあったのでそれを使います)\n設定項目もこれまでと同様に下記の項目です。\n通知名 詳細タイプ 通知をトリガーするイベント ターゲット このうち、 通知をトリガーするイベント に Pull request の項目で、とりあえず全てにチェックを入れておきます。\nまた、今回新たに追加された Approvals の項目にもチェックを入れておきます。\n今回ターゲットには、 CodeCommit からの通知イベントの json をそのまま Slack に通知する SNS トピックを指定しています。\n( そのまま というか、 通知イベント内の event['Records'][0]['Sns']['Message'] にあたる部分を返しています。)\nあとは Submit ボタンを押して通知設定は完了です。\n承認テンプレートの作成 今回は承認フローの通知を試すので、リポジトリに対して承認の設定をしておきます。\n下記のような承認テンプレートを作成し、ユーザ 1 人からの承認を必要とするようにします。\nプルリクエストの作成 では、このリポジトリに対してプルリクエストを作成してみます。\ndevelop ブランチから master ブランチへの PR を作成します。\nプルリクエストを作成すると、まもなくして Slack に通知が来ました。通知イベントの json は下記のような内容でした。\n{ \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;detailType\u0026#34;: \u0026#34;CodeCommit Pull Request State Change\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.codecommit\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2020-02-20T00:47:20Z\u0026#34;, \u0026#34;notificationRuleArn\u0026#34;: \u0026#34;arn:aws:codestar-notifications:ap-northeast-1:123456789012:notificationrule/4d66e120fe93497bccceXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;sourceReference\u0026#34;: \u0026#34;refs/heads/develop\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;isMerged\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;pullRequestStatus\u0026#34;: \u0026#34;Open\u0026#34;, \u0026#34;notificationBody\u0026#34;: \u0026#34;A pull request event occurred in the following AWS CodeCommit repository: qiita_demo. User: arn:aws:iam::123456789012:user/hoge. Event: Created. Pull request name: 6. Additional information: A pull request was created with the following ID: 6. The title of the pull request is: PR for apploving test. For more information, go to the AWS CodeCommit console https://ap-northeast-1.console.aws.amazon.com/codesuite/codecommit/repositories/qiita_demo/pull-requests/6?region=ap-northeast-1.\u0026#34;, \u0026#34;destinationReference\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;callerUserArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;pullRequestId\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;PR for apploving test\u0026#34;, \u0026#34;revisionId\u0026#34;: \u0026#34;5853753132e2cb33a83e7617af03c69e288d6bc9dddaXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;repositoryNames\u0026#34;: [ \u0026#34;qiita_demo\u0026#34; ], \u0026#34;destinationCommit\u0026#34;: \u0026#34;ebe4b9a5f6f53d00e74aXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;pullRequestCreated\u0026#34;, \u0026#34;sourceCommit\u0026#34;: \u0026#34;0a560861bae31d5d65a3XXXXXXXXXXXXXXXXXXXX\u0026#34; }, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:codecommit:ap-northeast-1:123456789012:qiita_demo\u0026#34; ], \u0026#34;additionalAttributes\u0026#34;: {} } ブランチ、コミットの情報などが入っています。\nプルリクエストの承認 ここで、別ユーザ (cc-applove-user-1) にてプルリクエストの承認をしてみます。\n承認すると、まもなく次のような通知イベントが届きました。\n{ \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;detailType\u0026#34;: \u0026#34;CodeCommit Pull Request State Change\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.codecommit\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2020-02-20T00:50:49Z\u0026#34;, \u0026#34;notificationRuleArn\u0026#34;: \u0026#34;arn:aws:codestar-notifications:ap-northeast-1:123456789012:notificationrule/4d66e120fe93497bccceXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;approvalStatus\u0026#34;: \u0026#34;APPROVE\u0026#34;, \u0026#34;sourceReference\u0026#34;: \u0026#34;refs/heads/develop\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;isMerged\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;pullRequestStatus\u0026#34;: \u0026#34;Open\u0026#34;, \u0026#34;notificationBody\u0026#34;: \u0026#34;A pull request event occurred in the following AWS CodeCommit repository: qiita_demo. User: arn:aws:iam::123456789012:user/cc-applove-user-1. Event: Updated. Pull request name: 6. Additional information: A user has changed their approval state for the pull request. State change: APPROVE. For more information, go to the AWS CodeCommit console https://ap-northeast-1.console.aws.amazon.com/codesuite/codecommit/repositories/qiita_demo/pull-requests/6?region=ap-northeast-1.\u0026#34;, \u0026#34;destinationReference\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;callerUserArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/cc-applove-user-1\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;pullRequestId\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;PR for apploving test\u0026#34;, \u0026#34;revisionId\u0026#34;: \u0026#34;5853753132e2cb33a83e7617af03c69e288d6bc9dddaXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;repositoryNames\u0026#34;: [ \u0026#34;qiita_demo\u0026#34; ], \u0026#34;destinationCommit\u0026#34;: \u0026#34;ebe4b9a5f6f53d00e74aXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;pullRequestApprovalStateChanged\u0026#34;, \u0026#34;sourceCommit\u0026#34;: \u0026#34;0a560861bae31d5d65a3XXXXXXXXXXXXXXXXXXXX\u0026#34; }, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:codecommit:ap-northeast-1:123456789012:qiita_demo\u0026#34; ], \u0026#34;additionalAttributes\u0026#34;: {} } detail.approvalStatus が APPROVE になっています。\n続いて、一旦この承認を取り消してみます。\n下記のような通知イベントが届きました。\n{ \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;detailType\u0026#34;: \u0026#34;CodeCommit Pull Request State Change\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.codecommit\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2020-02-20T00:52:34Z\u0026#34;, \u0026#34;notificationRuleArn\u0026#34;: \u0026#34;arn:aws:codestar-notifications:ap-northeast-1:123456789012:notificationrule/4d66e120fe93497bccceXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;approvalStatus\u0026#34;: \u0026#34;REVOKE\u0026#34;, \u0026#34;sourceReference\u0026#34;: \u0026#34;refs/heads/develop\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;isMerged\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;pullRequestStatus\u0026#34;: \u0026#34;Open\u0026#34;, \u0026#34;notificationBody\u0026#34;: \u0026#34;A pull request event occurred in the following AWS CodeCommit repository: qiita_demo. User: arn:aws:iam::123456789012:user/cc-applove-user-1. Event: Updated. Pull request name: 6. Additional information: A user has changed their approval state for the pull request. State change: REVOKE. For more information, go to the AWS CodeCommit console https://ap-northeast-1.console.aws.amazon.com/codesuite/codecommit/repositories/qiita_demo/pull-requests/6?region=ap-northeast-1.\u0026#34;, \u0026#34;destinationReference\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;callerUserArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/cc-applove-user-1\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;pullRequestId\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;PR for apploving test\u0026#34;, \u0026#34;revisionId\u0026#34;: \u0026#34;5853753132e2cb33a83e7617af03c69e288d6bc9dddaXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;repositoryNames\u0026#34;: [ \u0026#34;qiita_demo\u0026#34; ], \u0026#34;destinationCommit\u0026#34;: \u0026#34;ebe4b9a5f6f53d00e74aXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;pullRequestApprovalStateChanged\u0026#34;, \u0026#34;sourceCommit\u0026#34;: \u0026#34;0a560861bae31d5d65a3XXXXXXXXXXXXXXXXXXXX\u0026#34; }, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:codecommit:ap-northeast-1:123456789012:qiita_demo\u0026#34; ], \u0026#34;additionalAttributes\u0026#34;: {} } detail.approvalStatus が REVOKE になりました。\n再度承認して、最後にプルリクエストをマージします。\nちなみに、承認ルールを上書きした場合には下記のような通知イベントが届きます。\n{ \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;detailType\u0026#34;: \u0026#34;CodeCommit Pull Request State Change\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.codecommit\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2020-02-20T00:43:48Z\u0026#34;, \u0026#34;notificationRuleArn\u0026#34;: \u0026#34;arn:aws:codestar-notifications:ap-northeast-1:123456789012:notificationrule/4d66e120fe93497bccceXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;sourceReference\u0026#34;: \u0026#34;refs/heads/develop\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: \u0026#34;Thu Feb 20 00:30:17 UTC 2020\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;isMerged\u0026#34;: \u0026#34;False\u0026#34;, \u0026#34;pullRequestStatus\u0026#34;: \u0026#34;Open\u0026#34;, \u0026#34;notificationBody\u0026#34;: \u0026#34;A pull request event occurred in the following AWS CodeCommit repository: qiita_demo. User: arn:aws:iam::123456789012:user/cc-applove-user-1. Event: Updated. Pull request name: 5. Additional information: An override event has occurred for the approval rules for this pull request. Override status: OVERRIDE. For more information, go to the AWS CodeCommit console https://ap-northeast-1.console.aws.amazon.com/codesuite/codecommit/repositories/qiita_demo/pull-requests/5?region=ap-northeast-1.\u0026#34;, \u0026#34;destinationReference\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;callerUserArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/cc-applove-user-1\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;Thu Feb 20 00:30:17 UTC 2020\u0026#34;, \u0026#34;pullRequestId\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;PR for apploving test\u0026#34;, \u0026#34;revisionId\u0026#34;: \u0026#34;02a04395842c449148dc0f880d45ed75a04b637e4ae450e1810b5ec14bb7281e\u0026#34;, \u0026#34;repositoryNames\u0026#34;: [ \u0026#34;qiita_demo\u0026#34; ], \u0026#34;destinationCommit\u0026#34;: \u0026#34;ebe4b9a5f6f53d00e74aXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;pullRequestApprovalRuleOverridden\u0026#34;, \u0026#34;sourceCommit\u0026#34;: \u0026#34;0a560861bae31d5d65a3XXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;overrideStatus\u0026#34;: \u0026#34;OVERRIDE\u0026#34; }, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:codecommit:ap-northeast-1:123456789012:qiita_demo\u0026#34; ], \u0026#34;additionalAttributes\u0026#34;: {} } detail.overrideStatus が OVERRIDE になっています。\nプルリクエストのマージ 今回は 3way merge でマージしてみました。\nその際の通知イベントは下記のとおりです。\n{ \u0026#34;account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;detailType\u0026#34;: \u0026#34;CodeCommit Pull Request State Change\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.codecommit\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2020-02-20T00:55:40Z\u0026#34;, \u0026#34;notificationRuleArn\u0026#34;: \u0026#34;arn:aws:codestar-notifications:ap-northeast-1:123456789012:notificationrule/4d66e120fe93497bccceXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;sourceReference\u0026#34;: \u0026#34;refs/heads/develop\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: \u0026#34;Thu Feb 20 00:55:36 UTC 2020\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;isMerged\u0026#34;: \u0026#34;True\u0026#34;, \u0026#34;pullRequestStatus\u0026#34;: \u0026#34;Closed\u0026#34;, \u0026#34;notificationBody\u0026#34;: \u0026#34;A pull request event occurred in the following AWS CodeCommit repository: qiita_demo. User: arn:aws:iam::123456789012:user/hoge. Event: Updated. Pull request name: 6. Additional information: The pull request merge status has been updated. The status is merged. For more information, go to the AWS CodeCommit console https://ap-northeast-1.console.aws.amazon.com/codesuite/codecommit/repositories/qiita_demo/pull-requests/6?region=ap-northeast-1.\u0026#34;, \u0026#34;destinationReference\u0026#34;: \u0026#34;refs/heads/master\u0026#34;, \u0026#34;callerUserArn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/hoge\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;Thu Feb 20 00:47:08 UTC 2020\u0026#34;, \u0026#34;pullRequestId\u0026#34;: \u0026#34;6\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;PR for apploving test\u0026#34;, \u0026#34;revisionId\u0026#34;: \u0026#34;5853753132e2cb33a83e7617af03c69e288d6bc9dddaXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;repositoryNames\u0026#34;: [ \u0026#34;qiita_demo\u0026#34; ], \u0026#34;destinationCommit\u0026#34;: \u0026#34;ebe4b9a5f6f53d00e74aXXXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;pullRequestMergeStatusUpdated\u0026#34;, \u0026#34;mergeOption\u0026#34;: \u0026#34;THREE_WAY_MERGE\u0026#34;, \u0026#34;sourceCommit\u0026#34;: \u0026#34;0a560861bae31d5d65a3XXXXXXXXXXXXXXXXXXXX\u0026#34; }, \u0026#34;resources\u0026#34;: [ \u0026#34;arn:aws:codecommit:ap-northeast-1:123456789012:qiita_demo\u0026#34; ], \u0026#34;additionalAttributes\u0026#34;: {} } detail.isMerged が True 、 detail.pullRequestStatus が Closed 、 detail.mergeOption が THREE_WAY_MERGE になっています。\nまとめ CodeCommit リポジトリへのプルリクエストに対して通知を受け取ることができるようになったので試してみた話でした。\nどの通知イベントについても言えるのは detail.notificationBody に詳細なメッセージが入っているので、 Slack に通知する際にはこのメッセージをメインに流せば良さそうな気がします。これまではプルリクエストに対する承認を人間の手で誰かしらに通知していましたが、今回のアップデートで、開発がよりスムーズに進められるようになりそうです。(そもそも今までなんでなかったんでしょうね\u0026hellip;)\n前回の CodeBuild のときにも思いましたが、めちゃくちゃ簡単に通知の設定ができるのは良いですね。\n",
    "permalink": "https://michimani.net/post/aws-codecommit-pr-notification/",
    "title": "CodeCommit リポジトリへのプルリクエストの承認を SNS 経由で Slack に通知してみた"
  },
  {
    "contents": "DynamoDB のバックアップからテーブルをリストアする際に、他のリージョンにもリストアできるようになったみたいなので、実際に試してみました。\n目次 概要 やってみる 1. バックアップを作成する 2. リージョンを指定してリストアする リストア先のリージョンで確認する CLI でリストアしてみる まとめ 参考 概要 DynamoDB テーブルのバックアップからリストア (復元) する際に、元のテーブルと別のリージョンにリストアできるようになったようです。今回は、実際に 東京 (ap-northeast-1) で取得したバックアップを バージニア北部 (us-east-1) リージョンへリストアしてみました。\nAWS 公式のアナウンスは下記記事を参照してください。\nYou can now restore Amazon DynamoDB table backups as new tables in other AWS Regions やってみる 手順としては下記の通りです。\nバックアップを作成する リージョンを指定して復元する バックアップの作成については、普段からバックアップ取得している場合は不要です。\n1. バックアップを作成する 今回は 東京 (ap-northeast-1) リージョンにある Thread テーブルを対象にします。バックアップが存在しないので、まずは手動でバックアップを作成します。\nバックアップが作成されました。\n今回はオンデマンドバックアップからの復元を試しますが、ポイントインタイムリカバリ (PITR) からの復元については AWS のブログ記事を参照してください。\nRestore Amazon DynamoDB backups to different AWS Regions and with custom table settings | AWS Database Blog 2. リージョンを指定してリストアする 続いて、作成したバックアップからリストアします。\n先ほど作成したバックアップを選択して バックアップの復元 ボタンを押します。\nすると、 クロスリージョンの復元 という項目があるので、ここでリストア先のリージョンを選択します。\n後の項目は適宜変更してください。\n最後に テーブルの復元 ボタンを押すと、リストアが開始されます。\nリストア先のリージョンで確認する 復元ボタンを押した直後は、リストア先のリージョンではステータスが リストア中 となっています。\nリストアには最大で数時間かかると書かれていますが、今回は 5 分ほどでステータスが 有効 になりました。\nCLI でリストアしてみる 同じことを AWS CLI でやってみます。\nAWS CLI のバージョンは、最新の v2.0.0 です。\n$ aws --version aws-cli/2.0.0 Python/3.7.4 Darwin/18.7.0 botocore/2.0.0dev4 コマンドとしては DynamoDB の restore-table-from-backup です。\n$ aws dynamodb restore-table-from-backup \\ --target-table-name ThreadFromBk \\ --backup-arn arn:aws:dynamodb:ap-northeast-1:123456789012:table/Thread/backup/01582064716226-XXXXXXXX \\ --region us-east-1 An error occurred (ValidationException) when calling the RestoreTableFromBackup operation: Invalid Request: sseSpecificationOverride must be provided for cross-region restores エラーになりました。\nどうやらクロスリージョンリストアには SSESpecificationOverride が必要なようです。\nただ、 v2.0.0 では restore-table-from-backup のオプションとして SSESpecificationOverride を渡すオプションがありません。 --generate-cli-skeleton オプションで確認してみても項目が見当たりません。\n$ aws dynamodb restore-table-from-backup --generate-cli-skeleton { \u0026#34;TargetTableName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;BackupArn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;BillingModeOverride\u0026#34;: \u0026#34;PAY_PER_REQUEST\u0026#34;, \u0026#34;GlobalSecondaryIndexOverride\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;INCLUDE\u0026#34;, \u0026#34;NonKeyAttributes\u0026#34;: [ \u0026#34;\u0026#34; ] }, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 0, \u0026#34;WriteCapacityUnits\u0026#34;: 0 } } ], \u0026#34;LocalSecondaryIndexOverride\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;INCLUDE\u0026#34;, \u0026#34;NonKeyAttributes\u0026#34;: [ \u0026#34;\u0026#34; ] } } ], \u0026#34;ProvisionedThroughputOverride\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 0, \u0026#34;WriteCapacityUnits\u0026#34;: 0 } } ということで、 v1 の最新版である v1.18.2 で試してみます。\nv1 と v2 の共用については、下記の記事を参考に環境を作りました。\nvenvを使ってAWS CLI（v1）とAWS CLI v2を使い分ける（Mac, Linux編） ｜ Developers.IO (aws-cli-v1) $ aws --version aws-cli/1.18.2 Python/3.7.5 Darwin/18.7.0 botocore/1.15.2 この環境で --generate-cli-skeleton オプションで確認してみると SSESpecificationOverride の項目が存在しています。\n(aws-cli-v1) $ aws dynamodb restore-table-from-backup --generate-cli-skeleton { \u0026#34;TargetTableName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;BackupArn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;BillingModeOverride\u0026#34;: \u0026#34;PROVISIONED\u0026#34;, \u0026#34;GlobalSecondaryIndexOverride\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;KEYS_ONLY\u0026#34;, \u0026#34;NonKeyAttributes\u0026#34;: [ \u0026#34;\u0026#34; ] }, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 0, \u0026#34;WriteCapacityUnits\u0026#34;: 0 } } ], \u0026#34;LocalSecondaryIndexOverride\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;INCLUDE\u0026#34;, \u0026#34;NonKeyAttributes\u0026#34;: [ \u0026#34;\u0026#34; ] } } ], \u0026#34;ProvisionedThroughputOverride\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 0, \u0026#34;WriteCapacityUnits\u0026#34;: 0 }, \u0026#34;SSESpecificationOverride\u0026#34;: { \u0026#34;Enabled\u0026#34;: true, \u0026#34;SSEType\u0026#34;: \u0026#34;KMS\u0026#34;, \u0026#34;KMSMasterKeyId\u0026#34;: \u0026#34;\u0026#34; } } help で確認してみても、 --sse-specification-override の説明があります。\n(aws-cli-v1) $ aws dynamodb restore-table-from-backup help ... OPTIONS ... --sse-specification-override (structure) The new server-side encryption settings for the restored table. Shorthand Syntax: Enabled=boolean,SSEType=string,KMSMasterKeyId=string JSON Syntax: { \u0026#34;Enabled\u0026#34;: true|false, \u0026#34;SSEType\u0026#34;: \u0026#34;AES256\u0026#34;|\u0026#34;KMS\u0026#34;, \u0026#34;KMSMasterKeyId\u0026#34;: \u0026#34;string\u0026#34; } では、先ほどのコマンドを実行してみます。\n(aws-cli-v1) $ aws dynamodb restore-table-from-backup \\ --target-table-name ThreadFromBk \\ --backup-arn arn:aws:dynamodb:ap-northeast-1:123456789012:table/Thread/backup/01582064716226-XXXXXXXX \\ --sse-specification-override Enabled=false \\ --region us-east-1 { \u0026#34;TableDescription\u0026#34;: { \u0026#34;AttributeDefinitions\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;ForumName\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;Subject\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], \u0026#34;TableName\u0026#34;: \u0026#34;ThreadFromBk\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;ForumName\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;Subject\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;TableStatus\u0026#34;: \u0026#34;CREATING\u0026#34;, \u0026#34;CreationDateTime\u0026#34;: 1582071746.299, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 }, \u0026#34;TableSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;TableArn\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-1:123456789012:table/ThreadFromBk\u0026#34;, \u0026#34;TableId\u0026#34;: \u0026#34;bd27f2d1-61c3-41e4-b91a-XXXXXXXXXXXX\u0026#34;, \u0026#34;BillingModeSummary\u0026#34;: { \u0026#34;BillingMode\u0026#34;: \u0026#34;PROVISIONED\u0026#34; }, \u0026#34;RestoreSummary\u0026#34;: { \u0026#34;SourceBackupArn\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-northeast-1:123456789012:table/Thread/backup/01582064716226-XXXXXXXX\u0026#34;, \u0026#34;SourceTableArn\u0026#34;: \u0026#34;arn:aws:dynamodb:ap-northeast-1:123456789012:table/Thread\u0026#34;, \u0026#34;RestoreDateTime\u0026#34;: 1582064716.226, \u0026#34;RestoreInProgress\u0026#34;: true } } } (aws-cli-v1) $ aws dynamodb list-tables --region us-east-1 { \u0026#34;TableNames\u0026#34;: [ \u0026#34;ThreadFromBk\u0026#34; ] } リストアが正常に実行され、バージニア北部 (us-east-1) リージョンにテーブルが作成されました。\nCLI から実行する場合は、今 (2020/02/19) のところ v1 でのみ可能なようです。\nまとめ DynamoDB のバックアップからテーブルをリストアする際に、他のリージョンにもリストアできるようになったみたいなので、実際に試してみた話でした。\nリージョンを跨いだリストアと聞くと面倒な印象がありましが、非常に簡単にリストアすることができました。\n今回はマネジメントコンソールから実行しましたが、 CLI からも実行できるようなので、そちらもあらためて試してみようと思います。 CLI でのクロスリージョンリストアについては、現時点 (2020/02/19) では v1 でのみ実行可能なようですが、問題なく実行できました。\n参考 You can now restore Amazon DynamoDB table backups as new tables in other AWS Regions Restore Amazon DynamoDB backups to different AWS Regions and with custom table settings | AWS Database Blog ",
    "permalink": "https://michimani.net/post/aws-restore-dynamodb-table-to-other-region/",
    "title": "DynamoDB でバックアップから別リージョンへのリストアができるようになったみたいなので試してみた"
  },
  {
    "contents": "API Gateway で作成した API の認証方法として API キーを使用する場合に、簡単に API キーを生成できるようなシェルスクリプトを作ってみました。\n目次 概要 詳細 使用料プランの作成 API キーの生成と使用量プランへの関連付け まとめ 概要 既に API Gateway で API が作成されていることを前提とします。そして、シェルスクリプトで実現するのは、下記の内容です。\n使用量プランの作成 API キーの生成と使用量プランへの関連付け 使用量プランの作成 については CloudFormation テンプレートを使用して作成します。\nAPI キーの生成と使用量プランへの関連付け はシェルスクリプトを使って実現します。\n今回紹介するシェルスクリプト内では、 AWS CLI のコマンドを実行しているので、 AWS CLI のインストールおよびクレデンシャルの設定は済んでいることを前提としています。動作確認した AWS CLI のバージョンは、 1.17.7 および 2.0.0 です。\nまた、今回紹介するシェルスクリプト内では jq コマンドを使用しているので、それもインストールされていることを前提とします。\n詳細 では、それぞれのシェルスクリプト、および CloudFormation テンプレートをみていきます。\n使用料プランの作成 使用量プランの作成には、下記のような CloudFormation テンプレートを使用します。ファイル名は api-usage-plan.yml としておきます。\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;API Gateway - usage plan template.\u0026#34; Parameters: TargetApiId: Type: String Resources: UsagePlan: Type: \u0026#34;AWS::ApiGateway::UsagePlan\u0026#34; Properties: ApiStages: - ApiId: !Ref TargetApiId Stage: api Description: Usage plan for API UsagePlanName: ApiUsagePlan Outputs: UsagePlanID: Description: ID of created usage plan Value: !Ref UsagePlan このテンプレートのデプロイには、下記のようなスクリプトを使用します。ファイル名は 01-api-usage-plan.sh として、上記のテンプレートと同じディレクトリに配置します。\n#!/bin/bash CHANGESET_OPTION=\u0026#34;--no-execute-changeset\u0026#34; if [ $# = 1 ] \u0026amp;\u0026amp; [ $1 = \u0026#34;deploy\u0026#34; ]; then echo \u0026#34;deploy mode\u0026#34; CHANGESET_OPTION=\u0026#34;\u0026#34; fi CFN_TEMPLATE=\u0026#34;$(dirname $0)/api-usage-plan.yml\u0026#34; CFN_STACK_NAME=ApiUsagePlan aws cloudformation deploy --stack-name ${CFN_STACK_NAME} --template-file ${CFN_TEMPLATE} --parameter-overrides ChaliceDeployedApi=${TARGET_API_ID} ${CHANGESET_OPTION} if [ $# = 1 ] \u0026amp;\u0026amp; [ $1 = \u0026#34;deploy\u0026#34; ]; then echo \u0026#34;\\n----------------------------------------------\\n\u0026#34; echo \u0026#34;Following string is ID of created usage plan. Use it for creating api key belongs to an usage plan.\\n\u0026#34; aws cloudformation describe-stacks --stack-name ${CFN_STACK_NAME} | jq \u0026#34;.Stacks[0].Outputs[0].OutputValue\u0026#34; | sed -E \u0026#34;s/\\\u0026#34;//g\u0026#34; fi TARGET_API_ID には事前に対象の API ID を環境変数として設定しておきます。\n$ export TARGET_API_ID=\u0026lt;your-target-api-id\u0026gt; そして、下記のコマンドでテンプレートをデプロイします。\n$ sh ./01-api-usage-plan.sh deploy deploy mode Waiting for changeset to be created.. Waiting for stack create/update to complete Successfully created/updated stack - ApiUsagePlan ---------------------------------------------- Following string is ID of created usage plan. Use it for creating api key belongs to an usage plan. abc123 デプロイに成功すると、作成された使用量プランの ID (abc123) が出力されます。\nちなみに、テンプレートをデプロイするためのこのシェルスクリプトは、以前に下記のセッションで紹介されており、良さそうだなと思ったので今回使ってみました。\nCloudFormationの全てを味わいつくせ！「AWSの全てをコードで管理する方法〜その理想と現実〜」 #cmdevio ｜ Developers.IO コマンドの引数に deploy の文字列を渡していますが、これを渡さなかった場合は CloudFormation の change set のみが作成され、実際にリソースは作成されません。\nA CloudFormation template for creating API Gateway usage plan. API キーの生成と使用量プランへの関連付け API キーの生成と使用量プランへの関連付けは、次のような流れになります。\nAPI キーを生成する 生成した API キーを使用量プランに関連づける ステップが 2 段階あるので、手作業だと面倒です。これを下記の 1 つのスクリプトでやってしまいます。ファイル名は generate_api_key.sh とします。\n#!/bin/bash if [ $# != 2 ] || [ $1 = \u0026#34;\u0026#34; ] || [ $2 = \u0026#34;\u0026#34; ]; then echo \u0026#34;Two parameters are required\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;1st : string for API Key name (ex. user1)\u0026#34; echo \u0026#34;2nd : string for Usage Plan ID (ex. abc123)\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;example command\u0026#34; echo \u0026#34;\\t$ sh ./generate_api_key.sh user1 abc123\u0026#34; exit fi API_KEY_NAME=$1 USAGE_PLAN_ID=$2 # create api key CREATE_RES=$(aws apigateway create-api-key --name $API_KEY_NAME --enabled) CREATED_API_KEY_ID=$(echo $CREATE_RES | jq \u0026#34;.id\u0026#34; | sed -E \u0026#34;s/\\\u0026#34;//g\u0026#34;) CREATED_API_KEY_VALUE=$(echo $CREATE_RES | jq \u0026#34;.value\u0026#34; | sed -E \u0026#34;s/\\\u0026#34;//g\u0026#34;) # add api key to usage plan ADD_RES=$(aws apigateway create-usage-plan-key --usage-plan-id $USAGE_PLAN_ID --key-id $CREATED_API_KEY_ID --key-type API_KEY) ADDED_KEY_ID=$(echo $ADD_RES | jq \u0026#34;.id\u0026#34; | sed -E \u0026#34;s/\\\u0026#34;//g\u0026#34;) if [ $ADDED_KEY_ID != $CREATED_API_KEY_ID ]; then echo \u0026#34;Failed to generate a API Key\u0026#34; exit fi echo \u0026#34;API Key generated successfully.\u0026#34; echo $CREATE_RES | jq \u0026#34;.\u0026#34; スクリプト内にも説明を書いていますが、次のように使用します。\n$ sh ./generate_api_key.sh api-user-1 abc123 API Key generated successfully. { \u0026#34;id\u0026#34;: \u0026#34;aa11bb2222\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5pdADt75RU24blTbviyZq8o1XjmR9rwB9NlRggnx\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;api-user-1\u0026#34;, \u0026#34;enabled\u0026#34;: true, \u0026#34;createdDate\u0026#34;: \u0026#34;2020-02-17T20:45:03+09:00\u0026#34;, \u0026#34;lastUpdatedDate\u0026#34;: \u0026#34;2020-02-17T20:45:03+09:00\u0026#34;, \u0026#34;stageKeys\u0026#34;: [] } 成功すると、生成された API キーの情報が出力されます。\nShell script that generate API key of Amazon API Gateway まとめ API Gateway で作成した API の認証方法として API キーを使用する場合に、簡単に API キーを生成できるようなシェルスクリプトを作ってみた話でした。\nシェルスクリプト内では AWS CLI のコマンドを実行しているだけですが、ひとつのスクリプトにしておくことで手作業の手間が省けるのは大きいですね。また、諸々の操作をコマンド化しておくことで、もしその操作を CI/CD に組み込むことになった際にも、比較的簡単に導入ができそうです。\nCloudFormation 楽しいおじさんになりつつあるとともに、 AWS CLI 楽しいおじさんにもなりつつあります。\n",
    "permalink": "https://michimani.net/post/aws-generate-apigateway-api-key/",
    "title": "AWS CLI で API Gateway の API キーを生成するシェルスクリプトを作ってみた"
  },
  {
    "contents": "ここ 2 週間くらいで CloudFormation のテンプレートをいくつか書く機会があり、だんだんと CloudFormation 楽しいおじさんになりつつあります。\n今回は、 API Gateway のカステムドメインを CloudFormation のテンプレートで設定してみた時の話です。\n目次 前提 API Gateway でのカスタムドメイン設定 CloudFormation でさくっとやる ハマったポイント まとめ 前提 API Gateway のカスタムドメインを CloudFormation \u0008を使って設定します。\nAPI 自体は既に存在しており、その API に対してカステムドメインを設定します。\nまた、今回は下記の状態を前提とします。\nカスタムドメインとして使用するドメインが Route 53 にてホストゾーンとして作成されている カスタムドメインとして使用するドメイン (サブドメイン) に対する SSL 証明書が ACM にて発行されている API Gateway でのカスタムドメイン設定 まずは手でマネジメントコンソールをぽちぽちしてやる場合を想定して、 API Gateway でのカスタムドメイン設定の手順をおさらいしておきます。\nカスタムドメイン名作成 API Gateway のコンソールで左のメニューにある カスタムドメイン名 (Custom domain names) からカスタムドメイン名を作成します。\n項目を埋めて 保存 を押すと、 SSL 証明書の初期化が始まります。この処理には最大で 40 分ほどかかります。\nベースパスマッピングを追加 カスタムドメイン名を保存したら、続いては ベースパスマッピング を追加します。これは何かというと、作成したカスタムドメイン名と、 API Gateway の API を紐づける設定のことです。\n項目は パス 、 送信先 と ステージ があります。\n例えば、 API Gateway の生の URL が https://aabbcc123456.execute-api.us-east-1.amazonaws.com/prod だとして、カスタムドメインを適用後に https://api.example.com/ でアクセスしたい場合は、それぞれを下記のように設定します。\nパス : / 送信先 API : API ID が aabbcc123456 の API ステージ : prod Route 53 にレコードセットを追加 API Gateway 側でカスタムドメイン名の作成とベースパスマッピングの追加が終わったら、最後に Route 53 にレコードセットを追加します。\n先ほどの例の通り api.example.com を使用したい場合は、ドメイン名 api.example.com の A レコード (エイリアス) を作成します。エイリアスのターゲットは、先ほど作成したカスタムドメインの ターゲットドメイン名 にです。これは API Gateway のコンソールでも確認できますが、 Route 53 のエイリアスのターゲット候補にカスタムドメイン名とともに出てくるので、そこから選択すれば OK です。ただし、カスタムドメイン名を作成してすぐには出てこないので、出てこない場合は出てくるまで待ちます。\nCloudFormation でさくっとやる 上記でやった手順を CloudFormtaion でさくっとやってしまいます。\n早速ですが、実行するテンプレートは下記の通りです。\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: \u0026#34;Setup API Gateway custom domain name\u0026#34; Parameters: AcmArn: Type: String CustomDomainName: Type: String ApiID: Type: String DomainHostZoneId: Type: String Resources: ApiGatewayCustomDomainName: Type: \u0026#34;AWS::ApiGateway::DomainName\u0026#34; Properties: CertificateArn: !Ref AcmArn DomainName: !Ref CustomDomainName BasePathMapping: Type: \u0026#34;AWS::ApiGateway::BasePathMapping\u0026#34; DependsOn: ApiGatewayCustomDomainName Properties: DomainName: !Ref CustomDomainName RestApiId: !Ref ApiID Stage: api CustomDomainRecord: Type: \u0026#34;AWS::Route53::RecordSet\u0026#34; Properties: Name: !Ref CustomDomainName Type: A HostedZoneId: !Ref DomainHostZoneId AliasTarget: DNSName: !GetAtt ApiGatewayCustomDomainName.DistributionDomainName HostedZoneId: !GetAtt ApiGatewayCustomDomainName.DistributionHostedZoneId リソースを 3 つ定義していますが、それぞれが上で説明した手順と対応しています。また、 Parameters の各値については環境変数など、外から設定することを想定しています。\n実際にこのテンプレートを実行してリソースが生成されるまでには 5 分程度の時間がかかります。\nとは言っても手でぽちぽちやるより圧倒的に楽なので、やっぱり CloudFormation 良いなっていう感じです。\nハマったポイント 上で紹介したテンプレートは無事にリソースが作成された時のテンプレートですが、これに至るまでに AWS::Route53::RecordSet AliasTarget 部分の記述でハマったポイントがあったので紹介しておきます。基本的にリファレンスをちゃんと読めば解決する問題でした。\nまず、一番最初に書いていたのはこんなテンプレートでした。\nCustomDomainRecord: Type: \u0026#34;AWS::Route53::RecordSet\u0026#34; Properties: Name: !Ref CustomDomainName Type: A HostedZoneId: !Ref DomainHostZoneId AliasTarget: DNSName: !Ref ApiGatewayCustomDomainName HostedZoneId: !Ref DomainHostZoneId AliasTarget.DNSName には上で作成したカスタムドメイン名のリソースを参照するようにしています。無茶してますね。\nAliasTarget.HostedZoneId には、設定するドメインのホストゾーン ID を指定しています。これはなんか良さそうな気もします。\nではこのテンプレートを実行してどうなったでしょうか。\n\u0026ldquo;creates a CNAME or alias loop in the zone.\u0026rdquo; まず、 AliasTarget.DNSName の設定ミスでスタックのエラーになりました。(適宜、改行しています)\n[Tried to create an alias that targets api.example.net., type A in zone ZZ88889999AAAA, but that target was not found, RRSet of type A with DNS name api.example.net. is not permitted as it creates a CNAME or alias loop in the zone.] 理由としては、 DNS の名前解決がループしてしまっていたからです。冷静に考えると、 ApiGatewayCustomDomainName のリソースはカスタムドメイン名なので、それを DNS のターゲットとして指定するとループするのは容易にわかります。\nというこ実行しました。\nCustomDomainRecord: Type: \u0026#34;AWS::Route53::RecordSet\u0026#34; Properties: Name: !Ref CustomDomainName Type: A HostedZoneId: !Ref DomainHostZoneId AliasTarget: DNSName: !GetAtt ApiGatewayCustomDomainName.DistributionDomainName HostedZoneId: !Ref DomainHostZoneId ホストゾーン内にターゲットのドメインが存在しない 今度は別のエラーになりました。(適宜、改行しています)\n[Tried to create an alias that targets d11234efghabcd.cloudfront.net., type A in zone ZZ88889999AAAA, but the alias target name does not lie within the target zone, Tried to create an alias that targets d11234efghabcd.cloudfront.net., type A in zone ZZ88889999AAAA, but that target was not found] 理由としては、ターゲットドメイン名が指定された Hosted Zone に存在しないというものでした。\n答えは下記のように公式リファレンスにも書かれていました。\n設定ファイルで AWS リソースに使用している HostedZoneId の値が正しくありません。HostedZoneId キーの値は、各リージョンの AWS リソースの一意な ID であり、ドメイン名のホストゾーン ID ではないことに注意してください。\nAWS CLI を使用して Amazon Route 53 のリソースレコードセットを作成する際に発生するエラーのトラブルシューティング 上の間違ったテンプレートでは HostedZoneId に HostZoneId を指定しています。よく見ると全然違いますよね。 ホステッド なのか ホスト なのか、全然違います。\nここで指定すべきなのは、作成されたカスタムドメイン名が持っているターゲットドメイン名がホストされている ゾーンの ID です。Route 53 で管理しているドメインの ゾーン ID とは違うわけです。(それはそう)\nというこ正解でした。\nCustomDomainRecord: Type: \u0026#34;AWS::Route53::RecordSet\u0026#34; Properties: Name: !Ref CustomDomainName Type: A HostedZoneId: !Ref DomainHostZoneId AliasTarget: DNSName: !GetAtt ApiGatewayCustomDomainName.DistributionDomainName HostedZoneId: !GetAtt ApiGatewayCustomDomainName.DistributionHostedZoneId まとめ API Gateway のカスタムドメインを CloudFormation \u0008を使って設定した話でした。\nCloudFormation 初心者で設定値のところでいろいろ失敗しましたが、やっぱり最終的に自分の考えているリソースが出来上がるのは最高に楽しいですね。これまでは CDK を使って CloudFormation テンプレートを実行してたのですが、これを機に生のテンプレートを書くこともチャレンジしていこうと思います。\nその際、 VS Code のプラグインを入れることでテンプレートの記述がめちゃくちゃ楽になるので、もしこれから CFn テンプレート書こうと思っている方でまだプラグインをいれていない方は、今すぐに入れましょう！\nvscode-cfn-lint - Visual Studio Marketplace awslabs/aws-cfn-lint-visual-studio-code: Provides IDE specific integration to cfn-lint. https://github.com/awslabs/cfn-python-lint Linterを使ってCloudFormationの間違いに爆速で気づく ｜ Developers.IO ",
    "permalink": "https://michimani.net/post/aws-setup-apigateway-custom-domain-using-cloudformation/",
    "title": "API Gateway のカスタムドメインを CloudFormation で設定してみた"
  },
  {
    "contents": "Hugo で作ったブログの記事下に、その記事と同じカテゴリの記事一覧を表示します。WordPress とか はてなブログ とかだとウィジェットで簡単に設置できるようになってますが、静的サイトの場合は少しだけ工夫が必要です。\n目次 前提 やること 現在の記事のカテゴリ名を取得する 対象のカテゴリ内の記事一覧を取得する 記事一覧を表示させる 最終的にどうなったのか まとめ 前提 Hugo 0.62.0 Taxonomies を使ってカテゴリ管理している やること 各テーマディレクトリ内にある layouts/_default/single.html に修正を加えます。\n今回は indigo のテーマで実装するので、対象のファイルは themes/indigo/layouts/_default/single.html となります。\n現在の記事のカテゴリ名を取得する 同一カテゴリの記事を表示するために、現在の記事のカテゴリ名を取得する必要があります。\nHugo では下記の記述により、現在の記事のカテゴリ名を取得できます。\n{{ .Params.categories }} ただし、これではカテゴリ名が配列で取得されるので、下記の記述を用いて一番最初のカテゴリ名を取得します。\n{{ index .Params.categories 0 }} また、カテゴリ名は後ほど使用するので、次のようにして変数として保持しておきます。\n{{ $c := index .Params.categories 0 }} 対象のカテゴリ内の記事一覧を取得する 続いてカテゴリ内の記事一覧を取得します。\nカテゴリ名は先ほど取得して $c という変数に保持しているので、それを利用して下記のような記述で取得できます。\n{{ index .Site.Taxonomies.categories (lower $c) }} .Site.Taxonomies.categories は dict 型になっていて、カテゴリ名をキーとして値に記事の配列を持っています。例えば aws というカテゴリであれば .Site.Taxonomies.categories.aws とすることで aws カテゴリの記事一覧を取得できます。\nlower $c としているのは、 .Site.Taxonomies.categories のキーが小文字になっているからです。前項で取得したカテゴリ名は大文字を含んでいる可能性があるため、この記述で小文字に変換しています。\nlower | Hugo 記事一覧についても後ほど複数回使用することになるため、変数に保持しておきます。\n{{ $cposts := index .Site.Taxonomies.categories (lower $c) }} 記事一覧を表示させる 最後に、取得した同一カテゴリの記事一覧を表示させます。\nHugo では range 関数を使うことで配列の中身を取り出すことができるので、下記のようにして記事一覧を表示させます。\n{{ range $cposts }} {{ .Render \u0026#34;li\u0026#34; }} {{ end }} {{ .Render \u0026quot;li\u0026quot; }} の部分で各記事の情報を themes/indigo/layouts/_default/li.html に渡しています。この li.html というのはトップページの記事一覧にも使われているテンプレートなので、この一覧表示のためだけに新たにテンプレートを作成すればトップの一覧とは別のデザインで表示することも可能です。\nまた、上記の表記では同一カテゴリ内の記事が全て一覧で表示されてしまうので、下記のようにして表示件数を制限することも可能です。\n{{ range first 5 $cposts }} {{ .Render \u0026#34;li\u0026#34; }} {{ end }} これで 5 件だけ表示されます。\n最終的にどうなったのか ここまでで同一カテゴリ内の記事一覧表示はできるようになりました。最後に、実際にこのブログで表示させるためにどうしているか、例として載せておきます。\n{{ if and (.Params.categories) }} {{ $c := index .Params.categories 0 }} \u0026lt;div class=\u0026#34;same-category-posts-area\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Posts in \u0026#34;{{ $c }}\u0026#34; category\u0026lt;/h2\u0026gt; {{ $cposts := index .Site.Taxonomies.categories (lower $c) }} {{ range $cposts }} {{ .Render \u0026#34;li\u0026#34; }} {{ end }} {{ if gt (len $cposts) 5 }} \u0026lt;a class=\u0026#34;more-btn\u0026#34; href=\u0026#34;/categories/{{ lower $c }}/\u0026#34;\u0026gt;more ...\u0026lt;/a\u0026gt; {{ else }} \u0026lt;a class=\u0026#34;more-btn\u0026#34; href=\u0026#34;/categories/\u0026#34;\u0026gt;other categories\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} まず最初の if 文 {{ if and (.Params.categories) }} では、そもそも現在の記事にカテゴリが設定されているかを確認してます。設定されていれば表示するし、なければ表示してません。\nあとは、 同一カテゴリ内の記事が 5 件以上あればカテゴリ別記事一覧ページへのリンクを、 5 件より少なければカテゴリ一覧ページへのリンクを、それぞれ表示するようにしています。\nまとめ Hugo で作ったブログの記事下に、その記事と同じカテゴリの記事一覧を表示してみた話でした。\nこういったウィジェット (のようなもの) は動的に生成しているイメージが強く、静的サイトで実現するのは難しいかなと思っていたのですが、少し工夫することで実現できました。今回紹介したように Hugo には様々な関数が用意されているので、それらを組み合わせることでさらにリッチなコンテンツを表示できるようになるかもしれません。\n",
    "permalink": "https://michimani.net/post/development-show-other-posts-of-same-category-on-hugo/",
    "title": "[Hugo] 記事下に同じカテゴリの記事一覧を表示する"
  },
  {
    "contents": "JAWS DAYS 以外の JAWS UG イベントに初めて行ってきました。いろんな支部があるようなのですが、まずは初心者支部からということで考えていたのですが、毎回すぐに定員いっぱいになってしまっていたので、今回やっと参加することができました。\n今回は Fin-JAWS とのコラボということで、金融系における AWS の事情もお話を聞くことができました。\nJAWS-UG 初心者支部#22 Fin-JAWSコラボ＆ミニハンズオン会[リモート枠あり] - connpass 目次 タイムテーブル 各セッションの概要 会場諸注意/初心者支部とは/テーマ説明 | 初心者支部運営 澤田さん/太田さん ミニハンズオン：AWSアカウントを作ったら最初にやるべきこと-セキュリティ編- 初心者支部運営 迫さん セッション①：Fin-JAWSの紹介と金融の現状 Fin-JAWS支部運営 釜山さん セッション②：データはどこからくるの？〜AWSとオンプレとの違いで学んだあれやこれ〜 | 大熊さん セッション③：FISCから学ぶAWSセキュリティことはじめ | 新井さん セッション④：マルチアカウント運用のはじめかた（Japan Degital Design） | 小野さん まとめ タイムテーブル タイムテーブルは下記の通りです。\n時間 内容 登壇者 18:30- 受付開始 - 19:00-19:05 会場諸注意/初心者支部とは/テーマ説明 初心者支部運営 澤田さん/太田さん 19:05-19:35 ミニハンズオン：AWSアカウントを作ったら最初にやるべきこと-セキュリティ編- 初心者支部運営 迫さん 19:37-19:50 セッション①：Fin-JAWSの紹介と金融の現状 Fin-JAWS支部運営 釜山さん 19:50-20:00 休憩 - 20:00-20:20 セッション②：データはどこからくるの？〜AWSとオンプレとの違いで学んだあれやこれ〜 大熊さん 20:20-20:30 セッション③：FISCから学ぶAWSセキュリティことはじめ 新井さん 20:30-20:50 セッション④：マルチアカウント運用のはじめかた（Japan Degital Design） 小野さん 20:50-20:55 アンケート回答 - 場所は、目黒セントラルスクエア 21 階のアマゾンウェブサービスジャパン株式会社でした。\n各セッションの概要 会場諸注意/初心者支部とは/テーマ説明 | 初心者支部運営 澤田さん/太田さん JAWS UG とは Japan AWS User Group 初心者支部とは Fin-JAWS 金融系に特化した内容 澤田さん、太田さん、迫さんは初心者支部の運営初心者 ミニハンズオン：AWSアカウントを作ったら最初にやるべきこと-セキュリティ編- 初心者支部運営 迫さん アカウントを作成したらまずやることはいっぱいある 今日はセキュリティに関するところ ルートアカウントのアクセスキーの削除 ルートアカウントの MFA 有効化 IAMユーザーの作成 IAMグループの作成 パスワードポリシーの設定 CloudTrail の有効化 GuardDuty の有効化 結果のサンプルを出力できる AWSアカウントを作成したら最初にやるべきこと -セキュリティ編- - Qiita AWSアカウントを作ったら最初にやるべきこと ～令和元年版～ ｜ Developers.IO セッション①：Fin-JAWSの紹介と金融の現状 Fin-JAWS支部運営 釜山さん 釜山さん 日本電気 は にほんでんき ではなく にっぽんでんき Fin-JAWS とは 目的毎の支部 金融と Fin Tech の JAWS-UG 参加条件は、金融や FinTech に興味があればok 金融事業や FinTech 企業における AWS の勉強会 2017 年に第一回、その後 2019 年に第二回 (いろいろあって) re:Invent 現地でも活動 Twitter @finjaws1 Fin-JAWS (金融とFinTechに関するJAWS支部) | Doorkeeper 金融のクラウド事情 MUFG ショック (2017 年) これまでは細々としたところで使われていた 最初に大きな部分をクラウドに移行することを宣言したのが MUFG クラウド導入状況 (H 29 年度) 都銀、信託については 100% 基幹系に関しては導入なしが 90 % 以上 全体的に理解が進んでいない コンサルする際にも、クラウドの基本的な説明から入ることが多い クラウドバイデフォルトの原則 システムを導入するに当たってクラウドを第一に選択しましょうということ クラウド利用の促進 大阪リージョン開設に伴ってソニー銀行が基幹系に導入へ 遅れているというイメージだが、進んでいるところは進んでいる 最後に宣伝 JAWS DAYS 行きましょう！ JAWS DAYS 2020 | Your Next Cloud Journey セッション②：データはどこからくるの？〜AWSとオンプレとの違いで学んだあれやこれ〜 | 大熊さん 20200129 jawsug bgnr22 from Kahori Takeda 大熊さん 2018 年に最初の仕事は Route 53 と SES の設定 2019 年は JWAS DAYS に参加したり re:Invent に行ったりして AWS 熱が凄かった 今回が初めての登壇 データはどこから来るの 金融のデータといえば株価 株価は専用線？そうでもない いろんな方法で、いろんなフォーマットでデータを取得 メール、 PDF 、クローリング \u0026hellip; これまでは取得したデータを人出で入力していた その部分を AWS (Lambda) で自動化した話 前段の Lambda で取得と加工 営業日判定、計算、 SFTP 送信用の Lambda AWS (特にサーバレス) の良さって何？ サーバ、ネットワークの構築作業不要 高可用性 夜中、休日にハードの障害連絡が来ない 慣れてきた頃の大失敗 S3 バケットの数万ファイルを移動 PUT トリガーで Lambda を実行 Lambda の同時実行上限に引っかかった 他の Lambda が動かなくなった S3 の PUT イベントや CloudWatch からのイベントキューは制御 (削除したり) できない 間に SQS を挟んでいればどうにかなったかも (全部に SQS 挟むのが良いわけではない) アカウント全体での Lambda のスロットリングが必要 セッション③：FISCから学ぶAWSセキュリティことはじめ | 新井さん 新井さん インフラ -\u0026gt; スマホ UI/UX -\u0026gt; サーバサイド 金融系のお客様を担当 Fargate をよく使う FISC ガイドライン AWS x 金融 と聞いて Google 検索すると 事例、ガイドライン、セキュリティ \u0026hellip; FISC ガイドライン 対策、何かあった時の対応方法・手順 約 300 項目 AWS と FISC の関係性 AWS から FISC に関するガイドラインを提供している FISC の項目に対する AWS の見解が整備されている 責任共有モデルに基づいて、自分たちでもセキュリティ対策をする必要がある 抑止 -\u0026gt; 予防 -\u0026gt; 検出 -\u0026gt; 回復 FISC 準拠のユースケース ALB + EC2 + RDS データの保護 ACM 、 EBS の暗号化、 RDS の暗号化、 アプリケーションレベルでの暗号化、 S3 へのアクセスは VPC エンドポイントを利用 など 暗号鍵の管理 KMS 、 CloudHSM ソフトウェアの管理 SSM リソース管理 CloudWatch 不正アクセス対策 WAF 、 Shield 、 GuardDuty 、 Config ウイルス対策 サードパーティを利用 Inspector、 SSM で検知・パッチ適用 本人確認 IAM 大規模災害対策 マルチ AZ 今後はマルチリージョン化も まとめ AWS は FISC ガイドラインをベースにセキュリティ設計を提供している ただし FISC の最新版 (第9版) に対する AWS のリファレンスガイドがまだない セッション④：マルチアカウント運用のはじめかた（Japan Degital Design） | 小野さん JAWS-UG初心者支部 - 2020-01-29 - マルチアカウント運用のはじめかた from Yutaro Ono 小野さん インフラ管理 MUFG グループの新会社 銀行本体ではできない FinTech 関連のことをやっていく マルチアカウントのための機能 AWS Organizations ひとつのアカウントを組織のルートアカウントとする その下に子アカウント作る 請求をルートアカウントに一括して請求できる OU (Organization Unit) によるアカウント管理 マルチアカウント運用の初めかた ルートになるアカウントを決める 非常に強い権限を持つことになる 課金 子アカウントへのスイッチロール権限 自動的に IAM ロールが作成される 通常運用はしない前提 新しいアカウントを作って Organizations のルートとする 既存のアカウントを 招待 する 作成時に 子アカウント として新規にアカウントを作ることも可能 アカウントの分け方 なぜひとつを共有するのはダメなのか 権限の制限が難しくなる AWS の世界で使いやすい分け方を考える 会社の部署単位でわけるのはオススメできない 会社の組織はよく変わる あとで困らないために 1 アカウント 1 用途 Organizations のルートアカウント 請求や子アカウントの管理だけ 課金管理 CloudTrail 組織の CloudTrail という機能 OrganizationAccountAccessRole 子アカウントに Assume Role できるようになる AdministratorAccess の権限を持っている IAM ユーザ管理専用アカウント スイッチロールの元になるような感じ 組織で使う IAM ユーザを管理 セキュリティ通知用アカウント GuardDuty などの通知をまとめる 組織の CloudTrail ログを S3 に保存する Config などの設定を集約 検証環境 開発者ごとに分けるケースと、共用アカウントを利用するケース 運用環境 サービスごとにアカウントを分ける まとめ ひとつひとつのアカウントをシンプルに アカウントを作るだけなら無料 利用料金をアカウント単位で管理 AWS Control tower など、マルチアカウント管理向けの機能も強化されてきている 偶発的な間違いを防止 japan-d2/secure-stack-template-aws: Financial-grade Secure Stack Template for AWS CloudFormation. AWS をセキュアに始められるテンプレート まとめ JAWS-UG 初心者支部#22 Fin-JAWSコラボ＆ミニハンズオン会 に参加してきた話でした。\n初の JAWS-UG イベントでしたが、もちろんですが AWS に関する内容ばかりで楽しかったです。初心者支部ということで内容的には既に知っていることがほとんどでした。ただ、 FinTech 関係の話やマルチアカウントに関する話はしっかり聞いたことがなかったので、へぇ〜なるほど〜 っていう感じで聞いていて面白かったです。\n次回は 2/19 にハンズオンをやるそうなので、気になる方はぜひ参加してみてはどうでしょうか。\nJAWS-UG 初心者支部#24 サーバレスハンズオン勉強会 - connpass ",
    "permalink": "https://michimani.net/post/event-jaws-ug-beginner-22/",
    "title": "[レポート] JAWS-UG 初心者支部#22 Fin-JAWSコラボ＆ミニハンズオン会 に行ってきました"
  },
  {
    "contents": "AWS CodePipeline で実行中のパイプラインを停止できるようになったので、実際に試してみました。\n目次 概要 やってみる AWS CLI で停止してみる 実行中のパイプラインの を取得するには まとめ 概要 AWS CodePipeline Enables Stopping Pipeline Executions 上記 AWS 公式のお知らせにもあるように、 AWS CodePipeline で実行中のパイプラインを、コンソールおよび SDK から停止できるようになりました。\nお知らせの概要は下記の通りです。\nCodePipile で実行中のパイプラインを簡単に停止できるようになりました。これまでは下記のいずれかの方法しかありませんでした。\nステージ間の遷移を無効にし、実行中のパイプラインに優先する新しいパイプラインの実行を待つ 実行中のパイプラインがタイムアウトするのを待つ 本日より、 CodePipeline コンソールおよび AWS CodePipeline SDK を使用してパイプラインの実行を停止することができるようになりました。\nということで、実際に試してみます。\nやってみる CodePipeline コンソールと SDK によって停止できるということですが、コンソールからの停止については既にクラメソさんのブログで紹介されていたので、ここでは割愛します。\n[アップデート]CodePipeline 途中で実行を停止できるようになりました ｜ Developers.IO なので、今回はもう一方の方法である SDK で\u0026hellip;と思ったのですが、最新の AWS CLI でも停止が可能なので、そちらで試してみます。\nAWS CLI で停止してみる 使用までに AWS CLI をアップデートしたところ、バージョンは 1.17.7 でした。\n❯ aws --version aws-cli/1.17.7 Python/3.7.5 Darwin/18.7.0 botocore/1.14.7 停止に使用するコマンドは aws codepipeline stop-pipeline-execution です。\n必須のパラメータは --pipeline-name と --pipeline-execution-id です。\nコンソール上で停止する際の選択肢として 停止して待機 と 停止して中止 がありますが、それぞれ --abandon と --no-abandon のオプションで指定します。デフォルトは --abandon が有効になっています。(つまり、 停止して中止)\n今回は、このブログの CI/CD に使っている michimani.net-pipeline という名前のパイプラインを対象にします。\nパイプラインを開始する ❯ aws codepipeline start-pipeline-execution --name michimani.net-pipeline { \u0026#34;pipelineExecutionId\u0026#34;: \u0026#34;e2d0a696-4151-49be-9b40-73a088619c1c\u0026#34; } パイプラインを停止する ❯ aws codepipeline stop-pipeline-execution --pipeline-name michimani.net-pipeline \\ --pipeline-execution-id e2d0a696-4151-49be-9b40-73a088619c1c { \u0026#34;pipelineExecutionId\u0026#34;: \u0026#34;e2d0a696-4151-49be-9b40-73a088619c1c\u0026#34; } 停止実行後のステータス確認 ❯ aws codepipeline list-pipeline-executions --pipeline-name michimani.net-pipeline --max-items 1 | jq .pipelineExecutionSummaries [ { \u0026#34;pipelineExecutionId\u0026#34;: \u0026#34;e2d0a696-4151-49be-9b40-73a088619c1c\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;Stopped\u0026#34;, \u0026#34;startTime\u0026#34;: 1579698780.514, \u0026#34;lastUpdateTime\u0026#34;: 1579698888.742, \u0026#34;sourceRevisions\u0026#34;: [ { \u0026#34;actionName\u0026#34;: \u0026#34;Source\u0026#34;, \u0026#34;revisionId\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;revisionSummary\u0026#34;: \u0026#34;add enable inner adsense parameter\u0026#34;, \u0026#34;revisionUrl\u0026#34;: \u0026#34;https://github.com/michimani/michimani.net/commit/1234567890\u0026#34; } ], \u0026#34;trigger\u0026#34;: { \u0026#34;triggerType\u0026#34;: \u0026#34;StartPipelineExecution\u0026#34;, \u0026#34;triggerDetail\u0026#34;: \u0026#34;arn:aws:iam::12345678:user/hoge\u0026#34; } } ] ちなみに --no-abandon オプションを指定した場合は、しばらくしてから Stopped になりました。\n停止して待機 なのか 停止して中止 なのかは下記のコマンドで確認できます。\naws codepipeline get-pipeline-state --name michimani.net-pipeline 現在のパイプラインのステータスを取得できるので、該当のアクションの actionStates.latestExecution.status と latestExecution.status の値で確認することができます。\n--no-abandon オプションを指定した場合 (停止して待機 の場合) は、進行中のアクションを実行してからパイプラインを停止するので actionStates.latestExecution.status が Succeeded (もしくは失敗していれば Failed) 、 latestExecution.status が Stopped になっています。\n一方で、オプションを指定しない または --abandon オプションを指定した場合 (停止して中止 の場合) は、進行中のアクションを待たずにパイプラインを停止するので、 actionStates.latestExecution.status が Abandoned、 latestExecution.status が Stopped になっています。\nまた、 停止して中止 の場合は、コンソール上で赤字で Abandoned と表示されます。\n実行中のパイプラインの を取得するには 今回はパイプラインの開始も CLI で実行したため、パイプラインの実行 ID は開始コマンド実行時に確認できましたが、すでに実行中のパイプラインの実行 ID = pipelineExecutionId を取得するには下記のコマンドを実行します。\naws codepipeline list-pipeline-executions --pipeline-name michimani.net-pipeline \\ | jq .pipelineExecutionSummaries \\ | jq \u0026#39;map(select(.status == \u0026#34;InProgress\u0026#34;))\u0026#39; \\ | jq \u0026#39;.[].pipelineExecutionId\u0026#39; これで実行中 (= ステータスが InProgress) のパイプラインの pipelineExecutionId を取得できます。\nまとめ AWS CodePipeline で実行中のパイプラインを停止できるようになったので、実際に AWS CLI で試してみた話でした。\nこれまで CodePipeline でパイプラインが開始されると、実行中に止めたくなっても新たに優先されるパイプラインを開始したり、実行中のパイプラインがタイムアウトするまで待つ必要がありました。\n今回のアップデートで即座に停止 (中止) することができるようになったので、そういう状態の時に待つ必要がなくなり、時間が節約できますね。\n",
    "permalink": "https://michimani.net/post/aws-stop-codepipeline-execution/",
    "title": "AWS CodePipeline のパイプライン実行を停止できるようになったので CLI で試してみた"
  },
  {
    "contents": "去年から Todoist でタスク管理をしているのですが、調べてみると Todoist でタスクやプロジェクトを扱うための API が用意されていることがわかりました。今回はその API を利用して Todoist にタスクを追加するスクリプトを Python で書いてみた話です。\n目次 概要 作ったもの Todosit の Python モジュールを使う タスクを追加する プロジェクト一覧を取得する まとめ 概要 Todoist のタスクは自分で作成したプロジェクトで分類することができます。どのプロジェクトにも属さないタスクは Inbox に入ります。\nなので、今回のスクリプトでもプロジェクトを任意のパラメータとして指定できるようにします。\n作ったもの GitHub に置いています。\nmichimani/todoist-cli: This is simple command line tool for Todoist. 以下、簡単な解説です。\nTodosit の Python モジュールを使う Todoist API を使用するといっても、 Python のモジュールがあるのでそれを使います。\nちなみにスクリプトは Python 3.x (3.7.5) で実行するので、あらかじめ環境を作っておきます。\n$ python3 -m venv .venv \u0026amp;\u0026amp; source ./.venv/bin/activate pip で Todoist のモジュールをインストールします。\n$ pip install todoist-python ドキュメントは下記ですが、見辛いです。(というかメンテナンスされてない？？)\nOverview — todoist-python 1.0 documentation Todoist API 本体のドキュメントは下記です。\nAPI Documentation | Todoist Developer タスクを追加する タスクを追加するスクリプトは下記のような感じです。\nimport argparse import configparser import os import todoist p = argparse.ArgumentParser() p.add_argument(\u0026#39;task_name\u0026#39;, help=\u0026#39;task name for new task\u0026#39;) p.add_argument(\u0026#39;-p\u0026#39;, \u0026#39;--project_id\u0026#39;, default=\u0026#39;\u0026#39;, help=\u0026#39;project ID for new task added\u0026#39;) args = p.parse_args() c = configparser.ConfigParser() c.read(os.path.dirname(os.path.abspath(__file__)) + \u0026#39;/../config.ini\u0026#39;, \u0026#39;UTF-8\u0026#39;) TODOIST_TOKEN = c.get(\u0026#39;todoist\u0026#39;, \u0026#39;token\u0026#39;) def add_task(task_name, project_id=\u0026#39;\u0026#39;): api = todoist.TodoistAPI(TODOIST_TOKEN) if project_id != \u0026#39;\u0026#39;: api.add_item(task_name, project_id=project_id) else: api.add_item(task_name) api.commit() if __name__ == \u0026#39;__main__\u0026#39;: add_task(args.task_name, args.project_id) ハイライトしている部分が todoist-python モジュールを使用している部分です。\ntodoist.TodoistAPI() の引数には Todoist App の access token を渡します。\naccess token は、個人で使うレベルであれば Todoist App を作成した際に生成される Test Token を使えば OK です。\nOAuth 認証で取得する場合は下記のリポジトリを参考にしてください。 michimani/todoist-token-generator: This is a simple web site to generate access token of Todoist App. タスクの追加は TodoistAPI オブジェクトを生成して add_item() を呼びます。\ntask_name は必須で、 project_id は任意です。\nこのスクリプトは下記のように使います。\n$ python add-task.py \u0026#34;Task Name\u0026#34; -p 12345678XX -p はオプションで、指定した場合はプロジェクトに対してタスクが追加されます。省略した場合は Inbox にタスクが追加されます。\n期限やリマインダーは追加せず、とりあえずタスクを追加するような形で作ってみました。\nプロジェクト一覧を取得する プロジェクトに対してタスクを追加する際には、project_id が必要になります。\nTodoist の Web app だとプロジェクト名のリンクから project_id を確認することはできますが、面倒です。\nなので、プロジェクト一覧も API で取得してみます。\nプロジェクト一覧を取得するスクリプトは下記の通りです。\nimport argparse import configparser import json import os import shutil import todoist p = argparse.ArgumentParser() p.add_argument(\u0026#39;-f\u0026#39;, \u0026#39;--full_sync\u0026#39;, default=\u0026#39;0\u0026#39;, help=\u0026#39;\u0026#34;1\u0026#34; for full sync\u0026#39;) args = p.parse_args() c = configparser.ConfigParser() c.read(os.path.dirname(os.path.abspath(__file__)) + \u0026#39;/../config.ini\u0026#39;, \u0026#39;UTF-8\u0026#39;) TODOIST_TOKEN = c.get(\u0026#39;todoist\u0026#39;, \u0026#39;token\u0026#39;) CACHE_DIR = os.path.dirname(os.path.abspath( __file__)) + \u0026#39;/../caches/sync_cache/\u0026#39; SYNC_CACHE_FILE = CACHE_DIR + TODOIST_TOKEN + \u0026#39;.json\u0026#39; LIST_LINE_FORMAT = \u0026#39;{id: \u0026lt;15}: {name: \u0026lt;30}\u0026#39; def list_projects(): if args.full_sync == \u0026#39;1\u0026#39; and os.path.exists(CACHE_DIR): shutil.rmtree(CACHE_DIR) if os.path.exists(SYNC_CACHE_FILE): with open(SYNC_CACHE_FILE) as f: sync_data = json.load(f) else: api = todoist.TodoistAPI(TODOIST_TOKEN, cache=CACHE_DIR) sync_data = api.sync() projects = sync_data[\u0026#39;projects\u0026#39;] print(\u0026#39;\\n\u0026#39;) print(LIST_LINE_FORMAT.format(id=\u0026#39;PROJECT ID\u0026#39;, name=\u0026#39;PROJECT NAME\u0026#39;)) print(\u0026#39;{s:-\u0026lt;15}:{s:-\u0026lt;30}\u0026#39;.format(s=\u0026#39;\u0026#39;)) for project in projects: print(LIST_LINE_FORMAT.format(id=project[\u0026#39;id\u0026#39;], name=project[\u0026#39;name\u0026#39;])) print(\u0026#39;\\n\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: list_projects() プロジェクト一覧に限らず、 Todoist から情報を取得する際には sync() を使用します。\n今回は TodoistAPI オブジェクトを生成する際に access token の他に cache というパラメータを追加しています。デフォルトでは ~/.todoist-sync/ となっているのですが、今回はこのリポジトリ内のディレクトリを指定しています。\nsync() を実行すると、実行結果 (JSON) が cache で指定したディレクトリに保存されます。ファイル名は {access token}.json となります。\n更新された情報を取得するにはキャッシュファイルを削除してから sync() を実行する必要があるので、スクリプト実行時のオプションで full_sync するかどうかを指定できるようにしています。\n上記のスクリプトを実行すると、プロジェクトの ID と名前が出力されます。\n$ python src/list-projects.py PROJECT ID : PROJECT NAME ---------------:------------------------------ 12345678XX : Inbox 12345678XX : プロジェクト1 12345678XX : プロジェクト2 12345678XX : プロジェクト3 まとめ Todoist API を利用して Todoist にタスクを追加するスクリプトを Python で書いてみた話でした。\nふとしたときにササっとタスクを追加しようと思うと、やはりターミナルから実行できたほうが便利なので、とりあえずタスク追加するだけであればこれくらい最低限の機能でいいかなーという感じです。\nただ、最近知ったんですが Slack に Todoist のアプリがあって、それを使えば Slack から同じようなことができるんですよね\u0026hellip;。まあ、それはそれで。\ntodoist と Slack 連携できたのか...。https://t.co/GAqpFbSyIA\n\u0026mdash; よっしー@CBR853RR (@michimani210) January 17, 2020 ",
    "permalink": "https://michimani.net/post/development-add-task-to-todoist-via-python/",
    "title": "Todoist にタスクを追加するスクリプトを Python で書いてみた"
  },
  {
    "contents": "技術的なことを検索して出てくるサイトやブログ記事の中で参考 URL が記されている場合があります。その URL が本当に URL だけのときって全然参考にならないよなー、リンクにはタイトル付与してほしいなー っていう話です。\n何を言っているのか ちょっと何言ってるかわかんないです by サンドウィッチマン 富澤\nということで、簡単に説明すると、たとえば下記のような参考 URL の表記はパッと見でどんなページを参考にしているかわかるでしょうか？\n参考 URL https://aws.amazon.com/jp/ec2/pricing/ https://www.michinoeki-mania.com/entry/michinoeki/2015/02/072276 https://qiita.com/michimani/items/bf347cd0c1575989ea6f 1 つ目は、 URL のパスからなんとなく推測できそうな気がします。\n2 つ目は、 2015 年の 2 月に書かれたブログかなというくらいはわかりますが、何のサイトでどんな内容なのかまではわかりません。\n3 つ目は、 Qiita の記事であることはわかりますが、内容は全くわかりません。\nつまり、実際にそのリンクを踏んで見ないとどんな内容のページを参考にしたのかがわかりません。\n何かを調べて、さらにそこで参考にしているページまでチェックしようとしている人にとって、この表記だと不親切です。\nでは、下記のような表記だとどうでしょうか。\n参考 URL 料金 - Amazon EC2 | AWS 道の駅で野宿するときに気を付けている5つの事 - みちのえきまにあ Qiita から Hugo 用の markdown を生成するやつ - Qiita パッと見でどんなページかわかります。\nわざわざリンクを踏んで内容を確認しに行くまでもなく、タイトルから参考になるかどうかをある程度判断することができるので、時間の節約にもなり親切です。\nリンクにタイトルを付ける 何を今更っていう感じですが、リンクにタイトルを付けるには下記のように記述します。\nHTML の場合\n\u0026lt;a href=\u0026#34;https://aws.amazon.com/jp/ec2/pricing/\u0026#34;\u0026gt;料金 - Amazon EC2 | AWS\u0026lt;/a\u0026gt; Markdown の場合\n[料金 - Amazon EC2 | AWS](https://aws.amazon.com/jp/ec2/pricing/) でもこれめんどくさい ただ、ブログを書いているときにこれらの表記を本文に書くのって結構めんどくさいです。\nWordPress とか はてなブログ とかの WYSIWYG でリンク作成したり、独自でスニペット作ってたり、毎回手で入力してる場合もあるかと思います。\nURL はアドレスバーをコピーすればいいですが、記事のタイトルに関してはそれぞれのページのタイトル部分を確認する必要があります。\nリンクとして URL だけを書いているのは、おそらくそういった操作が面倒だからだと思います。\nワンクリックでコードを取得するブックマークレット ということで作ったので、下記のブックマークレットです。\nmichimani/copy_link_of_current_page_as_md | GitHub Gist これをブックマークレットとして適当な名前で保存して、ブラウザのブックマークバーにでも置いておけば、あとはリンクを生成したいページを開いてそこをクリックするだけです。\nMarkdown 用のリンクコードがクリップボードにコピーされます。\n実態としては下記の JavaScript を実行しています。\nconst titleTag = document.getElementsByTagName(\u0026#34;title\u0026#34;), pageUrl = location.href; let pageTitle = pageUrl; titleTag.length \u0026gt; 0 \u0026amp;\u0026amp; titleTag[0].innerHTML !== \u0026#34;\u0026#34; \u0026amp;\u0026amp; (pageTitle = titleTag[0].innerHTML); const md = `[${pageTitle}](${pageUrl})`; navigator.clipboard \u0026amp;\u0026amp; navigator.clipboard.writeText(md); タイトルタグが見つからない、または空文字の場合は、仕方なく URL のリンクを生成しています。が、まあ、そんなページはこのご時世ほぼ無いでしょう。(無いと信じたい)\nまとめ リンクにはタイトル付与してほしいなー っていう話でした。\nQiita とか個人のブログとかを Markdown で書いている方は、このブックマークレットを使って訪問者に優しいリンクを作りましょう。\n",
    "permalink": "https://michimani.net/post/development-copy-link-with-title-for-markdown/",
    "title": "参考 URL として URL だけ貼ってあるのって参考にならなくない？"
  },
  {
    "contents": "ふと GitHub のリポジトリに付いたスター数を数えてみたいと思ったので、 GitHub の API を利用してカウントしてみました。\n目次 作ったもの 使い方 補足 GitHub の API Python の argparse モジュール まとめ 作ったもの カウント用のスクリプトは Python で作りました。 (3.7.5)\n使い方 使用する場合は python3 -m venv .venv \u0026amp;\u0026amp; source ./.venv/bin/activate などで Python 3 の環境を作って \u0026amp;\u0026amp; 入って、その中で下記コマンドを実行します。\n$ python count-github-stargazers.py michimani Count stargazers of michimani\u0026#39;s GitHub repositries... auto-start-stop-ec2 : 1 qiita-to-hugo : 1 resize-s3-image : 1 start-stop-ec2-python : 1 TOTAL : 4 補足 今回作ったスクリプトについて簡単に補足を書いておきます。\nGitHub の API 今回使用したのは、ユーザのリポジトリ一覧を取得する API です。\nスクリプト中にもありますが、エンドポイントは下記 URL となっています。\nGET https://api.github.com/users/:username/repos 認証は不要で、指定したユーザの Public な リポジトリ一覧を取得することができます。\nper_page パラメータでページングが可能で、 page パラメータでページを指定できます。\nその他、下記のオプションで取得できるリポジトリの種類、ソート順を指定することができます。(詳細については公式ドキュメントを参照してください)\ntype : all / owner / member で指定します。デフォルトは owner です。\nsort : 下記の値でソート順を指定します。デフォルトは full_name です。\ncreated updated pushed full_name direction : asc / desc でソート順の昇順/降順を指定します。デフォルトは、sort=full_name の場合は asc 、その他の場合は desc です。\nRepositories # List user repositories | GitHub Developer Guide Python の argparse モジュール 今までも Python で簡単な CLI ツールのようなものを作っていましたが、今回は argparse という Python の標準モジュールを使用してみました。\nCLI ツールを作るとなると、引数やオプションの処理を実装する必要があります。それらの 必須/任意 の管理などは面倒なのですが、 argparse モジュールを使用することにより引数やオプションの管理が非常に簡単になります。\nargparse \u0026amp;mdash; コマンドラインオプション、引数、サブコマンドのパーサー — Python 3.8.1 ドキュメント 今回のスクリプトでは GitHub のユーザ名のみを必須の引数とするシンプルなものですが、引数が渡されなかった場合に定型文を出力してくれたり、デフォルトで -h オプションを使用することで各引数・オプションのヘルプを出力することもできます。\n引数なしで実行した場合\n$ python count-github-stargazers.py usage: count.py [-h] user_name count.py: error: the following arguments are required: user_name ヘルプ\n$ python count-github-stargazers.py -h usage: count.py [-h] user_name positional arguments: user_name GitHub user name optional arguments: -h, --help show this help message and exit argparse モジュールについては下記の記事を参考にさせていただきました。\n[テンプレ付き]PythonでCLIツールを作るときのTips ｜ Developers.IO まとめ GitHub のリポジトリに付いたスター数をカウントするスクリプトを Python で作ってみた話でした。\nスター数をカウントしたいというか argparse モジュールを使ってみたかった感じになってますが、自分のリポジトリに付いてるスター数をカウントしたくなったら参考にしてみてください。\nどうでもいいですが、スター数の名前が単純に stared_count じゃなくて stargazers_count なのがなんかかっこいいですよね。スターゲイザーっていう響きが。\n",
    "permalink": "https://michimani.net/post/development-count-github-stargazers/",
    "title": "GitHub のリポジトリに付いたスター数をカウントしてみた"
  },
  {
    "contents": "静的サイトジェネレータ Hugo を AWS 環境でホスティング、さらに自動ビルドを実行できる環境を AWS CDK で作ってみました。\nHugo を AWS 環境で運用したいという方の参考になれば幸いです。\n目次 概要 前提 作ったもの 使い方 新しい年が始まったので 概要 タイトルの通り、 AWS CDK を使って静的サイトジェネレータ Hugo で生成したサイトをホスティング、さらに CodeCommit への Push をトリガーにした自動ビルド環境を構築します。\nできあがる環境は下図のようになります。\n前に単純な静的サイトを S3 でホスティングする環境を CDK で構築しましたが、その静的サイト部分を Hugo にした形です。\n本当は Route 53 と S3 の間に CloudFront を置いて HTTPS でアクセスできるようにしたほうがいいのですが、 AWS CDK の CloudFront モジュールは現在 (v1.1.9.0) は public preview なので利用しない形で実装しています。\n前提 AWS CDK がインストールされている (執筆時点で最新の 1.19.0 を使用します) Route 53 で独自ドメインのホストゾーンが作成されている\n今回は既存のホストゾーンに対してレコードを追加するような構成を考えます。 作ったもの 作った CDK プロジェクトはこちらです。\nmichimani/hugo-pipeline: This is a AWS CDK project that create building pipeline of HUGO site. GitHub に置いているので、 clone して使います。\n使い方 まずは clone してきます。\n$ git clone https://github.com/michimani/hugo-pipeline.git $ cd hugo-pipeline npm パッケージのインストールと、 CDK プロジェクトの初期化をします。\n$ npm install $ cdk bootstrap config ファイルをサンプルから作ります。\n$ cp stack-config.yml.sample stack-config.yml config ファイルは下記のような YAML ファイルになっています。\ncommon: hugo_version: 0.62.0 region: \u0026#34;\u0026lt;deploy-target-region\u0026gt;\u0026#34; route53: zone_name: \u0026#34;\u0026lt;existed-hosted-zone-name\u0026gt;\u0026#34; zone_id: \u0026#34;\u0026lt;existed-hosted-zone-id\u0026gt;\u0026#34; subdomain_host: \u0026#34;\u0026lt;hostname-of-sub-domain (optional)\u0026gt;\u0026#34; codepipeline: branch: master 各項目の設定値の詳細は下記の通りです。\ncommon hugo_version : 利用する Hugo のバージョンを指定します。この記事を書いている時点 (2020/1/1) では 0.62.0 が最新です。 region : AWS リソースを作成するリージョンを指定します。 route53 zone_name : あらかじめ Route 53 に登録してあるホストゾーンのゾーン名を指定します。 zone_id : ゾーン名のゾーン ID を指定します。ゾーン ID は Route 53 のコンソール等で確認します。 subdomain_host : ゾーン名のサブドメインとしてホスティングする場合は、ホスト名部分をしていします。サブドメインを使用しない場合は省略します。 codepipeline branch : ビルドのトリガーとなる Push 先のブランチ名を指定します。 下記コマンドを実行して、スタックが生成されるかをテストします。\n※ただし、テストについては未完成で、テストが通っても cdk deploy が失敗する場合があります。\n$ npm run test テストが通ったら synth して実際に生成される CFn テンプレートを確認します。\n$ cdk synth ... 特に問題なければデプロイコマンドを実行します。\nただし、下記のように 2 つのコマンドを実行するようにします。\n$ cdk deploy \u0026amp;\u0026amp; cat ./cdk.out/codecommit_info.txt すると、デプロイ完了後に下記のような出力がされます。\nPlease run following AWS CLI command to get generated CodeCommit repositry info. aws codecommit get-repository --repository-name repo-name --region region-code 指示の通り CLI コマンドを実行すると、 CDK によって生成された CodeCommit リポジトリの URL を確認することができます。\naws codecommit get-repository --repository-name repo-name --region region-code { \u0026#34;repositoryMetadata\u0026#34;: { \u0026#34;accountId\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;repositoryId\u0026#34;: \u0026#34;12345678-abcd-1234-efgh-d0f1ea0f43ab\u0026#34;, \u0026#34;repositoryName\u0026#34;: \u0026#34;repo-name\u0026#34;, \u0026#34;repositoryDescription\u0026#34;: \u0026#34;This is a repository of Hugo site.\u0026#34;, \u0026#34;defaultBranch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;lastModifiedDate\u0026#34;: 1577774989.781, \u0026#34;creationDate\u0026#34;: 1577774643.78, \u0026#34;cloneUrlHttp\u0026#34;: \u0026#34;https://git-codecommit.region-code.amazonaws.com/v1/repos/repo-name\u0026#34;, \u0026#34;cloneUrlSsh\u0026#34;: \u0026#34;ssh://git-codecommit.region-code.amazonaws.com/v1/repos/repo-name\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:codecommit:region-code:123456789012:repo-name\u0026#34; } } この URL を リモートリポジトリとして、 Hugo のリポジトリに登録し、指定したブランチに Push すればビルドが開始されます。\n(最後が雑)\n新しい年が始まったので 静的サイトジェネレータ Hugo を AWS 環境でホスティング、さらに自動ビルドを実行できる環境を AWS CDK で作ってみた話でした。\n2020 年になって新しい年が始まったので、これを機会にブログを始めようと思っている方もいると思います。\nはてなブログやワードプレスもいいですが、 Hugo をはじめとした静的サイトジェネレータでブログを運用してみるのも面白いと思います。\n静的サイト自体は様々な方法、環境で運用することはできますが、 AWS 環境で運用しようとしている方の参考になれば幸いです。\n",
    "permalink": "https://michimani.net/post/aws-develop-hugo-pipeline-using-cdk/",
    "title": "AWS CDK で Hugo のビルド環境とホスティング環境を作ってみた"
  },
  {
    "contents": "昨年に引き続き、今年も自分のアウトプット事情について振り返ってみます。が、今年はアウトプットに加えてインプットもたくさんしたので、インプット事情についても合わせて振り返ります。\n目次 2019 年の目標にしていたこと インプット事情 ほぼ毎月なにかしらのイベント・セミナーに参加していた ハンズオンめっちゃ良い セミナーは同じようなトピックでも全然違う アウトプット事情 去年より記事数が増えた GitHub のリポジトリにスターがついた AWS 認定事情 2020 年はどうするか 2019 年の目標にしていたこと 2018 年の 12 月にはこんな記事を書いていました。\nその最後で\nとりあえず 外に出る ことを目標にしたいと思います。\nブログでのアウトプットもそうですが、セミナー参加、コミュニティへの参加などで色んな人と情報共有していきたいと考えてます。\nそれに加えて、 AWS 関連の認定も取得に向けても頑張りたいですね。\nと書いています。\n今回はこれらの目標が達成できたのか、振り返ってみます。\nインプット事情 2018 年はインプットの方法としては主に各サービスのアップデートだったり、業務で使う言語について (Python、 JavaScript) 調べてみたり、という方法でした。\nイベントへの参加も後半くらいから意識し始めて、 Developers.IO 2018 と AWS Start-upゼミ::デモ祭り に参加しました。\nAWS Loft Tokyo で開催された AWS Start-upゼミ に行ってきました これらのイベント参加が自分にとってはモチベーションの向上に繋がると感じたので、 2019 年はより多くのイベント・セミナーに参加しようと意気込んでいたわけです。\nほぼ毎月なにかしらのイベント・セミナーに参加していた 実際に 2019 年に参加したイベント・セミナーは下記の通りです。\n2 月 JAWS DAYS 2019 知っ得ハンズオンシリーズ はじめての Serverless 3 月 PHPerKaigi 2019 4 月 知っ得 AWSハンズオン for Developers はじめての AI サービス 6 月 目黒JavaScriptもくもく会 #2 7 月 AKIBA.AWS #14 番外編〜AWS Update LT大会〜 AWS Cloud Development Kit (CDK) GA 記念 Meetup 8 月 知っ得ハンズオン はじめてのレコメンデーション - Amazon Personalize 10 月 AWS Amplify \u0026amp; Chalice Hands-on Workshop 11 月 Developers.IO 2019 Tokyo AWS Amplify \u0026amp; Chalice ハンズオン #03 〜怠惰なプログラマ向け お手軽アプリ開発手法〜 12 月 AWS re:Invent ふりかえり勉強会 re:Growth 2019 TOKYO ということで、ほぼ毎月なにかしらのイベントに行ってました。\nハンズオンめっちゃ良い 特に意識していたのが、実際に手を動かすハンズオンには積極的に参加するようにしていました。\nセミナーで話を聞くことで内容は入ってくるのですが、やはり実際に手を動かさないとわからないことも多いです。自ら調べて手を動かすという方法もありますが、やはりしっかりと整備されたハンズオン資料を利用して学ぶのは効率的で、とても良い時間になったと感じています。\nハンズオンはほとんど昨年オープンした AWS Loft Tokyo で開催されたものに参加しました。 AWS Loft Tokyo は毎月さまざまなイベントやハンズオンが開催されていて、時間も仕事終わりにいける時間ということもあり非常に助かっています。しかも無料なものがほとんどです。\nイベントの参加には AWS Loft Tokyo のワーキングアカウントは不要で、イベントごとの登録になるので、是非みなさん活用しましょう。\nイベント・セミナー一覧 - AWS Loft Tokyo | AWS セミナーは同じようなトピックでも全然違う いくつかのイベント・セミナーに参加してみて思ったのは、同じようなトピックのものでも登壇・発表する人によって見る角度が全然違うなということです。\nセッションを聞くたびに新しい情報を得ることができますし、たとえ重複する内容であっても再確認という意味合いで理解を深めることができます。\n1 年を通して様々なイベント・セミナーに参加しているとその点については強く感じていて、ぶっちゃけ 2 月に行った JAWS DAYS では セッションの内容難しいな、わけわからんな、ふーん って思ってたのが、12 月に参加した re:Growth 2019 では、すっと入ってくる内容が多かったです。\n継続的に情報を収集していくのは大事だなと、強く感じた 1 年でした。\nアウトプット事情 アウトプットの方法としては、メインはこのブログで、あと今年は簡単なコードでも GitHub 、 Gist にコードを後悔するようにしました。\nそれぞれでどんな成果があったか振り返ります。\n去年より記事数が増えた アウトプットは、メインはこのブログですね。\n去年は 1 年間で 48 本の記事を書きました。今年も引き続き更新しようと思っていましたが、結果的には 54 本 記事を書いていました。\n平均するとほぼ週 1 本ペースで、なんやかんやで継続して書けたなーと思っています。\n内容は AWS 関連でやってみたことが大部分を占めていて、あとはイベント・セミナーの参加レポート、静的サイトのこと、ガジェット系という分類です。\nありがたいことにはてブでテクノロジーカテゴリのホッテントリに入ることも数回あり、ブログの PV 数もじわじわ伸びています。\n今年の今日までの PV グラフ。\nプチはてブ砲があったり、そもそものベースが伸びてきてたりして、来年も頑張ろうっていう感じです。 pic.twitter.com/4nmy1O1nCc\n\u0026mdash; よっしー@CBR853RR (@michimani210) December 27, 2019 PV 数が増えると単純に書くモチベーションが上がってきますが、自分で見返して参考にすることも多いです。その時点で最低でも一人の役には立っているので、ちょっとしたことでも書いておくのは大事だなと思いました。\nGitHub のリポジトリにスターがついた 実際に自分が色々やってみたときに書いたコードを GitHub や Gist におくようになったんですが、いくつかのリポジトリにスターがつきました。\nmichimani/auto-start-stop-ec2: This is a CDK application for automatically starting / stopping EC2 instances. This is a sample of AWS Lambda function that notifies CodeBuild Notification (only state changes) to Slack via Amazon SNS. GitHub でスターをつける基準は人それぞれあると思いますが、つけてもらった側はとても嬉しいですね。\n自分は実際に使ってみて役に立ったリポジトリにスターをつけるようにしているんですが、自分につけられたスターもそんな理由だったら嬉しいなという、ポジティブな捉え方をしています。\nGitHub にパブリックなリポジトリをおくということは他人に自分のソースを見られるということになります。\nコードからは技術力も見透かされると思うのでプレッシャーはありますが、なにも言われずに一人だけで完結するよりはいいかなと思っています。\nAWS 認定事情 それに加えて、 AWS 関連の認定も取得に向けても頑張りたいですね。\nいかにも、言うだけ言ってやらない人みたいな書き方をしてます。\nが、結果的には AWS 認定の Associate レベル 3 種類に合格することができました。\n具体的な勉強方法についてはそれぞれ別記事で書いていますが、合格に向けた明示的な勉強に割いた時間は少なく、やはり 1 年を通して AWS 関係のイベント・セミナーに参加していたことで、ベースの知識がついていたことが大きかったと思います。\nもちろん認定合格がゴールではないですが、自分の力を対外的にアピールするための手段として、合格できたことは良かったです。\n2020 年はどうするか 2019 年の自分のインプット・アウトプット事情について振り返ってみました。\n結果的には 2018 年の最後に軽く宣言した目標をほぼクリアすることができました。\n2020 年は、今年の流れを引き続き続けていきます。\nそして、 AWS 認定に関しては Professional レベル 2 種類に合格できるよう頑張ります。\nあと、少し出遅れた感じはありますが、機械学習に関する技術についても勉強していきたいと思っています。\n今年の re:Invent では機械学習に関するサービスのリリース・アップデートがとても多かったので、勉強しろよと言われてるような気がしなくもないです。(単純)\nもちろん AWS に関することだけを勉強するわけではなく、関連する技術についても勉強していきます。\nもっとサーバレスやりたい。\n変化の早い世界で毎日のように新しい情報が入ってくるのはとても楽しいので、来年も楽しみながら頑張ります。\n",
    "permalink": "https://michimani.net/post/other-retrospect-in-2019/",
    "title": "2019 年の自分のインプット・アウトプット事情について振り返ってみる"
  },
  {
    "contents": "個人的スピードキューブ元年の 2019 年ふりかえり。今回は競技編ということで、タイムや覚えた手順、参加した大会・イベントなどについて振り返ってみます。\n今年買ったキューブ編はこちら。\nタイム・手順 まず、去年の 12月時点での単発ベスト (非公式) タイムは、28 秒くらいでした。 (動画のみで、正確な記録が残ってなかった)\n平均は 35 秒くらいで、それでもキューブを再開して 1 ヶ月程度で 15 秒くらいタイムが縮まっていました。\nそんななかで 2019 年は、 15 秒を切る というのを目標にしていました。\nそのためには手順もいっぱい覚えないとけないなーという状態でした。\n今年の1月。めちゃくちゃ上手くいったとき。 pic.twitter.com/xSBO8o9iuG\n\u0026mdash; よっしー@CBR853RR (@michimani210) December 26, 2019 単発タイムではほぼ目標達成 ということで結論からいうと、単発のベストタイムは 15.15 を出すことができました。\nsingle: 15.15 タイム一覧: 1. 15.15 L\u0026#39; F2 R B2 L D\u0026#39; R\u0026#39; B\u0026#39; R F2 R2 L2 F\u0026#39; U2 D2 B D2 B L2 B\u0026#39; それ以外でも、 sub 20 を頻繁に出すことができるようになりました。\nちょっと、めっちゃ好タイム出た！ pic.twitter.com/YP0llUlLRk\n\u0026mdash; よっしー@CBR853RR (@michimani210) November 12, 2019 今年の 5 月くらいからは csTimer という Web アプリでタイムを計測しているのですが、 3194 回ソルブして ao5 、 ao 12 それぞれのベストタイムが 23.71 、 25.86 でした。\nまた、 ao 100 は 27.92 でした。\navg of 5: 23.71 タイム一覧: 1. 25.24 R D\u0026#39; L D L2 F\u0026#39; U D2 B\u0026#39; L2 U2 B L2 D2 F\u0026#39; L2 F\u0026#39; R2 B2 D R2 2. 23.25 L B2 D F2 D\u0026#39; L2 F2 D2 B2 U\u0026#39; R2 D2 R B\u0026#39; R\u0026#39; F\u0026#39; L R2 U R2 3. (25.79) B2 L2 R2 D2 F2 U\u0026#39; B2 L2 D\u0026#39; F2 L2 U\u0026#39; R\u0026#39; B\u0026#39; L2 D B\u0026#39; L\u0026#39; D\u0026#39; U\u0026#39; 4. (21.26) B\u0026#39; L\u0026#39; U2 R2 F\u0026#39; L2 R2 F\u0026#39; U2 R2 D2 F2 D2 F U F\u0026#39; U2 B D R F2 5. 22.65 R2 U\u0026#39; D2 R D2 F U\u0026#39; F\u0026#39; D\u0026#39; B\u0026#39; L2 F R2 F\u0026#39; U2 D2 F2 U2 R2 D2 avg of 12: 25.86 タイム一覧: 1. (23.33) R D2 L\u0026#39; U B\u0026#39; D2 L U\u0026#39; B D F2 D\u0026#39; L2 D B2 U\u0026#39; R2 U 2. 26.11 R\u0026#39; U L2 U2 R2 U\u0026#39; F2 U B2 U\u0026#39; L2 B2 U\u0026#39; L B F D\u0026#39; U\u0026#39; F\u0026#39; D2 U 3. 23.46 U B2 F2 U\u0026#39; F2 U B2 R2 D R2 D\u0026#39; B\u0026#39; R F2 R\u0026#39; D R\u0026#39; B F\u0026#39; L B\u0026#39; 4. 24.78 U D2 L\u0026#39; F\u0026#39; B U2 B2 L F U\u0026#39; R2 L2 F2 R2 D\u0026#39; F2 R2 U2 D L2 U\u0026#39; 5. 28.13 B\u0026#39; F\u0026#39; D2 F\u0026#39; L2 B R2 U2 R2 U2 F\u0026#39; L2 R F R U\u0026#39; L2 R B\u0026#39; R2 U2 6. (30.26) B2 R2 B2 F2 D F2 D2 R2 F2 U2 B L\u0026#39; F\u0026#39; D L2 R\u0026#39; B\u0026#39; D\u0026#39; L2 R 7. 26.76 B2 D2 F\u0026#39; L2 F L2 R2 U2 L2 U2 B2 L\u0026#39; F R2 D B\u0026#39; R\u0026#39; D F\u0026#39; U2 8. 23.60 D L2 B\u0026#39; L2 F D2 U2 B D\u0026#39; U\u0026#39; L F\u0026#39; L\u0026#39; B2 R\u0026#39; F\u0026#39; U 9. 27.56 U F L\u0026#39; U2 B2 F2 R B2 L D2 R\u0026#39; F2 R D U R\u0026#39; F L\u0026#39; F R 10. 24.97 R F2 L F\u0026#39; L2 U F D L F U2 B D2 B2 L2 F\u0026#39; R2 U2 F D2 B\u0026#39; 11. 25.03 B2 L B2 D F R\u0026#39; B2 D2 B\u0026#39; D2 L2 F\u0026#39; D2 B U2 B R2 12. 28.17 D L2 F U\u0026#39; B\u0026#39; R2 B D2 F2 D B2 L2 U\u0026#39; R2 U F2 R csTimer - Professional Rubik\u0026amp;rsquo;s Cube Speedsolving/Training Timer 最近は回していても sub 30 は確実で、ほぼ 20 秒台前半のタイムに落ち着いています。\n手順は CFOP の途中まで 手順は CFOP をマスターしようと頑張ってますが、まだまだ途中です。\n手順表は tribox さんの CFOP 手順表を使っています。\ntriboxストア / tribox CFOP 手順表 この手順表におけるそれぞれのフェーズのパターン数と覚えたパターン数の割合は下記の通りです。\nF2L (4/41) OLL (8/57) PLL (14/21) 最近は F2L を練習しているのですが、ペアを見つけるのに時間がめちゃくちゃかかっている状態です。\nOLL は、とりあえずクロスを作ってからという状態です。\nPLL は、あと少しです。\nOLL、PLL はスムーズに回せている気がしているので、来年は F2L しっかり使えるようにしていきたいです。\nちなみに、tribox さんの手順書は今年の秋に 第 2 版として新しい手順表も出ているようです。\ntriboxストア / tribox CFOP 手順表 第2版 イベント イベントというか、平日の夜にやっている 大人のためのキューブ勉強会 というものに何度か参加しました。\n内容としては、平日の夜に集まって各々キューブを回して、実際に大会で使用するタイマーを使って ao 5 を計測します。その際のジャッジは 3x3x3 元世界チャンピオンの中島悠 @naka_jima_yu さんで、試技後にアドバイスもしていただけます。\n大人のためのキューブ勉強会 - connpass ここに参加したことで、先ほど紹介した csTimer の存在を教えてもらえたり、解き方に対するアドバイスをもらえたり、そもそもキューブに対するモチベーションが上がったりと、年間通してキューブ生活を送っていく上でとても良い経験になりました。\n今年の後半はなかなか参加できなかった (開催もなかった) ので、来年はまた開催されれば参加したいと思います。\n大会 大会というとハードルが高く感じていましたが、実際に参加してみるとそうでもなかったです。\nひとつ驚いたのは、大会参加者の年齢ですね。僕が参加したのは夏休み期間にあった大会 (記録会) だったのでその影響もあるのかもしれませんが、参加者のほとんどは小・中学生でした。もちろん大人の方もいらっしゃいましたが、平均年齢は 20 代前半、もしくは 10 代だったかもしれません。\n大会に参加することで良いことは、他の人のソルブを間近で見ることができることです。\n自分よりタイムが早い人もたくさんいますし、大会によっては日本記録を持っているような人が参加していたりするので、そういう人たちのソルブを見ていると参考になる部分も多いです。\nまた一番嬉しいのは、自分のソルブが公式のタイムとして記録されるということです。\n大会の参加には WCA に登録する必要があります。(登録は無料) そして大会に参加すると、その情報に紐づいた公式タイムが記録され、後に過去に参加した大会やタイムなどを参照することもできます。\n夏以降もたくさんの大会があったのですが、残念ながら予定が合わず参加できませんでした。\n来年は記録会ではなくステージ制の大会に参加してみたいと思います。\nまとめ 個人的スピードキューブ元年の 2019 年ふりかえり。今回は競技編ということで、タイムや覚えた手順、参加した大会・イベントなどについて振り返ってみました。\n1 年間での伸びとしては緩やかかもしれませんが、確実にタイムは縮まってきています。\n2020 年はさらなるタイム短縮と、大会にもどんどん参加してみたいと思います。\n前回 書いたように安価で高機能なキューブもたくさんあるので、スピードキューブ始めてみませんか。\n",
    "permalink": "https://michimani.net/post/other-speedcube-summary-2019-solve/",
    "title": "2019 年のスピードキューブ活動について振り返ってみる - 競技編"
  },
  {
    "contents": "主に技術ブログを書く際に、コードやターミナルの出力結果をコードブロックとして表示したい場面があります。 今回は、 Hugo で作成したブログの記事内で、コードブロックに良い感じに Syntax Highlight CSS を当てる方法を整理してみます。\n前提 Hugo のバージョン : 0.60.0 以降 Markdown parser : goldmark\nHugo の Markdown parser は、 0.60.0 以降、それ以前の blackfriday から goldmark に変更されており、 blackfriday を使用したい場合は config.toml にて明示的に指定しなければいけなくなりました。 Configure Markup | Hugo 使用する theme : indigo\n※ここは実際に使用している theme に適宜置き換えて読んでください Syntax Highlight 用の CSS を生成する まずは Syntax Highlight 用の CSS を生成します。\nといっても、下記のコマンドを実行するだけです。\n$ hugo gen chromastyles --style=github \u0026gt; ./static/css/syntax.css あとは、生成されたファイルを読み込むように ./themes/indigo/layouts/partials/head.html 内に下記を追記します。\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ .Site.BaseURL }}css/syntax.css\u0026#34; /\u0026gt; 以上で Syntax Highlight の準備が整いました。\nChroma Style Gallery CSS 生成コマンドの --style=github の部分で、 Syntax Highlight のスタイル (Chroma Style) を指定しています。この部分を変更することで、さまざまなスタイルの Syntax Highlight を表現することができます。\nスタイルの一覧は Chroma Style Gallery で確認できるので、好みのスタイルを探して --style オプションで指定します。\nMarkdown で装飾する Markdown でコードブロックを表現する際には、下記のように記述します。 (Python のスクリプトの場合)\n```python import json def lambda_handler(event, context): # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } ```　上の Markdown は下記のように表示されます。\nimport json def lambda_handler(event, context): # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } Hugo では、行番号を表示したり、特定の行に網掛けをつけることができます。\n行番号を表示する 行番号を表示したい場合は、下記のような記述をします。\n```python {linenos=true} import json def lambda_handler(event, context): # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } ```　上の Markdown は下記のように表示されます。\n1import json 2 3def lambda_handler(event, context): 4 # TODO implement 5 return { 6 \u0026#39;statusCode\u0026#39;: 200, 7 \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) 8 } 特定の行に網掛けをつける 特定の行に網掛けをつけたい場合は、下記のような記述をします。\n```python {hl_lines=[1, 3, \u0026#34;5-8\u0026#34;]} import json def lambda_handler(event, context): # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } ```　上の Markdown は下記のように表示されます。\nimport json def lambda_handler(event, context): # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } もちろん、行番号表示との併用も可能です。\n```python {linenos=true, hl_lines=[1, 3, \u0026#34;5-8\u0026#34;]} import json def lambda_handler(event, context): # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) } ```　1import json 2 3def lambda_handler(event, context): 4 # TODO implement 5 return { 6 \u0026#39;statusCode\u0026#39;: 200, 7 \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Hello from Lambda!\u0026#39;) 8 } ※Syntax Highlight のテーマによっては網掛けで見辛くなることがあります\nまた、 Syntax Highlight に対応した言語とその指定方法については Hugo の公式ドキュメントを確認してください。\nSyntax Highlighting | Hugo まとめ Hugo で作成したブログの記事内でコードブロックに良い感じに Syntax Highlight CSS を当てる方法について整理してみました。\n静的サイトジェネレータは Markdown で簡単に記事が作成できる分、ちょっとした装飾や見た目のカスタムが地味に面倒だったりします。 Hugo のコードブロックについても、バージョン 0.60.0 より前では自分でショートコードを作って対応する必要がありました。\nそれが 0.60.0 以降では今回のように簡単にコードブロックを装飾できるようになったので、特に技術ブログでは恩恵を受けられると思います。\nMarkdown でブログか聞きたいけど、はてなとか WordPress は編集画面がもたつくし、そもそもページの表示も高速にしたい！という方はぜひ Hugo でブログ書きましょう。\n",
    "permalink": "https://michimani.net/post/development-hugo-syntax-highlight/",
    "title": "Hugo で作成した記事内のコードブロックに Syntax Highlight CSS を当てる"
  },
  {
    "contents": "去年の 11 月頃、高校 3 年のとき以降ほぼ放置していたルービックキューブを再び回し始めました。そして GAN354 M という、当時の高性能キューブを購入し、スピードキューブの世界に飛び込みました。\n2019 年はある意味スピードキューブ元年と言っていいくらいキューブに触れていた年だったので、年の瀬のこの時期に色々と振り返ってみます。\n長くなりそうなので、この記事では今年買ったキューブの紹介をします。\n目次 気付いたらいっぱい買ってた 各キューブのレビュー GAN356 X Numerical IPG Stickerless GAN249 M V2 Stickerless GAN354 M Stickerless GAN356 i Stickerless GAN356 XS Stickerless QiYi Thunderclap 3x3x3 V3 M Stickerless YuXin HuangLong 3x3x3 M DaYan GuHong V3 M Stickerless ステッカータイプとステッカーレスタイプ まとめ 気付いたらいっぱい買ってた 今年はたくさんのキューブを買いました。\n昨年買った GAN354 M もかなり高性能なキューブで感動していましたが、それを上回るような高性能キューブ、スマホと連動するスマートキューブなどなどバラエティに富んだキューブを次々と買ってしまいました。下記の 8 種類です。\nGAN356 X Numerical IPG Stickerless GAN249 M V2 Stickerless GAN354 M Stickerless GAN356 i Stickerless GAN356 XS Stickerless QiYi Thunderclap 3x3x3 V3 M Stickerless YuXin HuangLong 3x3x3 M DaYan GuHong V3 M Stickerless (購入順)\n3x3x3 が 7 種類、 2x2x2 が 1 種類です。\nGAN シリーズについては、次々に面白そうなキューブが登場したのでついついポチってしまいました。\nたくさんありますが、それぞれの特徴と回し心地について簡単にレビューしていきます。\n各キューブのレビュー 今年買ったキューブについてレビューしていきます。\nなお、各キューブについては triboxストア または smartship store - スピードキューブ向けパズル・立体パズル販売店 で購入いたしました。\nどちらのショップも注文から発送までのスピードが早く、取り扱っているキューブ (に限らず立体パズル全般) の種類が豊富です。\nGAN356 X Numerical IPG Stickerless 一番の特徴は、バネの調整段階が 3 段階で設定できることと、磁力の強さを変更できることです。\nまずバネの調整について。\nこれまで各キューブにはバネの調整機構はあったものの、単にネジを回すだけのものが多く、その調整段階は無段階でした。無段階ということで微調整ができるというメリットはありますが、 6 面分すべてのバネの強さを均一に調整することは非常に難しいです。\nGAN356 X では、固定の段階数にすることによって各面のバネの調整が均一にできるようになっています。\n次に磁力の調整について。\nキューブの回転面にマグネットが搭載されており、回転をアシストしてくれるようなキューブはたくさんあります。が、 GAN356 X ではそのマグネットアシストの磁力を調整することができます。\nバネと磁力、双方があらかじめ用意された段階で調整が可能なので、より安定した設定を実現したり、他の人と同じ設定を再現したりすることが簡単にできるようになっています。\n参考価格： 6,900 円\nGAN249 M V2 Stickerless 初めての 2x2x2 キューブなので他のキューブとの比較はありませんが、マグネットアシスト (調整不可) 付き、バネの調整も可能です。 バネは、付属する専用の工具を使って六角のパーツを回転させることで調整可能です。\n参考価格： 2,200 円\nGAN354 M Stickerless GAN354 M は去年買ったんですが、当時一番使いやすいキューブだったので、もう１つ購入しました。\n詳細なレビューについては過去の買いているので、そちらを参照してください。\n2 個買うくらい非常に良いキューブです。\n名前の通り約 54 mm と少し小さめなので、手の小さい自分にはマッチしています。\nただし、バネの調整に関しては無段階のため各面で均一に調整することが難しいですが、バッドポイントはそこくらいです。\n参考価格： 4,700 円\nGAN356 i Stickerless パッと見で他のキューブと違うのは、充電クレードルに乗っているというところです。\nGAN356 i はスマホアプリと連動するスマートキューブで、タイム計測はもちろん、ソルブ時の各フェーズでかかった時間、手数、回転のスムーズさなどを数値で確認することができます。\nこれがスマートキューブ...凄い！ pic.twitter.com/6y7yZACqz0\n\u0026mdash; よっしー@CBR853RR (@michimani210) September 28, 2019 基本的な構造は GAN356 X と同じですが、下記が異なります。\nバネの調整が 2 段階であること 磁力の調整はできないこと 表面がマット加工されていること あくまでもこれはスマホと連動させて楽しむキューブです。\n参考価格： 8,800 円\nGAN356 XS Stickerless GAN356 X の後継モデルにあたるキューブで、バネの調整、磁力の調整、内部構造がパワーアップしています。\nまずバネの調整について。\nGAN356 X と同様にあらかじめ設定された段階で設定が可能になっていますが、調整方法が変わっています。\nGAN356 X では GES というパーツを用いて、 12 段階で調整が可能でした。 GAN356 SX では 新たに採用された GES Pro というパーツを用いて、 24 段階で調整が可能になっています。\n次に磁力の調整について。\nGAN356 X では、マグネットパーツを別の強さの違うものと入れ替えて調整することが可能でした。GAN356 XS では別パーツは不要になり、キューブ本体についているマグネットパーツ部分を 内 または 外 に移動させることで強さを調整することができるようになっています。\n下記が GAN356 X からの違いになります。\nバネの調整がより細やかに 磁力の調整に別パーツが不要に 内部構造の変更 表面がマット加工されている かなり軽い 冬に手が乾燥する身としては表面のマット加工が辛いですが、現時点では一番回しやすいキューブで、単発で出たタイム (非公式) も一番早いです。\n参考価格： 6,545 円\nQiYi Thunderclap 3x3x3 V3 M Stickerless 安価でマグネットアシストが付いているキューブです。\nGAN シリーズのようにバネ調整がチープ (プラスドライバーでの調整) だったり、磁力の調整がなかったりしますが、回し心地は非常に良いです。\nGAN シリーズはコーナーカットが優秀で、ある程度無理な位置であっても回転させることができますが、Thunderclap はそれらに比べるとしっかりとした位置に持っていってからでないと回転させることができないという印象です。\nこれまで GAN シリーズばかり使っていたので感覚はかなり違いますが、だからと言ってタイムが全然出ない訳ではありません。\n気分転換に回しています。\n大きさは、 GAN354 M よりも若干大きく、 GAN356 X (XS) よりは小さいくらいです。\n参考価格： 1,265 円\nYuXin HuangLong 3x3x3 M 買ったキューブの中で、唯一のステッカータイプのキューブです。\nこちらも安価でマグネットアシストが付いているキューブで、磁力の調整は不可、バネの調整はプラスドライバーで行います。\n回し心地としては、 Thunderclap と同様にしっかりと回さないといけないという印象ですが、この価格でマグネットアシスト付きと考えると、良いキューブだと思います。\nただ、やはり自分にはステッカーレスのキューブが合っているなと思いました。\n大きさは、 GAN354 M よりも若干大きく、 GAN356 X (XS) よりは小さいくらいです。\n参考価格： 2,200 円\nDaYan GuHong V3 M Stickerless こちらも安価でマグネットアシストが付いているキューブで、磁力の調整は不可、バネの調整はプラスドライバーで行います。\nいま世に出ているキューブの中で、回している時の音が最も静かだと言われているキューブ () の廉価版にあたります。\n廉価版とはいえ確かに音は静かで、持っているキューブの中でも一番音が静かです。\n大きさは GAN354 M と同じくらいの大きさです。\n参考価格： 1,375 円\nステッカータイプとステッカーレスタイプ 今年買ったキューブのうち、 YuXin HuangLong 3x3x3 M だけが ステッカータイプ で、残りは全て ステッカーレスタイプ です。\n違いとしては各面の色のステッカーが貼ってあるのか、キューブそのものに色がついているのかという違いです。\nこれによって、手触りと見える色に違いが出てきます。\n特に 見える色 についてはタイムに大きく関わってくる要素だと思っています。\nステッカータイプは、黒のキューブに各色のステッカーが貼ってあります。ステッカーは各面の縁まで覆っている訳ではなく、下地の黒の部分が残っているのが普通です。\nそのため、見えている色は 6 面の 6 色 ＋ 下地の 1 色 の全部で 7 色になります。\nスピードキューブでは各色をみて先読みをしたりするので、その際に不要な色が目に入ってくるとノイズになります。\n一方でステッカーレスタイプでは、キューブ本体に色がついているため各面の 6 色以外の色は存在しません。\nまた、各面を真正面から見た状態でも側面の色が若干わかります。\nステッカータイプと比べると各色の明度が高く、蛍光色っぽくなっているものがほとんどです。\nステッカータイプとステッカーレスタイプの違いとしてこの 見える色 については非常に大きい違いです。もちろん、触り心地や色の感じ方によって最適なキューブは変わってきますが、どっちを買えば良いか迷っている方はとりあえずステッカーレスを買うことをおすすめします。\nまとめ 2019 年のスピードキューブ活動について、今回は今年買ったキューブの紹介でした。\n安価なキューブから高性能なキューブまで色々買いましたが、個人的に使いやすいキューブベスト 3 は、 1 位から順に GAN356 XS Stickerless 、 GAN356 X Numerical IPG Stickerless 、 GAN354 M Stickerless です。\nやはり GAN シリーズのキューブはとても回しやすく、実際に好タイムも出ているので自然と回す時間も長くなっています。\nただ、キューブひとつに 4,000 〜 6,000 円というのはなかなかパッと出せる金額ではないので、とりあえずスピードキューブ始めてみたいという方には QiYi Thunderclap 3x3x3 V3 M Stickerless がおすすめです。実際、今回紹介した中で 4 番目に選ぶとしたらこのキューブです。\n千円台でマグネットアシストもついて、回し心地もバネの調整次第で良い感じになるので、コスパはかなり高いです。\nおそらく来年もたくさんの新しいキューブが登場すると思うので、キューブコレクターとして (?) さらに色々なキューブを試してみようと思います。\n",
    "permalink": "https://michimani.net/post/other-speedcube-summary-2019-cube/",
    "title": "2019 年のスピードキューブ活動について振り返ってみる - 購入したキューブ編"
  },
  {
    "contents": "AWS 認定資格のうち、アソシエイトレベルの 3 種類 (DVA, SAA, SOA) に合格したので、その勉強法とかについて書いていきます。\n目次 TL;DR 前置き 受験した試験について Developer Associate (DVA) SysOps Administrator Associate (SOA) 試験対策 (明示的な) 対策期間 (明示的でない) 対策期間 まとめ TL;DR Developer Associate (DVA) と SysOps Administrator Associate (SOA) に 1 ヶ月の対策で合格しました 明示的な試験対策以外にも日頃からの情報収集が大事 来年は Professional 2 種類 合格したい 前置き タイトルでは アソシエイトレベル 3 種類 と書いていますが、今回の記事では 12 月に受験して合格した Developer Associate (DVA) と SysOps Administrator Associate (SOA) について書きます。\nSolutions Architect Associate (SAA) については 6 月に受験して合格した際に記事を書いているので、そちらも参考にしてみてください。\nまた、 AWS 認定資格そのものについても上の記事で書いてますので、今回は省略します。\n受験した試験について 今回受験したのは Developer Associate (DVA) と SysOps Administrator Associate (SOA) の 2 試験です。\nDVA は開発者向け、 SOA は運用者向けの資格試験という感じですが、内容的には SAA と重複するような部分も多くあります。\nじゃあ、実際にどのあたりが違うのか、 SAA との違い、追加で試されているなと思った部分などを、個人の感想レベルですが書いていきたいと思います。\nDeveloper Associate (DVA) 開発者向けの資格試験ということで、 AWS のリソースを使った開発における問題が出るようになっています。\n例えば、\nCLI からリソースを操作するときの具体的なコマンド DynamoDB のユニットキャパシティの計算 ある操作をする際に実行すべき API など、実際にある程度 AWS 環境での開発をしていないとわからないような問題が出てきます。\nまた問題にされるサービスもより開発に近いサービスになっていたり、それぞれのサービスを利用する際の詳しい手順や実際のユースケースでの課題解決方法なども出てきます。\n下記はサービスと問題の例です。\nSQS 効率的なメッセージの取得方法 Lambda リトライされる条件、同時実行数制限 CodeCommit, CodeBuild, CodeDeploy, CodePipeline 実行時のエラー理由 (権限不足など) API Gateway 複数環境へのデプロイ などなど、とにかく触ってないとわからないなという印象でした。\n実際に自分が受験したときは ECR, ECS といったコンテナ関連の問題も出題されており、そのへんはほとんど触ったことがなかったので苦戦しました。\nこれらはあくまでも SAA からの追加要素なので、 SAA の中にあった可用性やセキュリティに関する問題も出ますし、それらの内容もより突っ込んだというか、実際に設定する値とかまで踏み込んでいるという印象です。\nAWS 認定デベロッパー – アソシエイト認定 SysOps Administrator Associate (SOA) 運用者向けの資格試験ということで、 AWS 環境でのアプリケーションの運用に関する問題が出るようになっています。\n例えば、\nオンプレ環境からの AWS 環境への移行 オンプレ環境と AWS 環境の連携時のネットワーク設定 適切な IAM ロールの設定 などです。ざっくりとまとめると、様々なケースでのベストプラクティスを適切に導き出せるか、というポイントが大きいと思います。\nなので、 DVA と同じく SOA に関しても、ある程度 AWS 環境でいろんなサービスを触っていたり、様々なユースケース、事例を知っている必要があります。\n特にセキュリティに関する内容は出題数も多くなっていて、どのサービスで何ができるのか (異常を検出するだけなのか、その後の修正・パッチ適用までできるのか など) を理解していないといけません。\nSAA からの追加でよく問題に出てくるサービスと、その例については下記のような感じです。\nAWS Systems Manager EC2 へのセキュリティパッチ適用 パラメータの管理 CloudFormation 既存リソースの適切なアップデート手順 Route 53 各ルーティングポリシーの特徴 Direct Connect ユースケース、メリット DVA と比較すると、実際に触ってはいなくても情報さえ入れておけば乗り切れるところもありますが、やはり触っていることに越したことはないです。\nSOA についても SAA に出ていた内容が引き続き出たり、より深く聞いてきたりします。\n例えば IAM に関してだと、実際に実行したいアクションに必要なポリシーは何か、というところも抑えておくとスコアが伸びると思います。\nAWS 認定 SysOps アドミニストレーター – アソシエイト認定 試験対策 では、実際に DVA と SOA の受験に対する対策として何をやってきたかを書いていきます。\n(明示的な) 対策期間 SAA に受験・合格したのが 7 月頭で、そこから 12 月中旬まで約半年ありますが、何か特別なことをし始めたのは試験の 1 ヶ月前からです。\n11 月中旬になって「他のアソシエイト試験、来年に持ち越すのはなんか気持ち悪いなー。なんとなく行けそうな気がするし受けるか」と思って受験日を決めました。\n何回も試験会場に行くのは面倒なので、 2 つの試験を同日の午前と午後に入れました。\nなかなかハードスケジュールですが、自分はそれくらいしないと火が着かないので、追い込まれてやる気が出る人にはおすすめです。\n1 ヶ月間でやったこと じゃあその 1 ヶ月で何をやったかというと、資格対策本を読んで、模擬試験を受けて、各サービスの概要を調べて、もう一度同じ資格対策本を読む\u0026hellip;というのをやりました。スケジュール感は下記のとおりです。\n時期 やったこと 11 / 13 資格対策本を読み始める 12 / 11 模擬試験を受ける 12 / 11 〜 各サービスの概要を調べる (AWS 公式ページ) 12 / 18 資格対策本を読み終える (1回目) 12 / 21 資格対策本を読み終える (2回目) 12 / 22 (試験当日) 資格対策本の気になるところを読む だいぶ詰め込みすぎました。\n対策本について 今回使用した対策本は AWS認定アソシエイト3資格対策~ソリューションアーキテクト、デベロッパー、SysOpsアドミニストレーター~ という本です。\nAWS 認定資格の試験対策として物理的な本を買うというのは、正直乗り気ではありませんでした。というのも、日々アップデートがされるサービスに対して、ある一時点の情報をもとにして作成されたものでは対策が十分でない場合があるからです。\nあとは、対策本を買ってしまうと、資格の取得が最終目的になってしまうようなイメージもあって、敬遠していました。\nただ今回はこの本を買ってみて、まあまあ満足しています。\nアソシエイト 3 種類の対策本となっていることから、分野ごと (コンピューティング、ストレージ、セキュリティ など) に章立てされて各サービスの解説がされており、それぞれの章末には問題が数問ついています。\nまた、各分野に対してどの試験でよく聞かれるかという分類もされているので、各試験の対策として必要な部分を読むことができます。\nさらに、 DVA や SOA の範囲に入ってくるであろうサービスに関しては、 AWS のチュートリアルページのリンクも記載されています。そこも合わせてチェックできればとても効果的です。\nただし、各章の章末問題に関しては正直難易度が低すぎるので、直接実際の試験問題の対策になることはありませんでした。あくまでもその章に出てくるサービスの概要確認というレベルで捉えたほうがいいです。\n模擬試験について 今回は模擬試験を試験当日の 10 日前くらいに受けたんですが、この時点では DVA の正答率が 75 % 、 SOA の正答率が 65 % でした。 ちなみに本試験では 72 % (720/1000) が合格ラインです。\nただ、模擬試験についてはあくまでも模擬試験なので、ここでの正答率に一喜一憂するのは良くないです。\n前回 と同じように、わからなかった問題に出てくるサービスについては公式ページで概要と、「よくある質問」ページを見て理解を深めます。\n模擬試験の受験には 2,000 円 + 税/回 かかるので何度も受験するのは現実的ではないですが、 1 回は受けておいたほうがよいと思います。\nまた、個人的には模擬試験を受けるタイミングは、試験対策を始めてからあまり日が経っていない頃が良いと思います。\n理由としては、本試験直前に模擬試験を受けて、スコアが低いと精神的ダメージを受けるからです。\n対策開始当初であればスコアが低いのは当たり前なのでダメージも小さく、また早期に自分の苦手なところが発見できるというメリットもあります。\nこれに関しては人それぞれ (と言ってしまえばおしまい) ですが、勉強の仕方としては良いかなと思います。\n(明示的でない) 対策期間 まあこれは日頃からどれだけ AWS 対して触れているか (サービス利用、情報収集) ということです。\n実際に対策本を読んで模擬試験を受けて\u0026hellip;とやったのは 1 ヶ月程度でしたが、 SAA 合格からの半年、というかこの 1 年は AWS に関して触れている時間が結構長かったと感じています。\n例えば、 AWS 関連のセミナーとかハンズオンに参加してみたり、以下のブログや公式のお知らせで日々のアップデートを追いかけたりしていました。\nWhat’s New at AWS – Cloud Innovation \u0026amp;amp; News AWS Startup ブログ クラスメソッド発「やってみた」系技術メディア | Developers.IO もちろん、それらから得られた情報すべてを理解して自分のものにできているわけではないですが、「あのサービスってこういう使い方するんやー」とか「新しくあんな機能追加されたんやー」とか、なんとなくでも情報を得ることはできます。\nDVA や SOA では、実際の開発や運用などのより具体的な内容に関して出題されるので、セミナーや各種ブログで情報を追っていたのは大きかったと思います。\nまとめ AWS 認定資格 アソシエイトレベルの 3 種類 (DVA, SAA, SOA) のうち、 DVA と SOA の勉強法とか SAA との違いについて書きました。\nなかなかハードスケジュールでしたが、なんとか年内にアソシエイト 3 種類合格することができて良かったです。\nAWS に関するイベントに行くと、登壇者のスライドの事項紹介ページで認定バッチが並んでいる場面をよく見ますが、自分もとりあえず 3 つ並べる事ができるようになって、ニヤニヤしています。\nただ、認定資格の取得でおわりではなく、引き続き日々のアップデートを追いかけて、実際に手を動かして、さらに理解を深めていく必要があると思っています。\n来年は Professional レベルの 2 種類の取得を目標にがんばります。\nSAA の合格体験記は検索でもたくさんヒットするのですが、 DVA と SOA に関してはあまりそういった記事がないという印象だったので、受験を考えている方の参考になれば幸いです。\n",
    "permalink": "https://michimani.net/post/aws-dva-soa-associate-exam/",
    "title": "AWS 認定資格アソシエイトレベル 3 種類すべてに合格しました"
  },
  {
    "contents": "AWS CDK を使って独自ドメインを使用した静的サイトを S3 でホスティングするための環境を構築してみたので、今回はその話です。\n前置き これは AWS初心者 Advent Calendar 2019 18 日目の記事です。\n4 日目に引き続き 2 枠目になりますが、空いていたので滑り込みで入れさせてもらいました。\n4 日目に書いた記事はこちら。\n概要 タイトルの通り、 AWS CDK を使って独自ドメインを使用した静的サイトを S3 でホスティングするための環境を構築します。\n本当は Hugo という静的サイトジェネレータの CI/CD 環境を CDK を使って整えようとしていたのですが、一旦その前段階として、とりあえず S3 で独自ドメインの静的サイトをホスティングする環境を構築してみます。\n最終的にできあがる環境は、下図のようになります。\nめちゃくちゃシンプルです。\n前提 AWS CDK がインストールされている (執筆時点で最新の 1.18.0 1.19.0 を使用します)\n※この記事公開当日に 1.19.0 がリリースされました。 Route 53 で独自ドメインのホストゾーンが作成されている\n今回は既存のホストゾーンに対してレコードを追加するような構成を考えます。 やること 流れとしては下記の通りです。\nCDK プロジェクトを作成 実装 デプロイ とりあえず分けましたが、一番ボリュームが大きいのが 2. 実装 になります。 (それはそう)\nでは、一つずつ順番にやっていきます。\nなお、ここから紹介するコードについては GitHub に置いていますので、適当にいじってみてください。 (アドバイスもいただけると幸いです)\nmichimani/static-website-sample: This is a sample project of static web site hosted on S3 for TypeScript development with CDK. 1. CDK プロジェクトを作成 まずは CDK プロジェクトを作成します。\n$ mkdir static-website-sample $ cd static-website-sample $ cdk init app --language=typescript $ tree . -L 1 . ├── README.md ├── bin │ └── static-website-sample.ts ├── cdk.context.json ├── cdk.json ├── cdk.out ├── jest.config.js ├── lib │ └── static-website-sample-stack.ts ├── node_modules ├── package-lock.json ├── package.json ├── test └── tsconfig.json こんな感じのディレクトリ構成が作られます。\nCDK のロジックは lib/static-website-sample-stack.ts に記述していきます。\n2. 実装 では、メインの実装です。\n必要なモジュールをインストール 実装を始める前に、必要なモジュールをインストールします。今回使用するモジュールは、 @aws-cdk/aws-s3, @aws-cdk/aws-s3-deployment, @aws-cdk/aws-route53, @aws-cdk/aws-route53-targets, js-yaml, @types/js-yaml です。\n$ npm install @aws-cdk/aws-s3 @aws-cdk/aws-s3-deployment @aws-cdk/aws-route53 @aws-cdk/aws-route53-targets js-yaml @types/js-yaml 独自の設定ファイルの読み込み 今回のポイントとして、独自ドメインは動的に変更できるように独自の設定ファイルに分離させます。\nということで、まずはその独自の設定ファイルから実装していきます。\n設定ファイル自体は YAML 形式で stack-config.yml として作成します。\ncommon: region: ap-northeast-1 route53: zone: michimani.net zone_id: ABCD1234567890 sub_domain: static-website-sample これらの値の定義を lib/Config.ts というファイルで作成します。内容は下記のとおりです。\nimport yaml = require(\u0026#39;js-yaml\u0026#39;); import fs = require(\u0026#39;fs\u0026#39;); export interface commonConfig { region: string; } export interface Route53Config { zone: string; zone_id: string; sub_domain?: string; } export interface StackConfig { common: commonConfig, route53: Route53Config } const stackConfig: StackConfig = yaml.safeLoad(fs.readFileSync(\u0026#39;stack-config.yml\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})); export class StackConfig { constructor() { const stackConfig = yaml.safeLoad(fs.readFileSync(\u0026#39;stack-config.yml\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})); return stackConfig; } } この設定を読み込ませるために、 bin/static-website-sample.ts に下記のように実装します。\n#!/usr/bin/env node import \u0026#39;source-map-support/register\u0026#39;; import cdk = require(\u0026#39;@aws-cdk/core\u0026#39;); import { StaticWebsiteSampleStack } from \u0026#39;../lib/static-website-sample-stack\u0026#39;; import * as Config from \u0026#39;../lib/Config\u0026#39;; const app = new cdk.App(); const stackConfig = new Config.StackConfig(); new StaticWebsiteSampleStack(app, \u0026#39;StaticWebsiteSampleStack\u0026#39;, stackConfig, { env: { region: stackConfig.common.region }, }); また、 StaticWebsiteSampleStack で stackConfig を受け取れるように、 lib/static-website-sample-stack.ts のコンストラクタに変更を加えます。\n+ import * as config from \u0026#39;./Config\u0026#39;; export class StaticWebsiteSampleStack extends cdk.Stack { - constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) { + constructor(scope: cdk.Construct, id: string, stackConfig: config.StackConfig, props?: cdk.StackProps) { super(scope, id, props); これで、 stack-config.yml に記述した値を使用できるようになりました。\n可変の値の設定方法については色々なやり方があると思うので、もっと良い方法があればぜひ教えていただきたいです。\n各リソースを生成 諸々の準備が整ったので、各リソースを生成するためのコードを lib/static-website-sample-stack.ts に書いていきます。\nまず、最初にインストールしたモジュールを読み込みます。\nimport s3 = require(\u0026#39;@aws-cdk/aws-s3\u0026#39;); import s3deploy = require(\u0026#39;@aws-cdk/aws-s3-deployment\u0026#39;); import route53 = require(\u0026#39;@aws-cdk/aws-route53\u0026#39;); import route53targets = require(\u0026#39;@aws-cdk/aws-route53-targets\u0026#39;); また、今回は独自ドメインを使用する際に、サブドメインを使用する場合と使わない場合があります。それによって作成するリソースに設定する値も変わってくるので、サブドメインを使用するかどうかをフラグで持っておきます。\nconst useSubDomain: boolean = (typeof stackConfig.route53.sub_domain !== \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; stackConfig.route53.sub_domain !== \u0026#39;\u0026#39; \u0026amp;\u0026amp; stackConfig.route53.sub_domain !== null); リソース作成の順番としては、次のようになります。\nS3 バケットの作成 バケット内に静的ファイルを設置 Route 53 で S3 バケットに対するレコードを追加 確認 では、順番に実装部分を見ていきます。\nまずは S3 バケットの作成です。\nconst bucketName: string = (useSubDomain === true) ? `${stackConfig.route53.sub_domain}.${stackConfig.route53.zone}` : stackConfig.route53.zone; const bucket: s3.Bucket = new s3.Bucket(this, \u0026#39;StaticWebsiteSampleBucket\u0026#39;, { bucketName, publicReadAccess: true, websiteIndexDocument: \u0026#39;index.html\u0026#39;, websiteErrorDocument: \u0026#39;error.html\u0026#39;, removalPolicy: cdk.RemovalPolicy.DESTROY }); ポイントとしては、 バケット名を独自ドメインと同じ形式にする というところです。これは独自ドメインを使用して S3 で静的サイトをホスティングする際の条件になっています。\n例: 独自ドメインを使用して静的ウェブサイトをセットアップする - Amazon Simple Storage Service 続いて、このバケットに HTML などの静的ファイルを設置します。\n今回は ./public というディレクトリを作り、その中に index.html と error.html を作成しておきます。\n$ tree ./public ./public ├── error.html └── index.html 実際にオブジェクトを設置するコードは下記のようになります。\nconst sampleHtmlPut = new s3deploy.BucketDeployment(this, \u0026#39;SampleHtmlDeploy\u0026#39;, { destinationBucket: bucket, sources: [s3deploy.Source.asset(\u0026#39;./public\u0026#39;)] }); ここで destinationBucket には先程作成した bucket を指定しています。 CDK ではこのような形で各リソースの依存関係を簡単に構築することができます。\n最後に、 Route 53 で独自ドメインのレコード設定を実装します。\nconst myZone: route53.IHostedZone = route53.HostedZone.fromHostedZoneAttributes(this, \u0026#39;MyZone\u0026#39;, { zoneName: stackConfig.route53.zone, hostedZoneId: stackConfig.route53.zone_id }); const record: route53.ARecordProps = { zone: myZone, target: route53.AddressRecordTarget.fromAlias(new route53targets.BucketWebsiteTarget(bucket)), recordName: (useSubDomain === true) ? stackConfig.route53.sub_domain : stackConfig.route53.zone } const hostRecord = new route53.ARecord(this, \u0026#39;StaticWebsiteSampleRecord\u0026#39;, record); 前半部分では、ゾーン名とゾーン ID から既存のホストゾーンをインポートしています。\nそして、そのゾーンに対してレコードを追加しています。\n先ほどと同じように、レコードのエイリアスとなる S3 バケットの情報は、最初に作成した bucket を渡すことで成立しています。\nrecordName の値は、サブドメインを使用する場合はその ホスト部分のみ を設定します。サブドメインを使用しない場合は、 recordName を省略するか、ゾーン名を設定します。\n実装は以上となります。\nちなみに、 bin/static-website-sample.ts 内で明示的にリージョンを指定してるのは、 Route 53 のレコードのエイリアスとして S3 バケットを指定する際の制限となります。\n指定せずにビルドしようとすると、下記のようなエラーが出ます。\nCannot use an S3 record alias in region-agnostic stacks. You must specify a specific region when you define the stack (see https://docs.aws.amazon.com/cdk/latest/guide/environments.html) Subprocess exited with error 1 Environments - AWS Cloud Development Kit (AWS CDK) 3. デプロイ 実装が完了したので、デプロイします。\nデプロイコマンドは cdk deploy ですが、その前に、実際に出力される CloudFormation テンプレートを cdk synth コマンドで出力してみます。\n$ cdk synth { sourceHash: \u0026#39;6416c21be320b522db64c705872c0a54d788e3df57b34a5f0d1e8602d7521430\u0026#39; } Resources: StaticWebsiteSampleBucketABCDEFGH: Type: AWS::S3::Bucket Properties: BucketName: static-site-sample.michimani.net WebsiteConfiguration: ErrorDocument: error.html IndexDocument: index.html UpdateReplacePolicy: Delete DeletionPolicy: Delete Metadata: aws:cdk:path: StaticWebsiteSampleStack/StaticWebsiteSampleBucket/Resource StaticWebsiteSampleBucketPolicyF002703A: Type: AWS::S3::BucketPolicy Properties: Bucket: Ref: StaticWebsiteSampleBucketABCDEFGH PolicyDocument: Statement: - Action: s3:GetObject Effect: Allow Principal: \u0026#34;*\u0026#34; Resource: Fn::Join: - \u0026#34;\u0026#34; - - Fn::GetAtt: - StaticWebsiteSampleBucketABCDEFGH - Arn - /* Version: \u0026#34;2012-10-17\u0026#34; Metadata: aws:cdk:path: StaticWebsiteSampleStack/StaticWebsiteSampleBucket/Policy/Resource ... という感じで出力されます。\nCDK のコードとしては 40 行程度ですが、 CloudFormation テンプレートにすると 200 行近くになります。\nテンプレートを出力してみると、あらためて CDK の恩恵を感じられます。\nでは、 cdk deploy コマンドでデプロイしてみます。\n$ cdk deploy { sourceHash: \u0026#39;6416c21be320b522db64c705872c0a54d788e3df57b34a5f0d1e8602d7521430\u0026#39; } This deployment will make potentially sensitive changes according to your current security approval level (--require-approval broadening). Please confirm you intend to make the following modifications: IAM Statement Changes ┌───┬─────────────────────────────────────────────────────────────────────────────┬────────┬─────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────┬───────────┐ │ │ Resource │ Effect │ Action │ Principal │ Condition │ ├───┼─────────────────────────────────────────────────────────────────────────────┼────────┼─────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────┤ │ + │ ${Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole.A │ Allow │ sts:AssumeRole │ Service:lambda.amazonaws.com │ │ │ │ rn} │ │ │ │ │ ├───┼─────────────────────────────────────────────────────────────────────────────┼────────┼─────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────┤ │ + │ ${StaticWebsiteSampleBucket.Arn} │ Allow │ s3:Abort* │ AWS:${Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRol │ │ │ │ ${StaticWebsiteSampleBucket.Arn}/* │ │ s3:DeleteObject* │ e} │ │ │ │ │ │ s3:GetBucket* │ │ │ │ │ │ │ s3:GetObject* │ │ │ │ │ │ │ s3:List* │ │ │ │ │ │ │ s3:PutObject* │ │ │ ├───┼─────────────────────────────────────────────────────────────────────────────┼────────┼─────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────┤ │ + │ ${StaticWebsiteSampleBucket.Arn}/* │ Allow │ s3:GetObject │ * │ │ ├───┼─────────────────────────────────────────────────────────────────────────────┼────────┼─────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────┤ │ + │ arn:${AWS::Partition}:s3:::${AssetParameterscb52f748a3861e8690fd5f1fc2d76ed │ Allow │ s3:GetBucket* │ AWS:${Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRol │ │ │ │ 9a8bfc149334ddca4e31d2ba441ff5634S3Bucket5A5BA922} │ │ s3:GetObject* │ e} │ │ │ │ arn:${AWS::Partition}:s3:::${AssetParameterscb52f748a3861e8690fd5f1fc2d76ed │ │ s3:List* │ │ │ │ │ 9a8bfc149334ddca4e31d2ba441ff5634S3Bucket5A5BA922}/* │ │ │ │ │ └───┴─────────────────────────────────────────────────────────────────────────────┴────────┴─────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────┴───────────┘ IAM Policy Changes ┌───┬────────────────────────────────────────────────────────────────────────────┬────────────────────────────────────────────────────────────────────────────────┐ │ │ Resource │ Managed Policy ARN │ ├───┼────────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────┤ │ + │ ${Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole} │ arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole │ └───┴────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────┘ (NOTE: There may be security-related changes not in this list. See https://github.com/aws/aws-cdk/issues/1299) Do you wish to deploy these changes (y/n)? y StaticWebsiteSampleStack: deploying... StaticWebsiteSampleStack: creating CloudFormation changeset... 0/9 | 9:22:15 PM | CREATE_IN_PROGRESS | AWS::CDK::Metadata | CDKMetadata 0/9 | 9:22:15 PM | CREATE_IN_PROGRESS | AWS::IAM::Role | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole (CustomCDKBucketDeploymentRANDOMSTRING99999999999999999999ServiceRole89A01265) 0/9 | 9:22:15 PM | CREATE_IN_PROGRESS | AWS::Route53::RecordSet | StaticWebsiteSampleRecord (StaticWebsiteSampleRecord0C67AEAA) 0/9 | 9:22:15 PM | CREATE_IN_PROGRESS | AWS::S3::Bucket | StaticWebsiteSampleBucket (StaticWebsiteSampleBucketED610F77) 0/9 | 9:22:16 PM | CREATE_IN_PROGRESS | AWS::IAM::Role | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole (CustomCDKBucketDeploymentRANDOMSTRING99999999999999999999ServiceRole89A01265) Resource creation Initiated 0/9 | 9:22:17 PM | CREATE_IN_PROGRESS | AWS::CDK::Metadata | CDKMetadata Resource creation Initiated 0/9 | 9:22:17 PM | CREATE_IN_PROGRESS | AWS::Route53::RecordSet | StaticWebsiteSampleRecord (StaticWebsiteSampleRecord0C67AEAA) Resource creation Initiated 1/9 | 9:22:17 PM | CREATE_COMPLETE | AWS::CDK::Metadata | CDKMetadata 1/9 | 9:22:17 PM | CREATE_IN_PROGRESS | AWS::S3::Bucket | StaticWebsiteSampleBucket (StaticWebsiteSampleBucketED610F77) Resource creation Initiated 2/9 | 9:22:32 PM | CREATE_COMPLETE | AWS::IAM::Role | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole (CustomCDKBucketDeploymentRANDOMSTRING99999999999999999999ServiceRole89A01265) 3/9 | 9:22:38 PM | CREATE_COMPLETE | AWS::S3::Bucket | StaticWebsiteSampleBucket (StaticWebsiteSampleBucketED610F77) 3/9 | 9:22:41 PM | CREATE_IN_PROGRESS | AWS::IAM::Policy | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole/DefaultPolicy (CustomCDKBucketDeploymentRANDOMSTRING99999999999999999999ServiceRoleDefaultPolicy88902FDF) 3/9 | 9:22:41 PM | CREATE_IN_PROGRESS | AWS::S3::BucketPolicy | StaticWebsiteSampleBucket/Policy (StaticWebsiteSampleBucketPolicyF002703A) 3/9 | 9:22:42 PM | CREATE_IN_PROGRESS | AWS::IAM::Policy | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole/DefaultPolicy (CustomCDKBucketDeploymentRANDOMSTRING99999999999999999999ServiceRoleDefaultPolicy88902FDF) Resource creation Initiated 3/9 | 9:22:43 PM | CREATE_IN_PROGRESS | AWS::S3::BucketPolicy | StaticWebsiteSampleBucket/Policy (StaticWebsiteSampleBucketPolicyF002703A) Resource creation Initiated 4/9 | 9:22:43 PM | CREATE_COMPLETE | AWS::S3::BucketPolicy | StaticWebsiteSampleBucket/Policy (StaticWebsiteSampleBucketPolicyF002703A) 5/9 | 9:22:58 PM | CREATE_COMPLETE | AWS::IAM::Policy | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999/ServiceRole/DefaultPolicy (CustomCDKBucketDeploymentRANDOMSTRING99999999999999999999ServiceRoleDefaultPolicy88902FDF) 5/9 | 9:23:01 PM | CREATE_IN_PROGRESS | AWS::Lambda::Function | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999 (CustomCDKBucketDeploymentRANDOMSTRING9999999999999999999981C01536) 5/9 | 9:23:03 PM | CREATE_IN_PROGRESS | AWS::Lambda::Function | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999 (CustomCDKBucketDeploymentRANDOMSTRING9999999999999999999981C01536) Resource creation Initiated 6/9 | 9:23:04 PM | CREATE_COMPLETE | AWS::Lambda::Function | Custom::CDKBucketDeploymentRANDOMSTRING99999999999999999999 (CustomCDKBucketDeploymentRANDOMSTRING9999999999999999999981C01536) 6/9 | 9:23:06 PM | CREATE_IN_PROGRESS | Custom::CDKBucketDeployment | SampleHtmlDeploy/CustomResource/Default (SampleHtmlDeployCustomResource33CAFD7F) 7/9 | 9:23:21 PM | CREATE_COMPLETE | AWS::Route53::RecordSet | StaticWebsiteSampleRecord (StaticWebsiteSampleRecord0C67AEAA) 7/9 | 9:23:45 PM | CREATE_IN_PROGRESS | Custom::CDKBucketDeployment | SampleHtmlDeploy/CustomResource/Default (SampleHtmlDeployCustomResource33CAFD7F) Resource creation Initiated 8/9 | 9:23:46 PM | CREATE_COMPLETE | Custom::CDKBucketDeployment | SampleHtmlDeploy/CustomResource/Default (SampleHtmlDeployCustomResource33CAFD7F) 9/9 | 9:23:47 PM | CREATE_COMPLETE | AWS::CloudFormation::Stack | StaticWebsiteSampleStack ✅ StaticWebsiteSampleStack Stack ARN: arn:aws:cloudformation:ap-northeast-1:123412341234:stack/StaticWebsiteSampleStack/9857da30-1ffe-11ea-a5a2-000000000000 こんな感じで、生成されるリソースが表で出力され、 Do you wish to deploy these changes (y/n)? と聞かれます。そこで y をタイプすると、デプロイが開始されます。\nこのタイミングで、マネジメントコンソール上でもスタックのイベントが進行していることを確認することができます。\n4. 確認 では、ちゃんとデプロイできているのか確認してみます。\n今回は static-site-sample.michimani.net のドメインで静的サイトをホスティングしてみたので、そのドメインで確認してみます。\n通常ページ\n$ http http://static-site-sample.michimani.net HTTP/1.1 200 OK Content-Length: 392 Content-Type: text/html Date: Mon, 16 Dec 2019 12:59:06 GMT ETag: \u0026#34;770c6c57182c470d8748793c36c8d070\u0026#34; Last-Modified: Mon, 16 Dec 2019 12:23:42 GMT Server: AmazonS3 x-amz-id-2: ZuSWrclJxk4uQ9XCaNurCnUZqfV/O48OrHUOHpkXdrfMsRYpJ/gCWF6MpMfdRynasbQHpKXUO/0= x-amz-request-id: 614F5DF6A50B13A8 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Static Web Site Sample\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Static Web Site Sample\u0026lt;/h1\u0026gt; \u0026lt;small\u0026gt; This is a sample of static web page that crated by AWS CDK project. \u0026lt;/small\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; エラーページ\n$ http http://static-site-sample.michimani.net/not-exists-path/ HTTP/1.1 404 Not Found Content-Length: 405 Content-Type: text/html Date: Mon, 16 Dec 2019 13:00:00 GMT ETag: \u0026#34;88431d0e820cb2e640475abbae30e298\u0026#34; Last-Modified: Mon, 16 Dec 2019 12:23:42 GMT Server: AmazonS3 x-amz-error-code: NoSuchKey x-amz-error-detail-Key: not-exists-path/index.html x-amz-error-message: The specified key does not exist. x-amz-id-2: tysmHKDiqPdDuPRmQ2aKSjm1H5TfrB8GBJYKhZ9VZBVbUhQBlGKeXFPIvEWLi98rbwaZwA98YLM= x-amz-request-id: CB5A4EA6A195621A \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Static Web Site Sample\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Static Web Site Sample - Error Page\u0026lt;/h1\u0026gt; \u0026lt;small\u0026gt; This is a sample of static web page that crated by AWS CDK project. \u0026lt;/small\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ちゃんとデプロイできてますね。\nまとめ AWS CDK を使って独自ドメインを使用した静的サイトを S3 でホスティングするための環境を構築してみた話でした。\nシンプルな構成ではありますが、コマンドひとつでリソースが立ち上がり、各種連携がされるのは、やはり IaaS の良いところですね。\n今回は CloudFormation テンプレートを直接書くのではなく、 AWS CDK を利用しました。\nCDK を利用することによって、 TypeScript や Python, Java といった言語で CloudFormation テンプレートを作成することができます。テンプレート用の YAML を書くのはちょっと難しい\u0026hellip; と感じている人 (僕ですが) にも、 CDK であれば IaaS デビューしやすいかなと思います。\nただ、 CDK はあくまでも CloudFormation テンプレートを生成するために後から出てきたツールのため、生の CloudFormation テンプレートでは実現できても CDK では実現できない機能・リソースもあります。\n例えば、 CDK の CloudFront モジュールは現在 (1.18.0 1.19.0) は developer preview (public beta) となっています。そのため、 CloudFront を S3 の前段において SSL 化された静的サイトを構築しようとすると、 CDK では (production では) 実現できません。(Public Preview のため)\nそのほか、より複雑な構成を実現しようとすると、 CDK では難しいという場合もあるようです。\nこれに関しては CDK の GA 記念ミートアップの際にも話が出ていました。\nとは行っても CDK のアップデートスピードはかなり早いので、今後のアップデートには期待したいです。\n少し脱線しましたが、 CDK は IaaS デビューには凄くいいツールだと思うので、ぜひ簡単なリソース作成から試してみてはいかがでしょうか。\n",
    "permalink": "https://michimani.net/post/aws-create-static-website-hotsted-on-s3-using-cdk/",
    "title": "AWS CDK を使って独自ドメインの静的サイトを S3 でホスティングする環境を構築してみた"
  },
  {
    "contents": "クラスメソッドさん主催のイベント re:Growth 2019 TOKYO @アマゾンウェブサービスジャパン に行ってきたので、そのレポートです。\n内容は AWS re:Invent 2019 のふりかえりです。ラスベガスのお土産もありました！\n目次 概要 タイムスケジュール 各セッションの概要と感想 re:InInvent 総括 (AWS 金森さん) AWS Wavelength のこれから (株式会社ソラコム 片山さん) 「DeepComposer」〜 Deep Learning を学ぶ最高のツール (キムさん) Nitro Infrastructure 2019 (渡邊さん) セキュリティ系まとめと、みんなで使おう Detective (臼田さん) EKS on Fargate (城岸さん) ECS の次世代スケーリング戦略「Capacity Provider」 (濱田さん) 2019 年の ElastiCache アップデートと運用について (深澤さん) IAM/S3 Access Analyzer (吉井さん) Transit Gateway のアップデートまとめ (島川さん) まとめ 概要 re:Growth はクラスメソッドさんが主催するイベントで、前の週にラスベガスで開催されていた AWS re:Invent のふりかえり勉強会 という位置付けです。\n勉強会 となっていますが、内容としては re:Invent に参加したクラメソの社員さんたちが新サービスについての話を 5 〜 10 分という短い時間で次々に発表していくというものでした。\nイベント時間は 2 時間でしたが、あっというまに時間が過ぎていました。\nなお、イベントのハッシュタグは #cmregrowth でした。\n21階は初めて来た (@ アマゾンウェブサービスジャパン株式会社 - @awscloud_jp in 目黒区, 東京都) https://t.co/KZI0ALbMCP\n\u0026mdash; よっしー@CBR853RR (@michimani210) December 11, 2019 タイムスケジュール タイムスケジュールは次のような感じです。 2 時間に濃い内容がめちゃくちゃ詰まってます。\n18:30 受付 19:00 クラメソ菊池さんからのご挨拶 10:05 re:InInvent 総括 (AWS 金森さん) 19:20 AWS Wavelength のこれから (株式会社ソラコム 片山さん) 19:35 「DeepComposer」〜 Deep Learning を学ぶ最高のツール (キムさん) 19:45 Nitro Infrastructure 2019 (渡邊さん) 19:55 休憩 20:05 セキュリティ系まとめと、みんなで使おう Detective (臼田さん) 20:15 EKS on Fargate (城岸さん) 20:20 ECS の次世代スケーリング戦略「Capacity Provider」 (濱田さん) 20:25 2019 年の ElastiCache アップデートと運用について (深澤さん) 20:30 IAM/S3 Access Analyzer (吉井さん) 20:35 Transit Gateway のアップデートまとめ (島川さん) 20:40 クロージング 各セッションの概要と感想 各セッションの概要 (メモ) と簡単な感想を書いていきます。\nすでに登壇資料を公開していただいているものもありますので、そのリンクも載せています。\nre:InInvent 総括 (AWS 金森さん) re:Invent は 学習型 カンファレンス 来場者数は 65,000+ 、日本からは 1,700+ Game Day は後日 AWS Loft Tokyo でも開催 期間中は 74 件のアップデート。事前に 116 件のアップデート SNS で注目の新サービス TOP 3 Amazon Braket AWS OutPosts AWS Deep Composer とにかく回を増すごとに規模が大きくなってきていて、日本からの来場者もどんどん増えているようです。自分も来年は参加したい\u0026hellip;\nAWS Wavelength のこれから (株式会社ソラコム 片山さん) AWS Wavelengthのこれから | re:Growth 2019 Tokyo Wavelength\n5G の取り組みに向けたサービス 5G の特徴のうち、 低遅延 にフォーカスしたサービス 基地局と MNO の間を 趙低遅延 にする MNO のサーバ内に VPC を構築 5G ネットワークに AWS サービスを張り出すサービス AWS/KDDI/ソラコム で仕様を検討中 MEC : Mobile Edge Computing\nアプリケーションを Wavelength ゾーンにデプロイ する感じ\nローカル 5G\nクラメソ 5G が実現するかも Wavelength という単語はちらっと聞いたことがありましたが、内容にいついては全く知りませんでした。\n登壇資料内では詳細な図を用いて、 Wavelength によってネットワーク構成がどうなるのか触れられています。知らない部分が多くすごく壮大な話のような気がしていますが、なんかローカル 5G は凄そう。 (小並)\n「DeepComposer」〜 Deep Learning を学ぶ最高のツール (キムさん) [資料公開] re:Growth 2019 Tokyoで DeepComposer のことで発表してきました #reinvent #cmregrowth ｜ Developers.IO Deep Leaning\n言葉は知ってる コードレベルでは少し 詳細はほぼいない cycleGAN\n論文の数式は複雑に見えるが直感的にはわかりやすい(？)\nあらかじめモデルは用意されているが、自分で作ることもできる\nそれが SageMaker\nGANs : きっと役に立つ\nGANs のコード\n各レイヤーごとに 1 行 (くらい) のコードで書ける Depp Learning の世界でホットな GAN (Generative Adversarial Network) (GANs : Generative Adversarial Networks) に関する内容から入って、最終的には「Deep Leaning 始めよう！」という話でした。(ざっくり)\nDepp Learning というと複雑な数式やコードに直面して挫折する人もいますが、今回発表された DeepComposer や、 SageMaker 関連の各新サービスを使えばすごく簡単に始められるということでした。今年の re:Invent では機械学習関連のアップデートが多かったので、自分も機械学習について勉強していこうと思いました。\nNitro Infrastructure 2019 (渡邊さん) [資料公開] Nitro インフラストラクチャの進化、あるいは如何に AWS が Nitro を大切に思っているかという話を re:Growth 2019 TOKYO で話してきました #reinvent #cmregrowth ｜ Developers.IO Nitro 仮想ホスト基盤のこと\nNitro Hypervisor, Nitro Card, Nitro Security Chip \u0026hellip;\n2017 年に初めて公開\nNitro のおかげでセキュリティも向上\nNitro はすごい\nAWS の各種サービス (EC2, Lambda\u0026hellip;) を支える基盤の話。Nitro の登場によってコンピューティングの性能向上、セキュリティの向上、コストの最適化などが実現し、特に、機械学習業界やメディア業界は大きな恩恵を受けられているようです。\nAWS の根幹を担う Nitro、すごい！\n2017年 AWS「Nitro作ったよ〜」#cmregrowth\n\u0026mdash; よっしー@CBR853RR (@michimani210) December 11, 2019 セキュリティ系まとめと、みんなで使おう Detective (臼田さん) 「AWS上でインシデントの調査 Amazon Detective」というタイトルでre:Growth 2019に登壇しました #reinvent #cmregrowth ｜ Developers.IO AWS WAF v2 中の仕組みを変えて別機能としてリリース v1 は classic AWS WAF という名称に JSON で WebACL を定義 (簡単になった) ルール 10 個制限廃止 API も wafv2 に統合 IMDSv2 インスタンスメタデータ v2 メタデータ取得には Token が必要 v1 を無効化できる Amazon Detective インシデント調査が捗るサービス これまで 検知 : Gard Duty 調査 : Cloud Trail, VPC フローログなどを Athena で解析 ← これが大変だった Detective だとその大変な調査部分がすごく簡単になる Findings プロトコルごとのリクエスト数 通常より多いと赤で表示されたりする マルチアカウント、マルチリージョンで集約が可能 まずはプレビューに申し込みましょう セキュリティ系アップデートのまとめと、 Amazon Detective の話でした。\nAmazon Detective はインシデントを検知したあとの調査をとても楽にしてくれるサービスで、検知したインシデントをグラフで表示したり、そこから詳細な情報を追うことも簡単にできるようです。\n東京リージョンでも使えるので、まずはプレビューに申し込みましょう！とのことでした。\nEKS on Fargate (城岸さん) 「Fargate for EKS」というタイトルでre:Growth 2019に登壇しました #reinvent #cmregrowth ｜ Developers.IO Kubernetes おさらい コンテナオーケストレーションツール k8s クラスタ Master : クラスタ自体を管理する Worker : コンテナの実行環境 Master をマネージドしてくれたのが EKS これまで EC2 の上に Pod を立てる これから (EKS on Fargate) Fargate の上に Pod が立つ 制約 ELB は ALB のみ パブリック IP が付与できない などなどの制約 コスト的に安くなるかはアプリケーション次第 正直 Kubernetes に関してはすごく難しい！というイメージだったんですが (今もですが) 、冒頭のおさらいのところで少しイメージができた気がしました。\n今回の EKS on Fargate (Fargate for EKS とも言っていたような) は、 EKS でのコンテナ実行環境に EC2 ではなく Fargate を使うことができるようになるというもので、 EC2 の管理に費やしていた時間を削減できるというものでした。 (だと認識しました)\nただし利用には制約がいくつかあるため、導入できるかどうか、また、導入してコスト削減につながるかどうかはアプリケーション次第らしいです。\nPod が ポッ と上がる感じ #cmregrowth\n\u0026mdash; よっしー@CBR853RR (@michimani210) December 11, 2019 ECS の次世代スケーリング戦略「Capacity Provider」 (濱田さん) Capacity Providerとは？ECSの次世代スケーリング戦略を解説する #reinvent #cmregrowth ｜ Developers.IO Capacity Provider 既に ECS を使っている人にはぜひ Fargate を使ってる人は Fagate Spot をぜひ タスク実行の設定を柔軟に Capacity Provider (要素) と Capacity Provider Strategy (組み合わせ) の違い 色々要素が集まって最終的にクラスタに適用 Capacity Provider Strategy の戦略 Base (最小タスク数) と Weight (タスク数比率) をうまいことやる ユースケース AutoScale で ECS を使っている場合は今すぐに導入したほうがいい AZ のバランシングを Capacity Provider に任せる FARGATE_SPOT 落ちる？ 全然落ちない (今なら) 使い放題！ (かも) ECS におけるインフラを、より柔軟に設定できるようになるという Capacity Provider の話でした。ECS 使った経験がないのですが、登壇スライド内に出てくる図が非常にわかりやすく、イメージしやすかったです。 (13, 20 ページ目)\nまた Fargate を使っている場合 (ECS on Fargate) 、 FARGATE_SPOT を使うことでコストが一律 70 % オフになるという話もありました。 FARGATE_SPOT いわゆるスポットインスタンスのようなものなので急に落ちたりする可能性もありますが、今のところ全然落ちる気配がないとのことです。(使っている人が少ないから？)\n今年はコンテナ関連に少し足を踏み込んだ程度だったので、来年はもう少し踏み込みたいと思います。\nFargate Spot、10タスクマジで全然落ちない。55時間経過。そろそろお値段が気になる年頃…\n\u0026mdash; 濱田孝治（ハマコー） (@hamako9999) December 11, 2019 2019 年の ElastiCache アップデートと運用について (深澤さん) ElastiCacheの概念が変わる！？re:Growth 2019で登壇した資料を共有！！ ｜ Developers.IO ダウンタイムなしでルケールアップ/ダウン が可能 リーダーエンドポイント : クライアントから意識せずにノードの追加/削除が行える 名前変更コマンド : コマンドをオーバーライドできる (似ているようで危険なコマンドなど) セルフメンテナンスアップデート アップデートが利用可能なタイミングを通知できる ElastiCache のアップデートに関するお話でした。\nダウンタイムなしに、とか、クライアントからは意識せずに、というのは運用面から考えても嬉しいなとと思いました。\n今回のアップデートに関しては Redis のみ とか、 Redis によっても対応しているバージョンが違ったりするようなので、そこは注意が必要かなと思いました。\nIAM/S3 Access Analyzer (吉井さん) S3 / IAM Access Analyzer についてre:Growth 2019に登壇しました #cmregrowth #reinvent ｜ Developers.IO 外部プリンシパルから自分のリソースへのアクセスを検出する\nS3, KMS, IAM, Lambda, SQS\n実際にアクセスがあったかではなく、アクセスされる可能性があるかどうかを検出する\n追加料金はなし\nIAM で設定するけどリージョンごとの設定が必要\nとりあえず有効にしよう！\n自分のリソースが外部からアクセスされる可能性があるかを検出する Access Analyzer のお話でした。\nポイントは、 アクセス されたこと を検出するのではなく、 アクセス される可能性があること を検出するというところです。\n無料なので、とにかく全リージョンで今すぐに有効にしてください！という熱がすごかったです。\n本日はご来場ありがとうございました。\n寝る前にAccess Analyzerを有効にしてからお休みください。#cmregrowth\n\u0026mdash; YOSHII Ryo (@YoshiiRyo1) December 11, 2019 Transit Gateway のアップデートまとめ (島川さん) [資料公開]re:Growth 2019 TokyoでTransit Gatewayのアップデートまとめ」発表してきました #reinvent #cmregrowth ｜ Developers.IO 4 つのアップデート AWS Transit Gateway Multicast AWS Transit Gateway inter-Region Peering 複数リージョンでの Transit Gateway 運用が楽になる AWS Transit Gateway Network Manager ネットワーク運用を簡単に 障害点の早期発見 AWS Accelerated Site to site VPN Connections (長い！) パフォーマンス向上 オンプレから一番近いエッジロケーションにルーティング Transit Gateway のアップデートに関するお話でした。\n今年は Transit Gateway についての話を聞く機会が多かったなという印象で、実際に使ってはいないものの、その機能とかできることについては雰囲気で理解しているという状態でした。\n今回のアップデート内容についても、「なるほど、便利になりそう」くらいのイメージはできました。\nあと、全然関係ないですけど、とても良い声でした。\nまとめ クラスメソッドさん主催のイベント re:Growth 2019 TOKYO @アマゾンウェブサービスジャパン に行ってきたという話でした。\nre:Invent 期間中、次々に更新されるブログを追っていて、各アップデートの内容についてはサラッと目を通していましたが、実際に参加された社員さんの生の声、熱量を感じることができたのはとても良かったです。\n冒頭にも書きましたが、 2 時間があっという間に感じられるくらい濃い内容でした。そしてやっぱりクラスメソッドの人たちは登壇資料がわかりやすいしトークもスムーズで、凄いなと思いました。\nあと、今回はクラスメソッドさんからお土産もいただきました！\nT シャツはひとり一枚、その他のステッカーやボトル (みたいなやつ) などその他諸々についてはご自由に という感じ。これとは別に、帰りにステッカーを 2 枚ほど追加でいただきました\u0026hellip;\nお土産ありがとうございます！\n今年の 2 月に JAWS Days に行ったときは、正直セッションの内容を理解できないことも多かったんですが、約 1 年経って、各サービスのアップデート情報を聞いて、「ああ、あのサービスか」くらいの感じにはなってきました。\n引き続き AWS の情報は頑張って追っていって、来年はコンテナと機械学習関連のサービスをもっと使ってみたいなと思いました。\n",
    "permalink": "https://michimani.net/post/event-regrowth-2019/",
    "title": "[レポート] AWS re:Invent ふりかえり勉強会 re:Growth 2019 TOKYO @ AWS ジャパン に行ってきました"
  },
  {
    "contents": "Hugo で作成したブログの各記事内に、はてなブックマークのブクマ件数を表示させてみました。\nはてなブログ (そもそも同じはてなのサービスなので) とか WordPress とかだとブクマ件数を表示されていることが多いですが静的サイトではあまり見ないので、多少無理がありますがやってみました。\n目次 前置き 前提 概要 実際の流れ 1. ブクマ件数を取得して値を保存する Lambda を定期的に実行する 2. 各記事内で保存されたブクマ件数を取得する (JavaScript) まとめ 前置き これは 静的サイトジェネレーター Advent Calendar 2019 - Qiita 11 日目のエントリです。\n空いていたので滑り込みで入れさせてもらいました。\n普段このブログでは AWS やその他技術的なことを書いています。そのほか、ガジェットのレビューなども書いています。 前提 ブクマ件数の表示には AWS の各種サービス (Lambda, S3, CloudWatch Events) を利用します。が、なるべく料金が発生しないような形で利用します。\nまた、対象となる Hugo のサイトも AWS 上で運用している前提で書きます。\n概要 はてなブックマークには、特定の URL に付いているブクマ件数を取得する API が提供されています。\nはてなブックマーク件数取得API - Hatena Developer Center この API を利用して各記事のブクマ件数を取得し、その値を記事内に表示します。単純ですね。\nなお、 API の利用に関しては、はてなの利用規約を参照ください。\nAPI規約・ライセンス・商標について - Hatena Developer Center 実際の流れ 実際にブクマ件数の取得から表示までの流れは、下記のようになります。\nブクマ件数を取得して値を保存する Lambda を定期的に実行する 各記事内で保存されたブクマ件数を取得する (JavaScript) シンプルですね。\nただ、 Hugo は静的サイトなので、既に生成された html や sitemap.xml 等を使って上記のことを実現する必要があるので、それが今回のポイントです。\nでは、各フェーズについて詳しく見ていきます。\n1. ブクマ件数を取得して値を保存する Lambda を定期的に実行する まずは各記事のブクマ件数を取得して、その値を保存する Lambda を作成します。\n値は、 json ファイルにして S3 に保存します。 Dynamo DB や RDS はコストやアーキテクチャの面から考えて利用しません。\nブクマ件数を取得する対象の記事 URL は、当たり前ですが新しく記事を追加するたびに増えます。その度にブクマ件数取得対象 URL を増やしているとキリがないので、取得対象の記事 URL は sitemap.xml から取得します。\nsitemap.xml には各記事の URL 以外に、タグ別ページやプロフィールページも含まれています。\n今回取得したいのは各記事のブクマ件数のみなので、対象は /post/* の URL のみになります。\nちなみに Hugo で生成される sitemap.xml は下記のようになっています。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; standalone=\u0026#34;yes\u0026#34; ?\u0026gt; \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://michimani.net/post/aws-auto-start-stop-ec2-stack-using-cdk/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2019-08-07T12:58:46+09:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://michimani.net/post/aws-service-terms-for-using-beta-services/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2019-08-05T11:07:58+09:00\u0026lt;/lastmod\u0026gt; \u0026lt;/url\u0026gt; ... \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://michimani.net/\u0026lt;/loc\u0026gt; \u0026lt;lastmod\u0026gt;2019-08-19T18:16:11+09:00\u0026lt;/lastmod\u0026gt; \u0026lt;priority\u0026gt;0\u0026lt;/priority\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://michimani.net/about/\u0026lt;/loc\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;https://michimani.net/contact/\u0026lt;/loc\u0026gt; \u0026lt;/url\u0026gt; \u0026lt;/urlset\u0026gt; ブクマ件数を取得するためには、件数取得 API https://bookmark.hatenaapis.com/count/entry に、クエリパラメータ url として件数を取得したい URL を指定し、 GET リクエストを送ることで取得できます。\n例えば https://michimani.net/post/aws-auto-start-stop-ec2-stack-using-cdk/ のブクマ数を取得したい場合は下記のようになります。\n$ http \u0026#34;https://bookmark.hatenaapis.com/count/entry?url=https://michimani.net/post/aws-auto-start-stop-ec2-stack-using-cdk/\u0026#34; HTTP/1.1 200 OK Cache-Control: public, max-age=3600, s-maxage=3600 Connection: keep-alive Content-Length: 2 Content-Type: text/plain Date: Tue, 10 Dec 2019 12:50:26 GMT Server: nginx Via: 1.1 3c0190220d7b3ab896def13f86f295aa.cloudfront.net (CloudFront) X-Amz-Cf-Id: 7cPpfmH9vUlgPcZUeNZ-7mXmdxOWExJik_q5h8Dsq1FwdbgBXx7BOQ== X-Amz-Cf-Pop: NRT20-C1 X-Cache: Miss from cloudfront 31 シンプルにブクマ件数のみが返ってきます。\nちなみに上のコマンドで使っている httpie は curl に代わる便利なコマンドです。ぜひ使ってみてください。\nHTTPie – command line HTTP client このようにして取得したブクマ件数を、下記のような json ファイルとして S3 に保存します。\n{\u0026#34;cnt\u0026#34;: 31} ファイル名は、各記事が特定できるように /post/ 以降の文字列とします。\nこの例だと aws-auto-start-stop-ec2-stack-using-cdk.json とします。\nオブジェクトのキー名としては /data/htbcnt/aws-auto-start-stop-ec2-stack-using-cdk.json としています。こうすることで、 https://michimani.net/data/htbcnt/aws-auto-start-stop-ec2-stack-using-cdk.json でアクセスできるようになります。\nこの 取得 -\u0026gt; S3 へ保存 を、 sitemap.xml から取得した記事の数だけ実行します。\nなぜ各記事ごとに json ファイルを作成するかというと、後に各記事からこの json ファイルを参照する JavaScript を記述するのですが、その時にロードするファイルサイズを減らすためです。\n1 つの json ファイル内に各記事のブクマ数を保持することもできますが、記事数が増えてくるとファイルサイズも増えるので、このような形にしています。\nということで、実際に実行している Lamnda (Python 3.7) は下記のようになります。\n(gist に置いてます Get share count in Hatena Bookmark of each of posts in Hugo site, and save it as json files on S3. )\nfrom time import sleep import boto3 import json import logging import re import traceback import urllib.parse import urllib.request import xml.etree.ElementTree as ET HATEBU_CNT_API = \u0026#39;https://bookmark.hatenaapis.com/count/entry?url=\u0026#39; S3_BUCKET = \u0026#39;\u0026lt;your-s3-bucket-name\u0026gt;\u0026#39; SITE_MAP_KEY = \u0026#39;sitemap.xml\u0026#39; HUGO_HOST = \u0026#39;\u0026lt;your-hugo-site-host\u0026gt;\u0026#39; # eg) https://michimani.net s3 = boto3.resource(\u0026#39;s3\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def get_hatebu_count(post_url): count = 0 hatebu_url = HATEBU_CNT_API + urllib.parse.quote(post_url) try: with urllib.request.urlopen(hatebu_url) as res: count = int(res.read()) except Exception as e: logger.error(\u0026#39;Hatebu count request failed: %s\u0026#39;, traceback.format_exc()) return count def get_post_url_list(): post_url_list = [] try: s3_object = s3.Object(bucket_name=S3_BUCKET, key=SITE_MAP_KEY) sitemap = s3_object.get()[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) xml_root = ET.fromstring(sitemap) ns = {\u0026#39;post\u0026#39;: \u0026#39;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#39;} reg = re.compile(\u0026#39;^\u0026#39; + re.escape(HUGO_HOST + \u0026#39;/post/\u0026#39;) + \u0026#39;.+\u0026#39;) for url_part in xml_root.findall(\u0026#39;post:url/post:loc\u0026#39;, ns): if reg.match(url_part.text): post_url_list.append(url_part.text) except Exception as e: logger.error(\u0026#39;Get post url failed: %s\u0026#39;, traceback.format_exc()) return post_url_list def put_hatebu_count_file(post_url, hatebu_count): try: object_key = get_key_from_post_url(post_url) s3obj = s3.Object(S3_BUCKET, object_key) data = json.dumps({\u0026#39;cnt\u0026#39;: hatebu_count}, ensure_ascii=False) s3obj.put(Body=data) except Exception as e: logger.error(\u0026#39;Put count data failed: %s\u0026#39;, traceback.format_exc()) def get_key_from_post_url(post_url): return \u0026#39;data/htbcnt/{post_key}.json\u0026#39;.format( post_key=post_url.replace(HUGO_HOST + \u0026#39;/post/\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;\u0026#39;)) def count_needs_update(post_url, new_count): res = False try: object_key = get_key_from_post_url(post_url) cnt_data_obj = s3.Object(bucket_name=S3_BUCKET, key=object_key) cnt_data_raw = cnt_data_obj.get()[\u0026#34;Body\u0026#34;].read().decode(\u0026#34;utf-8\u0026#34;) cnt_data = json.loads(cnt_data_raw) if new_count \u0026gt; cnt_data[\u0026#39;cnt\u0026#39;]: res = True except Exception: print(\u0026#39;Hatebu count file does not exists.\u0026#39;) res = True return res def lambda_handler(event, context): post_list = get_post_url_list() for post_url in post_list: sleep(0.5) count = get_hatebu_count(post_url) if count_needs_update(post_url, count) is True: put_hatebu_count_file(post_url, count) logger.info(\u0026#39;Updated for \u0026#34;{}\u0026#34;, new Hatebu count is \u0026#34;{}\u0026#34;\u0026#39;.format(post_url, count)) else: logger.info(\u0026#39;No update requred for \u0026#34;{}\u0026#34;\u0026#39;.format(post_url)) これを、 CloudWatch Events のスケジュールで定期実行します。\n定期実行の間隔については適当ですが、このブログだとほぼブクマは付かないので、日本時間の 6:00 〜 24:00 の間に 15 分間隔で実行しています。これでも多い気がしますが\u0026hellip;。\nそのときの cron 式は 0/15 0-13,21-23 * * ? * となります。 CloudWatch Events のスケジュールで cron 式を指定する場合はのタイムゾーンは UTC になるので注意です。\nこれでブクマ件数の取得と保存ができました。\n2. 各記事内で保存されたブクマ件数を取得する (JavaScript) 続いては、保存したブクマ数を各記事内から取得します。\nJavaScript で取得するのですが、記述するのは各テーマの下にある single.html です。このブログでは indigo のテーマを使用しているので、 ./themes/indigo/layouts/_default/single.html に処理を追加します。\n実際には下記の記述を、 single.html の一番下に追加します。\n\u0026lt;script\u0026gt; showHatebuCount(); function showHatebuCount() { const postKey = location.pathname.replace(\u0026#39;/post/\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;/\u0026#39;, \u0026#39;\u0026#39;); const htbCntData = `/data/htbcnt/${postKey}.json`; fetch (htbCntData).then(res =\u0026gt; { const hatebuBadgeElm = document.getElementById(\u0026#39;hatebu-count-badge\u0026#39;); if (res.ok) { res.json().then(cntData =\u0026gt; { hatebuBadgeElm.innerText = cntData.cnt; }); } }); } \u0026lt;/script\u0026gt; ちなみに上のスクリプト内で指定している ID atebu-count-badge は、このブログの各記事下に置いている SNS 用シェアボタンのこの部分になります。\n表示する場所については適宜場所を作って表示してください。\nまとめ Hugo で作成したブログの各記事内に、はてなブックマークのブクマ件数を表示させてみた話でした。\n静的サイトのメリットは何と言ってもロードの速さなのでなので、ブクマ件数についてもあらかじめ静的なファイルで生成しておこうというのが今回のポイントです。\nもし同じような思いを持っている方がいれば、参考にしてみてください。また、Hugo に限らず静的サイトで既にブクマ数の表示をされている方がいれば、その方法についても知りたいなーという思いです。\n",
    "permalink": "https://michimani.net/post/development-show-hatebu-count-in-hugo-posts/",
    "title": "Hugo で作成したブログの各記事内に Lamnda と S3 を使って はてなブックマークのブクマ件数を表示させてみた"
  },
  {
    "contents": "国内ではメジャーなドメイン登録サービス (レジストラ) である お名前.com。ここで独自ドメインを取得している方は多いと思います。今回は、お名前.com で取得したドメインを AWS の DNS サービスである Amazon Route 53 に移管した時の話です。\n目次 前置き 前提 やること ステップ 0: 最上位ドメインの移管に関する要件 ステップ 1: Amazon Route 53 で最上位ドメインがサポートされていることを確認する ステップ 2: DNS サービスを Amazon Route 53 または別の DNS サービスプロバイダに移管する ステップ 3: 現在のレジストラで設定を変更する ステップ 4: ネームサーバーの名前を取得する ステップ 5: 移管をリクエストする ステップ 6: AISPL (インド) のお客様のみ : 移管料金を支払う ステップ 7: 確認と承認 E メールのリンクをクリックする ステップ 8: ドメイン設定を更新する 一回失敗しました まとめ 前置き これは AWS初心者 Advent Calendar 2019 4 日目のエントリです。\n空いていたので滑り込みで入れさせてもらいました。\n普段このブログでは AWS やその他技術的なことを自分のメモレベルで書いています。そのほか、ガジェットのレビューなども書いています。 前提 今回 移管するのはこのブログのドメインである michimani.net です。\nこの記事を公開するときは既に移管は完了していると思いますが、移管前から稼働しているサービス (期限が切れていない有効なドメイン) ということになります。\nまた、レジストラは お名前.com ですが、 DNS サービスはドメイン登録の移管前から Route 53 に移行してあります。\nやること 基本的には下記の AWS 公式ページにあるステップに沿って作業を進めていきます。\nドメイン登録の Amazon Route 53 への移管 - Amazon Route 53 上記ページから抜粋すると、やることは以下の 8 ステップに分けられます。\nステップ 0: 最上位ドメインの移管に関する要件 ステップ 1: Amazon Route 53 で最上位ドメインがサポートされていることを確認する ステップ 2: DNS サービスを Amazon Route 53 または別の DNS サービスプロバイダに移管する ステップ 3: 現在のレジストラで設定を変更する ステップ 4: ネームサーバーの名前を取得する ステップ 5: 移管をリクエストする ステップ 6: AISPL (インド) のお客様のみ : 移管料金を支払う ステップ 7: 確認と承認 E メールのリンクをクリックする ステップ 8: ドメイン設定を更新する ということで、各ステップの詳細について書いていきます。\nステップ 0: 最上位ドメインの移管に関する要件 まず、移管しようとしているドメインが下記の要件を満たしている必要があります。\n移管前のレジストラに登録また移管されてから 60 日が経過していること 有効期限が切れていて復元が必要な場合は、復元してから 60 日が経過していること ドメインのステータスが以下の いずれでもない こと clientTransferProhibited pendingDelete pendingTransfer redemptionPeriod serverTransferProhibited 登録からの日数についてですが、これは お名前.com ドメインNavi の ドメイン一覧 から対象のドメインを選択すると、詳細画面で確認できます。\nmichimani.net については登録したのが 2017年 だったので問題ありません。\n続いて有効期限ですが、こちらも登録日と同じ画面で確認できます。\nまた、 whois コマンドで確認することもできます。\n$ whois michimani.net | grep Expiry Registry Expiry Date: 2022-12-25T09:10:45Z 続いてドメインのステータスですが、こちらも whois コマンドで確認できます。\n$ whois michimani.net | grep \u0026#34;^Domain Status\u0026#34; Domain Status: ok https://icann.org/epp#ok また、これらの要件以外にも移管に際して注意しておく内容があります。\n移管中に問題が発生した場合 AWS サポートに問い合わせます。\nドメインの有効期限 移管に際して、移管後の有効期限が移管前の有効期限と変わる場合があります。\nTLD によっては、移管前の有効期限を保持したり、有効期限を 1 年延長したり、または有効期限を移管日の 1 年後に変更する場合があります。\n今回は汎用 TLD である .net なので、 Route 53 への移管では 新しい有効期限は前のレジストラの有効期間に 1 年を加えたもの になります。\n移管するドメインの TLD によっては有効期限が短くなる可能性もあるので、注意が必要です。\n詳細については下記ページを参照してください。\nドメインを Amazon Route 53 に移管するとドメイン登録の有効期限が受ける影響 - Amazon Route 53 移管にかかる料金 Route 53 に移管する場合、発生する移管料金は TLD ごとに決まっています。\n.net であれば 11.00 USD となります。\nその他の TLD については下記ページ内の TLD 別の最新料金表 の Transfer Price を確認してください。\n料金 - Amazon Route 53 | AWS ステップ 1: Amazon Route 53 で最上位ドメインがサポートされていることを確認する 移管しようとしているドメインの TLD が Route 53 でサポートされているかを確認します。\nよっぽど特殊な TLD でなければサポートされていると思いますが、念のため下記ページに対象の TLD が存在するかを確認しておきます。\nAmazon Route 53 で登録できるドメイン - Amazon Route 53 ステップ 2: DNS サービスを Amazon Route 53 または別の DNS サービスプロバイダに移管する ドメインの移管前に、 DNS サービスを Route 53 に移行しておきます。\n今回これは既に実施済みなので詳細な説明は割愛しますが、手順としては下記ページの通りです。\nRoute 53 を使用中のドメインの DNS サービスにする - Amazon Route 53 DNS サービスを移管する際には各レコードの TTL を一時的に短くする必要があります。既存の DNS 設定で TTL が長く設定してあるレコードに関しては、余裕をもって DNS サービスの変更を実施します。\nステップ 3: 現在のレジストラで設定を変更する 現在のレジストラ、つまり お名前.com で下記の項目について設定を変更 (および確認) します。\nドメイン登録者の連絡先が最新であることを確認 ドメイン詳細画面で 登録者情報 にある連絡先、特にメールアドレスが正しいかどうかを確認します。\n移管に際しての承認などはこのメールアドレスに届くので、受信可能なメールアドレスである必要があります。なお、お名前.com で登録者情報のメールアドレスを変更するには少し時間がかかります。 (1 〜 2時間くらい)\nドメインのロックが解除されていること確認 ドメイン機能一覧の ドメイン移管ロック から設定を確認し、設定されていれば OFF にします。\nドメインの DNSSEC を無効にする お名前.com では有料オプションになっています。設定している場合は無効にします。\nWhois 代行サービスを解除する これは お名前.com のみかもしれませんが、 Whois 代行サービスを設定している場合は解除します。\nここを設定したままだと、 Route 53 から移管をリクエストしたあとに お名前.com 側から不承認にされてしまいます。(一度やってしまいました)\n認証コードを取得する ここまでの項目について全て確認および変更ができたら、 お名前.com のドメイン詳細画面で 認証コード を取得します。\nドメイン詳細画面で AuthCode の欄で 表示 ボタンを押すと 認証コードが表示されます。\nステップ 4: ネームサーバーの名前を取得する このステップに関しては、 Route 53 を DNS サービスとして利用する場合には不要となります。\n今回はすでに DNS サービスは Route 53 に移行済みなので、特にやることはありません。\nステップ 5: 移管をリクエストする Route 53 のコンソールからドメインの移管をリクエストします。\nダッシュボードから 既存のドメインの移管 のリンクをクリックします。\n移管するドメインを入力して、カートに追加して、次へ進みます。\n認証コード の入力を求められるので、先ほど お名前.com で取得した認証コードを入力します。\nネームサーバのオプション では ドメインと同じ名前の Route 53 ホストゾーンからネームサーバーをインストールする を選択します。\nホストゾーン は、 ステップ 2 で設定済みであるホストゾーンを選択します。\n次の画面では登録者情報を入力します。\n次の画面で入力内容を確認し、リクエストを完了します。\nリクエストが完了すると、次に何が起こるかが表示されるので、目を通しておきます。\nこのあとは何通かメールが来たりしますが、それについては ステップ 7 で触れます。\nステップ 6: AISPL (インド) のお客様のみ : 移管料金を支払う このステップは該当しないのでスキップします。\nステップ 7: 確認と承認 E メールのリンクをクリックする 移管リクエストが完了すると、 AWS 、 お名前.com 双方からいろいろなメールが飛んできます。実際に飛んできた順番に、その種類 (件名) と対応について書いておきます。\n1. Amazon Registrar Sign-Up Confirmation これは Amazon Registrar に初めて登録する場合のみ送信されるようです。\n特に対応することはありません。\n2. Amazon Web Services Invoice Available [Account: ************] [Invoice ID: ********] ドメイン移管の料金に対する請求書が発行されましたというメールです。\n特に対応することはありません。\n3. 【重要】トランスファー申請に関する確認のご連絡 michimani.net お名前.com から、ドメイン移管を承認してくださいという内容のメールです。\n本文内に記載されているリンクから承認を実施します。\n4. トランスファー申請承認のご連絡 michimani.net お名前.com から、ドメイン移管の承認が完了しましたという内容のメールです。\n特に対応することはありません。\n5. Verify your email address or your domain michimani.net will be suspended AWS から、ドメイン登録者のメールアドレスを確認してくださいという内容のメールです。\n本文内に記載されているリンクをクリックして、メールアドレスが正しいことを証明します。\n6. Your email address has been successfully verified AWS から、ドメイン登録者のメールアドレスが承認されましたという内容のメールです。\n特に対応することはありません。\n7. Transferring the domain michimani.net to Route 53 succeeded AWS から、ドメインの移管が完了しましたという内容のメールです。\n特に対応することはなく、移管自体はこれで完了となります。\nまた、移管リクエストから移管完了までの進行状況については、 Route 53 のダッシュボードで確認することができます。\nコンソール上で移管リクエストが完了してから各メールが届くまでのおおよその時間については下記の通りでした。\nまた、承認等のアクションが必要なメールについては、メール受信後すぐに対応しました。\n1 : 2 分 2 : 5 分 3 : 20 分 4 : 35 分 5 : 35 分 6 : 35 分 7 : 90 分 これに関してはあくまでも目安で、場合によってはドメインの移管が完了するまで 10 日ほどかかる可能性もあるようです。\nステップ 8: ドメイン設定を更新する Transferring the domain michimani.net to Route 53 succeeded のメールを受信したということは、無事にドメインの移管が完了したということになります。\nまず気になるのが、ドメインの有効期限がどうなっているのか です。\nドキュメントによると .net ドメインの場合は既存の有効期限にプラス 1 年されるとなっています。\n$ whois michimani.net | grep Expiry Registry Expiry Date: 2023-12-25T09:10:45Z 1 年延長されてますね。\nついでにドメインのステータスも確認してみます。\n$ whois michimani.net | grep \u0026#34;^Domain Status\u0026#34; Domain Status: clientTransferProhibited https://icann.org/epp#clientTransferProhibited Domain Status: transferPeriod https://icann.org/epp#transferPeriod 移管直後なので clientTransferProhibited および transferPeriod となっています。\n移管が完了した後は、そのドメインに対して Route 53 側で以下の設定を更新します。(必要であれば)\nドメインのロック 自動更新 延長登録期間 DNSSEC このうち、 ドメインのロック、自動更新、延長期間登録 については、 Route 53 コンソールの 登録済みドメイン から変更します。\nドメイン一覧でも ドメインのロック、自動更新 それぞれの設定状況、有効期限が確認できます。\nドメイン名をクリックすると、詳細情報を確認、また各種設定を変更できます。\nDNSSEC の設定を有効にするためには、サードパーティの DNS サービスを利用する必要があるようです。詳しくは ドメインの DNSSEC の設定 - Amazon Route 53 を参照してください。\n以上でドメインの移管は完了となります。\nひとつひとつ確認しながらやると数時間かかるので、時間に余裕のあるときにやったほうがいいですね。\n一回失敗しました AWS のドキュメントに沿って移管作業を進めていましたが、作業の中で移管リクエストが一度不承認となってしまいました。\n理由としては、 お名前.com の設定で Whois 代行が有効になっていた からです。\n移管リクエストのあとに不承認になるという良い失敗をしたので、その時の状況についても書いておきます。\n移管リクエストの後は前項の ステップ 7 の通りメールがいくつか届きます。その際、 3 で届くはずの お名前.com からのメールのタイトルが 「トランスファー申請不承認のご連絡 michimani.net」 となっており、本文は下記のような内容でした。\nドメイン名：michimani.net\n上記ドメインのトランスファー申請につきまして、下記いずれかに該当するため申請を不承認といたしました。\n1.該当URLより不承認処理が行われた\n2.Whois情報公開代行サービスの設定が行われている\n3.ドメインの契約終了日まで7日以内（JPドメインのみ）\n4.期限内に承認処理が行われなかった\n再度申請を行う場合は、上記事項に該当しないことを確認した上で申請を行ってください。\nこの時点で Whois 代行だなと気付いたので、お名前.com のコンソールで Whois 代行サービスを解除しました。\n再度申請を行う とは、 Route 53 のコンソールから再度 移管リクエストを行うということになります。( ステップ 5 のやり直し)\n移管の進行状況についてはダッシュボードで確認できるので、再申請についてはこのステータスが 「ドメインの移管 が失敗」 になってから行いました。\nちなみに再申請を行うと、移管料金の請求に関するメールが再度送信されてきます。つまり、 2 回分の料金が 一時的に 発生します。 一時的に と書いたのは、移管リクエスト後になんらかの理由で不承認となった場合は、そのリクエストで発生した移管料金はちゃんと返金されるからです。このことはドキュメントにも下記のように書かれています。\nRoute 53 は、移管プロセスを開始する前に、ドメインの移管に対して料金を請求します。何らかの理由で移管が失敗した場合は、直ちにお客様のアカウントに移管の費用を返却します。\nなので、再申請になっても移管料金は重複して請求されないので、安心してください。\n実際 再申請後には、再申請の移管料金に関する請求メールと、失敗した分の移管料金の返金に関する請求メールが届きます。請求に関してはコンソールでも確認でき、下記のように 2 回分の移管料金と、1 回分の返金が発生しているのがわかります。\nまとめ ドメイン登録を お名前 .com から Route 53 に移管した話でした。\nそもそもなぜ移管しようと思ったかというと、お名前.com のコンソールが操作しづらい (遷移方法によっては UI が異なる、広告が出てくる\u0026hellip;) のと、諸々の案内メールが多すぎる (自動更新設定の催促、更新時に利用できるクーポンの案内\u0026hellip;) からです。\nまた、既に DNS サービスは Route 53 に移行済みだったので、ドメインの登録も Route 53 に移管してしまった方がスッキリしますよね。\n今回は お名前.com からの移管について書きましたが、おそらく他のレジストラからの移管もほぼ同様の流れだと思うので、同じような悩みを抱えている方は Route 53 への移管を検討してみてはどうでしょうか。\n",
    "permalink": "https://michimani.net/post/aws-domain-transfer-to-route53-from-onamae-com/",
    "title": "ドメイン登録を お名前.com から Route 53 に移管した"
  },
  {
    "contents": "発表から 3 ヶ月待って手に入れた Powerbeats Pro。使い始めた当初は快適さしかなかったのですが、使い続けているうちに良くないところも明らかになってきました。今回は Powerbeats Pro を 4 ヶ月使ってみて感じた良くないところについて書いていきます。\nPowerbeats Pro のおさらい Powerbeats Pro の良くなかったところ 充電できていないときがある iPhone との自動接続がうまくいかないときがある マスク、メガネとの同時使用が辛い ケースが大きすぎる まとめ Powerbeats Pro のおさらい Powerbeats Pro については以前にレビューを書きました。\nこの記事でも書いているとおり、 Powerbeats Pro は Apple の H1 チップを搭載した完全ワイヤレスイヤホンで、 AirPods と同様に iOS 端末との親和性 (初期セットアップ、自動接続) が高く、さらにカナル型ということもあり、まさに AirPods の上位互換 という印象でした。\n形状は AirPods とは大きく異なり、イヤーフックが付いていて本体およびケースのサイズ・重量ともに AirPods よりも大きくなっています。しかしながら装着した感じではそれによるデメリットはほとんど感じられず、むしろ安定感があって良いなという印象でした。\nカナル型ということもあり外部の音もある程度遮断されるため、通勤中でも快適に音楽を聴くことができ、もうこれは 2019 年のベストバイだなと確信していました。\nではそんな Powerbeats Pro のどの辺が良くなかったのでしょうか。\nPowerbeats Pro の良くなかったところ 2019 年のベストバイかと思われた Powerbeats Pro ですが、使っているうちに良くないなー、不便だなーと思うところがいくつかありました。\n充電できていないときがある Powerbeats Pro は AirPods と同様にケースに入れておくことで充電が可能です。\nケース内ではマグネットで固定されるようになっており、ケース内の充電端子と Powerbeats Pro 本体の充電端子が接するように固定されます。\nしかしこの固定が甘いというか、遊びがあるというか、充電端子同士が接しない状態でケースに収まってしまう (マグネットで固定されてしまう) ことがあるんです。\nその結果、左右どちらか、もしくは両方が十分に充電できていないという状況が何度かありました。\n対策としては、充電端子がしっかりと接したタイミングでケース外の LED ライトが一瞬赤く光るので、ケースに収めるときはその光を確認するようにしていました。\nAirPods のときは何も考えずにケースに入れれば良かったので、この点は不便でした。\niPhone との自動接続がうまくいかないときがある AirPods と同様に、 Powerbeats Pro は耳に装着すると同時に iPhone と接続され、効果音が鳴ります。\nしかし、この自動接続が片方しか接続されないということが割と頻繁にありました。そのときは効果音は左右どちらかからのみ聞こえてきて、その状態で音楽を再生しても片方からしか聞こえません。\nこうなった場合の対応方法としては、一度 Powerbeats Pro 本体を両方ともケースの中に入れ、数秒待ってからあらためて耳に装着します。すると、左右両方が正常に接続されます。\n一度ケースに収めるというのがポイントで、ただ耳から外して付け直すだけでは片方だけが接続される状態から抜け出すことはできませんでした。\nこの事象が起こる頻度が意外と高く、平日の通勤時間と昼休みに装着するとなると合計で 15 回の装着機会があるわけですが、そのうち 2 〜 3 回は発生していました。\nケースに戻しさえすれば大丈夫なんですが、さすがにめんどくさすぎますよね。\nマスク、メガネとの同時使用が辛い Powerbeats Pro にはイヤーフックが付いており、それがあることによって装着時の安定感が増します。\n前回のレビュー記事にも書きましたが、このイヤーフックは意外とストレスが無くて、長時間装着していてもほぼ痛くなりません。 (使い始めて数日は痛いかもしれません)\nしかし、同じく耳にかけるものとしてはマスクやメガネがあります。\nそれらとの同時使用はちょっと辛いです。私自身は PC 使用時しかメガネをかけないので、辛いのは主にマスク使用時です。\n購入した当時はマスクが必要な季節ではなかったのでほとんど意識していなかったのですが、寒くなってきてマスクが必要な季節になり、その辛さに気付いてしまいました。\nまずイヤーフックとマスクの紐という 2 種類のものを耳にかけるという点で、耳にストレスがかかります。どちらか一方ならほぼ気にならないんですが、両方となると、痛くなるまでの時間が短くなって辛いです。\nさらに、マスクの紐とイヤーフックが絡まることが多々あります。どちらかを外そうとした時に高確率で絡まってしまい、結局どちらも一旦外さないといけない、みたいなことにもなりました。\nメガネの場合は絡まるということは無いですが、イヤーフックとメガネのモダン部分との場所の取り合いが発生して、 Powerbeats Pro の装着時/脱着時にメガネも付け直す必要が出てきます。\n私の場合は Powerbeats Pro を使う場面ってほとんど電車での移動中なので、寒い季節や花粉の季節にはほぼマスクをかけています。\n購入した時期がちょうどマスクをかけない季節だったので考慮できていませんでしたが、ちょっと辛いポイントですね。\nケースが大きすぎる これはもう購入前からわかっていて、ある程度覚悟もしていたわけですが、やはりケースがかなり大きいです。\nあらためて AirPods と比較した画像を見てみると、その大きさはハンパないですね。\nケースの質感も滑りやすいものになっていて、手が乾燥していたり手袋をはめていたりすると滑って落としてしまうこともありました。\nまた、こういったイヤホンのケースなどはカバンの外側にある小さめのポケットに入れたりすることが多いと思うのですが、その際にもこの大きさがネックになってきます。もちろんカバンの種類にもよりますが。\nケース自体をストラップやカラビナ付きの保護ケース等に入れてぶら下げるという選択肢もあり、実際に Amazon でカラビナ付きのケースを買ってしばらくカバンにぶら下げていました。しかし、やはりその場合でも大きさがネックとなり、ぶら下げている感が強くなって気になります。また、大きく揺れることによってカラビナの付け根部分が破損しないか、といった心配もしてしまい、結局ぶら下げるのはやめました。\nPowerbeats Pro適用 イヤホン 充電ケース シリコン保護ケース 防水 防塵 耐衝撃 防傷 フック式キーホルダー付き 携帯に便利 Beats Powerbeats Proに対応（赤）\nあと、ケースの開き方が豪快で (という表現が適切かはわかりませんが) 、ハマグリみたいな開き方になるんですよ。\nこの大きさでその開き方となると、開いた状態だとさらに大きくなります。せっかく完全ワイヤレスイヤホンで装着している状態もスタイリッシュなのに、ケースからの出し入れがスマートじゃないんですよね。\nこの大きさに関しては購入前からわかっていたことではありますが、実際に使っているうちにやっぱり気になってくる部分でした。\nまとめ Powerbeats Pro を 4 ヶ月使ってみて感じた良くないところについての話でした。\n今回は良くない部分にフォーカスして書きましたが、 Powerbeats Pro は本当に良いイヤホンです！(説得力ないですが)\n良くない部分は確かにあるんですが、そのほかの部分で バッテリーの持ちだったり、高音質だったりで装着しているときの安定感だったりは AirPos には無いものです。\n発売当初はブラックのみが購入可能でしたが、 8 月にその他の 3 色 (アイボリー、モス、ネイビー) の発売が開始されてからは、電車内でも使っている人をよく見るようになりました。\n発売開始当時より価格も安くなっているので、 Powerbeats Pro 欲しかったけど価格面で敬遠していた方、カラーラインナップで我慢していた方はあらためてチェックしてみてはどうでしょうか。\n",
    "permalink": "https://michimani.net/post/gadget-review-weakpoint-of-powerbeats-pro/",
    "title": "[レビュー] Powerbeats Pro を 4 ヶ月使ってみて感じた良くないところ"
  },
  {
    "contents": "Amazon SES からメールを送信するには、メールアドレスまたはドメインを検証する必要があります。今回はドメインを検証してメールを送信するところまでやってみたのでそのメモです。\n目次 概要 やってみる 1. SES でドメイン検証 2. サンドボックステスト 3. SES からの送信制限解除をサポートに申請 4. 外部ドメインに対してメール送信 まとめ 概要 Amazon SES (以下、SES) からメールを送信してみます。\nSES からメールを送信するためには、送信元となる メールアドレスを検証 する方法と、送信元となるメールアドレスの ドメインを検証 する方法があります。\nメールアドレスの検証では、 test@michimani.net というアドレスを検証することでこのアドレスからメールを送信することができるようになります。\n一方、ドメインの検証では、 miichimani.net というドメインを検証することで、 test-desu@michimani.net や test-dayo@michimani.net など、ドメインさえ michimani.net であれば @ 以前はなんでもよくなります。\n今回は後者の ドメインを検証 してメールを送信してみます。\nちなみに Amazon SES は 2019年11月時点では東京リージョン (ap-northeast-1) で利用できないため、バージニア北部リージョン (us-east-1) で利用します。\nやってみる 具体的な手順は下記の流れになります。\nSES でドメイン検証 サンドボックステスト実施 SES からの送信制限解除をサポートに申請 外部ドメインに対してメール送信してみる 1. SES でドメイン検証 マネジメントコンソールで SES のダッシュボードを開き、サイドメニューの Domains を選択し、 Verify a New Domain ボタンを押します。\nドメインを入力するモーダルが表示されるので、検証したいドメインを入力して Verify This Domain ボタンを押します。\nGenerate DKIM Setting のチェックはしてもしなくても OK です。DKIM とは DomainKeys Identified Mail のことで、送信者が暗号化キーでメールを署名できる規格のことです。Amazon SES ではここでチェックすることによって全ての送信メールに DKIM 署名を追加することができます。また、ここでチェックしなくても、 SES の SendRawEmail API を使ってメールを送信する際に、個別に DKIM 署名を追加することができます。\n詳しくは下記のページを参照してください。\nAmazon SES における DKIM を使った E メールの認証 - Amazon Simple Email Service Verify This Domain を押すと、ドメイン検証に必要な DNS レコードが表示されます。\nドメインを管理している DNS サービスにてこれらのレコードを登録する必要がありますが、 Route 53 を使用する場合は、モーダル内の Use Route 53 ボタンを押すことで自動で設定してくれます。\nUse Route 53 ボタンを押すと登録する DNS レコードをチェックする画面に映ります。ここでは MX レコードに関する Warning が表示されます。\n内容としては、 この操作によって既存の MX レコードが上書きされるため、このドメインでメールの受信をする予定がなければチェックをしないように という内容です。\n登録するレコードにチェックを入れて Create Record Sets ボタンを押すと、 Route 53 にレコードが追加され、ドメインの検証が開始されます。\n検証結果は SES のドメイン一覧画面で確認できるので、しばらく時間を置いて次のキャプチャのようになっていれば検証完了です。\n2. サンドボックステスト ドメイン検証は完了しましたが、この時点では外部のドメインに対してメールを送信することができません。 (サンドボックス状態)\nなので、一旦いま検証が完了したドメイン宛にメールを送信してみます。\nドメイン一覧からドメインをチェックした状態で、上にある Send a Test Email ボタンを押します。\nすると、テストメール送信用のモーダルが表示されるので、 From , To, Subject, Body をそれぞれ入力します。To 欄には From と同じメールアドレスを指定します。\nSend Test Email ボタンを押してエラーが表示されずにモーダルが閉じれば、送信完了です。\n送信状態については、サイドメニューの Sending Statistics にある Deliveries のグラフで確認できます。\n3. SES からの送信制限解除をサポートに申請 サンドボックスにて送信できることが確認できたので、外部ドメインに対して送信ができるように、AWS のサポートセンターから SES 送信制限緩和を申請します。\nサポートセンターのページから Create case を押して, Service limit increase を選択します。\nLimit Type で SES 送信制限 を選択します。\nするといくつかの入力欄が出てきますが、全て空欄・未選択でもかまいません。\n続いて制限緩和を申請する対象のリージョン、緩和する対象とその値を指定します。\nリージョンは今回は バージニア北部 を選択します。\n緩和の対象は 希望する一日あたりの送信クォータ と 希望する最大送信レート から選べますが、 Add another request ボタンを押すことで複数の緩和対象を指定できるので、両方指定することもできます。\n今回はとりあえず 希望する一日あたりの送信クォータ を 100 で申請してみます。\n続いて申請の理由を入力します。\n最後にサポートからの返答言語、返答方法 (この場合は Web のみ選択可) を選択して、 Submit すれば申請完了です。\nあとはサポートからの返答を待つだけです。\nサポートの契約プランにもよると思いますが、 Basic プランだと制限緩和まで 13 時間くらいかかりました。余裕を持って申請したほうが良さそうです。\nちなみにサポートからの返答は下記のような内容でした。\n希望する一日あたりの送信クォータ を 100 で申請しましたが、結果として 50,000 になりました。もしかするとこれが最低クォータなのかもしれません。\n値を指定しなかった 希望する最大送信レート については 毎秒 14 メッセージ となりました。\nちなみに SES の制限緩和申請は初めてだったのですが、内容としてはクォータの制限緩和で、サンドボックスからの移動はその副産物みたいな感じなんででしょうか。\n4. 外部ドメインに対してメール送信 無事にサンドボックス状態を抜け出すことができた (はず) なので、実際に外部ドメインにメールを送信してみます。\n方法としては、サンドボックス状態で実施した方法と同じです。\n今回は Gmail 宛に送信してみます。\n送信してすぐに受信が確認できました。\nまた、 AWS CLI からも送信を試してみます。\n$ aws ses send-email \\ --from ses-cli@michimani.net \\ --to \u0026lt;hogehoge\u0026gt;@gmail.com \\ --subject \u0026#34;Test mail from Amazon SES via CLI\u0026#34; \\ --text \u0026#34;This is a tes mail from Amazon SES via CLI.\u0026#34; \\ --region us-east-1 { \u0026#34;MessageId\u0026#34;: \u0026#34;0100016e7c582b54-90ffeb69-f983-4a2c-b471-12345abcdefg-000000\u0026#34; } こちらも無事に受信できました。\nまとめ Amazon SES でドメイン検証をしてメールを送信してみた話でした。\nEC2 からメールを送信していると送信制限に引っかかったり、またその制限解除も Elastic IP ごとに申請が必要だったりしてスケーラビリティに欠けます。\n送信するメールの件数が少なかったとしても、送信には Amazon SES を使用した方が長期的にみると良さそうです。\nちなみに EC2 にホスティングしているアプリケーションから SES を利用してメールを送信する場合、毎月最初の 62,000 通は無料で、それ以降 1,000 通ごとに 0.10 USD の料金が発生します。\nその他、ファイルを添付した場合の追加料金や、送信だけでなく受信もした場合、また EC2 上のアプリではなく E メールクライアントやその他のソフトウェアから SES を使用した場合の料金については下記を参照してください。\n料金 - Amazon SES | AWS ",
    "permalink": "https://michimani.net/post/aws-send-mail-via-ses-using-domain/",
    "title": "Amazon SES でドメイン検証してメールを送信してみた"
  },
  {
    "contents": "先日新たなサービスが発表され、 AWS の Code シリーズ (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) のそれぞれから Amazon SNS と AWS Chatbot (beta) に対して通知を送ることができるようになりました。今回はこれらの中から、実際に CodeBuild から SNS 経由で Slack に通知を送ってみたという話です。\n追記 2020/02/26 CodeCommit の通知についても書きました。\n追記 2020/04/23 AWS Chatbot が一般利用可能 (GA) になりました。CLoudWatch Aram を Slack に通知してみた記事を書いたのでこちらも参考にしてみてください。\n目次 概要 CodeBuild から Slack に通知してみる そもそも何が嬉しいのか CodeBuild で通知の設定 実際にビルドしてみる まとめ 概要 冒頭にも書きましたが、 AWS の Code シリーズ (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) の各サービスから、 Amazon SNS および現在パブリックベータ版として公開されている AWS Chatbot に対して通知を送ることができるようになりました。\nAWS CodeCommit、AWS CodeBuild、AWS CodeDeploy、AWS CodePipeline 通知のご紹介 これまで Code シリーズからの通知は、 CodePipeline 内に Lambda を挟んだり、各サービス内で発生するイベントに対して CloudWatch Events を設定したりする必要がありましたが、この通知サービスの誕生によってその手間がなくなり、より簡単に Code シリーズからの通知を発信することができるようになります。\nこの通知サービスに関する詳しい内容は、上記の AWS 公式ページのほか、下記のクラスメソッドさんのブログを参照してみてください。\n【開発者必見】Codeシリーズに最適化された通知サービスNotificationsがリリースされました！ ｜ Developers.IO CodeBuild から Slack に通知してみる それでは早速この通知サービスを使って CodeBuild から SNS 経由で Slack に通知を飛ばしてみます。\nそもそも何が嬉しいのか やってみる前に、 CodeBuild から通知が飛ばせるようになったことで何が嬉しいのでしょうか。\nこの通知サービスの存在を知ってまず思ったのが、 ますます CodeBuild だけでできることが増えた！やったー！ ということです。\n実際のプロジェクトで CodeBuild だけを使うということはないと思いますが、個人で軽く動かしているような自動化フローでは、 AWS 料金節約のために CodeBuild だけで諸々やっている場合もあると思います。僕だけかもしれませんが。\nこれに関しては以前にブログで書きました。\nこのブログ内では、 GitHub への push をトリガーに CodeBuild で Vue プロジェクトのビルドを開始し、 S3 に配置、 CloudFront に invalidation 発行、さらにビルドが完了したことを Slack に通知するために curl コマンド を実行しています。\ncurl コマンドは buildspec.yml に書かれているため、動的な内容を含みづらいです。そのため、 Slack に通知される内容は、あくまでもビルドが終わったことだけで、それが成功したのか失敗したのか、それすらもわかりません。\nただ個人で動かしている自動化なので、とりあえず通知だけ来ればいいや、とブログ内では納得させていました。\nしかし、今回の通知サービスによって CodeBuild 自体から通知を飛ばすことができるようになったことで、ビルドに関する詳細な情報を SNS および Chatbot に外出しできるようになりました。 SNS のトピックに情報を渡せるということは Lambda にも情報を渡せるので、そこでゴニョゴニョすればよりリッチな通知を Slack に送れるようになるというわけです。\nこの通知サービス自体に料金は発生しませんし、 Code シリーズからの通知という性質からも SNS や Lambda で異常な料金が発生するということも考えにくいです。\nということで、以前のブログで節約のために削ったリッチさを補完できる、というかさらに良いものにできるようになるわけですね。\nCodeBuild で通知の設定 前置きが長くなりましたが、ここからは実際に通知の設定をしていきます。といっても非常に簡単です。\nSNS トピックを作成 まずは CodeBuild からの通知を受け取る SNS トピックを作成しておきます。もちろん既存の SNS トピックを使用しても OK です。\n今回は CodeBuildNotification という SNS トピックを作成しました。\n作成する際の注意点としては、 AWS の他のリソースから SNS トピックに対する権限をアクセスポリシーで設定する点です。具体的には、下記のような設定が必要になります。\n{ \u0026#34;Sid\u0026#34;: \u0026#34;AWSCodeStarNotifications_publish\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;codestar-notifications.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;SNS:Subscribe\u0026#34;, \u0026#34;SNS:Receive\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:ap-northeast-1:999999999999:CodeBuildNotification\u0026#34; } 詳しくは下記の公式ページを参照してください。\nTroubleshooting - Developer Tools Console CodeBuild で通知ルールを作成 続いて CodeBuild で通知ルールを作成します。\n通知はビルドプロジェクト単位で設定可能なので、ビルドプロジェクトのトップにある 通知 のプルダウンから 通知ルールの作成 と進んで通知ルールを作成します。\n設定項目は\n通知ルールの名前 詳細タイプ 通知をトリガーするイベント ターゲット です。全て既存の選択肢から選ぶ感じなので、数分で作成できます。\n通知ルールの名前と詳細タイプの設定します。\n通知をトリガーするイベントを洗濯します。\nCodeBuild では Build state と Build phase のそれぞれにトリガーできるイベントが用意されています。 Build state はビルド単位のイベントを、 Build phase はビルド内の phase 単位のイベントをトリガーとします。 phase 単位のイベントを拾っていると通知欄が荒ぶってしまうので、ここでは state のイベントのみをトリガーとするようにしておきます。\nターゲットを選択します。\nここでは、先ほど作成した SNS トピック CodeBuildNotification を指定します。複数のターゲットを指定することもできます。\n以上で通知ルールの作成が完了です。\n正常に作成できていれば、ビルドプロジェクトの設定にて作成した通知ルールを確認することができます。\nSlack 通知用の Lambda を作成 上記の設定でとりあえず CodeBuild からの通知を SNS に投げるところまではできるようになったので、その先は SNS に適当なサブスクリプションを設定して通知を受け取れば OK となります。\n今回は CodeBuild からの通知内容をいい感じに整形して Slack に通知するために、それ用の Lambda を作ってサブスクリプションに設定します。\n実際に CodeBuild から SNS 経由で Lambda に渡されるイベント情報と、それを良い感じに整形して Slack に通知する Lambda は gist に置いていますので、そちらを参照ください。(長くなってしまうので)\nThis is a sample of AWS Lambda function that notifies CodeBuild Notification (only state changes) to Slack via Amazon SNS. サンプルで置いているイベントの情報は、ステータス SUCCEEDED でビルドが完了したときのイベントです。\nBuild state のイベントとしては Succeeded, Stopped, Failed, In-progress がありますが、 In-progress 以外はこの構造をしています。\nIn-progress は何が違うかというと、 detail.additional-information.phases と detail.additionail-information.logs 内の group-name および stream-name が存在しません。まあ In-progress はビルド開始時のイベントなので納得ですね。\n実際にビルドしてみる 準備が整ったので、実際にビルドを実行してみます。\nちなみに curl コマンドを叩いていた時の通知はこんな感じでした。\n一応 attachment でそれっぽくしてはいますが、完全に固定のメッセージで味気ないですし。完全にただの \u0026ldquo;通知\u0026rdquo; です。\nそれが今回の一連の設定でこんな感じになります。\n良い感じですね。\nちなみにステータスが FAILED のときは赤色、 STOPPED のときは黄色になったりします。\nさらに phase レベルで SUCCEEDED になっていないものがあればその内容も付与するようになっています。\nそして、先日の Developers.IO のセッション内で、通知に CloudWatch Logs へのリンクを貼っておくと色々捗るよという話があったので、それも付与するようにしています。\nまとめ AWS CodeBuild からのイベント通知を SNS 経由で Slack に通知してみた話でした。\n今回は CodeBuild で全部やるという 節約のためにやってみた内容になりましたが、この通知サービスは実際の開発フローでも非常に便利なサービスだと思います。\n各 Code シリーズからの詳細な情報を得ることができるようになるため、各イベントをトリガーとした動作をより詳細に組み合わせることができそうです。\nAWS の Code シリーズで開発フローをリッチにしたい、通知周りをより簡単に設定したいという方は試してみてはどうでしょうか。\nThis is a sample of AWS Lambda function that notifies CodeBuild Notification (only state changes) to Slack via Amazon SNS. ",
    "permalink": "https://michimani.net/post/aws-codebuild-notification/",
    "title": "AWS CodeBuild から送信されるイベント情報を SNS 経由で Slack に通知してみた"
  },
  {
    "contents": "やってみた系ブログ Developers.IO でおなじみのクラスメソッド株式会社が主催するカンファレンスイベント Developers.IO 2019 Tokyo に行ってきたので、聴講したセッションの中で特に印象に残っているもの、参考になったもののレポートという名の感想を書いておきます。\nちなみに昨年も参加したのですが、今年は昨年以上のセッション数で、事前エントリー、当日の来場者もかなり多かったようです。\nhttps://t.co/8XSAiURBp3 2019東京にご参加頂きましてありがとうございました。1200名エントリー、晴天に恵まれて約900名ご来場、スタッフ約100名の大きなイベントとなりました。スポンサーの皆様、登壇者の皆さんもお疲れ様でした。\n\u0026mdash; さとし🍅 (@sato_shi) November 1, 2019 イベント当日はハッシュタグ #cmdevio でたくさんの方がツイートしているので、当日の雰囲気を味わいたい方はチェックしてみてください。\n目次 セッションごとの感想 Developers.IO CAFEのこれまでとこれから 〜顧客体験へのフォーカスから考える技術選択〜 AWSのすべてをコードで管理する方法〜その理想と現実〜 サービスを爆速で立ち上げるためのSaaSの活用 障害に備えたアーキテクチャを考える「あのとき何が起こった！？」 サーバーレスの基本とCI/CD構築 \u0026amp;amp; 運用 〜システムは動いてからが本番だ〜 全体の感想 セッションごとの感想 聴講したセッションごとの感想を簡単に書いていきます。\nなお、 Developers.IO のセッションの特徴としては、セッション後 (もしくはセッション前) に資料が公開されます。また、登壇者によっては資料だけでなくセッション中に話した内容や補足情報をまとめたブログをアップされている方もいるので、当日現地に行けなかった人たちにも優しいイベントになっています。\nちなみに会場は日本橋のベルサール東京日本橋でした。\nDevelopers.IO CAFEのこれまでとこれから 〜顧客体験へのフォーカスから考える技術選択〜 Developers.IO CAFEのこれまでとこれから 〜顧客体験へのフォーカスから考える技術選択〜 – Developers.IO TOKYO 2019 #cmdevio ｜ Developers.IO クラスメソッドの社長 横田聡さんによるセッションでした。\n内容としては、クラスメソッドが運営している完全キャッシュレスカフェ Developers.IO CAFE の開発に関する内容でした。開発に関する内容といっても具体的な技術の話というよりは、サービス開発において大事なことをたくさん知れたセッションでした。\n特に印象的だったのが、 体験をハックする という内容です。\nDevelopers.IO CAFE は Amazon Go を参考にしているのですが、 Amazon Go 自体はなんの仕組みも技術も公開されておらず、わかっているのは「入店して商品を取って退店すると決済が完了している」という 体験 のみ。その体験が要件定義となり仕様書となり\u0026hellip;で、実現できるための技術、開発は有志の社員の方々に丸投げだったようです。\n体験を形にするというのは非常に難しい気はしますが、ユーザのことを一番に考えると、本当に必要な機能やサービスはユーザの体験からわかるものなのかなーという納得感がありました。\nまた、 開発者や社内の人などの関係者だけでなく、実際のユーザ・カスタマーのフィードバックが大切だ という話もありました。\nDevelopers.IO CAFE ではカスタマーフィードバックをたくさんもらうために、開発-リリースのサイクルを何回も回すことを意識していて、結局 1 年間で 4 周もサイクルを回されたようです。さらに最近では企業や大学とコラボして期間限定の店舗をオープンするなどして、いろんな方面からカスタマーフィードバックを得ているということでした。\nさらには CAFE で働く人たちからもフィードバックをしてもらって、より心地よいユーザ体験を追究していくとのことでした。\nちょっと長くなりましたが、まとめると、サービス開発においては技術の選定や上司への提案からスタートするのではなく、まずは体験からスタートするのが良い というのが印象的でした。\nまた、横田さんは良い意味で社長感が無くて、とりあえずやってみる精神が非常に強い人だなという印象でした。\nAWSのすべてをコードで管理する方法〜その理想と現実〜 CloudFormationの全てを味わいつくせ！「AWSの全てをコードで管理する方法〜その理想と現実〜」 #cmdevio ｜ Developers.IO Infrastructure as Code に関するセッションでした。\n内容としては、 CloudFormation 大好きな 濱田孝治（ハマコー）@hamako9999 さんが主観たっぷりで CloudFormation について語る内容でしたが、 CloudFormation を既に使っている人にも、これから使おうとしている人にも刺さるような内容でした。\nちなみに自分は AWS CDK の GA に合わせて重い腰をあげて IaC デビューした身なのでほぼ後者にあたる人間だったのですが、参考になる部分がたくさんありました。\nまず最初は、そもそも Infrastructure as Code とはなにか？ というセクションだったのですが、ここで出てきた「AWS CLI との違い」による例がわかりやすかったです。\nAWS CLI は 処理を定義する ので、実行した回数だけリソースが増えていきます。一方で IaC は 状態を定義する ので、 (テンプレートを変更していなければ) 何度実行してもリソースは変化せず、数も増減しません。\nこの比較は、あー、なるほどと思いました。\nもうひとつは、 CloudFormation レンプレート作成時にどのような観点でスタックを分割するか という内容が参考になりました。\nCloudFormation ではなく AWS CDK での経験なのですが、複数のリソースを扱おうとすると、テンプレートはかなりのボリュームになります。例えば前にやった例では、 CloudWatch Events と Lambda で EC2 の自動起動・停止を実現する環境を CDK で作ったんですが、そのときに生成された CloudFormation テンプレートが 170 行にもなりました。テンプレートファイルに免疫がない自分にはこの行数でも ウッ となったんですが、もっと複雑なリソースの組み合わせになると 1000 行を超えることもあるようです。\nそうなってくると１つのテンプレートファイルで扱うのは非常に辛いため、いくつかのテンプレート・スタックに分割して管理するのが良いようです。\n具体的には、リソースの依存関係や変更が発生する頻度などが分割の条件 (というか考え方) になるようです。\n例えば、 VPC、Subnet、IGW、RouteTable、EIP などは変更がほぼないため、１つのテンプレートにまとめてしまってよくて、変更が頻繁に発生するであろう SecurityGroup はそれだけで１つのテンプレートにしてしまう、といった具合です。\n特にテンプレート・スタック分割はなるほど感が強く、内容としては CloudFormation での話でしたが、 CDK でのスタック分割にも適用できる考え方かなと思いました。\nCDK は TypeScript でまさにコードっぽくテンプレート (になるもの) を書くことができるので非常に便利ですが、やはり大元となる CloudFormation についても勉強が必要だなと思いました。\nサービスを爆速で立ち上げるためのSaaSの活用 サービスを爆速で立ち上げるためのSaaSの活用 – Developers.IO TOKYO 2019 #cmdevio ｜ Developers.IO タイトルの通り、 SaaS (Software as a Service) を使って爆速でサービス、特に SaaS を立ち上げようという話でした。そうです、 SaaS を活用して SaaS を作ろうという話です。\n利用者としてみる SaaS と、提供者としてみる SaaS の両方の良いところがわかりやすくまとめられていました。\n特に提供者としての SaaS は、パッケージ製品のようにユーザに個別で対応する必要がなく、今流行りのサブスクリプションモデルにもしやすい、ビジネスとしては美味しいところがたくさんあるというのが印象的でした。そんな SaaS は主に 認証、 課金・決済、CI/CD の 3 つの要素で構成されていますが、それらをスクラッチで開発するのではなく SaaS を活用しましょうという感じです。\n具体的には、認証まわりは Auth0、 課金・決済まわりは Stripe、 CI/CD は CircleCI がそれぞれ紹介されていました。この日の他のセッションでは、これらを 三種の神器 としてより具体的に紹介されているセッションもあったようです。\n【レポート】三種の神器」で始めるSaaS生活 – Developers.IO TOKYO 2019 #cmdevio ｜ Developers.IO SaaS は活用するもの という認識でしたが、それらを使って SaaS を作るという内容が面白かったです。また、 SaaS はビジネス的にもうまみがあるということから、今後は SaaS の開発をしているようなところが伸びていくのかなーと思いました。\n障害に備えたアーキテクチャを考える「あのとき何が起こった！？」 障害に備えたアーキテクチャを考える「あのとき何が起こった！？」 – Developers.IO TOKYO 2019 #cmdevio ｜ Developers.IO 「あのとき」とはもう皆さんご存知の 8月 23日 の AWS 東京リージョンの障害発生時ですね。\nこのセッションでは、あのときにクラスメソッドのブログ Developers.IO で何が起こったのか、どのような対応をしたのか、また、障害に対してどのような構成・準備が考えられるかについて話されていました。\nDevelopers.IO への障害の対応としては、 Slack への通知、 ALB や WAF のログなどから情報を得て的確な判断と行動をするという、お手本のような内容でした。\nただこれができたのは事前にしっかりと準備をされていたからなので、 Slackへの通知 や 何が起きているか判断できるメトリクスの取得 、 想定障害と復旧手順の準備 が大事だということでした。事前の準備でもう一つあったのが ログを S3/CWLogs に出力する という項目です。障害時には該当のサーバなどに入ることすらできない可能性があるので、外出ししておきましょうという話です。これ自体はわかってたのですが、その先の ログの検索手順の確立 というところまでやっておく必要があるというのがグサッときました。たしかに S3 や CWLogs に外出ししておくのはいいのですが、それらに対する検索って結構やりづらいですし、 Athena を使うにしても検索用のテーブルを作ったりする必要があって結構面倒です。なので、ログは外出しでとどまらずにその後の検索方法までしっかり確認して準備敷いておかないとけないと感じました。\nあと印象的だったのが、稼働率とコストの関係についての話です。\nあのときの障害によって各所では「マルチAZでは足りない、3つにしよう」とか「東京がダメな時のためにマルチリージョンにしよう」という話が結構出ていたと思います。しかし、それらの対策をするにはコストも大きくなります。セッションでは、稼働率 99% の時のコストを 1 として、99.9%、 99.99%、 Mulch Region のそれぞれを実現するために必要なコストとアーキテクチャについて説明されていました。ざっくりとですが、 99% が 1 に対して、 99.9% は 25.65、 99.99% は 36.81、 Multi Region は 47.72 という比率でのコスト増となるようです。3AZ 化やマルチリージョンという発想を出すのは簡単ですが、大きくなるコスト、複雑になる運用との優先度を冷静に考える必要があるなと思いました。\nサーバーレスの基本とCI/CD構築 \u0026amp; 運用 〜システムは動いてからが本番だ〜 サーバーレスの基本とCI/CD構築 \u0026amp;amp; 運用 〜システムは動いてからが本番だ〜 – Developers.IO TOKYO 2019 #cmdevio ｜ Developers.IO サーバレスの基本はさらっとで、サーバレス開発における運用の方法 (テスト環境の準備、テストコードの書き方、セキュアなデプロイ方法 etc\u0026hellip;) について、実際に担当されているプロジェクトを例に話されていました。で、この内容が非常に詳細にまとめられていて、サーバレス開発するにあたってはこのセッションの資料はかなり参考になるのではないかと感じました。\nGit のブランチの運用方法から始まり、 CircleCI のワークフローの例、AWS リソースに絡むテストコードの書き方、デプロイ時に使用する IAM ロールとアタッチするポリシーの分け方、さらには運用が始まってからの監視についての内容まで、本当に盛りだくさんの内容でした。\n実際のプロジェクトを例にした内容だったので、これからサーバレス開発を始めようと思っている人にも、すでに何かしらの運用フローで開発をしている人にも参考になるような内容でした。\nこのセッションはビアバッシュ形式での再演も実施されたセッションだったのでその当時は内容がはっきり入ってきていなかった部分もあったんですが、あらためて資料を見てみるとそのまとまり具合にただただびっくりしています。熟読したいと思います。\n全体の感想 Developers.IO 2019 Tokyo に行ってきた話でした。\n本当に各セッションに対する感想だけを書いたのですが、どのセッションも内容が濃くて参考になるものばかりで、 45 分のセッション時間があっという間に過ぎていました。\nあとこれは去年も感じたのですが、クラスメソッドの人たちは本当に技術力と登壇力が高いなーと感じました。資料も見やすくてセッションの運びもうまくて、ただただすごいなと思いました。\nまた、後に資料がすべて公開されるというのも良いですね。当日は会場設備の関係で一部のセッションではスクリーンの調子がよくなかったりというアクシデントはありましたが、資料は後で公開されると事前に告知されていたので、安心感がありました。\nあとは、ランチセッションで配られたお弁当が胃に優しいお弁当で、とても美味しかったです。\nさらに会場ではクラスメソッド、ゲスト企業のブース、コワーキングスペースなどもあり、空き時間も楽しめるようになっていました。\n今回技術的なインプットをたくさんすることができたので、得た内容、関連するに内容に対して実際に手を動かして自分の技術レベルを上げていきたいと思います。\n",
    "permalink": "https://michimani.net/post/event-developersio-1029-tokyo/",
    "title": "[レポート] Developers.IO 2019 Tokyo に行ってきました"
  },
  {
    "contents": "自分で開発したサービスを AWS 上で動かしたいとか、認定試験のために実際に触ってみたいとか、個人で AWS を利用したいシーンは多々あると思います。そんなとき気になるのは、毎月のコストです。勉強代だと思っても、できる限りコストは抑えたいところです。今回は、実際に私が個人で AWS を使う上でコスト削減のためにやっていること・気をつけていることを書いていきます。\n目次 コストアラートを設定する AWS Cost Explorer を使ってコストを管理する サービスの無料利用枠を活用する Amazon EC2 Amazon RDS Amazon DynamoDB AWS Lambda Amazon SNS AWS CodeCommit AWS CodePipeline まとめ コストアラートを設定する まずは、毎月の請求が来たときにびっくりすることがないようにコストアラートを設定します。\n毎月のコストの推移は CloudWatch のメトリクスで監視することができます。ただし請求データとアラームについては バージニア北部 (us-east) リージョンのみで表示・設定が可能となっています。\n手順としては普通の CloudWatch アラームの設定方法となんら変わりありません。\n対象とするメトリクスは 請求 \u0026gt; 概算合計請求額 \u0026gt; EstimatedCharges (USD) です。これに対して適当な閾値を設定して、 SNS で個人の Slack なりメールなりに通知すれば OK です。\nSNS トピックの作成については過去に書いているので参考にしてみてください。\nAWS Cost Explorer を使ってコストを管理する AWS のサービス群には AWS Cost Explorer というサービスがあります。これは AWS の使用状況とコストを可視化して、 どのサービス 、 どのリージョン 、 どの API オペレーション など、非常に細かいレベルでコストの発生状況を確認することができます。\n色々と分析の方法はあるんですが、個人的におすすめなのは タグ での分析です。\nAWS の各種リソースには\u0008 タグ として key/value 形式で情報を付与することができます。 そのタグの key を コスト配分タグ として有効化しておくと、AWS Cost Explorer にて同一 key に対して value ごとのコストがわかるようになります。\n私の場合は各リソースに Product というタグキーを設定して、そこにプロダクト名をつけています。そして Product タグ key をコスト配分タグとして有効化することで、下のキャプチャのようなグラフを表示することができます。\nタグ key をコスト配分タグとして有効化するのは、請求情報ページから実施します。\nサイドメニューの コスト配分タグ を選択します。\n既存の AWS リソースに設定されているタグ key が表示されているので、必要なものを選択して有効化します。\nAWS Cost Explorer でタグ別分析ができるのは、コスト配分タグとして有効化したあとからになります。たとえ対象のタグを有効化前から設定していたとしても、有効化までに発生したコストに関しては No Tagkey として計算されます。\nなので、できるだけ早くコスト配分用のタグを決めて有効化しておくことをおすすめします。\nこれによって、\u0008どんな内容で作成したリソースでどれだけのコストが発生しているかを知ることができます。\n個人で開発したサービスをいくつか持っているのであれば、サービスごとのコストがわかります。\n参加したハンズオンごとにタグの値を設定しておけば、のちにリソースの削除忘れなどに気付きやすくなるかもしれません。\nとにかく、どこにコストがかかっているのかを知るために AWS Cost Explorer を使いましょう。\nサービスの無料利用枠を活用する AWS の各サービスには無料利用枠が設定されているものがあり、ある程度の利用であれば料金が発生しません。なので、これを有効に活用しましょう。\nある程度 というのは、 利用開始からの一定期間 もしくは 特定の条件で無期限 のそれぞれのパターンがあります。\n各サービスの無料枠については下記の公式ページにまとめられています。\nAWS クラウド無料利用枠 | AWS この中から、特に利用頻度が高そうなもの、実際にお世話になっているものをいくつか抜粋してみます。\nAmazon EC2 750 時間/月 の利用時間 Linux、RHEL、SLES または Windows の t2.micro インスタンス 利用開始から 12 ヶ月間 750 時間というのは 24 で割ると 31.25 なので、 1 台の t2.micro インスタンスを無料で 1 ヶ月フル稼働させられるということになります。\n毎日半日だけ起動するのであれば、 2 台分使えます。\nただしこれは起動時間に対する料金の無料枠なので、起動中に発生したデータ転送にかかるコストは別途必要になります。\nまた、期間は利用開始から 12 ヶ月間となっているので、初めて利用を開始したら 1 年間はがっつり使いましょう。\nAmazon RDS 750 時間/月 の利用時間 db.t2.micro インスタンスが利用可能なデータベースエンジン 20 GB の汎用 (SSD) データベースストレージ 20 GB のデータベースバックアップおよび DB スナップショット用ストレージ 利用開始から 12 ヶ月間 RDS に関しても利用開始から 12 ヶ月間、毎月 750 時間の無料枠が利用できます。また、データベースストレージについても 20 GB (汎用 SSD) の無料枠が利用できます。\nひとつ残念なのが、無料枠の対象となるインスタンスタイプ db.t2.micro が全てのデータベースエンジンに対応していないんですよね。というか、 Amazon Aurora のみ対応しておらず、それ以外は対応しています。\nAmazon DynamoDB 読み/書き それぞれ 25 個のプロビジョニングキャパシティ 25 GB のデータストレージ DynamoDB ストリームからの 250 万回のストリーム読み込み要求 期間無制限 DynamoDB は毎月上記の無料枠が付与されています。なので、ちょっと触る程度だったり、本当に小規模なサービスであればずっと無料で利用することができます。 (これは DynamoDB に限った話ではないですが)\n読み/書き それぞれ 25 個のプロビジョニングキャパシティ と 25 GB のデータストレージ という値は個人で触る分には十分すぎる値で、私の場合は毎月少しずつストレージ容量は食ってますが、毎月 DynamoDB に対する料金は発生していません。(もちろん個人差はあります)\nDynamoDB 使ってサーバレス開発ちょっとやってみたいけどコストが\u0026hellip;と思っている方、十分な無料枠があるので安心してください。\nAWS Lambda 1,000,000 件/月の無料リクエスト 1 か月あたり最大 320 万秒のコンピューティング時間 期間無制限 Lambda は リクエスト回数 と コンピューティング時間 (秒) に対してコストが発生します。\nリクエスト回数は、その名の通り Lambda 関数を実行した回数になります。毎月 1,000,000 回ってかなりの数ですよね。\n最大 320 万秒のコンピュート時間についても、十分な利用枠だと思います。\nLamnda のコンピュート時間に対するコストは、 割り当てたメモリ と 実行時間 によって算出されます。ただしめちゃくちゃややこし良いので、詳細は Lambda の料金ページに記載されている例をご参照ください。\n料金 - AWS Lambda ｜AWS とは言っても、 Lambda に関しても個人で使う分には十分な無料枠なので、あまり神経質になる必要はないと思います。\nAmazon SNS 100 万件の発行 10 万件の HTTP/S 配信 1,000 件のメール配信 期間無制限 AWS での各種通知は Amazon SNS を利用することが多いと思います。しかし、上記のように十分な無料枠が期間無制限で毎月利用可能なので、上で設定したコストアラート含め、さまざまな通知に対して利用してもほぼ無料枠で収まるのではないでしょうか。\nAWS CodeCommit 1 か月あたり 5 人のアクティブユーザー 1 か月あたり 50 GB/月のストレージ 1 か月あたり 10,000 Git のリクエスト 期間無制限 CodeCommit に関してはだいぶ前にも書きましたが、個人で使うのであればほぼ無料で利用できるでしょう。無料枠からはみ出るとすれば、ストレージくらいな気がします。\nAWS CodePipeline 1 アクティブパイプライン/月 期間無制限 たった 1 つのパイプラインだけかよと思われるかもしれませんが、この アクティブパイプライン というのがポイントです。\nアクティブなパイプラインとは、30 日以上存在していて、その月に少なくとも 1 つのコード変更が発生したパイプラインです。その月に新しいコード変更がないパイプラインに対しては、料金は発生しません。アクティブなパイプラインは、1 か月に満たない分に対して按分計算されません\n料金 - AWS CodePipeline | AWS つまり、作成から 30 日以内のパイプラインであれば幾つ存在していても、かつ何回コードの変更があっても無料ということになります。\nありがちなパターンとしては、作成から 30 日以上経過しているもののほとんどコード変更のないパイプラインが複数あってもコストは発生しません。しかし、ある月にそれらのパイプラインのうち 2 つにコード変更が発生した場合、 1 つは無料枠の対象ですが、もう 1 つにはコストが発生してしまいます。\nCodePiplie の料金は 1.00 USD/アクティブパイプライン なので、このパターンだと CodePipeline だけで 1 USD のコストが発生してしまいます。\n自動デプロイはしたいけど節約もしたい\u0026hellip; という場合は、できることは制限されますが CodeBuild だけでやるという方法もあります。\nまとめ AWS を個人で使う際にコストを削減する方法について書きました。\n完全に無料で！というのは流石に難しいですが、無料枠の活用や Cost Explorer での見直しなどでコストの削減ができるのでは？という内容でした。コストの削減は個人レベルだけの話ではなく実際の業務においても重要な内容だと思うので、もしかしたら個人レベルで実施しているコスト対策が実務で生きる可能性もあります。\nAWS は個人で使うにしては高い！ という思いを持っている方には、そんなことないよと伝えたいです。\n",
    "permalink": "https://michimani.net/post/aws-way-to-save-cost-personal-account/",
    "title": "AWS を個人で使うときにコストを削減するためにやっていること"
  },
  {
    "contents": "DynamoDB の項目追加 put_item() については過去に書いたのですが、その時は必須パラメータのみを指定して動作を確認しました。今回は、オプションパラメータを使用して項目の追加と更新をやってみます。\n過去に書いた記事はこちら。\n目次 準備 テーブルの作成 サンプルデータのロード 項目の条件付き追加・更新 既に存在する項目と同じキーを持つ項目を追加しようとしたときに上書きされるのを防ぐ 特定の条件を満たす場合のみ更新する まとめ 準備 今回は AWS の公式ページにあるサンプルデータを使います。\nステップ 1: サンプルテーブルの作成 - Amazon DynamoDB いくつかサンプルのテーブルとデータが用意されていますが、その中の Thread テーブルのデータを使います。\nテーブルの作成 上のリンクではマネジメントコンソールでのテーブル作成手順が載っていますが、テーブルの作成も boto3 ライブラリを使ってやってみます。\nimport boto3 dynamodb_clinet = boto3.client(\u0026#39;dynamodb\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: response = dynamodb_clinet.create_table( AttributeDefinitions=[ { \u0026#39;AttributeName\u0026#39;: \u0026#39;ForumName\u0026#39;, \u0026#39;AttributeType\u0026#39;: \u0026#39;S\u0026#39; }, { \u0026#39;AttributeName\u0026#39;: \u0026#39;Subject\u0026#39;, \u0026#39;AttributeType\u0026#39;: \u0026#39;S\u0026#39; } ], TableName=\u0026#39;Thread\u0026#39;, KeySchema=[ { \u0026#39;AttributeName\u0026#39;: \u0026#39;ForumName\u0026#39;, \u0026#39;KeyType\u0026#39;: \u0026#39;HASH\u0026#39; }, { \u0026#39;AttributeName\u0026#39;: \u0026#39;Subject\u0026#39;, \u0026#39;KeyType\u0026#39;: \u0026#39;RANGE\u0026#39; } ], BillingMode=\u0026#39;PROVISIONED\u0026#39;, ProvisionedThroughput={ \u0026#39;ReadCapacityUnits\u0026#39;: 1, \u0026#39;WriteCapacityUnits\u0026#39;: 1 }, StreamSpecification={ \u0026#39;StreamEnabled\u0026#39;: False }, SSESpecification={ \u0026#39;Enabled\u0026#39;: False }, Tags=[ { \u0026#39;Key\u0026#39;: \u0026#39;Product\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;Sample\u0026#39; }, ] ) 色々と値を渡していますが、必須なのは下記のパラメータです。\nAttributeDefinitions\nパーティションキー、ソートキーで指定する属性を定義します TableName\nテーブル名です KeySchema\nパーティションキー、ソートキーの設定です その他 create_table() の詳細については下記ページを参照してください。\nDynamoDB — Boto 3 Docs 1.9.250 documentation | DynamoDB.Client.create_table サンプルデータのロード サンプルデータ (JSON) は下記ページからダウンロードできます。\nステップ 2: データをテーブルにロードする - Amazon DynamoDB 今回は Thread テーブルを使うので、 Thread.json のデータをロードします。\n上記ページでは AWS CLI でのロード方法が書かれているので、それに従ってサクッとロードしてしまいます。\n$ aws dynamodb batch-write-item --request-items file://data/Thread.json 項目の条件付き追加・更新 項目の条件付き追加を試してみます。\n条件付き とは、 put_item() 関数のオプションパラメータ ConditionExpression で指定する条件のことです。同じく条件を指定する Expected というパラメータもありますが、こちらは レガシーパラメータ とされていて、代わりに ConditionExpression を使うように書かれています。\nThis is a legacy parameter. Use ConditionExpression instead. For more information, see Expected in the Amazon DynamoDB Developer Guide .\nDynamoDB — Boto 3 Docs 1.9.250 documentation | DynamoDB.Table.put_item 既に存在する項目と同じキーを持つ項目を追加しようとしたときに上書きされるのを防ぐ 条件付きの追加でイメージしやすいのは、同じパーティションキー、ソートキーを持つ項目の追加かと思います。\nThread テーブルではパーティションキーが ForumName 、ソートキーが Subject なので、追加する項目は最低でもこの二つの属性を持っている必要があります。これは、 ConditionExpression パラメータで条件を指定しなくてもエラーとなります。\nということで、下記のような項目を追加してみます。\nadditional_item = { \u0026#39;ForumName\u0026#39;: \u0026#39;Amazon DynamoDB\u0026#39;, \u0026#39;Subject\u0026#39;: \u0026#39;DynamoDB Thread 2\u0026#39; } このパーティションキーとソートキーを持つ項目は、サンプルデータに含まれていました。\n$ aws dynamodb get-item --table-name Thread --key \u0026#39;{\u0026#34;ForumName\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;}, \u0026#34;Subject\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34;}}\u0026#39; { \u0026#34;Item\u0026#34;: { \u0026#34;LastPostedDateTime\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2015-09-15T19:58:22.514Z\u0026#34; }, \u0026#34;Replies\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Message\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB thread 2 message\u0026#34; }, \u0026#34;LastPostedBy\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34; }, \u0026#34;Answered\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34; }, \u0026#34;Views\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;Tags\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;items\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;attributes\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;throughput\u0026#34; } ] }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34; } } } なので、このまま条件を指定せずに追加すると、既存の項目が更新されます。\n一度やってみます。\nimport boto3 dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(\u0026#39;Thread\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: additional_item = { \u0026#39;ForumName\u0026#39;: \u0026#39;Amazon DynamoDB\u0026#39;, \u0026#39;Subject\u0026#39;: \u0026#39;DynamoDB Thread 2\u0026#39; } response = dynamo_table.put_item(Item=additional_item) 上のスクリプトを実行してから、あらためて項目を取得してみます。\n$ aws dynamodb get-item --table-name Thread --key \u0026#39;{\u0026#34;ForumName\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;}, \u0026#34;Subject\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34;}}\u0026#39; { \u0026#34;Item\u0026#34;: { \u0026#34;ForumName\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34; }, \u0026#34;Subject\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2\u0026#34; } } } このように、同一キーをもつ項目を追加した場合、既存の項目は完全に上書きされてしまいます。これによって属性の意図しない消失が発生します。\nこれを防ぐためには、 ConditionExpression パラメータで該当のキーが存在しない場合のみ追加するように条件を設定します。\nimport traceback import boto3 dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(\u0026#39;Thread\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: try: additional_item = { \u0026#39;ForumName\u0026#39;: \u0026#39;Amazon DynamoDB\u0026#39;, \u0026#39;Subject\u0026#39;: \u0026#39;DynamoDB Thread 2\u0026#39; } response = dynamo_table.put_item( Item=additional_item, ConditionExpression=\u0026#39;attribute_not_exists(ForumName) AND attribute_not_exists(Subject)\u0026#39; ) except Exception: print(traceback.format_exc()) 追加したのは ConditionExpression='attribute_not_exists(ForumName) AND attribute_not_exists(Subject)' の部分です。これを実行すると、下記のような ConditionalCheckFailedException エラーになり、項目が上書きされることはありません。\nbotocore.errorfactory.ConditionalCheckFailedException: An error occurred (ConditionalCheckFailedException) when calling the PutItem operation: The conditional request failed ConditionExpression パラメータ内で使用している attribute_not_exists() のような関数については、下記の公式リファレンスを参照してください。\n比較演算子および関数リファレンス - Amazon DynamoDB 特定の条件を満たす場合のみ更新する 下記のような項目があるとします。\n{ \u0026#39;Answered\u0026#39;: 0, \u0026#39;ForumName\u0026#39;: \u0026#39;Amazon CloudFront\u0026#39;, \u0026#39;LastPostedBy\u0026#39;: \u0026#39;User A\u0026#39;, \u0026#39;LastPostedDateTime\u0026#39;: \u0026#39;2015-09-22T19:58:22.514Z\u0026#39;, \u0026#39;Message\u0026#39;: \u0026#39;CloudFront thread 1 message\u0026#39;, \u0026#39;Replies\u0026#39;: 0, \u0026#39;Subject\u0026#39;: \u0026#39;CloudFront Thread 1\u0026#39;, \u0026#39;Tags\u0026#39;: [ \u0026#39;index\u0026#39;, \u0026#39;primarykey\u0026#39;, \u0026#39;table\u0026#39; ], \u0026#39;Views\u0026#39;: 5 } 上記の項目にて、閲覧数を表す Views の値を更新したい場合、その値は必ず増える必要があります。(閲覧数が減ることはほぼ考えないので)\n仮に更新後の新しい値を 10 とすると、更新対象項目の Views の値は 10 未満 である必要があります。この条件を ConditionExpression で指定します。\nこれまでの流れから考えると下記のような記述になりそうです。\nimport traceback import boto3 dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(\u0026#39;Thread\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: try: update_key = { \u0026#39;ForumName\u0026#39;: \u0026#39;Amazon CloudFront\u0026#39;, \u0026#39;Subject\u0026#39;: \u0026#39;CloudFront Thread 1\u0026#39;, } update_attr = { \u0026#39;Views\u0026#39;: { \u0026#39;Value\u0026#39;: 10 } } response = dynamo_table.update_item( Key=update_key, AttributeUpdates=update_attr, ConditionExpression=\u0026#39;Views \u0026gt; 10\u0026#39; ) except Exception: print(traceback.format_exc()) しかしこれを実行すると、下記のようなエラーとなります。 (適宜改行しています)\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the UpdateItem operation: Can not use both expression and non-expression parameters in the same request: Non-expression parameters: {AttributeUpdates} Expression parameters: {ConditionExpression} 今回のように比較演算子を使って条件を指定する場合、比較対象の値 (今回であれば 10) を 属性値 として指定する必要があります。\n具体的には、 update_item() に ExpressionAttributeValues パラメータを追加して、そこで比較用の値の属性値を指定します。\nさらに上記のエラーからわかるように、 AttributeUpdates と ConditionExpression は同時に使用することができません。というか、 boto3 のリファレンスでは AttributeUpdates はレガシーパラメータとされており、代わりに UpdateExpression を使うよう書かれています。\nThis is a legacy parameter. Use UpdateExpression instead. For more information, see AttributeUpdates in the Amazon DynamoDB Developer Guide\nDynamoDB — Boto 3 Docs 1.9.250 documentation | DynamoDB.Table.update_item さらに今回厄介なのが、更新対象の属性 Views が DynamoDB における予約後になっているという点です。\n式の属性名 - Amazon DynamoDB | 予約後 UpdateExpression などで指定する条件式には予約後の使用ができず、使用した場合はエラーとなってしまいます。\nそのような場合には ExpressionAttributeNames パラメータを使って属性に別名をつける必要があります。\nということで、 update_item() の引数を下記のように書き換えます。\nimport traceback import boto3 dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(\u0026#39;Thread\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: try: update_key = { \u0026#39;ForumName\u0026#39;: \u0026#39;Amazon CloudFront\u0026#39;, \u0026#39;Subject\u0026#39;: \u0026#39;CloudFront Thread 1\u0026#39;, } new_views = 10 response = dynamo_table.update_item( Key=update_key, UpdateExpression=\u0026#39;set #attr_views = :new_views\u0026#39;, ConditionExpression=\u0026#39;#attr_views \u0026lt; :new_views\u0026#39;, ExpressionAttributeNames={ \u0026#39;#attr_views\u0026#39;: \u0026#39;Views\u0026#39;, }, ExpressionAttributeValues={ \u0026#39;:new_views\u0026#39;: new_views } ) except Exception: print(traceback.format_exc()) これで Views の値を 10 に更新することができました。\nこのスクリプトを編集せずにもう一度実行してみると、条件式にマッチしないため ConditionalCheckFailedException が発生し、エラーとなります。\nbotocore.errorfactory.ConditionalCheckFailedException: An error occurred (ConditionalCheckFailedException) when calling the UpdateItem operation: The conditional request failed まとめ Python (boto3) で DynamoDB の条件付き項目追加・更新をやってみた話でした。\nこれを書いている時点では、 boto3 のリファレンスではレガシーパラメータの代わりになるパラメータに関する情報が詳しく書かれておらず、公式リファレンスへのリンクが貼られている状態です。\nそのためあちこち見るところが多くて苦戦しました。\n記事中にもリンクは貼っていますが、参考にしたページについては下記に記載しておきます。\n項目属性の指定 - Amazon DynamoDB 式の属性名 - Amazon DynamoDB 比較演算子および関数リファレンス - Amazon DynamoDB ステップ 3: 項目を作成、読み込み、更新、削除する - Amazon DynamoDB DynamoDB の予約語 - Amazon DynamoDB 今回は条件付きの追加と更新それぞれ 1 パターンずつ試しただけですが、あとは条件式の書き方の工夫になってくると思います。\nまた、今回出てきた条件を指定するパラメータの他にも、戻り値をどのような形式にするか指定するようなパラメータもあるので、またそれに関しても調べてみようと思います。\nDynamoDB 全然わからん\n\u0026mdash; よっしーCBR852RR (@michimani210) October 16, 2019 ",
    "permalink": "https://michimani.net/post/aws-operate-dynamodb-by-python/",
    "title": "Python (boto3) で DynamoDB の条件付き項目追加・更新をやってみる"
  },
  {
    "contents": "AKRacing のゲーミングチェア Pro-X シリーズの gray - 灰 を使い始めて約 1ヶ月がたちました。他の人のレビューでは王座だとか最高のイスだとか言われてますが、実際はどうなのか、レビューしてみます。\n目次 AKRacing のラインナップ Pro-X シリーズ 選んだ理由 組み立て 各部詳細と感想 全体的な感想 まとめ AKRacing のラインナップ AKRacing では下記のように様々なシリーズのチェアが販売されています。\nPremium Pro-X OVERTURE Nitro Wolf 極坐 このうち 極坐 は座椅子のような形ですが、それ以外はキャスターがついた、いわゆるオフィスチェアのような形です。\n価格も上のリストの順番通り上から高くなっており、 Premium シリーズはおよそ 60,000 円、 Wolf シリーズはおよそ 40,000 円となっています。極坐シリーズはおよそ 44,000 円です。\n極坐以外の 5 シリーズは、形状はよく似ていますが座面の厚さや材質、可動部分に違いがあります。特に気になるのは材質と可動部分なので、ざっと比較表を作ってみました。\nPremium Pro-X OVERTURE Nitro Wolf 材質 PUレザー PUレザー PUレザー PUレザー ファブリック生地 特徴 人間工学に基づいた設計 幅広の背もたれ、深めの座面によるゆったりとした座り心地 豊富なカラーバリエーション AKRacing シリーズのスタンダードモデル 基本設計・機能を踏襲したコスパ重視モデル アームレスト 上下/前後/回転 上下/前後/回転 上下 上下 上下 カラーバリエーション 3色 4色 6色 5色 3色 価格 約 62,000 円 約 55,000 円 約 46,000 円 約 44,000 円 約 41,000 円 ※価格については、この記事公開時点での Amazon でのおおよその価格です。\n各シリーズ共通の機能としては、下記のようなものがあります。\n最大 180° のリクライニンング ロッキング ヘッドレスト ランバーサポート ロッキング機能というのは、座面と背もたれの角度を固定したまま、全体を傾けることができるというものです。角度は最大 12° で、途中で固定することは出来ません。また、ロッキング機能自体の ON/OFF は昇降調整レバーの操作で切り替え可能です。\nPremium シリーズではこれに加えてチルト機能というものが搭載されており、座面の角度を変えた状態で固定できるようです。\nPro-X シリーズ いろんなシリーズがある中でも Pro-X シリーズ を選んだので、細かくみていきます。\n選んだ理由 まずは Pro-X シリーズ を選んだ理由です。\nこれまで使っていたのは、ニトリのワークチェア (N ターゲット) というイスです。アームレストもなく、座面の昇降機能とキャスターが付いているだけのシンプルなオフィスチェアです。価格も 4,000 円程度です。\n長い時間座らなければ特に不満はないのですが、最近は長時間座っていることも多くなったため、良いイスはないかと探していました。\n当初はゲーミングチェアの選択肢は無く、アーロンチェアのような THE オフィスチェア のような物を探していました。でも、めちゃくちゃ高いんですよね。(100,000 ~ 200,000 円くらいする)\nハーマンミラー アーロンチェア リマスタード グラファイト ポスチャーフィット Bサイズ アーム付き AER1B23DW ALP G1 G1 BB BK 23103 で、ゲーミングチェアが選択肢に入ってきました。まあ、長時間座ってる主な理由はスマブラをするためなので、ゲーミングチェアでもいいかなと。ただ、調べてみるとゲーミングチェアでデスクワークするのも全然ありみたいな感じで、しかも前述した THE オフィスチェアよりも安いんですよね。意外でした。\nゲーミングチェアといっても色々あると思うのですが、最初にヒットしたのが AKRacing のゲーミングチェアでした。\n今回、新しいイスを導入するにあたって絶対に譲れない条件は、 アームレストがあること でした。また、ただのアームレストではなく、 昇降 、 前後移動 、 回転 ができるものが欲しかったんです。となると、 AKRacing のシリーズ内では Pro-X シリーズ が該当します。\n組み立て Amazon で注文すると、組み立て前の状態で届きます。\n箱の大きさは 88 x 70 x 41 (cm) 、 重さは 28 kg あります。\nちなみに今回の配送で初めてデリバリープロバイダが使われました。趣旨とはずれるので割愛しますが、まあ不便でした。 組み立て前と言ってもパーツは下記の 4 つのみです。\n座面 背もたれ キャスター シリンダー カラーの組み立て説明書が付属しているので、それを見ながら組み立てます。説明書には 2 人で作業してくださいと書かれていますが、 1 人でもできました。所要時間は 30 分ほどでした。\nなお、組み立てに必要な工具 (六角レンチ) も付属しており、さらには作業用の軍手まで付いているので、届いたら即組み立て作業を開始できます。\n組み上がるとこんな感じです。色は グレイ、ホワイト、レッド、ブルーがありますが、グレイにしました。\n各部詳細と感想 では各部詳細を写真と一緒に見ていきます。\n材質 PU レザーと言われても\u0026hellip;っていう感じですが、表面がこんなふうになっている革ですね。\n高級な感じで、汚れも付きにくいです。液体をこぼしてもすぐには染み込まないので、すぐに拭き取れば問題ありません。\nまた、座っていても滑るという感覚がないので、座り心地も良いです。\nランバーサポート ゆったりとした座り心地が売りの Pro-X シリーズでは、これがないとお尻の位置が深すぎて辛いです。ランバーサポート自体はかまぼこ型になっているのでどうなんだろうと思いましたが、しっかり支えてくれてる感があってとても良いです。\n取り付け位置に関しても、きっちりと場所を固定するものではないので自分の好みの位置に微調整しながら使っています。\nフルフラットにして寝ようとするときには邪魔になるので、その時は外したほうがいいです。ただし、取り付けるためのベルトを座面と背もたれの隙間に通す必要があるので、あまり簡単につけたり外したりできないのが難点です。\nヘッドレスト 反発力はランバーサポートと同じくらいで、少し硬めです。こちらも取り付け位置はきっちり固定するものではないのですが、頭を乗せるタイミングでちょうど良い位置に持って来る感じです。頭を上げると、写真の位置に戻ってしまいます。\nそしてここでも Pro-X シリーズのゆったりさが感じられるのが、基本のヘッドレストの位置が結構高いんですよね。なので、あまり身長が高くない人だと良い位置に持ってくることができない可能性があります。私の身長は 170cm ですが、これ以上小さい人だと厳しいかなという印象です。\nじゃあ外してしまえばいいのかというと、それはそれで首が痛くなるので、難しいところです。\nアームマウント 一見すると硬そうに見えますが、ちょうどいい柔らかさで肘の一番硬い部分が当たっていても痛くはなりません。\nアームマウントは Pro-X シリーズを選択する理由にもなっている重要な部分です。\nまずは前後の調整。\n可動域は、上の写真から下の写真までの位置です。長さにすると、約 5 cm くらいです。個人的にはこれくらい動いてくれたら十分ですね。\n続いては回転。\n左・真ん中・右 の 3 段階で調整できます。回転させた状態で前後に移動させることも可能です。\n回転に関しては、正直可動域が狭いなという印象です。90° とは言わないものの、同じ角度でもう一段階左右に回転できれば最高でした。が、この少しの回転でも肘の置き場所に融通が利くので、回転すること自体には満足です。可動域には少し不満です。\nちなみに上下の昇降可動域は 7cm あり、これは十分な可動域です。\n座面の昇降 Pro-X シリーズに限らず、 AKRacing のゲーミングチェアには クラス4 ガスシリンダー というものが使われているようです。\n国際的第三者認証機関であるLGAよりDIN 4550の基準をクリアしているとの承認を受けた、耐久性が高く、安心・安全な製品です。（最大荷重150kg）\nそのおかげもあってか、昇降は非常に滑らかで、上げるときも急にドンっと上がってくるようなことはありませんし、下げるときも動きが非常に滑らかです。 ただし、昇降レバーがちょっと操作しづらい位置にあるのが残念ポイントです。\n見ての通り、昇降レバーは右後ろの位置にあります。これを操作しようとすると、アームレストの位置によっては腕が当たってしまいます。\n下げる時はただ体重をかけてレバーを引くだけでよいのですが、上げる時は地面に足をついて体を少し浮かせた状態でレバーを引く必要があります。座面の大きさ (厚さ・前後/左右の幅) が結構大きいので、座る姿勢のまま座面を上げようとするのは結構難しいです。\nこれは体の大きさ、バランス感覚によってさはあると思いますが、できれば右前の位置にあればよかったなという思いです。\nまとめ AKRacing のゲーミングチェア Pro-X シリーズのレビューでした。\n各機能に関する部分で感想を書きましたが、トータルでイスとしては最高です。\nゆったりした構造になっているので、座った状態でかなりリラックスできます。座面の上であぐらもかけます。\n長時間座っていても苦にならないので、気付いたらほぼ一日中スマブラやっていた なんていうこともありました。\nまた、180° リクライニングも嬉しいポイントで、フルフラットにすればイスの上で寝ることも可能です。\nその場合オットマンがあればなお良いですが、足を折りたたんで横になって寝られるくらいの広さと安定感があるので、身長が小さめの友達が泊まりに来たときなんかは寝床として提供できるかもしれません。\nただし文中でも触れたように、身長が 170cm より小さい人の通常使用には向いていないと思います。\nPro-X のコンセプトからもゆったりとした構造になっているため、どうしても大きすぎるという感覚になってしまうからです。170cm の自分でも多少大きいかなという印象はあります。\n特にアームレストなんかは体の小さい人には幅が広すぎて肘を置くために腕を外に広げないといけない可能性もあります。\n今回は実物を見ずに Amazon のレビューや他の人のレビューのみを参考にしましたが、やはりイスに関しては実物に実際に座ってみて、その感触を確かめてから導入する方がいいですね。(それはそう)\nもし購入を検討されている方は、ヨドバシカメラやビックカメラなどの家電量販店でもゲーミングチェアを扱っているところはありますので、ぜひ店舗で実際に座ってみてから考えてください。\n",
    "permalink": "https://michimani.net/post/gadget-review-akracing-pro-x/",
    "title": "[レビュー] AKRacing のゲーミングチェア Pro-X を使い始めたら座ってる時間が増えた"
  },
  {
    "contents": "AWS を操作するための Python ライブラリ boto3 を使って DynamoDB へデータを書き込む方法について調べていたところ、どうやらその方法が 3 パターンあるようです。今回は、それらについてのメモです。\n目次 概要 boto3 を使った DynamoDB への書き込み方法 DynamoDB.Client.put_item() DynamoDB.Table.put_item() DynamoDB.Table.batch_writer() DynamoDB.Client と DynamoDB.Table の違い まとめ 概要 boto3 を使って DynamoDB のデータを読み書きする方法については前にも書きましたが、今回は書き込みに絞って調べてみた内容です。\nちなみにこれを書いている時点 (2019/09) 時点での boto3 のバージョンは 1.9.236 です。\nboto3 を使った DynamoDB への書き込み方法 前の記事 ではざっくりと put_item() と書いてましたが、その put_item() にも 2 パターンあります。さらにその put_item() をバッチ処理的に実行できる batch_writer() というメソッドがあるので、全部で 3 パターンあるということになります。\n具体的には以下のメソッドです。\nDynamoDB.Client.put_item() DynamoDB.Table.put_item() DynamoDB.Table.batch_writer() それぞれどういう使い方をするのか見ていきます。 ​ DynamoDB.Client.put_item() DynamoDB — Boto 3 Docs 1.9.236 documentation #DynamoDB.Client.put_item これは 前の記事 でやった方法で、実装としては次のようになります。\ntable_name = \u0026#39;hinatazaka-blog\u0026#39; item = { \u0026#34;Author\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;富田鈴花\u0026#34;}, \u0026#34;Authorcode\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;15\u0026#34;}, \u0026#34;Blogkey\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;30775\u0026#34;}, \u0026#34;Entrydate\u0026#34;: {\u0026#34;N\u0026#34;: \u0026#34;201909062231\u0026#34;}, \u0026#34;Images\u0026#34;: {\u0026#34;L\u0026#34;: [ {\u0026#34;S\u0026#34;:\u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobzVcYf5.jpg\u0026#34;}, {\u0026#34;S\u0026#34;: \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobbmdhGD.jpg\u0026#34;}, {\u0026#34;S\u0026#34;: \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobNBUyrd.jpg\u0026#34;} ] }, \u0026#34;Title\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;東北自動車道\u0026#34;}, \u0026#34;Url\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;https://www.hinatazaka46.com/s/official/diary/detail/30775?ima=0000\u0026amp;cd=member\u0026#34;} } dynamo = boto3.client(\u0026#39;dynamodb\u0026#39;) dynamo.put_item(TableName=table_name, Item=item) TableName : string と Item : dict が必須のパラメータとなります。ちょっとややこしいのが、 Item で指定する dict の value は更に dict になっていて、そこでデータの型を指定する必要があります。 S とか L とかがそれです。\nこの方法だと Item として指定する dict の構成がちょっと繁雑に見えます。\nまた、任意のパラメータである ConditionExpression に色々と条件を指定することで、同一キー項目の上書きを防止したり、条件を満たす場合のみ更新したりすることができます。これはちょっと奥が深そうなので、別で書きたいと思います。\nDynamoDB.Table.put_item() DynamoDB — Boto 3 Docs 1.9.236 documentation #DynamoDB.Table.put_item これは DynamoDB.Client.put_item() よりもシンプルというか、直感的な使い方になります。\ntable_name = \u0026#39;hinatazaka-blog\u0026#39; item = { \u0026#34;Author\u0026#34;: \u0026#34;富田鈴花\u0026#34;, \u0026#34;Authorcode\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;Blogkey\u0026#34;: 30775, \u0026#34;Entrydate\u0026#34;: 201909062231, \u0026#34;Images\u0026#34;: [ \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobzVcYf5.jpg\u0026#34;, \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobbmdhGD.jpg\u0026#34;, \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobNBUyrd.jpg\u0026#34; ], \u0026#34;Title\u0026#34;: \u0026#34;東北自動車道\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;https://www.hinatazaka46.com/s/official/diary/detail/30775?ima=0000\u0026amp;cd=member\u0026#34; } dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(table_name) dynamo_table.put_item(Item=item) 必須パラメータは Item: dict のみで、指定する dict も内部で型の指定をする必要がありません。\nまた、DynamoDB.Client.put_item() と同様に任意のパラメータである ConditionExpression に色々と条件を指定することで、同一キー項目の上書きを防止したり、条件を満たす場合のみ更新したりすることができます。\nDynamoDB.Table.batch_writer() DynamoDB — Boto 3 Docs 1.9.236 documentation #DynamoDB.Table.batch_writer まずは使い方を見てみます。\ntable_name = \u0026#39;hinatazaka-blog\u0026#39; items = [ { \u0026#34;Author\u0026#34;: \u0026#34;富田鈴花\u0026#34;, \u0026#34;Authorcode\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;Blogkey\u0026#34;: 30775, \u0026#34;Entrydate\u0026#34;: 201909062231, \u0026#34;Images\u0026#34;: [ \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobzVcYf5.jpg\u0026#34;, \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobbmdhGD.jpg\u0026#34;, \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobNBUyrd.jpg\u0026#34; ], \u0026#34;Title\u0026#34;: \u0026#34;東北自動車道\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;https://www.hinatazaka46.com/s/official/diary/detail/30775?ima=0000\u0026amp;cd=member\u0026#34; }, { \u0026#34;Author\u0026#34;: \u0026#34;富田鈴花\u0026#34;, \u0026#34;Authorcode\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;Blogkey\u0026#34;: 30820, \u0026#34;Entrydate\u0026#34;: 201909100044, \u0026#34;Images\u0026#34;: [ \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mob12dNLt.jpg\u0026#34;, \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobLZTiXa.jpg\u0026#34; ], \u0026#34;Title\u0026#34;: \u0026#34;甘口と中辛のハイブリット、甘中辛\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;https://www.hinatazaka46.com/s/official/diary/detail/30820?ima=0000\u0026amp;cd=member\u0026#34; }, { \u0026#34;Author\u0026#34;: \u0026#34;富田鈴花\u0026#34;, \u0026#34;Authorcode\u0026#34;: \u0026#34;15\u0026#34;, \u0026#34;Blogkey\u0026#34;: 30855, \u0026#34;Entrydate\u0026#34;: 201909130035, \u0026#34;Images\u0026#34;: [ \u0026#34;https://cdn.hinatazaka46.com/files/14/diary/official/member/moblog/201909/mobSSIX7d.jpg\u0026#34; ], \u0026#34;Title\u0026#34;: \u0026#34;現国よりも数学が\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;https://www.hinatazaka46.com/s/official/diary/detail/30855?ima=0000\u0026amp;cd=member\u0026#34; } ] dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(table_name) with dynamo_table.batch_writer() as batch: for item in items: batch.put_item(Item=item) こんな感じで batch_writer() の中で put_item() を呼ぶことで、一度に複数のデータを書き込むことができます。\n普通に DynamoDB.Table.put_item() を複数回実行するのと何が違うのかというと、 batch_writer() を使った場合はよしなに再送とかをやってくれるんです。\nThe batch writer will automatically handle buffering and sending items in batches. In addition, the batch writer will also automatically handle any unprocessed items and resend them as needed. All you need to do is call put_item for any items you want to add, and delete_item for any items you want to delete.\nまた、ドキュメントにもあるように、 delete_item() も同時に batch writer に登録できるので、登録と削除を一括で実行することができます。\nDynamoDB.Client と DynamoDB.Table の違い ここまでで 3 パターンをみてきましたが、実質的には DynamoDB.Client.put_item() と DynamoDB.Table.put_item() の 2 パターンという味方ができます。\nどちらもオプションで詳細な書き込み条件を指定することができるという特徴がありますが、必須パラメータが違ったり Item で指定する dict の構成が違ったりします。じゃあこれどっちを使えばいいのかっていう疑問が出てきます。\nあらためてそれぞれの put_item() を使う実装方法をみてみると、 DynamoDB.Client.put_item() ではまず DynamoDB の Client オブジェクトを生成しています。\ndynamo = boto3.client(\u0026#39;dynamodb\u0026#39;) DynamoDB.Table.put_item() では、 DynamoDB の ServiceResource オブジェクトを生成して、そこから Table オブジェクトを生成しています。\ndynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) dynamo_table = dynamo.Table(table_name) つまり DynamoDB.Client.put_item() のほうが、より低レベルな層からデータの書き込みを実行していることになります。実際、 DynamoDB.Client には create_table() や describe_endpoints() といった、 DynamoDB 全体にかかるメソッドが用意されています。 一方で DynamoDB.Table には、 delete() や scan() といった、特定のテーブルに対する操作を行うメソッドしかありません。\nなので使い分けとしては、基本的には DynamoDB.Table.put_item() のほうを使えばいいのかなという感じです。 put_item() のたびにテーブル名を指定する必要もないですし、書き込むデータの dict も自然なので扱いやすいです。\n一方で、一つのプロセス内で複数のテーブルに対して処理を実行するような場合には DynamoDB.Client.put_item() を使うことでオブジェクトの生成回数を減らすことができます。結果的にこれがパフォーマンスの向上に繋がるかもしれませんが、そこまでシビアな実装をすることってあるのかな？という気もします。\nまとめ Python (boto3) を使って DynamoDB にデータを書き込む方法について調べてみた話でした。\n同じ名前のメソッドでも必須パラメータが違ったり指定するデータの構成が違ったりするので、初めて boto3 でデータ書き込みしようと思ったときは戸惑いました。今回書いた 3 パターンについては、その違いがわかったかと思います。\n途中でも書いていますが、 put_item() に指定できるオプションパラメータについては またあらためて調べて試してみます 下記の記事で書いたので、よかったら参考にしてみてください。\n",
    "permalink": "https://michimani.net/post/aws-put-item-to-dynamodb-by-boto3/",
    "title": "boto3 の put_item() で DynamoDB にデータを書き込む方法 3 パターン"
  },
  {
    "contents": "先日、このブログが Google Adsense の審査に通過しました。審査通過のために色々とやってみたので、その試行錯誤について書きます。同じように Hugo でブログやサイトを運営している方の参考になれば幸いです。\n目次 非承認となった理由 やったこと 自動生成されたコンテンツ アフィリエイトプログラム 無断で複製されたコンテンツ 誘導ページ まとめ 非承認となった理由 初めて申請を出したのが 2018年12月 で、そこから承認まで約 8 ヶ月かかりました。その間、いろいろ試行錯誤したものの、非承認となった理由は一貫して 価値の低い広告枠（コンテンツが複製されているサイト） というものでした。\nこれがどういった内容かというのは Google Search Console のヘルプ内に書かれていますが、抜粋すると、申請したサイトが下記のようなコンテンツとして評価されたという内容です。\n自動生成されたコンテンツ アフィリエイトプログラム 無断で複製されたコンテンツ 誘導ページ それぞれ、もう少し詳細に内容をみてみます。\n自動生成されたコンテンツ なんらかのプログラムによって自動的に生成されたコンテンツで、検索ランキングを操作することを目的としているようなコンテンツのことです。Google では、そういったユーザの役に立たないコンテンツに対しては評価が厳しいようです。\nアフィリエイトプログラム 既に他のアフィリエイトプログラムに参加しているサイトの場合は、コンテンツに貼られているバナーなどが実際の文章と関係がなかったり、その商品のレビューなどを他のサイトから引用してきただけで独自の文章がなかったりするコンテンツのことです。こういった内容だと、他のレビューサイトと同じような内容になってしまい、最終的にはユーザの利便性を妨げるとされています。\n無断で複製されたコンテンツ コンテンツに独自性がなく、他の評判の良いサイトなどからコンテンツを流用したようなコンテンツのことです。単純なコピーだけでなく、コンテンツ内に使用されている用語を類似語に変換したようなコンテンツも、無断で複製されたものとして判断されます。\n誘導ページ 最終的にアクセスされるコンテンツではなく、それらへの誘導を目的としたページのことです。例えば、カテゴリ別・タグ別のコンテンツ一覧ページなどがこれにあたります。\nより詳細な内容についいては、下記リンクから 価値のないコンテンツ の欄を参照してください。\n[手動による対策] レポート - Search Console ヘルプ やったこと Google からは具体的な改善方法は教えてもらえないので、上記の内容をヒントにして怪しいところを直しては申請、ダメならまた直して申請\u0026hellip;ということを繰り返します。\n上で挙げた 4 つの内容に関してやっていったことを書きます。\n自動生成されたコンテンツ Hugo は markdown で書いた記事から自動的に静的なコンテンツ (html) を生成してくれるツールです。なので、ある意味では自動生成ですが、コンテンツに関しては全て自分で手で書いている内容なので、おそらくこの件には抵触しないだろうという判断をしました。なので、これに関しては特に対策をしていません。\nアフィリエイトプログラム 申請時点で既に Amazon アソシエイト・プログラムに参加しており、商品バナーを貼っているコンテンツもありました。なので、一旦それらを全て削除して申請を行いました。\nただし、結果は変わらず非承認で、理由も 価値のないコンテンツ でした。\n無断で複製されたコンテンツ このブログを始めるまでは Qiita でいくつか記事を書いていましたが、それらの記事を全てこのブログに引っ越してきました。もちろん Qiita のほうでは記事を削除していましたが、もしかしたらそれが無断転載という判断をされたのかもしれないと思い、 Qiita から移行してきた記事を全て削除して申請を行いました。\nただし、結果は変わらず非承認で、理由も 価値のないコンテンツ でした。\nちなみに、 Qiita から Hugo へ記事を移行したいと考えている方は、簡単なツール (というかただのスクリプト) を公開しているので使ってみてください。\nmichimani/qiita-to-hugo: Generate markdown files for Hugo contents from Qiita. 誘導ページ Hugo では、カテゴリ別・タグ別の記事一覧ページも自動で生成してくれますが、それらは最終的なコンテンツではなく誘導ページとなります。で、これが静的サイトジェネレータの良いところであり悪いところでもあるんですが、カテゴリ数やタグ数が多くなると、必然的に誘導ページの数が増えます。また、各カテゴリ・各タグに属する記事数が多いと、それらの誘導ページ内でページングが発生し、さらに誘導ページの数が増えることになります。\nこのブログではカテゴリとタグ両方を使用していました。カテゴリ数は 7 と絞っていましたが、タグ数はかなりの数がありました。 (AWS のサービスごとにつけたりしてたので\u0026hellip;)\nということで、タグでの検索ができなくなるというデメリットはありますが、一旦タグ一覧の出力をやめて、申請を行いました。\n結果的にはこの対応によって審査を通過することができました！\nWordPress やその他の動的サイトではカテゴリ別・タグ別ページがカテゴリ数・タグ数に依存して増えるということは無いので、これは Hugo のというか、静的サイトジェネレータ特有の事象かと思います。\nHugo でタグ一覧を出力しない方法 Hugo では、デフォルトではカテゴリ一覧、タグ一覧は出力されません。\nこのブログでは過去にそれぞれの一覧ページを出力するような対応をしていたので、それをもとに戻した形になります。\nなので、対応はいたって簡単です。\nconfig.toml を下記のように変更します。\n[taxonomies] - tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; これでタグ一覧を出力しないようになるので、最終的なコンテンツでは無い誘導ページの数を減らすことができました。\nまとめ Hugo で作ったサイトで Google Adsense の審査に通過するときにやったことを書きました。\nAdsense の審査は年々厳しくなっているという話もありますが、 Google のポリシーに則ったサイトであればちゃんと審査は通るようです。今回はタグ一覧を出力しないという方法で解決したので、誘導ページの多さが問題だったと思われます。\nもし Hugo やその他の静的サイトジェネレータで作ったサイトで Adsense の審査通過に苦労されている方は、一度試してみると良いかもしれません。\n",
    "permalink": "https://michimani.net/post/pass-google-adsense-review-with-hugo-site/",
    "title": "Hugo で作ったサイトで Google Adsense の審査を通過したときにやったこと"
  },
  {
    "contents": "CloudWatch Logs には AWS の各サービスからあらゆるログが送信されるようになっていますが、SDK や CLI を使えば外部のアプリケーションからもログを送信することができます。今回は AWS CLI を使ってログを送信してみました。\nCloudWatch Logs とは CloudWatch Logs の構成要素 AWS CLI での操作 ログの送信 (準備) ログの送信 ログの確認 ログの検索 ログイベントの形式 まとめ CloudWatch Logs とは Amazon CloudWatch Logs を使用して、Amazon Elastic Compute Cloud (Amazon EC2) インスタンス、AWS CloudTrail、Route 53、およびその他のソースのログファイルの監視、保存、アクセスができます。\nAmazon CloudWatch Logs とは - Amazon CloudWatch Logs AWS のリソースに限らず、アプリケーションのログも溜めておくことができます。サーバレスアーキテクチャだったりスケーリングを意識したアーキテクチャだと、ログの置き場所に困りますが、そういう構成ではとりあえず CloudWatch Logs に投げるみたいな構成になっていると思います。\nCloudWatch Logs の構成要素 CloudWatch Logs では ロググループ と ログストリーム という要素があるので、それぞれについてざっと確認します。\nロググループ ロググループは、保持、監視、アクセス制御について同じ設定を共有するログストリームのグループです。ロググループを定義して、各グループに入れるストリームを指定することができます。1 つのロググループに属することができるログストリームの数に制限はありません。\nつまり、アプリケーション単位や画面単位といったくくりでログをグループ分けしている要素です。例えば Lambda の実行ログに関しては、関数単位でロググループが作成されています。\nログストリーム ログストリームは、同じソースを共有する一連のログイベントです。CloudWatch Logs へのログの各ソースによって、個別のログストリームが構成されます。\nロググループ内でさらにログを分割している要素がログストリームです。同じアプリケーションであっても、別のバージョンや別日付などでストリームを分けることになると思います。Lambda の実行ログを例にすると、関数のバージョン単位、一定時間単位でストリームが分割されています。\nロググループ内には数の制限なくログストリームを作成することができます。一つだけでもいいです。ただし、最低でも日付ごととかで分割してあると、あとでログを閲覧するときに見つけやすいです。\nまた、ストリームを分けたとしても、マネジメントコンソール上ではストリームを横断したログの検索が可能なので、心配ありません。\nログイベント いわゆるログのことです。あとで書きますが、 AWS CLI では put-log-events というサブコマンドを使ってログを送信します。\nAWS CLI での操作 では、実際に AWS CLI で CloudWatch Logs を操作してみます。\nCloudWatch Logs のコマンドは aws logs です。これに続けてサブコマンドを足していく感じです。\nコマンドの詳細については公式のドキュメントを参照してください。\nlogs — AWS CLI 1.16.230 Command Reference ログの送信 (準備) ログの送信には put-log-events サブコマンドを使うと書きましたが、実行時には下記のオプションが必要になります。\n--log-group-name : 対象のロググループ名 --log-stream-name : 対象のログストリーム名 --log-events : 送信するログ つまり、ログの送信するにはロググループとログストリームが既に存在している必要があります。なので、今回はそれらも AWS CLI で作成してみます。\nロググループ、ログストリームの作成 まずは ロググループです。\n$ aws logs create-log-group --log-group-name michimani-test-group 出力は特にありません。本当に作成されたのか確認してみます。\n$ aws logs describe-log-groups --log-group-name-prefix michimani { \u0026#34;logGroups\u0026#34;: [ { \u0026#34;logGroupName\u0026#34;: \u0026#34;michimani-test-group\u0026#34;, \u0026#34;creationTime\u0026#34;: 1567477587999, \u0026#34;metricFilterCount\u0026#34;: 0, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:logs:ap-northeast-1:XXXXXXXXXXXX:log-group:michimani-test-group:*\u0026#34;, \u0026#34;storedBytes\u0026#34;: 0 } ] } もちろんですが、既にグループが存在している場合はエラーが出力されます。\n$ aws logs create-log-group --log-group-name michimani-test-group An error occurred (ResourceAlreadyExistsException) when calling the CreateLogGroup operation: The specified log group already exists 続いてログストリームを作成します。\n$ aws logs create-log-stream \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 これも出力は特に何もないので、作成されているのか確認してみます。\n$ aws logs describe-log-streams \\ --log-group-name michimani-test-group \\ --log-stream-name-prefix test-stream-001 { \u0026#34;logStreams\u0026#34;: [ { \u0026#34;logStreamName\u0026#34;: \u0026#34;test-stream-001\u0026#34;, \u0026#34;creationTime\u0026#34;: 1567478248995, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:logs:ap-northeast-1:XXXXXXXXXXXX:log-group:michimani-test-group:log-stream:test-stream-001\u0026#34;, \u0026#34;storedBytes\u0026#34;: 0 } ] } これでログを入れる箱ができたので、ログを送信していきます。\nログの送信 ログの送信 (準備) で書いた通り、ログの送信にはロググループとログストリーム、そしてログのイベントデータを指定する必要があります。 イベントデータは --log-events オプションで指定しますが、方法が Shorthand Syntax と JSON Syntax の 2 通りあるのでそれぞれ見ていきます。\n例として、Hello CloudWatch Logs というログを送信するとします。\nShorthand Syntax ワンラインでタイムスタンプとメッセージを指定する方法です。\n$ aws logs put-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --log-events timestamp=1567478448995,message=\u0026#34;Hello CloudWatch Logs\u0026#34; JSON Syntax JSON 形式で指定する方法です。この方法だと、複数のログイベントを一度に送信することができます。\n[ { \u0026#34;timestamp\u0026#34;: 1567478448995, \u0026#34;message\u0026#34; : \u0026#34;Hello CloudWatch Logs\u0026#34; } ] この方法の場合は、 JSON をファイルとして保存しておいて、そのファイルを --log-events で指定する形になります。\n$ aws logs put-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --log-events file://events.json どちらの方式でも timestamp と message が必要になります。また、 timestamp は 13 桁 である必要があります。 では実際に JSON Syntax の方式でログを送信してみます。\n送信するイベントデータとして、下記のような JSON を準備します。\n[ { \u0026#34;timestamp\u0026#34;: 1567478448995, \u0026#34;message\u0026#34; : \u0026#34;Good morning CloudWatch Logs\u0026#34; }, { \u0026#34;timestamp\u0026#34;: 1567478548995, \u0026#34;message\u0026#34; : \u0026#34;Good afternoon CloudWatch Logs\u0026#34; }, { \u0026#34;timestamp\u0026#34;: 1567478648995, \u0026#34;message\u0026#34; : \u0026#34;God night CloudWatch Logs\u0026#34; } ] aws logs put-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --log-events file://events.json { \u0026#34;nextSequenceToken\u0026#34;: \u0026#34;49596402802751870934231393163316499531521111741320675602\u0026#34; } 結果として、 nextSequenceToken という値が出力されました。この値は、 引き続き同じログストリームにログを送信する際に必要 になります。仮に同じコマンドをもう一度実行してみると\n$ aws logs put-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --log-events file://events.json An error occurred (DataAlreadyAcceptedException) when calling the PutLogEvents operation: The given batch of log events has already been accepted. The next batch can be sent with sequenceToken: 49596402802751870934231393163316499531521111741320675602 エラーとともに、先ほどと同じ sequenceToken の値が記されています。ということで、 sequenceToken を指定して送信してみます。\naws logs put-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --log-events file://events.json \\ --sequence-token 49596402802751870934231393163316499531521111741320675602 { \u0026#34;nextSequenceToken\u0026#34;: \u0026#34;49596402802751870934231393650155762793610763307610816786\u0026#34; } 別の sequenceToken が返ってきました。つまり、同一のログストリームにログイベントを送信する場合は、 2 回目以降はその一つ前に送信した際に返ってきた sequenceToken を --sequence-token オプションで指定する必要があります。\nログの確認 では、上で送信したログを確認してみます。\n$ aws logs get-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ { \u0026#34;events\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1567478448995, \u0026#34;message\u0026#34;: \u0026#34;Good morning CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567483678013 }, { \u0026#34;timestamp\u0026#34;: 1567478448995, \u0026#34;message\u0026#34;: \u0026#34;Good morning CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567484033671 }, { \u0026#34;timestamp\u0026#34;: 1567478548995, \u0026#34;message\u0026#34;: \u0026#34;Good afternoon CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567483678013 }, { \u0026#34;timestamp\u0026#34;: 1567478548995, \u0026#34;message\u0026#34;: \u0026#34;Good afternoon CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567484033671 }, { \u0026#34;timestamp\u0026#34;: 1567478648995, \u0026#34;message\u0026#34;: \u0026#34;God night CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567483678013 }, { \u0026#34;timestamp\u0026#34;: 1567478648995, \u0026#34;message\u0026#34;: \u0026#34;God night CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567484033671 } ], \u0026#34;nextForwardToken\u0026#34;: \u0026#34;f/34955941955374514222924862267686763373673893786318209026\u0026#34;, \u0026#34;nextBackwardToken\u0026#34;: \u0026#34;b/34955937495225474516800233530579142916825806445936443392\u0026#34; } 同じ JSON を 2 回送信したので、それぞれ 2 つずつ同じログイベントが取得できました。\n一緒に帰ってきている nextForwardToken と nextBackwardToken は何でしょうか。\nAWS CLI で取得できるログイベントは、データサイズは 1 MB または イベント数が 10,000 件 が上限となっています。なので、それ以降 (Forward) もしくは それ以前 (Backward) のイベントを取得する際には --next-token で値を指定します。\nまた、取得するログイベント数の上限は --limit オプションでも指定できます。 --limit と --next-token の挙動を試してみます。\n$ aws logs get-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --limit 2 { \u0026#34;events\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1567478648995, \u0026#34;message\u0026#34;: \u0026#34;God night CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567483678013 }, { \u0026#34;timestamp\u0026#34;: 1567478648995, \u0026#34;message\u0026#34;: \u0026#34;God night CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567484033671 } ], \u0026#34;nextForwardToken\u0026#34;: \u0026#34;f/34955941955374514222924862267686763373673893786318209026\u0026#34;, \u0026#34;nextBackwardToken\u0026#34;: \u0026#34;b/34955941955374514222924861837722797446498107642019643394\u0026#34; } 直近の 2 件が取得できました。それ以前の 2 件を取得するために --next-token に nextBackwardToken の値を指定してみます。\n$ aws logs get-log-events \\ --log-group-name michimani-test-group \\ --log-stream-name test-stream-001 \\ --limit 2 \\ --next-token b/34955941955374514222924861837722797446498107642019643394 { \u0026#34;events\u0026#34;: [ { \u0026#34;timestamp\u0026#34;: 1567478548995, \u0026#34;message\u0026#34;: \u0026#34;Good afternoon CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567483678013 }, { \u0026#34;timestamp\u0026#34;: 1567478548995, \u0026#34;message\u0026#34;: \u0026#34;Good afternoon CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567484033671 } ], \u0026#34;nextForwardToken\u0026#34;: \u0026#34;f/34955939725299994369862548114114936108837743188276609025\u0026#34;, \u0026#34;nextBackwardToken\u0026#34;: \u0026#34;b/34955939725299994369862547684150970181661957043978043393\u0026#34; } 次の 2 件が取得できました。\nこんな感じで、前後のイベントを取得できます。\nログの検索 filter-log-events サブコマンドで、任意の文字列でログイベントを検索することができます。\nfilter-log-events で必須のオプションは --log-group-name のみで、 --log-stream-name は任意です。つまり、冒頭にも書きましたがログストリームを横断してロググループ全体で検索することが可能です。これは AWS CLI だけでなく、マネジメントコンソール上でも可能です。\nでは実際に試してみます。\n$ aws logs filter-log-events \\ --log-group-name michimani-test-group \\ --filter-pattern \u0026#34;Good morning\u0026#34; { \u0026#34;events\u0026#34;: [ { \u0026#34;logStreamName\u0026#34;: \u0026#34;test-stream-001\u0026#34;, \u0026#34;timestamp\u0026#34;: 1567478448995, \u0026#34;message\u0026#34;: \u0026#34;Good morning CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567483678013, \u0026#34;eventId\u0026#34;: \u0026#34;34955937495225474516800233530579142916825806445936443392\u0026#34; }, { \u0026#34;logStreamName\u0026#34;: \u0026#34;test-stream-001\u0026#34;, \u0026#34;timestamp\u0026#34;: 1567478448995, \u0026#34;message\u0026#34;: \u0026#34;Good morning CloudWatch Logs\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567484033671, \u0026#34;eventId\u0026#34;: \u0026#34;34955937495225474516800233960543108844001592590235009024\u0026#34; } ], \u0026#34;searchedLogStreams\u0026#34;: [ { \u0026#34;logStreamName\u0026#34;: \u0026#34;test-stream-001\u0026#34;, \u0026#34;searchedCompletely\u0026#34;: true } ] } ログイベントの形式 上の例で送ったログを、マネジメントコンソールで確認してみます。\nまあ、普通です。\nログの内容がシンプルなのでこれでもいいかもしれませんが、実際のログにはいろんな情報が含まれています。なので、それらの情報をマネジメントコンソール上で見やすくなるような形でログを送信してみます。\n方法としては、ログイベントの message を JSON 文字列にする です。具体的には下記のような JSON を準備して、送信します。\n[ { \u0026#34;timestamp\u0026#34;: 1567478748995, \u0026#34;message\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Ken\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;Good morning\\\u0026#34;}\u0026#34; }, { \u0026#34;timestamp\u0026#34;: 1567478848995, \u0026#34;message\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Kumi\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;Good afternoon\\\u0026#34;}\u0026#34; } ] マネジメントコンソールで確認してみると、下のキャプチャのようにログイベントを展開したときに JSON が整形されて表示されます。\nこの JSON 形式にしておくと視認性が良くなるというメリットもありますが、ログの検索時に JSON のキーと値を指定して検索することもできます。例えば name が Ken のログであれば、 {$.name = \u0026quot;Ken\u0026quot;} というフィルタで検索できます。\nもちろんこれは AWS CLI で操作するときにも使えます。\n$ aws logs filter-log-events \\ --log-group-name michimani-test-group \\ --filter-pattern \u0026#39;{$.name = \u0026#34;Ken\u0026#34;}\u0026#39; { \u0026#34;events\u0026#34;: [ { \u0026#34;logStreamName\u0026#34;: \u0026#34;test-stream-001\u0026#34;, \u0026#34;timestamp\u0026#34;: 1567478748995, \u0026#34;message\u0026#34;: \u0026#34;{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;Ken\\\u0026#34;,\\\u0026#34;message\\\u0026#34;:\\\u0026#34;Good morning\\\u0026#34;}\u0026#34;, \u0026#34;ingestionTime\u0026#34;: 1567486685224, \u0026#34;eventId\u0026#34;: \u0026#34;34955944185449034075987179626789594095612148516425498624\u0026#34; } ], \u0026#34;searchedLogStreams\u0026#34;: [ { \u0026#34;logStreamName\u0026#34;: \u0026#34;test-stream-001\u0026#34;, \u0026#34;searchedCompletely\u0026#34;: true } ] } ということなので、送信した後のログイベントのその後の扱い方を考えると、 JSON 形式で送信しておいた方が便利なような気がします。\nまとめ AWS CLI での CloudWatch Logs の操作してみた話でした。\n実際にアプリケーションからログを送信する際には、適度な粒度でログストリームを作成したりする必要があります。今回は CLI で試しましたが、各言語の SDK で実装する場合にも、基本的にはこんな感じの操作になるかと思います。\nやっぱり CLI で操作する楽しさって何か特別なものがありますよね。\n",
    "permalink": "https://michimani.net/post/use-cloudwatch-via-aws-cli/",
    "title": "AWS CLI で CloudWatch Logs にログを送信する"
  },
  {
    "contents": "Visual Studio Code で markdown を書く際に、いい感じに目次を作成してくれるのが Markdown TOC という拡張機能です。その Markdown TOC が上手く動作しなかったので、その対処方法について書きます。\nMarkdown TOC 冒頭にも書きましたが、 Visual Studio Code で markdown を書くときに目次を自動生成してくれる拡張機能です。\n最近ではブログを markdown で書くことも多いと思います。このブログも Hugo を使っていますが、ブログ記事は markdown で書いています。普通の markdown 文書であれば目次は不要かもしれませんが、ブログとなると目次をつけたくもなります。\nそんな時に便利なのが、 Markdown TOC というわけです。\nmarkdown 内にある見出し (#) をもとに、任意の場所に目次のコードを挿入してくれます。オプションで、挿入された目次から各見出しへのリンクも付与できます。\nちなみに Visual Studio Code の拡張機能検索で 「Markdown TOC」を検索すると複数でてきますが、今回触れているのは AlanWalk さんの Markdown TOC です。\nAlanWalk/markdown-toc: MarkdownTOC(Table Of Contents) Plugin for Visual Studio Code. 起こったこと こんな markdown があったとして、これに目次を挿入します。\nMarkdown TOC は、目次を挿入したい位置にカーソルを移動させて、そこで ^ + M, T を押すと目次のコードが挿入されます。上の markdown であれば次のようなコードが挿入されます。\n\u0026lt;!-- TOC --\u0026gt; - [Markdown TOC](#markdown-toc) - [起こったこと](#起こったこと) - [対処方法](#対処方法) - [まとめ](#まとめ) \u0026lt;!-- /TOC --\u0026gt; しかし、実際にコマンドを実行したところ、次のようになりました。\n\u0026lt;!-- TOC --\u0026gt;autoauto- [header 1](#header-1)auto - [header 1 - 1](#header-1---1)auto - [header 1 - 2](#header-1---2)auto- [header 2](#header-2)auto - [header 2 - 1](#header-2---1)auto - [header 2 - 1 - 1](#header-2---1---1)autoauto\u0026lt;!-- /TOC --\u0026gt; # は認識されているようですが、挿入されるコードが残念なことになってます。\n対処方法 AlanWalk/Markdown-TOC の issue に解決方法がありました。\n\u0026amp;ldquo;auto\u0026amp;rdquo; inserted instead of line breaks in TOC · Issue #65 · AlanWalk/markdown-toc どうやら Visual Studio Code の設定でファイルの EOL が auto になっていると発生するようです。\n確認したところ、確かに auto になっていました。\nこれを \\n に変更します。\nそしてもう一度実行してみると、正しく目次コードが挿入されました。\nまとめ Visual Studio Code の拡張機能 Markdown TOC が上手く動作しなかったときの対処方法についての話でした。\nこの Markdown TOC は、先に目次コードを挿入しておけば、あとから追加した見出しの分も自動でコードを追記してくれます。また、目次コードを作成する見出しの深さや、リストにする時にマーカーにするのか連番にするのかも設定できます。\nHugo もしくはその他の静的サイトジェネレータでブログを書いている方はほとんど markdown で記事を書いていると思うので、もしエディタに Visual Studio Code を使っているのであれば Markdown TOC はおすすめです。\n",
    "permalink": "https://michimani.net/post/vscode-markdown-toc-problem/",
    "title": "Visual Studio Code の拡張機能「Markdown TOC」が上手く動作しなかった時の対処方法"
  },
  {
    "contents": "AWS Loft Tokyo で開催された 知っ得ハンズオン はじめてのレコメンデーション - Amazon Personalize に参加してきたので、そのレポート・メモです。\n過去に参加した知っ得ハンズオンレポはこちら。\n目次 スケジュール 座学セッション ハンズオン まとめ おまけ スケジュール 18:00 - 18:30 受付 18:30 - 19:00 座学セッション 19:00 - 20:30 ハンズオン (各自黙々とやる感じ) 20:30 - 21:00 質問など 座学セッション 最初は座学セッションということで、今回やるハンズオンの説明と、\n以下、座学セッションのメモです。\n機械学習 と AI の言葉の違い 機械学習 アルゴリズムの育成 投資フェーズ データサイエンティスト SageMaker AI アルゴリズムをもとにビジネスロジックを作成 回収フェーズ フロントエンドエンジニア Recognition, Polly など Amazon Personalize 個人化推薦 (\u0026lt;\u0026gt; 非個人化推薦) 協調フィルタリングで実装 頑張れは Excel, SQL でも実装できる データが増えると再計算に時間がかかる 情報が少ないとレコメンドの精度が低い (コールドスタート) Personalize は Deep Learning ベース (Amazon.com と同様) 属性情報、インベントリ、アクティビティストリーム という 3 つの情報からパーソナライズ 商品状態が頻繁に変わる場合は アクティビティストリーム をオンにする これらを元に学習させると、レコメンド用の API が完成する inreractions, items, users 属性のうち、 interaction の CSV だけ学習可能 リアルタイムデータインジェクション 再学習の頻度については公開されていない インジェクションデータが 1,000 件を超えたら再学習される データが増えない場合はいつまでたっても再学習されない (手動でやる) コンソール (English only) または API でモデルを学習 データの投入 パース 学習 推論エンドポイントの生成 (キャンペーン と呼ぶ。一般的な機械学習用語では、 推論モデルのデプロイ ) 内蔵されているアルゴリズムは大きく 4 つ Search Personalization Related Items User Personalization HRNN 類似サービス Amazon Forecast : 精度の高い時系列予報サービス (プレビュー) ハンズオン ユーザー 610 人による 9,700 本の映画視聴履歴とその評価のデータ (約 10 万件) をもとに、オススメの映画をレコメンドしてくれるサービスを作るというハンズオンでした。\nデータは CSV 形式で配布されたので、それを利用しました。\n大まかな流れとしては次のような感じです。\nS3 に学習用データ (配布された CSV) をアップロード Amazon Personalize にて S3 のファイルを読み込み (約 20 分) 読み込んだデータをもとに学習 (約 50 分) 学習結果を元に キャンペーンを作成 (推論モデルのデプロイ) 作成したキャンペーンを使って推論環境を構築 (約 15 分) 推論環境からレコメンド結果を取得 こんなに簡単に推論環境ができてしまうのか？っていうくらいシンプルな手順でした。\n終始マネジメントコンソール上での操作でしたが、フローとしては下図のダッシュボードにある 4 フローです。\n構築した推論環境はマネジメントコンソール上でもテストできます。\nもちろん CLI からも実行可能です。\n$ aws personalize-runtime get-recommendations \\ --campaign-arn arn:aws:personalize:us-east-1:12345678XXXX:campaign/20190819-handson-campaign \\ --user-id 123 \\ --region us-east-1 { \u0026#34;itemList\u0026#34;: [ { \u0026#34;itemId\u0026#34;: \u0026#34;63992\u0026#34; }, { \u0026#34;itemId\u0026#34;: \u0026#34;166528\u0026#34; }, ... 略 ... { \u0026#34;itemId\u0026#34;: \u0026#34;109374\u0026#34; }, { \u0026#34;itemId\u0026#34;: \u0026#34;47610\u0026#34; } ] } まとめ AWS のレコメンドサービス Amazon Personalize のハンズオンに参加してきた話でした。\nAmazon Personalize に関しては普段の業務では使う機会がないので、このようなハンズオンを開催していただけるのはとても嬉しい限りです。\n今回は学習用のデータがあらかじめ用意されていたので、それを元にして簡単に推論環境を構築することができ、さらにレコメンド結果も簡単に取得することができました。座学セッションの中でもあったように、 機械学習 と AI という言葉の違いは自分の中でもしっかりと意識していきたいと思いました。Amazon Personalize に関しては AI の範囲にあるサービスなので、学習の部分を意識せずにアプリケーション内でレコメンドが簡単に使えるというサービスです。一方で学習用のデータを準備するのが大変かなという印象でした。\n業務では使うことがなさそうですが、面白いサービスだなと思いました。\nおまけ 今回はちょっと待ち時間が多かったのが少し残念でした。ハンズオンの流れでも書きましたが、 2 、 3 、 5 で待ち時間が発生します。\nただ、その間は AWS の他のレコメンドサービス Amazon Rekognition で遊んだりしていました。Amazon Rekognition はマネジメントコンソール上で画像ファイルをアップロードするだけで機能を試すことができるので、気になる方は使ってみてください。\n実際に得られるレスポンス (JSON) も確認できます。\n{ \u0026#34;Labels\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;Meal\u0026#34;, \u0026#34;Confidence\u0026#34;: 94.69331359863281, \u0026#34;Instances\u0026#34;: [], \u0026#34;Parents\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;Food\u0026#34; } ] }, { \u0026#34;Name\u0026#34;: \u0026#34;Dish\u0026#34;, \u0026#34;Confidence\u0026#34;: 94.69331359863281, \u0026#34;Instances\u0026#34;: [], \u0026#34;Parents\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;Meal\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;Food\u0026#34; } ] }, { \u0026#34;Name\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;Confidence\u0026#34;: 94.69331359863281, \u0026#34;Instances\u0026#34;: [], \u0026#34;Parents\u0026#34;: [] }, { \u0026#34;Name\u0026#34;: \u0026#34;Curry\u0026#34;, \u0026#34;Confidence\u0026#34;: 66.77191162109375, \u0026#34;Instances\u0026#34;: [], \u0026#34;Parents\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;Food\u0026#34; } ] }, { \u0026#34;Name\u0026#34;: \u0026#34;Gravy\u0026#34;, \u0026#34;Confidence\u0026#34;: 57.303733825683594, \u0026#34;Instances\u0026#34;: [], \u0026#34;Parents\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;Food\u0026#34; } ] }, { \u0026#34;Name\u0026#34;: \u0026#34;Bowl\u0026#34;, \u0026#34;Confidence\u0026#34;: 56.948299407958984, \u0026#34;Instances\u0026#34;: [], \u0026#34;Parents\u0026#34;: [] } ], \u0026#34;LabelModelVersion\u0026#34;: \u0026#34;2.0\u0026#34; } ",
    "permalink": "https://michimani.net/post/aws-handson-hajimete-personalize/",
    "title": "[レポート] 知っ得ハンズオン はじめてのレコメンデーション - Amazon Personalize @ AWS Loft Tokyo に行ってきました"
  },
  {
    "contents": "GA になってから間もない AWS CDK ですが、既にそのバージョンは 1.3.0 になっています。そんな AWS CDK を使って、特定のタグを持つ EC2 インスタンスの自動起動・停止をする環境を構築してみました。\n概要 CDK アプリケーションの作成 パッケージのインストール Lambda CloudWatch Events CloudFormation のテンプレートを出力してみる デプロイ まとめ 今回作ったもの 概要 特定のタグ名と値を持つ EC2 インスタンスを自動で停止・起動させる環境を構築します。\n自動停止・起動の処理は Lambda で実装し、それを CloudWatch Events のスケジュールで定期的に実行します。この方法自体は以前にも書いた通りですが、今回は Lambda 関数の作成やスケジュールの設定などの環境構築を AWS CDK でやってしまおう という話です。\nCDK アプリケーションの作成 それでは早速 AWS CDK アプリケーションを作成していきます。\nAWS CDK については公式のドキュメント、およびリファレンスをご参照ください。\nWhat Is the AWS CDK? - AWS Cloud Development Kit (AWS CDK) API Reference · AWS CDK CDK はこの記事作成時点で最新の 1.3.0 を使用します。\n$ cdk --version 1.3.0 (build bba9914) TypeScript で実装していきます。\n$ mkdir auto-start-stop-ec2 $ cd auto-start-stop-ec2 $ cdk init app --language=typescript Applying project template app for typescript Initializing a new git repository... Executing npm install... ... # Useful commands * `npm run build` compile typescript to js * `npm run watch` watch for changes and compile * `cdk deploy` deploy this stack to your default AWS account/region * `cdk diff` compare deployed stack with current state * `cdk synth` emits the synthesized CloudFormation template 次のようなディレクトリ構成ができあがります。\n. ├── README.md ├── bin │ └── auto-start-stop-ec2.ts ├── cdk.json ├── lib │ └── auto-start-stop-ec2-stack.ts ├── node_modules │ ├── @aws-cdk ... ├── package-lock.json ├── package.json └── tsconfig.json パッケージのインストール まずは必要なパッケージをインストールします。\nCDK では、コアとなる @aws-cdk/core の他に、必要となるサービスごとのパッケージを使用します。\n$ npm install @aws-cdk/aws-events @aws-cdk/aws-events-targets @aws-cdk/aws-lambda @aws-cdk/aws-iam 準備が整ったので、 lib/auto-start-stop-ec2-stack.ts を編集して必要なリソースを作成するコードを書いていきます。\nLambda まずは Lambda 関数です。\n今回は予め Python で書かれた Lambda 関数用の auto-start-stop-ec2.py ファイルを、新たに作成した lambda/ ディレクトリに用意しておき、それを使うように実装します。\nまた、 Lambda では EC2 インスタンスの起動と停止を実行するので、必要な IAM ポリシーも付与します。\nconst lambdaFn = new lambda.Function(this, \u0026#39;singleton\u0026#39;, { code: new lambda.InlineCode(fs.readFileSync(\u0026#39;lambda/auto-start-stop-ec2.py\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})), handler: \u0026#39;index.main\u0026#39;, timeout: cdk.Duration.seconds(300), runtime: lambda.Runtime.PYTHON_3_7, }); lambdaFn.addToRolePolicy(new iam.PolicyStatement({ actions: [ \u0026#39;ec2:DescribeInstances\u0026#39;, \u0026#39;ec2:StartInstances\u0026#39;, \u0026#39;ec2:StopInstances\u0026#39; ], resources: [\u0026#39;*\u0026#39;] })); ちなみに外部のファイルを読み込む際のパスは、 CDK アプリケーションのルートディレクトリからの相対パスになります。この lib/*.ts からの相対パスではありません。\nCloudWatch Events 続いて、 Lammda 関数を定期実行するための CloudWatch Events を実装します。\nconst stackConfig = JSON.parse(fs.readFileSync(\u0026#39;stack.config.json\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})); // STOP EC2 instances rule const stopRule = new events.Rule(this, \u0026#39;StopRule\u0026#39;, { schedule: events.Schedule.expression(`cron(${stackConfig.events.cron.stop})`) }); stopRule.addTarget(new targets.LambdaFunction(lambdaFn, { event: events.RuleTargetInput.fromObject({Region: stackConfig.targets.ec2region, Action: \u0026#39;stop\u0026#39;}) })); // START EC2 instances rule const startRule = new events.Rule(this, \u0026#39;StartRule\u0026#39;, { schedule: events.Schedule.expression(`cron(${stackConfig.events.cron.start})`) }); startRule.addTarget(new targets.LambdaFunction(lambdaFn, { event: events.RuleTargetInput.fromObject({Region: stackConfig.targets.ec2region, Action: \u0026#39;start\u0026#39;}) })); CloudWatch Events の定義としては、まず events.Rule オブジェクトを作成し、 addTarget() メソッドでターゲット (今回の場合は Lambda 関数) を追加するという形になります。\ntargets.LambdaFunction のコンストラクタの第 2 引数は任意のパラメータで、 CloudWatch Events で Lambda 関数を呼ぶ時の引数を指定することができます。\n今回は 起動と停止の時間、対象となる EC2 インスタンスがあるリージョンを stack.config.json という JSON ファイル内で定義しておいて、その値を使用するようにしています。\nちなみにここでも指定するファイルパスは、 CDK アプリケーションのルートディレクトリからの相対パスになります。\n{ \u0026#34;events\u0026#34;: { \u0026#34;cron\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;55 23 ? * SUN-THU *\u0026#34;, \u0026#34;stop\u0026#34;: \u0026#34;0 12 ? * * *\u0026#34; } }, \u0026#34;targets\u0026#34;: { \u0026#34;ec2region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34; } } これで CDK アプリケーションの実装は完了です。 auto-start-stop-ec2-stack.ts の全体としては下記のような感じです。\nimport events = require(\u0026#39;@aws-cdk/aws-events\u0026#39;); import iam = require(\u0026#39;@aws-cdk/aws-iam\u0026#39;); import lambda = require(\u0026#39;@aws-cdk/aws-lambda\u0026#39;); import targets = require(\u0026#39;@aws-cdk/aws-events-targets\u0026#39;); import cdk = require(\u0026#39;@aws-cdk/core\u0026#39;); import fs = require(\u0026#39;fs\u0026#39;); export class AutoStartStopEc2Stack extends cdk.Stack { constructor (app: cdk.App, id: string) { super(app, id); const stackConfig = JSON.parse(fs.readFileSync(\u0026#39;stack.config.json\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})); const lambdaFn = new lambda.Function(this, \u0026#39;singleton\u0026#39;, { code: new lambda.InlineCode(fs.readFileSync(\u0026#39;lambda/auto-start-stop-ec2.py\u0026#39;, {encoding: \u0026#39;utf-8\u0026#39;})), handler: \u0026#39;index.main\u0026#39;, timeout: cdk.Duration.seconds(300), runtime: lambda.Runtime.PYTHON_3_7, }); lambdaFn.addToRolePolicy(new iam.PolicyStatement({ actions: [ \u0026#39;ec2:DescribeInstances\u0026#39;, \u0026#39;ec2:StartInstances\u0026#39;, \u0026#39;ec2:StopInstances\u0026#39; ], resources: [\u0026#39;*\u0026#39;] })); // STOP EC2 instances rule const stopRule = new events.Rule(this, \u0026#39;StopRule\u0026#39;, { schedule: events.Schedule.expression(`cron(${stackConfig.events.cron.stop})`) }); stopRule.addTarget(new targets.LambdaFunction(lambdaFn, { event: events.RuleTargetInput.fromObject({Region: stackConfig.targets.ec2region, Action: \u0026#39;stop\u0026#39;}) })); // START EC2 instances rule const startRule = new events.Rule(this, \u0026#39;StartRule\u0026#39;, { schedule: events.Schedule.expression(`cron(${stackConfig.events.cron.start})`) }); startRule.addTarget(new targets.LambdaFunction(lambdaFn, { event: events.RuleTargetInput.fromObject({Region: stackConfig.targets.ec2region, Action: \u0026#39;start\u0026#39;}) })); } } あとは npm run build を実行して完了です。\nCloudFormation のテンプレートを出力してみる CDK アプリケーションとして実装した auto-start-stop-ec2-stack.ts は、最終行の空行も含めてちょうど 50 行でした。\nこれを CloudFormation の template (yaml) として出力してみます。\n$ cdk synth Resources: singletonServiceRole9C9ECF4A: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Statement: - Action: sts:AssumeRole Effect: Allow Principal: Service: Fn::Join: - \u0026#34;\u0026#34; - - lambda. - Ref: AWS::URLSuffix Version: \u0026#34;2012-10-17\u0026#34; ManagedPolicyArns: - Fn::Join: - \u0026#34;\u0026#34; - - \u0026#34;arn:\u0026#34; - Ref: AWS::Partition - :iam::aws:policy/service-role/AWSLambdaBasicExecutionRole Metadata: aws:cdk:path: AutoStartStopEc2Stack/singleton/ServiceRole/Resource singletonServiceRoleDefaultPolicyFDD8CA90: Type: AWS::IAM::Policy Properties: PolicyDocument: Statement: - Action: - ec2:DescribeInstances - ec2:StartInstances - ec2:StopInstances Effect: Allow Resource: \u0026#34;*\u0026#34; Version: \u0026#34;2012-10-17\u0026#34; PolicyName: singletonServiceRoleDefaultPolicyFDD8CA90 Roles: - Ref: singletonServiceRole9C9ECF4A Metadata: aws:cdk:path: AutoStartStopEc2Stack/singleton/ServiceRole/DefaultPolicy/Resource singleton69FEA30F: Type: AWS::Lambda::Function Properties: Code: ZipFile: \u0026gt; import boto3 import json import traceback def main(event, context): try: region = event[\u0026#39;Region\u0026#39;] action = event[\u0026#39;Action\u0026#39;] client = boto3.client(\u0026#39;ec2\u0026#39;, region) responce = client.describe_instances( Filters=[{\u0026#39;Name\u0026#39;: \u0026#39;tag:AutoStartStop\u0026#39;, \u0026#34;Values\u0026#34;: [\u0026#39;TRUE\u0026#39;]}]) target_instans_ids = [] for reservation in responce[\u0026#39;Reservations\u0026#39;]: for instance in reservation[\u0026#39;Instances\u0026#39;]: tag_name = \u0026#39;\u0026#39; for tag in instance[\u0026#39;Tags\u0026#39;]: if tag[\u0026#39;Key\u0026#39;] == \u0026#39;Name\u0026#39;: tag_name = tag[\u0026#39;Value\u0026#39;] break target_instans_ids.append(instance[\u0026#39;InstanceId\u0026#39;]) print(target_instans_ids) if not target_instans_ids: print(\u0026#39;There are no instances subject to automatic {}.\u0026#39;.format(action)) else: if action == \u0026#39;start\u0026#39;: client.start_instances(InstanceIds=target_instans_ids) print(\u0026#39;started instances.\u0026#39;) elif action == \u0026#39;stop\u0026#39;: client.stop_instances(InstanceIds=target_instans_ids) print(\u0026#39;stopped instances.\u0026#39;) else: print(\u0026#39;Invalid action.\u0026#39;) return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;message\u0026#34;: \u0026#39;Finished automatic {action} EC2 instances process. [Region: {region}, Action: {action}]\u0026#39;.format(region=event[\u0026#39;Region\u0026#39;], action=event[\u0026#39;Action\u0026#39;]) } except: print(traceback.format_exc()) return { \u0026#34;statusCode\u0026#34;: 500, \u0026#34;message\u0026#34;: \u0026#39;An error occured at automatic start / stop EC2 instances process.\u0026#39; } Handler: index.main Role: Fn::GetAtt: - singletonServiceRole9C9ECF4A - Arn Runtime: python3.7 Timeout: 300 DependsOn: - singletonServiceRoleDefaultPolicyFDD8CA90 - singletonServiceRole9C9ECF4A Metadata: aws:cdk:path: AutoStartStopEc2Stack/singleton/Resource singletonAllowEventRuleAutoStartStopEc2StackStopRule6999EBDE48367C0B: Type: AWS::Lambda::Permission Properties: Action: lambda:InvokeFunction FunctionName: Fn::GetAtt: - singleton69FEA30F - Arn Principal: events.amazonaws.com SourceArn: Fn::GetAtt: - StopRule00306666 - Arn Metadata: aws:cdk:path: AutoStartStopEc2Stack/singleton/AllowEventRuleAutoStartStopEc2StackStopRule6999EBDE singletonAllowEventRuleAutoStartStopEc2StackStartRule682FBE6A53E17D5B: Type: AWS::Lambda::Permission Properties: Action: lambda:InvokeFunction FunctionName: Fn::GetAtt: - singleton69FEA30F - Arn Principal: events.amazonaws.com SourceArn: Fn::GetAtt: - StartRule40F02F2E - Arn Metadata: aws:cdk:path: AutoStartStopEc2Stack/singleton/AllowEventRuleAutoStartStopEc2StackStartRule682FBE6A StopRule00306666: Type: AWS::Events::Rule Properties: ScheduleExpression: cron(0 12 ? * * *) State: ENABLED Targets: - Arn: Fn::GetAtt: - singleton69FEA30F - Arn Id: Target0 Input: \u0026#39;{\u0026#34;Region\u0026#34;:\u0026#34;ap-northeast-1\u0026#34;,\u0026#34;Action\u0026#34;:\u0026#34;stop\u0026#34;}\u0026#39; Metadata: aws:cdk:path: AutoStartStopEc2Stack/StopRule/Resource StartRule40F02F2E: Type: AWS::Events::Rule Properties: ScheduleExpression: cron(55 23 ? * SUN-THU *) State: ENABLED Targets: - Arn: Fn::GetAtt: - singleton69FEA30F - Arn Id: Target0 Input: \u0026#39;{\u0026#34;Region\u0026#34;:\u0026#34;ap-northeast-1\u0026#34;,\u0026#34;Action\u0026#34;:\u0026#34;start\u0026#34;}\u0026#39; Metadata: aws:cdk:path: AutoStartStopEc2Stack/StartRule/Resource CDKMetadata: Type: AWS::CDK::Metadata Properties: Modules: aws-cdk=1.3.0,@aws-cdk/assets=1.3.0,@aws-cdk/aws-applicationautoscaling=1.3.0,@aws-cdk/aws-autoscaling=1.3.0,@aws-cdk/aws-autoscaling-common=1.3.0,@aws-cdk/aws-autoscaling-hooktargets=1.3.0,@aws-cdk/aws-cloudformation=1.3.0,@aws-cdk/aws-cloudwatch=1.3.0,@aws-cdk/aws-ec2=1.3.0,@aws-cdk/aws-ecr=1.3.0,@aws-cdk/aws-ecr-assets=1.3.0,@aws-cdk/aws-ecs=1.3.0,@aws-cdk/aws-elasticloadbalancingv2=1.3.0,@aws-cdk/aws-events=1.3.0,@aws-cdk/aws-events-targets=1.3.0,@aws-cdk/aws-iam=1.3.0,@aws-cdk/aws-kms=1.3.0,@aws-cdk/aws-lambda=1.3.0,@aws-cdk/aws-logs=1.3.0,@aws-cdk/aws-s3=1.3.0,@aws-cdk/aws-s3-assets=1.3.0,@aws-cdk/aws-servicediscovery=1.3.0,@aws-cdk/aws-sns=1.3.0,@aws-cdk/aws-sns-subscriptions=1.3.0,@aws-cdk/aws-sqs=1.3.0,@aws-cdk/aws-ssm=1.3.0,@aws-cdk/core=1.3.0,@aws-cdk/custom-resources=1.3.0,@aws-cdk/cx-api=1.3.0,@aws-cdk/region-info=1.3.0,jsii-runtime=node.js/v10.15.1 CDK のメタデータも含まれていますが、約 170 行の YAML ファイルになりました。\nYAML で CloudFormation のテンプレートをしっかり書いたことはないで、これをゼロから書けと言われると辛いです。今回のようなシンプルな構成であれば良いですが、もっと複雑な構成になるとさらに辛そうです。\n前に参加した CDK Meetup では、 1,000 行を超えるテンプレートを書いた みたいな話もありました。\nデプロイ CDK アプリケーションが完成したので、あとはデプロイするだけです。\nデプロイもいたって簡単で、 cdk deploy コマンドを実行するだけです。\ncdk deploy This deployment will make potentially sensitive changes according to your current security approval level (--require-approval broadening). Please confirm you intend to make the following modifications: \u0026lt;diff が良い感じに出力されます\u0026gt; Do you wish to deploy these changes (y/n)? y AutoStartStopEc2Stack: deploying... AutoStartStopEc2Stack: creating CloudFormation changeset... 0/9 | 15:41:41 | CREATE_IN_PROGRESS | AWS::CDK::Metadata | CDKMetadata 0/9 | 15:41:41 | CREATE_IN_PROGRESS | AWS::IAM::Role | singleton/ServiceRole (singletonServiceRole9C9ECF4A) 0/9 | 15:41:41 | CREATE_IN_PROGRESS | AWS::IAM::Role | singleton/ServiceRole (singletonServiceRole9C9ECF4A) Resource creation Initiated 0/9 | 15:41:43 | CREATE_IN_PROGRESS | AWS::CDK::Metadata | CDKMetadata Resource creation Initiated 1/9 | 15:41:43 | CREATE_COMPLETE | AWS::CDK::Metadata | CDKMetadata ... 9/9 | 15:43:38 | CREATE_COMPLETE | AWS::CloudFormation::Stack | AutoStartStopEc2Stack ✅ AutoStartStopEc2Stack Stack ARN: arn:aws:cloudformation:ap-northeast-1:123456789876:stack/AutoStartStopEc2Stack/51a4b4f0-b8de-11e9-ad2d-0ea27dfbb62e 作成されるリソースの内容が出力され、 Do you wish to deploy these changes (y/n)? で y を入力すればリソースの作成が始まるので、あとは待つだけです。途中でエラーが発生すれば、その旨 出力されます。\nちなみに cdk deploy でデプロイされる先は、 AWS CLI の設定ファイル .aws/config および .aws/credentials に依存しています。なので、 --profile オプションを使えばデフォルト以外のアカウントへのデプロイも可能です。\nまとめ AWS CDK を使って CloudWatch Event + Lambda で EC2 を自動起動・停止させる環境を作ってみた話でした。\n今まで CloudFormation はほとんど触ったことがなく、 YAML でテンプレートを書くという作業にも食わず嫌いでした。そんなところに AWS CDK が現れて、 TypeScript, Python といった普段の開発に使っている言語でインフラコードが書けるというのは、かなり画期的なことだと思いました。\nCDK Meetup のときに AWS の福井さんが仰っていた 「Infrastructure as Code ではなくて Infrastructure is Code だ」 という言葉が非常にしっくりきました。\nCDK 自体に関しては GA になって 1 ヶ月も経っていませんが、既にバージョンが 1.3.0 になっており、引き続き結構なスピードでアップデートされていくのかなーという印象です。\nCDK きっかけでインフラのコード化に足を踏み入れたので、置いて行かれないようにしたいと思います。\n今回作ったもの 今回作った CDK アプリケーションは GitHub に置いていますので、よかったら使ってみてください。書き方おかしいとかあれば指摘も大歓迎です。\nmichimani/auto-start-stop-ec2: This is a CDK application for automatically starting / stopping EC2 instances. ",
    "permalink": "https://michimani.net/post/aws-auto-start-stop-ec2-stack-using-cdk/",
    "title": "AWS CDK を使って CloudWatch Event + Lambda で EC2 を自動起動・停止させる環境を作ってみた"
  },
  {
    "contents": "先日 AWS の Beta サービスを使用する時の注意点について書きましたが、その内容について念のため AWS サポートに問い合わせてみました。その回答が返ってきたのでその回答が返ってきたのでその回答内容と、あらためて注意点について振り返ってみます。\n前回までのおさらい AWS の Beta サービスを利用するにあたっては、利用に際して AWS Service Terms に記載されている 1.10 項に同意する必要があります。\nこの 1.10 項には 1.10.1 から 1.10.10 まで条文がありますが、ざっくり掻い摘まむと、\nサービスの内容変更、提供終了は AWS がいつでも実行できる 事前メンテナンスについて通知はされない バグ、瑕疵が含まれる可能性があるため AWS および関連会社は全ての保障をしない Beta サービスを使って得られた情報は開示しない という内容でした。ここまでは前回書いた内容です。\nAWS Service Terms の原文は以下。\nAWS Service Terms 上に挙げた内容のうち、最後に書いた Beta サービスを使って得られた情報は開示しない について、 AWS サポートに問い合わせてみました。\nAWS サポートへの問い合わせ 経緯 そもそも問い合わせようと思ったのは、先日某サービスが public beta として利用可能になり、そのサービスを実際に使ってみたというブログを書いたのが発端です。\nこの記事が公開後に想像以上に反応があり、 PV 数もブクマ数も過去最高値を記録しました。(普段が低すぎるのですが)\nそんななかで、 「この記事って AWS Service Terms の 1.10.8 に引っかかりませんか？」 という指摘をいただきました。\nお恥ずかしながら AWS Service Terms の beta サービスに関する記述に関しては把握していなかったので調べてみると、確かに引っかかりそうな内容でした。というか引っかかる可能性しか無いなと思いました。\nこういう条文を読むのは慣れていないので、今後のためにも AWS サポートに問い合わせてみることにしました。\nというのが経緯です。\n問い合わせ方法 AWS のマネジメントコンソールからサポートページを開いて、新たなケースを作成します。\nサポートプランは Basic なので、気長に返答が来るのを待ちます。\n問い合わせ内容としては、こんな感じで書きました。\nちなみに、 サポートケースの適切な作成方法については公式ページに書かれているので、参考にしましょう。\n技術的なお問い合わせに関するガイドライン - AWS サポート | AWS サポートからの回答 問い合わせたのは金曜日の午後だったので、土日を挟んで次の週の火曜日に一旦返答がありました。そこでは 担当部署に確認しておりますので、しばらくお待ちください という内容でした。\nそして、その次の週の月曜日に下記の通り回答がありました。\nやはり、 1.10.8 から読み取った内容に齟齬はなかったようで、 SNS やブログ等への開示は不可 のようです。\n書かれている通りといえばそれまでなんですが、確認できたのでよかったです。\nまた、回答の中にもあるように AWS カスタマーアグリーメント (PDF) の以下の文面も確認しておいた方がよさそうです。\n13.9 秘密保持および公表\nサービス利用者は、AWS 秘密情報を本契約上認められる、提供される本サービス内容の利用に関連してのみ使用することができるものとする。サービス利用者は、本契約期間中およびその終了後 5 年間、いかなる時も、AWS 秘密情報を開示してはならないものとする。サービス利用者は、AWS 秘密情報の開示、流布、または不正使用を防止するために、同様の性質を持つ自己の秘密情報を保持するために採用する手段を最低限含む、すべての合理的な方法をとるものとする。サービス利用者は、本契約またはサービス利用者による提供される本サービス内容の利用に関して、いかなるプレスリリースも、その他の発表や広告もしないものとする。\nつまり、 beta サービス利用中に取得した情報 (= 秘密情報) は、たとえそのサービスが正式リリースされたとしても、 AWS を利用している期間中およびその終了後の 5 年間は開示してはいけないということになります。\nなので、正式リリースされああとに 「beta のときはこうでした」 みたいなのもダメなんですね\u0026hellip;。\n実際にサポートに確認してみてよかったです。\nご対応いただいたサポートの方、ありがとうございました。\nまとめ AWS の Beta サービスを使用する時の条件についてサポートに問い合わせてみた話でした。\nBeta サービスとはいえ新しいサービスが使えるようになるとすぐに使いたくなってしまいますが、 AWS Service Terms に書かれている条件に合意したうえでの利用というのを認識しないとダメだなと思いました。\nなんか当たり前のことを再確認したような内容でしたが、参考になれば幸いです。気になる方はぜひ一度 AWS Service Terms を読んでみてください。\n",
    "permalink": "https://michimani.net/post/aws-service-terms-for-using-beta-services/",
    "title": "AWS の Beta サービスを使うときの条件について AWS サポートに問い合わせてみた"
  },
  {
    "contents": "CodePipeline を使うとビルド、デプロイ、結果の通知までを簡単に構築できますが、作成後 30 日以降は料金が発生してしまいます。なので、節約のために CodeBuild で全部やってしまおうという話です。\n目次 前提 CodePipeline と CodeBuild の料金 CodePipeline CodeBuild やっていたこと/やること CodeBuild の設定変更 失ったもの ※追記あり (2019/11/12) まとめ 前提 Vue.js で書かれた静的 Web サイトの CI/CD を CodePipeline を使って実現している ソースの変更は頻繁には行われないが、最低でも月に 1 回はある GitHub への Push をトリガーにビルド・デプロイ、 Slack への通知ができれば OK あくまでも個人で運用している AWS アカウントの料金節約を目的としています。また、今回やってみる内容では、 CodePipeline で実現できていることを全て再現することはできません。諦めなければいけない挙動もあるので、その点はご理解ください。\nCodePipeline と CodeBuild の料金 タイトルで 節約のために と書いている通り、今回の内容で毎月の料金を節約することができるので、 CodePipeline と CodeBuild についてあらためて確認してみます。\nCodePipeline CodePipeline は、利用するパイプラインの数に対して料金が発生します。\nアクティブなパイプライン 1 つにつき、 1 USD (アクティブとは、30 日以上存在している 且つ その月に少なくとも 1 つのコード変更が発生) 作成後 30 日間は無料 無料利用枠として、アクティブなパイプラインを 1 つ利用可能 CodeBuild CodeBuild は、ビルドにかかった時間 (分) に対して料金が発生します。(1 分未満は切り上げ)\nまたその単価は、利用するコンピューティングタイプによって変わります。\nタイプ メモリ (GB) vCPU Linux (USD/分) Windows (USD/分) build.general1.small 3 2 0.005 該当なし build.general1.medium 7 4 0.010 0.018 build.general1.large 15 8 0.020 0.036 無料利用枠として、 1 ヶ月あたりビルドを 100 分使用できます。\nCodeBuild を使って該当の Vue.js アプリケーションのビルド・デプロイするのにかかる時間は 1 - 2 分程度なので、 build.general1.small を使用する場合は 1 回につき 0.02 USD 程度ということになります。\nCodePipeline を使っている場合だと、プラス 1 USD (1 回きりですが) が発生します。\nやっていたこと/やること まずは CodePipeline を使っていたときの構成についてです。\nイメージとしてはこんな感じです。\nGitHub の master ブランチへの Push をトリガーにパイプラインが開始 CodeBuild でビルド、生成されたソースを S3 へ配置 パイプラインの実行結果を Slack へ通知するための Lambda を実行 これを、次のような構成にします。\nシンプルになりました。\n構成はこれでいいとして、 CodeBuild の設定、 buildspec.yml も変更する必要があります。具体的には下記のポイントです。\n送信元を GitHub に変更 プライマリソースのウェブフックイベントを設定\nmaster ブランチへの Push でビルドが開始されるようにします buildspec.yml 内で Slack への通知を追加 では、設定の変更方法についてみていきます。\nCodeBuild の設定変更 送信元を GitHub に変更 まずは送信元を GitHub に変更します。が、既に CodePipeline に紐づいているビルドプロジェクトを変更する場合、送信元はマネジメントコンソールで変更することができません。なので CLI で変更します。\n一旦、既存の設定を JSON 形式で取得します。\n$ aws codebuild batch-get-projects --names \u0026#34;BuildProjectName\u0026#34; \u0026gt; ./current.json { \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BuildProjectName\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:codebuild:ap-northeast-1:123456789012:project/BuildProjectName\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;CODEPIPELINE\u0026#34;, }, \u0026#34;artifacts\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;CODEPIPELINE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;BuildProjectName\u0026#34;, \u0026#34;packaging\u0026#34;: \u0026#34;NONE\u0026#34; }, \u0026#34;cache\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;NO_CACHE\u0026#34; }, \u0026#34;environment\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;LINUX_CONTAINER\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;aws/codebuild/nodejs:10.14.1\u0026#34;, \u0026#34;computeType\u0026#34;: \u0026#34;BUILD_GENERAL1_SMALL\u0026#34;, \u0026#34;environmentVariables\u0026#34;: [], \u0026#34;privilegedMode\u0026#34;: false }, \u0026#34;serviceRole\u0026#34;: \u0026#34;arn:aws:iam::123456789012:role/service-role/codebuild-BuildProjectName-service-role\u0026#34;, \u0026#34;timeoutInMinutes\u0026#34;: 60, \u0026#34;encryptionKey\u0026#34;: \u0026#34;arn:aws:kms:ap-northeast-1:123456789012:alias/aws/s3\u0026#34;, \u0026#34;tags\u0026#34;: [], \u0026#34;created\u0026#34;: 1557990804.228, \u0026#34;lastModified\u0026#34;: 1562114621.841, \u0026#34;badge\u0026#34;: { \u0026#34;badgeEnabled\u0026#34;: false } } ], \u0026#34;projectsNotFound\u0026#34;: [] } こんな感じの JSON が取得できるので、これを元にして次のような JSON ファイルを新たに作成します。(仮に update.json とします)\n変更点としては、 source と artifacts です。\nenvironment.environmentVariables については、後の Slack 通知で使用する環境変数になります。多分 Slack の Webhook URL は Systems Manager のパラメータストアから、 KMS で暗号化した上で取得したほうがよさそうです。このへんはまだちょっとわかってないので、一旦プレーンテキストで入れてます。\n{ \u0026#34;name\u0026#34;: \u0026#34;BuildProjectName\u0026#34;, \u0026#34;source\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;GITHUB\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;https://github.com/michimani/repo.git\u0026#34; }, \u0026#34;artifacts\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;NO_ARTIFACTS\u0026#34; }, \u0026#34;environment\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;LINUX_CONTAINER\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;aws/codebuild/standard:2.0\u0026#34;, \u0026#34;computeType\u0026#34;: \u0026#34;BUILD_GENERAL1_SMALL\u0026#34;, \u0026#34;environmentVariables\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;notifyTitle\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;BuildProjectName build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;PLAINTEXT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;slackHookUrl\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;https://hooks.slack.com/services/123456789/ABCDEFGHI/abcdefghij1234567890abcd\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;PLAINTEXT\u0026#34; } ], \u0026#34;privilegedMode\u0026#34;: false } } この JSON ファイルを使って CodeBuild の設定を変更します。\n$ aws codebuild update-project --cli-input-json file://update.json 正しく変更できれば、変更後の設定が出力されます。\nプライマリソースのウェブフックイベントを設定 続いてはビルド開始の設定です。これについてはマネジメントコンソールで操作しました。\nCodeBuild のビルドプロジェクトから対象のプロジェクトを選択して、送信元の編集画面を開きます。\nちなみに、この時点でソースプロバイダが GitHub になっていることを確認できます。\n画面の下の方にある プライマリソースのウェブフックイベント で、ビルド開始条件を設定します。\nイベントタイプ で プッシュ を選択し、 これらの条件でビルドを開始する の HEAD_REF に ^refs/heads/master$ と入力して更新します。\nこの master 部分を変更することで、特定のブランチへのイベントを検知することができます。\nbuildspec.yml 内で Slack への通知を追加 最後に、 Slack への通知処理を buildspec.yml 内に追加します。処理といっても、下記の curl コマンドを post_build フェーズの commands に追加するだけです。\ncurl -H \u0026#39;Content-Type:application/json\u0026#39; -d \u0026#34;{\u0026#39;channel\u0026#39;:\u0026#39;dev\u0026#39;,\u0026#39;icon_emoji\u0026#39;:\u0026#39;:codebuild:\u0026#39;,\u0026#39;attachments\u0026#39;:[{\u0026#39;author_name\u0026#39;:\u0026#39;CodeBuild\u0026#39;,\u0026#39;title\u0026#39;:\u0026#39;$notifyTitle\u0026#39;,\u0026#39;color\u0026#39;:\u0026#39;#00FF00\u0026#39;,\u0026#39;fields\u0026#39;:[{\u0026#39;value\u0026#39;:\u0026#39;Build and deploy were successed.\u0026#39;}]}]}\u0026#34; $slackHookUrl POST している JSON は次のような構造になっています。\n{ \u0026#34;channel\u0026#34;: \u0026#34;dev\u0026#34;, \u0026#34;icon_emoji\u0026#34;: \u0026#34;:codebuild:\u0026#34;, \u0026#34;attachments\u0026#34;: [ { \u0026#34;author_name\u0026#34;: \u0026#34;CodeBuild\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;$notifyTitle\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#00FF00\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;Build and deploy were successed.\u0026#34; } ] } ] } 実際に通知されるとこんな感じになります。\n以上で、 CodeBuild のみでビルド・デプロイ・通知までできるようになりました。\n失ったもの ※追記あり (2019/11/12) CodePipeline を使用しなくなったことで料金の節約には成功しましたが、 CI/CD のクオリティとしては下がっています。\n特に問題なのが、 ビルドに失敗したことがわからない という点です。\nCodePipeline を使っていた時は、仮に CodeBuild の処理中にエラーが発生した場合はあパイプラインの失敗となります。失敗したというステータスは Slack 通知用の Lambda 内で取得できるので、成功しても失敗しても、どちらの場合も通知で確認することができます。\n一方、今回のように CodeBuild だけでやろうとすると、 CodeBuild 内でのエラー・失敗をキャッチして通知する術がありません。\n本当に無理やりやろうとするなら、 buildspec.yml 内の各フェーズの最初に通知コマンドを実行して、最後のフェーズの通知が来なかったらその前段階で失敗したということはわかるかもしれません。\n※追記 2019/11/12\nAWS の Code シリーズ向けに新たな通知サービスがリリースされ、各 Code シリーズから個別に SNS および AWS Chatbot (beta) にイベント通知を送信できるようになりました。\nその結果、失ったと思われたものはクオリティが上がって戻ってきました！詳細は下記のブログで書いてます。\nまとめ 節約のために CodePipeline をやめて CodeBuild に全部任せてみたという内容でした。\n失ったもの で書いたように、 CI/CD のクオリティとしてはだいぶ低くなります。GitHub への Push でビルド・デプロイしたい！通知はとりあえず何かくれば OK！ くらいのゆるーい感じであれば、 CodeBuild だけでもなんとかなりそうです。\nそもそも他の CI/CD サービス使えばいいのかもしれませんが\u0026hellip;。\n",
    "permalink": "https://michimani.net/post/aws-use-only-codebuild/",
    "title": "節約のために CodePipeline をやめて CodeBuild に全部任せてみた"
  },
  {
    "contents": "2019 年 4 月に発表された Beats の完全ワイヤレスイヤホン Powerbeats Pro が、やっと日本でも発売開始となりました。カナダとアメリカでは 5 月に発売が開始されていましたが、そこから約 2 ヶ月、待ちくたびれました。\nそんな Powerbeats Pro を使い始めて 1 週間経ったので、レビューしていきます。基本的には AirPods との比較となります。\n目次 Powerbeats Pro とは 外観 同梱物 使用感 iOS デバイスとの接続 装着感 音質 操作性 バッテリー まとめ Powerbeats Pro とは まずは Powerbeats Pro についてのおさらいです。\nPowerbeats Pro は Apple H1 チップを搭載した 完全ワイヤレスイヤホンです。Apple H1 チップとは、第 2 世代 AirPods に搭載されているものと同じチップです。初代 AirPods および Beat X に搭載されていたものは W1 チップで、それの改良版ということになります。具体的には、より広範囲な通信範囲で安定した接続を実現する、また、デバイスとの接続時間が短縮されています。\nまた、 beat といえばやはり音質がいいと言うイメージですが、この Powerbeats Pro でも ダイナミックな音域とノイズアイソレーションによりバランスのとれた迫力サウンドを実現 しています。\nまあ、簡単に言うと、 AirPods が高音質になってカナル型になったもの っていう感じです。(あくまで個人的なイメージです)\nPowerbeats Pro - Beats by Dre AirPods - Apple（日本） 外観 では Powerbeats Pro の外観を、 AirPods と比較してみてみます。ここで比較しているのは初代 AirPods ですが、第 2 世代の AirPods もほぼ同じ形なので特に違和感はないと思います。\nまずは外箱。\nすでにカッコ良さが溢れてます。\n続いてケースの外観です。\nかなり大きいです。\nイヤホン本体の形状が大きく異なるので仕方ないですが、これに関しては AirPods のほうが勝る点ですね。\n続いてイヤホン本体の外観です。\nAirPods がインナーイヤー型だったのに対して、Powerbeats Pro はカナル型で、より安定した装着感を得るためにイヤーフックがついています。そのため、イヤホン本体も大きいです。\n詳細な数値は下記の通りです。(左右各)\n重量 サイズ AirPods 4 g 16.5 × 18.0 × 40.5 mm Powerbeats Pro 20.3 g 56 mm (高さ) AirPods (第2世代) with Charging Case - 技術仕様 同梱物 同梱物は下記の通りです。\n完全ワイヤレスのPowerbeats Proイヤフォン 充電ケース 4 種類のサイズのイヤーチップ Lightning - USB-A 充電ケーブル クイックスタートガイド 充電は相変わらず Lightning です。ここは USB Type-C でもよかったんじゃないかなとは思います。ちなみに付属している Lightning ケーブルは黒なので、ちょっと珍しいアイテムです。\n使用感 では、肝心の使用感について、次の 5 項目に分けてレビューしていきます。\niOS デバイスとの接続 音質 操作性 装着感 バッテリー iOS デバイスとの接続 冒頭にも書きましたが Powerbeats Pro には AirPods と同じ Apple H1 チップが搭載されています。そのため、 iOS デバイスとの接続も非常に簡単です。\niOS デバイスの近くで Powerbeats Pro のケースを開けると、 iOS デバイス側ですぐに検出されます。\nここで 接続 を押せば接続完了です。以降は、耳に Powerbeats Pro をつけると同時に自動で接続され、すぐに音楽を聴くことができます。このあたりは本当に AirPods と同じです。\nちなみに、耳に装着した時になる効果音は AirPods とは異なります。\n装着感 外観の項目でイヤホン本体の重量について書きましたが、左右それぞれ数値的には AirPods の 5 倍 の重量です。が、装着した感じは全く気になりません。\nさすがに使い始めて 3 日間くらいは、 1 時間くらい装着した時点で耳の裏 (イヤーフックが当たる部分) が痛くなりましたが、今ではもう全然痛くないです。\nPowerbeats Pro はカナル型なので、重量がすべてイヤーフックに掛かるわけではありません。耳の穴に入れてる時点でほぼ固定されており、イヤーフックはあくまでも補助的な役割です。そのため、数値上は 5 倍の重量ですが、装着した感じは全く違和感ありません。\nまあ、 AirPods は本当に装着感が無いので、それと比べるとつけてる感はありますが。\n装着感とは少し違いますが、この形状ゆえに掃除がしにくいです。AirPods と比べて皮膚と接している面積も多くなるため、どうしても皮脂などの汚れがつきやすくなっています。夏場などは特に。\n本体は防水仕様なので汗による故障はきにする必要がありませんが、掃除は少しやりにくいですね。\n音質 これはもう、一言、良いです。\nカナル型ということで音漏れの心配がほぼなく、また外からのノイズも防ぐことができるので、 AirPods と比較した場合 より高音質に感じます。\nただ、カナル型と言っても外の音を完全にシャットダウンするわけではなく、音量を小さくすれば普通に会話できる程度の音は聞こえます。\nAirPods ではどうしても外の音がかなり入ってきてしまうので、通勤では正直使いづらかったです。しかし、 Powerbeats Pro はもう最高ですね。電車内、駅のホームでも快適に音楽を聴くことができます。\n操作性 Powerbeats Pro には左右両方に物理的なボタンが 2 つ (3 つ) 付いています。\nまず 1 つ目は音量調整ボタン。\n本体の上部についている横長のボタンです。装着した時には、本体を親指と人差し指で上下から挟んで、人差し指で押せる場所にあります。両端を押すことで、音量の UP/DOWN ができます。左右両方についていますが、音量は共通になっています。\n2 つ目は、本体の側面にある b と書かれているボタンです。\nここは AirPods でいうとタップで操作していた部分にあたります。Powerbeats Pro では物理的なボタンになっていますが、押し心地はかなり軽いです。\nこの b ボタンは、押す回数によって動作が変わります。\nシングル : 音楽の再生/停止 ダブル : 曲送り トリプル : 曲戻し 長押し : Siri 起動 AirPods ではタップ回数に応じて機能を (ほぼ) 自由に割り振ることができましたが、 Powerbeats Pro では上記で固定になっています。まあ、十分な操作性能ですね。\nバッテリー バッテリーに関しては、第 2 世代 AirPods よりも持ちが良いです。\nAirPods のバッテリーに関しては次のように書かれています。\n24 時間以上のバッテリー駆動時間 (充電ケースを使用した場合) 最大 5 時間の再生時間 (1 回の充電で) わずか 15 分の充電で最大 3 時間の再生時間 一方、 Powerbeats Pro のバッテリーに関しては次のように書かれています。\n充電ケースを使用すれば、 24 時間以上の再生が可能 最長 9 時間の再生 5 分の充電で約 90 分再生 つまり、 1 回の充電で音楽再生できる時間も、急速充電によって確保できる再生時間も、 Powerbeats Pro のほうが長いんです。\n本体サイズのおかげでバッテリー容量を確保できているのかもしれませんが、長時間使用できるというはありがたいですね。\nまとめ Apple H1 チップを搭載した Beats の完全ワイヤレスイヤホン Powerbeats Pro のレビューでした。\n発表から発売までかなり待たされましたが、これは買ってよかったです。個人的 2019 年ベストバイに堂々のランクインです。\nAirPods も非常に良いイヤホンでした。特に iOS デバイスとの親和性は最高です。が、やはりインナーイヤー型では快適に音楽を聴くことができず、不便さを感じていました。\nしかし、性能はほぼそのままでカナル型になった Powerbeats Pro 、これはもう最高のイヤホンです。\nしいて欠点を挙げるとすれば、充電ケースがかなり大きいということですかね。でもそれ以外は完璧です。不満はありません。\nちなみに現時点 (7/31) で購入できるのは ブラック のみで、アイボリー、モス、ネイビー についてはまだ購入できません。(発売開始時期も未定です) 8/23、ついに他の 3 色 (アイボリー、モス、ネイビー) の発売も開始されました！個人的にはネイビーがすごく良い雰囲気出してるなーっていう感じです。 Apple Store および Amazon で購入できるので、 AirPods の性能はいいけど遮音性が\u0026hellip;と思っている方はぜひ Powerbeats Pro を使ってみてください。耳が幸せになれます。\n※追記 2019/11/28\nPowerbeats Pro の良くないなと感じるところについても書きました。\n",
    "permalink": "https://michimani.net/post/gadget-review-powerbeats-pro/",
    "title": "[レビュー] Powerbeats Pro を使い始めたらそこには快適さしかなかった ー さらば AirPods ー"
  },
  {
    "contents": "AWS では Beta や Preview としてサービスが利用できるようになることがあります。それらの、まだ正式リリースされていないサービスを使用する際に注意すべきことをメモしておきます。\nAWS Service Terms AWS のサービスを利用する際の規約があります。それが AWS Service Terms です。\n各種言語に翻訳されたものもありますが、あくまでそれらは便宜上提供されているものなので、英語版を正とします。\nAWS Service Terms 内容としては、 AWS で利用できる各サービスと、認定試験およびトレーニングに関する規約です。その一部として、 Beta Service Participation というパートがあり、 Beta サービスの利用に関する規約が書かれています。\nこれを書いている時点 (2019/07/28) では、最終更新日が 2019/07/22 となっています。AWS Service Terms は頻繁に更新されているようなので、この記事をご覧になる頃には情報が古くなっている可能性があります。\nBeta Service Participation AWS Service Terms の中の 1.10 が Beta Service Participation となっていて、 Beta サービスの利用に関する規約となっています。\nここでいう Beta サービスとは、“beta”, “preview”, “pre-release”, または “experimental” などと書かれている試験的なサービスおよびリージョンのことを指します。\n項目としては 1.10.1 から 1.10.10 までの 10 項目で構成されており、わかりやすいところだと下記のような内容があります。\nサービスの内容変更、提供終了は AWS がいつでも実行できる 事前メンテナンスについて通知はされない バグ、瑕疵が含まれる可能性があるため AWS および関連会社は全ての保障をしない まあ、 Beta なので自己責任で使ってねという感じです。それはそうでしょう。納得です。\nひとつ気になったのが、 1.10.8 の内容です。\n1.10.8 Beta Materials, Test Observations, Suggestions concerning a Beta Service or Beta Region, or any other information about or involving (including the existence of) any Beta Service or Beta Region are considered AWS Confidential Information. You will not disclose (including, but not limited to, in a press release or public statement) any Beta Materials, Test Observations, Suggestions concerning a Beta Service, or any other information about or involving (including the existence of) any Beta Service, except as agreed by AWS in writing.\n翻訳すると、 Beta サービスの素材、テスト結果、その他 Beta サービスに関する情報 (Beta サービスの存在自体を含む) については全て AWS の秘密情報とされ、それらを一切開示しない という内容です。\nつまり、 Beta として提供されたサービスをちょっと触ってみて、それで得られた情報 (ログやコンソール上の表示など) をブログなどで公開することは、この 1.10.8 に抵触します。\nBeta サービスによっては、マネジメントコンソールの Get Started 的な画面で By using this service you agree to Section 1.10 \u0026ldquo;Beta Service Participation\u0026rdquo; in AWS Service Terms といった内容の文章が書かれており、利用するにあたっては AWS Service Terms の 1.10 に同意する必要があります。\nなので、 一切開示しない にも同意することになります。たとえそのサービスが Public beta であってもです。\nせっかく試してみるなら、その結果を共有したくなるものだと思うんですが、仕方ないですね。\nまとめ AWS の Beta サービスを使うときに注意することについて書きました。\n実は最近 public beta として公開されたサービスについて実際に触ってみたブログを書いたんですが、どうやらその記事も怪しいと言うことで削除いたしました。\nこのブログ始まって以来のプチバズり、ブクマも 50 以上つけていただいていたのですが、申し訳ございません。\nまあ、やってる内容は AWS 公式のブログでやっていることと同じなのでそちらを参照してください。\n一応、本当にダメなのか AWS のサポートには問い合わせているので、何か進展あれば追記したいと思います。\n",
    "permalink": "https://michimani.net/post/aws-use-beta-service/",
    "title": "AWS の Beta サービスを使うときに注意すること"
  },
  {
    "contents": "先日 GA となった AWS CDK のミートアップ AWS Cloud Development Kit (CDK) GA 記念 Meetup が AWS Loft Tokyo で開催されました。\nCDK についてはほぼ使用経験がないのですが、今後使えるようになりたいと思い参加してきました。そのレポートです。\nちなみに、事前の申込み時点でのイベントタイトルは AWS Cloud Development Kit (CDK) Meetup でしたが、当日のスライドでは GA 記念 が追加されていました。\nイベントの概要 各セッションのレポート (という名のメモ) AWSのインフラはプログラミングコードで構築！AWS Cloud Development Kit入門 CDKを用いたモダンなECSクラスタの構築と運用 はてなブログタグとCDK Ansible \u0026#43; CloudFormation を、AWS CDK に移行する方法 まとめ・感想 イベントの概要 2018年8月に Developer Preview として発表された AWS Cloud Development Kit (AWS CDK) は AWSの環境を管理するコードをTypeScript や Python など一般のプログラミング言語を使って記述できるフレームワークです。 CDKはAWSリソースに対するオブジェクト指向の抽象レイヤを提供するほか、ライブラリのデフォルト値としてAWSのベストプラクティスを組み込み、少ないコード量で記述できるよう設計されています。 このCDKについて解説し、お客様事例を共有いただくMeetupを開催します。すでにCDKを活用されているはてな様、DeNA様にご登壇いただきます。 イベントページ から引用 参加対象者としては下記のように書かれていました。\nコードを使ってAWSの環境を構築・運用したいと思っている方 すでに AWS CloudFormation を利用していて、テンプレートの記述方法を改善したいと思っている方 CDKを使ったAWS環境管理の事例を知りたいと思っている方 私はこの条件で言うと コードを使ってAWSの環境を構築・運用したいと思っている方 と CDKを使ったAWS環境管理の事例を知りたいと思っている方 に該当する状態でした。\nただ、イベント参加時点ではインフラのコード化については未経験 (CFn と CDK のチュートリアルレベル) で、これを機に IaC に対するモチベーションを高めて勉強していこうという気持ちで参加しました。\nタイムスケジュールは下記の通りでした。\n6:30PM–7:00PM 受付 7:00PM–7:05PM ごあいさつ・事前説明 7:05PM–7:50PM AWSのインフラはプログラミングコードで構築！AWS Cloud Development Kit入門（仮）\nアマゾンウェブサービスジャパン株式会社\nSolutions Architect 福井厚 7:50PM–8:10PM CDKを用いたモダンなECSクラスタの構築と運用\n株式会社はてな サービス・システム開発本部 システムプラットフォーム部 SRE\nby id:cohalz 原田陽太 (ハラダヨウタ) 氏 8:10PM–8:30PM はてなブログタグとCDK\n株式会社はてな サービスシステム開発本部 第1グループ ブログ統合チーム アプリケーションエンジニア\nby id:aereal 中澤亮太 (ナカザワリョウタ) 氏 8:30PM–9:00PM Ansible + CloudFormation を、AWS CDK に移行する方法\n株式会社ディー・エヌ・エー\nby 佐藤学 氏 9:00PM–9:05PM クロージング 各セッションのレポート (という名のメモ) 各セッションのレポート という名のメモです。メモレベルなので表記ゆれ・誤字脱字多いと思います。\nAWSのインフラはプログラミングコードで構築！AWS Cloud Development Kit入門 アマゾンウェブサービスジャパン株式会社\nSolutions Architect 福井厚\nそもそも CDK が生まれた背景とは 手作業は始めるのは簡単だが繰り返しの作業ではミスが起きやすい API コールが失敗したら？どうなったらリソースが利用可能な状態？ロールバックは？ CFn を使うとそのあたりが容易、再作成も可能に。ただし文法が辛い サードパーティでも辛さがある CDK とは CDK Apprication \u0026gt; Stack (s) \u0026gt; Construct (s) \u0026gt; Construct (s) \u0026hellip; Construct は AWS CDK App の基本ビルディングブロック 単一のリソースを表現したり、複数の AWS CDK App を連携することも可能 AWS Construct Library (抽象化レベル) low-level : 様々なプロパティを明示的に指定する必要がある high-level : デフォルトパラメータを使ってより簡単に記述できる Patterns : あらかじめ用意された複数リソースの関係を利用できる\nComposition と呼ばれる CFn にはあるが CDK にはない場合の回避策 CFn constructs を直接利用する CFN リソースがない場合は cd.CfnResource を使う (汎用の CFN リソース) addOverRide、addDeleteOverride で上書きする CFn のカスタムリソースを設定する Stack は CFn の Stack と (ほぼ) 同意 1週間前くらいに GA Release v1.0.0 · aws/aws-cdk · GitHub GA 版の対応言語は Python, TypeScript。Java, C# は Developer Preview AWS::CDK::METADATA というリソースは全てのスタックに自動的に付与される cdk bootstrap リージョン、アカウントの設定 指定しない場合は AWS CLI で指定している credential が使用される cdk diff デプロイ済みと変更ごとの差分を確認できる サンプルコーディング demo 1 init app と init sample-app がある。後者ではサンプルスクリプトが自動で生成される bin 配下のスクリプトがメイン。lib 配下に Stack のスクリプト 基本的にパラメータにはデフォルト値が設定されている。必要な値のみ指定する cdk synth で CFn テンプレートが生成される cdk deploy でデプロイされる demo 2 VPC: AZ 数はデフォルトでは指定のリージョン内すべて。最大数を指定して数を制限できる。 ECS: fromAsset() メソッドの実行のみで ビルド、 ECR への push までやってくれる。 まとめ Infrastructure IS Code (not AS) AWS CDK Intro Workshop :: AWS Cloud Development Kit (AWS CDK) Workshop SAM と CDK の住み分け それぞれが進化していく 多少のセキュリティチェックは含まれている CDKを用いたモダンなECSクラスタの構築と運用 株式会社はてな サービス・システム開発本部 システムプラットフォーム部 SRE\nby id:cohalz 原田陽太 (ハラダヨウタ) 氏\nどうして CDK を採用したか クラウド環境は AWS アプリケーションをコンテナ化している CFn テンプレートは横展開が大変 プログラミング言語で書ける、パッケージ化しやすい、強力な CLI ツール ECS のライブラリを作る 公式ライブラリはあったが昨日が足りない ecs.config を使いやすく、SSM エージェントの導入、スポットインスタンス対応 既存のツール、仕組みではデプロイやパッケージの更新が難しい CDK なら簡単に設定できる cohalz GitOps GitHub -\u0026gt; CodeBuild -\u0026gt; ECR stg -\u0026gt; prd の PR を自動生成、stg 環境にデプロイ インフラとアプリのコードを分離できた インフラ構成が GitHub のマスター 今後の課題: cdk diff どうする？まだまだ手作りだが\u0026hellip; 開発時にやったこと Jest のスナップショット 実行結果をファイルに書き込んで前回の実行時との差分を確認できる CDK のアップデート時 Renovate と組み合わせてテストを通ればマージ アップデートやバグへの対応 社内 Slack 積極的にバグ報告、PR まとめ 慣れた言語でかける 以上のメリットがある IaC とそのデプロイツールとして利用できる まだまだ出たとこなので知見共有が大事 はてなブログタグとCDK 株式会社はてな サービスシステム開発本部 第1グループ ブログ統合チーム アプリケーションエンジニア\nby id:aereal 中澤亮太 (ナカザワリョウタ) 氏\nCDK とわたし プッロビジョニングツールが好き TS 大好き CDK とはてなブログ タグ インフラは全て AWS ECS でホスト 複数のコンポーネントからなる (API, GraphQL, SSR サーバ) CFn を検討したが SG 作成で同じようなコードが多い クロススタック参照の順序に気を遣う 差分チェック\u0026hellip; CDK DP だったが乗っていくことを決めた 最悪脱出できる (cdk synth で CFn テンプレート出力できる) ECS デプロイと CDK CDK でアプリケーションの面倒までみるのか (CDK app 内にアプリケーションのバージョンタグを記載) アプリケーションのデプロイ ツールのレイヤからみると Orchestration レイヤリングの話 (難しい\u0026hellip;) ECS アプリケーションの観点では、様々んコンポーネントを隠蔽してくれている 構成管理の観点では、つながりを意識しないといけない 懸念 ロールバックが遅くならないか？ Stack を細かく分けて対応 アプリケーションエンジニアから見た CDK ソフトウェアの抽象化に対するアンテナ CDK は Clean Architecture を実現できている 秩序立ったレイヤリングとその恩恵 CDK への期待 Higher-order construct Ansible + CloudFormation を、AWS CDK に移行する方法 株式会社ディー・エヌ・エー\nby 佐藤学 氏\n現行 (Ansible + CloudFormation) Jinja2 というテンプレートファイルから CFn のテンプレートファイルを生成 CFn にデプロイ CDK を知る 現行の構成を AWS 社員向けに説明したところ、袋叩きにされた CDK への移行 Step1: CFn テンプレートの作成を CDK に移行 (synth で出力、現行と差分がないか) Ansble で出力したものと CDK で出力したものの (物理的な) 差分をゼロにする TypeScript のほうがサンプルが多い (多かった) Metadata が出力されると diff が辛いので、オプションで出力しないようにする Step2: CDK を使ってデプロイ まとめ 移行は 2 ステップ CFn テンプレートの出力 と デプロイ 質問 CDK で出力される Stack の順序は？ -\u0026gt; 名前順 まとめ・感想 AWS CDK GA 記念 Meetup の参加レポート (メモ) でした。\n感想としては、セッション内容のレベルたけー\u0026hellip; です。私が理解できたのは最初の福井さまのセッション (の一部) くらいで、あとはもう「へぇ〜\u0026hellip;」という感じで聞いていました。インフラのコード化についてはまだ自分の未体験ゾーンということもありますが、構成管理に対する考え方とか、そもそも出てくるワードの意味とか、わからん部分が多かったです。まあこれは仕方ないかなということで、今後がんばろうという良い刺激を受けることができました。\nCDK のミートアップということでしたが、核となるは CloudFormation の話だなという感じでした。CDK については、とにかく\n主要な言語 (TypeScript, Python \u0026hellip;) で書ける 少量、直感的な記述で CFn テンプレートを生成できる というところがメリットだという話でした。\nただそのメリットをより感じられるのは、これまでに手で CFn テンプレートを書いていた人や、サードパーティのツールを使って生成した人なのかなと感じました。\nなので、このイベントきっかけで CDK を触り始めた・インフラのコード化してみようと思った自分にとっては、 CDK を使って構成管理をしていく前に、 CloudFormation についても勉強していかないといけないなと感じました。といっても、どうやって勉強していけばいいのかわからないので、これまでマネジメントコンソールでぽちぽち構築してきた諸々を再現できるような CDK アプリケーションを作ってみようかなと考えてます。\nあと、やっぱりはてなの方々ってかなりレベル高いことやってるんやなーという印象を受けました。\n3 年前くらいに転職活動してる時、実ははてなさんにも書類送ったんですが、あの時の自分では全くレベルが達してなかったですね。(じゃあ今は達してるのかというとそうではない)\n近々新機能 (サービス) となるブログタグがリリースされるということなので、楽しみにしたいと思います。\n長くなりましたが、AWS CDK 、IaC やりはじめたばっかりの自分でも、とにかく簡単に構成管理できるという印象です。\nこれを機にインフラのコード化はじめてみるのも良いかもしれません。\nあと、どうでもいいんですが、ミートアップ終わったあと (21:30くらい) に目黒三大とんかつ屋の とんき に行こうとしたんですけど、その時間でも待ちがあったんです。さすが人気店。雨も降ってたので諦めて帰りました。。\n来週も Loft Tokyo 行くので今度は待ってみてもいいかな\u0026hellip;\nAwswakaran.tokyo.powered By Aws ",
    "permalink": "https://michimani.net/post/event-aws-cdk-meetup/",
    "title": "[レポート] AWS Cloud Development Kit (CDK) GA 記念 Meetup @ Loft Tokyo に行ってきました"
  },
  {
    "contents": "クラスメソッドさんが主催する AWS の勉強会 AKIBA.AWS に初めて参加してきましたので、その参加レポートです。\n今回は 14 回目ということでしたが、普段の開催方式とは違って AKIBA.AWS #14 番外編〜AWS Update LT大会〜 として開催されました。2 時間 (ほぼ) 休憩なしで LT 10 本という濃密な時間でした。\nAWS.AKIBA とは LT の内容 1：VPCとオンプレをつなぐ環境のUpdate 2：re:Invent2018以降のECSアップデートをざっと確認しよう！ 3：Code Seriesのアップデートについて 4：S3界の革命児！S3 Batchのご紹介 5：知っているかX-rayのアップデートを！！ 6：AWS Batchの使い所のご紹介 7：Amazon WorkSpaces 直近のアップデートご紹介 8：Terraform v0.12 9：SSMのアレ 10：AWS Configのアップデートをご紹介 まとめ AWS.AKIBA とは Developers.IO で AWS に関することやその他技術的なことをアウトプットし続けているクラスメソッドさんが主催する AWS の勉強会です。\n毎回あるテーマについて 基礎・応用・ガチ というレベルを付けて開催されています。\n今回は 番外編 ということで、昨年の re:Invent 2018 以降にアップデートがあった内容を中心に、 AWS のさまざまなサービスについて合計 10 本の LT を実施するという回でした。\n今回の開催概要については connpass のイベントページ をご覧ください。\nLT の内容 各 10 分の LT が 10 本ありましたので、それぞれの項目タイトルとざっくりとした内容、また、各登壇者様がそれぞれのブログ記事にてスライドや詳細なコメントを書いておられるので、そのリンクを載せておきます。(公開されているものに限り。見つけたら追記していきます。)\n1：VPCとオンプレをつなぐ環境のUpdate 登壇者 : 北野佑一さん\nオンプレと AWS を繋ぐ話 (Transit Gateway)\nSite to Site VPN / Direct Connect (DX)\nオンプレと複数の VPC を繋ぐ方法\nひとつずつ接続を構築する → 数が増えるとしんどい Direct COnnect Gateway VPC の相互接続はできない オンプレと VPC と VPC を繋ぐ\nVPC 同士は VPC ピアリング → 数が増えるとしんどい Transit Gateway (今までは、EC2 にソフトウェアをインストールしてハブにしていた) 既存の VPN 設定を、オンプレ側の設定変更なしに Transit Gateway に移行可能になった Direct Connect Gateway との接続をサポート (東京リージョンはまだ) 料金は高い AKIBA.AWS #14 番外編 で、オンプレとVPCをつなぐ環境のUpdateについて話しました #akibaaws 2：re:Invent2018以降のECSアップデートをざっと確認しよう！ 登壇者 : 岩城匠朗さん\nContainer Roadmap (GitHub で公開されている) ECS の Blue/Green デプロイ これまでは Green の環境の作り込みが必要だった CodeDeploy がよしなにやってくれる CFn で対応予定 (進行中) ECS Secrets Management これまではアプリケーション側で複合が必要だったが、不要になった run ECS task definitions locally ローカル環境でのテストには別途 ローカル用の docker-compose ファイルが必要だった タスク定義を元にローカル環境でテストが可能になった ECS のアップデートは激しい。Road Map で動きを確認しよう 以下、スライド内で参考記事として挙げられていたブログです。\n【アップデート】ECSタスク定義を利用したローカル環境でのテスト実行が可能に！ 【祝！】ECSへの機密情報受け渡しがCloudFormationに対応しました！ 3：Code Seriesのアップデートについて 登壇者：奥拓哉さん\nCodeCommit\nCLI, SDK での commit をサポート (ただし辛い) 2 つのマージ戦略 (PR 時のマージ方法) タグ付け CodeBuild\nリソースグループのタグ付け API ローカルキャッシュのサポート (これまでは S3 にしか置けなかった) 複数の言語のビルドをサポート CodeDeploy\nCodePipeline\n大きなアップデートはない CodeDeploy 以外で VPN エンドポイントをサポート\nモダンにはなっているが、circle CI や GitHub と比べるとまだまだ感は否めない\n【登壇】AKIBA.AWS #14 AWS Update LT大会 Code Seriesのアップデートについて #akibaaws 4：S3界の革命児！S3 Batchのご紹介 登壇者：大前諒祐さん\nS3 Batch Operations\nCLI, SDK, API からジョブを作成した場合は、確認待ちのステータスをスキップすることができる\nAKIBA.AWS 第14回 特別編で「Amazon WorkSpaces 直近のアップデートご紹介」を発表しました #akibaaws ｜ DevelopersIO 5：知っているかX-rayのアップデートを！！ 登壇者：金泰雨さん\nAWS X-Ray とは\nトレース (全体) セグメント (パケットの流れ) サブセグメント (リソース間のデータ通信) Analytics (新機能)\nFilter Expresion 各セグメント、サブセグメントの情報を詳細に確認できる URL でフィルタの適用が可能 サンプリングルールをコンソール上で設定できるようになった\nAKIBA.AWS #14 番外編〜AWS Update LT大会〜 で「知っているかX-Rayのアップデートを！！」という内容で登壇しました #akibaaws 6：AWS Batchの使い所のご紹介 登壇者：西山祐司さん\nSQS, ECS, AutoScaling などのクラスタ管理が不要\nBlack Belt の資料が古い\n配列ジョブの対応\n1 つのジョブ登録で複数のジョブの実行が可能になった (大量ファイルへの対応) CloudWatch イベントに対応\nJob キューに登録されてから実行 -\u0026gt; CloudWatch イベントをトリガーにして実行可能に タイムアウトによる自動停止\njob が残り続ける -\u0026gt; タイムアウトの job は強制終了できるように AWS Batch の使い所\n依存関係のある Job の実行 S3 -\u0026gt; AWS Batch (1) -\u0026gt; S3 -\u0026gt; AWS Batch (2) 深層学習など AKIBA.AWS #14 番外編〜AWS Update LT大会〜 で「S3界の革命児！S3 Batchのご紹介」という内容で登壇しました #akibaaws 7：Amazon WorkSpaces 直近のアップデートご紹介 登壇者：小野塚正忠さん\n仮装デスクトップサービス\nWeb ブラウザからも利用できる\nURI ごとに仮装環境を用意できるのは便利そう AKIBA.AWS 第14回 特別編で「Amazon WorkSpaces 直近のアップデートご紹介」を発表しました #akibaaws ｜ DevelopersIO 8：Terraform v0.12 登壇者：数枝正樹さん\nInfrastructures as Code (AWS のサービスではない)\nHCL という言語\n0.12\n差分表示 変数の使用が簡潔に For, For-Each module (関数みたいなもの) の入出力に使える型が柔軟に指定できるようになった Pragmatic Terraform on AWS (電子書籍) を読もう\n#akibaaws でTerraform v0.12について話しました 9：SSMのアレ 登壇者：柴田卓哉さん\nEC2 をはじめとする AWS のリソースの構成管理を集めたもの\n役割が変わるごとにサービス名が変わっている\nもともとは Windows の構成管理\n【登壇】AKIBA.AWS 第14回 特別編で「SSMのアレ（仮題）」という発表をしました #akibaaws 10：AWS Configのアップデートをご紹介 登壇者：中川翔太さん\nAWS のリソース構成管理\n構成変更などの履歴を管理、評価\n評価のタイミングは、定期的/変更時 リソースの修復\nSSM のドキュメントにアクションを記載 クエリ機能\nリソース情報に対して SQL を実行して集計可能 ex) 特定のセキュリティグループに関連するリソースを取得する クエリ結果の出力ができるといいなー 料金体系の変更\n課金がルール数ベースから評価回数ベースに変更 とにかく安くなる 登壇】AKIBA.AWS 第14回 特別編で「AWS Configのアップデートをご紹介」という発表をしました #akibaaws まとめ LT が 10 分 x 10 本で、あっという間の 2 時間でした。\nいくつかの内容については既に Developers.IO 経由で目にしていたものもありましたが、今回初めて聞く内容や、まだ触ったことのないサービスに関する話も聞くことができたので、自分の中に一気に情報が入ってきたなーと感じました。(理解できているかはおいといて)\n特に面白そうだなともったのは AWS X-Ray です。\nAWS X-Ray はアプリケーションで処理するリクエストについて、各リソース間でのデータやパケット情報を収集・閲覧できるサービスです。使いどころ云々というよりかは、なんか可視化できるって良いですよね というレベルでの面白さです。\n期限なしの毎月の無料枠も設定されているので、気軽に触れますね。\nあと思ったのが、今回登壇されていた方々が全て入社から 1 年未満の方 (5月入社の人もいた) ばっかりで、クラスメソッドさんの急成長ぶりを感じました。何目線だっていう話ですが。。\n普段ブログで大変お世話になっていますが、やはり実際に耳で聞くと情報の入ってき方も違うなと感じたので、また機会があれば参加したいと思います。\n",
    "permalink": "https://michimani.net/post/event-aws-akiba-updating-lt/",
    "title": "AKIBA.AWS #14 番外編〜AWS Update LT大会〜 に行ってきました"
  },
  {
    "contents": "先日お邪魔した 目黒JavaScriptもくもく会 で AWS IoT エンタープライズボタン というデバイスの話が出ていました。どうやら AWS Lambda をはじめとした AWS の各サービス (SNS とか DynamoDB とか) と非常に簡単に連携できる IoT デバイスらしいということなので、とりあえず買って試してみました。\n目次 AWS IoT エンタープライズボタン とは 今回やること 実際の手順 1. Slack の Webhook URL を取得 2. Slack にメッセージを送信する Lambda 関数を作成 3. AWS IoT エンタープライズボタン を AWS IoT 1-Click に登録 4. AWS IoT 1-Click で Lambda 関数の呼び出しを設定 5. AWS IoT エンタープライズボタン を押す まとめ AWS IoT エンタープライズボタン とは AWS IoT ボタンは、Amazon Dash Button ハードウェアをベースにしたプログラミング可能なボタンです。このシンプルな Wi-Fi デバイスは設定が簡単で、特定のデバイス向けのコードを記述することなく、AWS IoT Core、AWS Lambda、Amazon DynamoDB、Amazon SNS、およびその他多くのアマゾン ウェブ サービスの使用を開始する開発者向けに設計されています。\nAWS IoT ボタン（プログラミング可能なダッシュボタン）| AWS Amazon Dash Button は既にに販売が終了していますが、ベースは Amazon Dash Button で、ボタンを押した時のアクションをハックできるのが AWS IoT エンタープライズボタン です。公式の説明文にもあるように、 Wi-Fi でインターネットに接続し、AWS の各種サービスと連携することができます。連携できるサービスには AWS Lambda も含まれています。Lambda で関数を実行できるということは、ほぼ何でもできるということなので、アイデア次第では色んなことに使えそうです。\nAWS IoT エンタープライズボタン は普通に Amazon で売っているので、誰でも購入できます。\n今回やること AWS IoT エンタープライズボタンをトリガーにして、 Slack にメッセージを送信する Lambda 関数を実行してみます。\n手順としては下記の通りですが、 Slack の Webhook URL の取得については割愛します。\nSlack の Webhook URL を取得 Slack にメッセージを送信する Lambda 関数を作成 AWS IoT エンタープライズボタン を AWS IoT 1-Click に登録 AWS IoT 1-Click で Lambda 関数の呼び出しを設定 AWS IoT エンタープライズボタン を押す 実際の手順 では、手順をひとつずつみていきます。\nちなみに今回は 東京リージョン を使います。 1. Slack の Webhook URL を取得 割愛します。\n2. Slack にメッセージを送信する Lambda 関数を作成 まずは Slack にメッセージを送信する Lambda 関数を作成します。\nPython 3.7 で実装します。\nimport json import logging from base64 import b64decode from urllib.request import Request, urlopen from urllib.error import URLError, HTTPError SLACK_CHANNEL = \u0026#39;iot-test\u0026#39; HOOK_URL = \u0026#39;https://hooks.slack.com/services/ABCDEFGHI/JKLMNOPQR/stuvwxyz1234567890abcdef\u0026#39; logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): logger.info(\u0026#34;Event: \u0026#34; + str(event)) message = str(json.dumps(event, indent=2)) slack_message = { \u0026#39;channel\u0026#39;: SLACK_CHANNEL, \u0026#39;icon_emoji\u0026#39;: \u0026#39;:robot_face:\u0026#39;, \u0026#39;attachments\u0026#39;: [ { \u0026#39;color\u0026#39;: \u0026#39;#6293ca\u0026#39;, \u0026#39;fields\u0026#39;: [ { \u0026#39;value\u0026#39;: \u0026#34;```\\n%s\\n```\u0026#34; % (message) } ] } ] } req = Request(HOOK_URL, json.dumps(slack_message).encode(\u0026#39;utf-8\u0026#39;)) try: response = urlopen(req) response.read() logger.info(\u0026#34;Message posted to %s\u0026#34;, slack_message[\u0026#39;channel\u0026#39;]) except HTTPError as e: logger.error(\u0026#34;Request failed: %d %s\u0026#34;, e.code, e.reason) except URLError as e: logger.error(\u0026#34;Server connection failed: %s\u0026#34;, e.reason) event で渡される辞書データを成形された json 文字列をメッセージとして送信するだけの関数です。\n3. AWS IoT エンタープライズボタン を AWS IoT 1-Click に登録 次に AWS IoT エンタープライズボタン を IoT デバイスとして登録します。\n登録には AWS の IoT 1-CLick というサービスと、 iOS/Android 向けに提供されている AWS IoT 1-Click アプリ (以下、アプリ) を使います。\nAWS IoT 1-Click アプリは、 App Store および Google Store からダウロードします。今回は iOS 版を使いますが、 Android 版でもほぼ同じだと思います。\n大まかな手順としては次の通りです。\nIoT 1-CLick でデバイス ID を入力する アプリを使ってデバイスを登録する ではこれもひとつずつ見ていきます。\n1. IoT 1-CLick でデバイス ID を入力する AWS のマネジメントコンソールから IoT 1-Click を探して IoT 1-Click のオンボードボードを表示します。\n表示したら、 デバイスの登録 ボタンを押して デバイスID を入力します。\n登録ボタンを押すとデバイスの反応待ちになるので、一旦そのままにしておきます。\n2. アプリを使ってデバイスを登録する アプリ起動後は、上で操作したものと同じ AWS アカウント (IAM IAM ユーザ) でログインします。また、リージョンは アジアパシフィック (東京) を選択しておきます。\nそして、セットアップ画面の Wi-Fi を設定 ボタンから AWS IoT エンタープライズボタンの Wi-Fi設定をします。\nデバイス ID は手入力、または、 AWS IoT エンタープライズボタン が入っていた箱の側面に貼ってあるバーコードを読み取ることで入力可能です。私は何の躊躇もなしに外箱を捨ててしまったので、手入力しました。外箱はしっかり残しておきましょう。\nちなみにデバイス ID は AWS IoT エンタープライズボタン本体の裏側に記載されているので、万が一箱を捨ててしまっても大丈夫です。ただし、めちゃくちゃ字が小さいです。\nデバイス ID を入力するとペアリングが始まるので、AWS IoT エンタープライズボタン を 6 秒間 (青い光が点滅するまで) 押します。アプリ側で認識されると、続いて Wi-Fi の設定に移ります。(キャプチャがありません、ご了承ください)\n接続したい SSID をセレクトボックスから選択して、パスワードを入力します。すると「Wi-Fi 設定中です」みたいなメッセージが表示されてローディングアニメーションが表示されます。\nここでそのまま放置すると接続エラーになってしまう ので、ローディングアニメーションが表示されたら AWS IoT エンタープライズボタン を 1 回押します。\nWi-Fi 設定が完了すると、マネジメントコンソール上でも認識されるので、デバイスの登録は完了です。\nちなみにアプリ側での Wi-Fi 設定は途中でキャンセルするとデバイス ID の入力からやり直しになります。なので、箱を捨ててしまっているとかなり面倒なことになります。(なりました)\n4. AWS IoT 1-Click で Lambda 関数の呼び出しを設定 デバイスが登録できたら、あとはマネジメントコンソール上でプロジェクトの設定をします。\nここで出てくるキーワードがいくつかあるので、それぞれの説明を書いておきます。\nプロジェクト IoT デバイスで何かをするためのプロジェクトです。これが大元です。\nプレイスメント IoT デバイスを配置する場所 (概念) です。\nプロジェクト内に複数作成できます。\nひとつのプレイスメントには複数のデバイスを登録できますが、その最大数は後述するテンプレートに依存します。\nまた、複数のプレイスメントで同じデバイスを共有することはできません。例えば A と B のプレイスメントがある場合に、デバイス1 を A と B の両方に配置することはできません。\nプレイスメントにはデバイスと合わせて、属性 と 値 の形で任意の情報を持たせることができます。\nテンプレート デバイステンプレート、プレイスメント という項目で構成されています。\n一つのプロジェクトに対して一つ存在します。\nデバイステンプレートは、\u001fそのプロジェクトで使うデバイスのテンプレートを 1 \u0008個以上作成します。\n例えば、 SMS を送信するデバイス、 Email を送信するデバイス、 Lambda 関数を実行するデバイス という 3 つのデバイステンプレートを作成することができます。\nプレイスメントは前述したとおり 属性 と 値 で任意の情報を持たせることができるので、そのテンプレートを作成します。個別のプレイスメントでそれらを上書きすることができます。\n\u0026hellip;と書きましたが、文字だけでは伝わりにくいので実際に作ったプロジェクトのテンプレートとプレイスメントの内容を見ながら説明を加えてみます。\nまずはテンプレートです。\nデバイスは 3 つ登録しています。それぞれ名前の通りですが、 SMS を送信するデバイス、 Email を送信するデバイス、 Lambda 関数を実行するデバイス です。テンプレートではあくまでも何かをするデバイスの枠を用意しているというイメージです。\nテンプレート内のプレイスメントですが、ここで設定しているものは全て必須属性となっています。プレイスメントは基本的に任意の属性・値を設定できますが、今回のように SMS 送信、 Email 送信 を行うデバイスを登録する場合は、それぞれ SMS 送信先電話番号とメッセージ内容、 Email 送信先メールアドレスと件名と本文 が必須の属性となります。もちろん、これら以外に任意の属性を追加することも可能です。\n続いて、プレイスメントです。\nテンプレートで設定したデバイス 3 つの枠はありますが、デバイス ID が空欄になっています。つまり、どの枠にもデバイスが登録されていません。このように、必ずしもデバイスを登録する必要はなく、例えばこのプレイスメントでは SMS 送信のデバイスだけを登録する、といったことが可能です。\nまた、プレイスメントごとに属性を変えることで、このプレイスメントに登録されたデバイスが押された場合には この電話番号へ SMS を送るといった形で、プレイスメントごとに詳細な設定をすることができます。(SMS 送信だと phoneNumber の値を変更する)\nただし、実行する Lambda 関数はデバイステンプレートで指定したものから変更することができません。なので、もしプレイスメントごとに実行する Lambda 関数を変更したい場合は、 A という関数を実行するデバイス、 B という関数を実行するデバイス\u0026hellip;といった形でデバイステンプレートを設定して、各プレイスメントで実行したいデバイステンプレートにデバイスを登録するのが良いと思います。\n前置きが長くなりましたが、実際にプロジェクトを作成していきます。基本的に上で説明した設定を埋めていく感じです。\nサイドメニューの 管理 から プロジェクト を選択して 作成 ボタンからプロジェクトを作成します。\nまずはプロジェクトの名前と説明を入力します。\n続いてテンプレートの設定です。\n今回は Lambda 関数を実行するだけなので、デバイステンプレートには Lambda 関数実行用のデバイスを設定します。\nLambda 関数実行の場合は必須のプレイスメント属性は無いので、適当に属性と値を設定しておきます。\nこれでプロジェクトは完成です。\n引き続いてプレイスメントを作成します。\nプレイスメントの作成では、プレイスメント名と、デバイステンプレートで用意したデバイスの枠にデバイスを設定、あとはプレイスメントの属性を設定します。\nで、今回の目的を達成するためのプロジェクトの設定が次のようになります。(上のキャプチャと名前が異なる部分があります)\nこれでプロジェクトの作成とプレイスメントの設定が完了したので、あとは AWS IoT エンタープライズボタン を押すだけです\u0026hellip;!\n5. AWS IoT エンタープライズボタン を押す ということで、とりあえず 1 回押してみます。すると数秒後に Slack にメッセージが飛んできます。\nこれが IoT \u0026hellip;。\n上のキャプチャのとおりですが、 IoT 1-Click から Lambda に渡される event の内容はこんな json になっています。\n{ \u0026#34;deviceInfo\u0026#34;: { \u0026#34;deviceId\u0026#34;: \u0026#34;G030AB12345678CD\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;button\u0026#34;, \u0026#34;remainingLife\u0026#34;: 99.35, \u0026#34;attributes\u0026#34;: { \u0026#34;projectRegion\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;projectName\u0026#34;: \u0026#34;MyProject\u0026#34;, \u0026#34;placementName\u0026#34;: \u0026#34;MyPlacement\u0026#34;, \u0026#34;deviceTemplateName\u0026#34;: \u0026#34;runLambda\u0026#34; } }, \u0026#34;deviceEvent\u0026#34;: { \u0026#34;buttonClicked\u0026#34;: { \u0026#34;clickType\u0026#34;: \u0026#34;SINGLE\u0026#34;, \u0026#34;reportedTime\u0026#34;: \u0026#34;2019-07-08T12:38:25.312Z\u0026#34; } }, \u0026#34;placementInfo\u0026#34;: { \u0026#34;projectName\u0026#34;: \u0026#34;MyProject\u0026#34;, \u0026#34;placementName\u0026#34;: \u0026#34;MyPlacement\u0026#34;, \u0026#34;attributes\u0026#34;: { \u0026#34;floor\u0026#34;: \u0026#34;99\u0026#34;, \u0026#34;room\u0026#34;: \u0026#34;999\u0026#34;, \u0026#34;building\u0026#34;: \u0026#34;9\u0026#34; }, \u0026#34;devices\u0026#34;: { \u0026#34;runLambda\u0026#34;: \u0026#34;G030AB12345678CD\u0026#34; } } } とりあえず 1 回 と書いたのには訳があって、 AWS IoT エンタープライズボタン では次の 3 パターンの押し方を認識します。\n1 回 2 回 長押し (2秒くらい) そして、それぞれの押し方によって渡される event 内の deviceEvent.buttonClicked.clickType が SINGLE 、 DOUBLE 、 LONG になります。\nLambda 側で何か処理を分岐させる場合は、プレイスメントの属性と、このクリックタイプ を条件にすると良さそうです。\nちなみに deviceInfo.remainingLife は AWS IoT エンタープライズボタン の寿命になっています。ボタン自体は約 2,000 回押すことができると書かれていますが、プログラム上で扱うにはこの値を使って「あと○回押せます」とかもできそうです。\nまとめ AWS IoT エンタープライズボタン を使って Lambda を実行して Slack にメッセージを送ってみたという話でした。\nいままで IoT 関係にはとっつきにくかったんですが、びっくりするくらい簡単に IoT っぽいことができました。Lambda が実行できるということはほぼなんでもできるということなので、発想次第では本当にいろんなことができそうです。\nIoT 関係に踏み込みたいけど何から始めていいのかよくわかわんっていう人には、 AWS IoT エンタープライズボタン おすすめです。\n",
    "permalink": "https://michimani.net/post/aws-iot-1-click-tutorial/",
    "title": "AWS IoT エンタープライズボタンで Lambda 関数を実行して Slack にメッセージを送信してみた"
  },
  {
    "contents": "技術ブログでよく見る「この記事は公開されてから1年以上経過しています」というメッセージを、 Hugo で作ったブログの記事にも表示するようにしてみました。\nやること 下記の JavaScript を head 内に記述します。\nfunction showInfoNotifiesOldPost() { const nowDate = new Date(); const oldTs = 60 * 60 * 24 * 365 * 1000; // 1 year (millisecond) const nowTs = nowDate.getTime(); const postDateElem = document.querySelector(\u0026#39;p.post-date \u0026gt; time.dt-published\u0026#39;); if (postDateElem !== null) { try { const postDateStr = postDateElem.getAttribute(\u0026#39;datetime\u0026#39;); const postDate = new Date(postDateStr); const postTs = postDate.getTime(); if (nowTs - postTs \u0026gt; oldTs) { const entrySectionElem = document.querySelector(\u0026#39;section.content.e-content\u0026#39;); if (entrySectionElem !== null) { const infoElem = \u0026#39;\u0026lt;div id=\u0026#34;old-entry-notifier\u0026#34;\u0026gt;この記事は公開されてから 1 年以上経過しています。情報が古い可能性がありますので、ご注意ください。\u0026lt;/div\u0026gt;\u0026#39;; entrySectionElem.insertAdjacentHTML(\u0026#39;afterbegin\u0026#39;, infoElem); } } } catch { console.warn(\u0026#39;failed to get post date.\u0026#39;); } } } window.onload = () =\u0026gt; { showInfoNotifiesOldPost(); }; スタイルを調整します。\n#old-entry-notifier { background-color: #ffcc80; border: 2px solid #ff6d00; font-weight: 900; margin: 15px 0; padding: 15px; } 以上です。\nshow-old-entry-message.js | gist ",
    "permalink": "https://michimani.net/post/development-show-old-entry-message/",
    "title": "Hugo で作ったブログの古い記事にメッセージを表示する"
  },
  {
    "contents": "AWS 認定の入り口と言ってもいい存在のソリューションアーキテクトアソシエイト (SAA) の試験を先日受けてきて、無事に合格できました。受験に至った経緯とか対策とか、そのあたりについて書いていきます。\n※追記\nアソシエイトレベル残り 2 つの試験 (DVA, SOA) についても受験記を書きました。\n目次 AWS 認定について 受験した経緯 AWS (とその他) の経験値 AWS 以外のこと AWS のこと 受験までの流れ 受験日の決定・予約 試験対策 受験日当日 まとめ AWS 認定について 諸々の話の前に、AWS 認定についておさらいです。概要は AWS 認定についての公式ページ から拝借した下の図をご覧ください。\n冒頭でソリューションアーキテクトは AWS 認定の入り口と書きましたが、本来なら クラウドプラクティショナー がその位置付けになると思います。クラウドプラクティショナー試験では、 AWS クラウドとは何か 、 AWS クラウドの価値提案について 、 請求、アカウントマネジメント、料金モデル といった AWS の基本的な部分が出題範囲となっています。受験対象としても \u0026ldquo;Six months of fundamental AWS Cloud and industry knowledge\u0026rdquo; となっているので、業界に入ってから、または AWS を触り始めて半年くらいのレベルということになります。\nAWS 認定クラウドプラクティショナー | AWS クラウドプラクティショナー以外では、大きく分けて下記の 3 つのカテゴリがあります。\nArchitect - アーキテクト Operations - 運用担当者 Developer - 開発者 それぞれその名前の通り、自分がどういう分野で勉強、成長していくかのパスになっています。詳しくは AWS 認定の概要ページ にある 学習パス のところを見てください。\nそして上記のカテゴリに対して、 Associate - アソシエイト と Professional - プロフェッショナル の 2 つのレベルが用意されています。それぞれのレベルの受験対象としては、\nアソシエイト・・・AWS を使用して問題を解決するソリューションの実装経験が 1 年程度 プロフェッショナル・・・AWS を使用したソリューションの設計、運用、およびトラブルシューティングの総合的な経験が 2 年程度 とされています。\nまた、これらの他に Speciality レベルとして、ネットワーク、セキュリティ、Alexa Skil Builder といった、より細かい分野に対する認定試験も用意されています。\n上記をふまえて、今回受験したのは、アーキテクトのアソシエイトレベルということになります。\n受験した経緯 AWS を触り始めてちょうど 2 年くらい経つので、自分の AWS に関する知識の確認がしたいと思ったのがきっかけです。\nまた、先日開催されていた AWS Summit では認定者ラウンジというものがあり、 AWS 認定試験に合格している人のみが入れるスペースとなっていました。今後もそういったイベントで何かしら恩恵を受けられそうというのもきっかけの一つです。\nAWS の 経験は 2 年なのでプロフェッショナルレベルも対象になってきますが、ひとまずはアソシエイトレベルを確実に、という感じです。\nAWS (とその他) の経験値 受験前の私の技術的な経験値としてはだいたい下記のとおりです。\nAWS 以外のこと 技術者歴 6 年 主に LAMP 環境での WEB アプリケーション開発 基本情報技術者試験は持ってる AWS のこと EC2 上で LAMP 環境構築して WEB アプリケーションの開発 オンプレで動いていた WEB サービスを AWS 環境に移行 (Route 53, ELB, EC2, RDS, S3, CloudFront\u0026hellip; といった基本的なアーキテクチャ) Route 53 + CloudFront + S3 で静的 WEB サイトのホスティング CodePipeline (CodeCommit, CodeBuild, CodeDeploy) で CI/CD の設定 API Gateway + Lambda + DynamoDB で簡単な API の作成 CloudWatch Event で Lambda の定期実行 その他、チュートリアルレベルで触ったことがあるサービス Athena, Cognito, Polly, Rekognition \u0026hellip; おそらく AWS で運用/開発をする際に使うであろうサービスについて、基本的な部分は触ったことがある という感じです。\nちなみに 業務内 : 業務外 の割合は、ざっくりとですが 4 : 6 くらいです。業務ではどうしても偏ったサービス (EC2, S3, RDS, Lambda くらい) しか使わないので、プライベートの時間で他のサービスを触ってました。\n受験までの流れ 受験しようと思ってから受験日当日までの流れは下記の通りです。\n受験日の決定・予約 試験対策 受験日当日 この間、約 1 ヶ月です。個人的には期日が決まっていた方がやる気が出るので、試験対策の前に受験日を決めてしまいました。\n受験日の決定・予約 試験の予約の詳しい手順については割愛します。\n東京近郊であれば受験できるテストセンターはたくさんあるので、受験日はほぼ選び放題という印象です。ただしテストセンターによっては平日のみしか受験できなかったりするので、そこは注意が必要です。\n私は土日でも受験可能な 銀座CBTS歌舞伎座テストセンター (Google Map) を選択しました。\n日比谷線 東銀座駅直結なのでアクセスしやすいです。\nちなみに SAA 本試験の受験料は、 15,000 円 (税抜) です。\n試験対策 やったことを順番に書き出してみます。\nサンプル問題を解く 間違えた問題に関するサービスについて調べる (ざっくり) 模擬試験を受験する 間違えた問題に関するサービスについて調べる (追い込み) 以上です。ひとつずつ詳しくみていきます。\nサンプル問題を解く まずは SAA にどんな問題が出題されるのかを確認するために、 AWS 公式ページからダウンロードできるサンプル問題を解きました。\nAWS 認定ソリューションアーキテクト – アソシエイト | AWS 問題数は 10 問だけですが、出題される問題の雰囲気はつかめると思います。\nまずこの問題を解いた時点で 8 問は正解していたので、案外いけるのでは？と思っていました。\n間違えた問題に関するサービスについて調べる (ざっくり) サンプル問題で間違えた問題に関するサービス、初めて聞くサービスについて概要を確認します。参考にするページは、 AWS の各サービスの公式ページです。\n例えば私の場合は Elastic Beanstalk ってなんやねん って感じだったので、下記ページを参照しました。\nAWS Elastic Beanstalk（ウェブアプリの実行と管理）| AWS 各サービスのページにはそれぞれ 概要、特徴、料金、開始方法、リソース、よくある質問 という項目があるので、各項目を確認します。確認した要点としては以下のような点です。\n何ができるサービスなのか どういった状況で使うサービスなのか 料金発生の仕組み (単位時間、容量、インスタンスタイプなど) 利用する際に発生する疑問点 (よくある質問 の項目で確認) ここで \u0026quot;(ざっくり)\u0026quot; と書いているのは、まずはサービスそのものについてアバウトでもいいので把握することを目的としているからです。\nこの時点ではまだサンプル問題の 10 問しか解いていないので、深掘りするにも対象が少なすぎます。一旦は概要を抑えて、次の模擬試験に備えます。\n模擬試験を受験する 認定試験の予約と同じ場所 (ページ) から模擬試験の予約をすることができます。ここも詳しい手順については割愛します。\n予約といっても日時を決めるわけではないので、受験料 2,000 円 (税抜) を支払うとすぐに受験できます。\nちなみに、私は本試験の 1 週間前に模擬試験を受験しました。\n問題数が 25 問、制限時間が 30 分であること、自分の PC で受験できること以外は、画面の構成など本試験と同様です。なので、試験問題もそうですが、試験画面にも慣れることができます。\n模擬試験を受験するにあたっては、本試験に向けて下記のことを実施できると良いです。\n言語設定を英語に切り替えて、英語の出題文を確認する 問題にフラグを付ける、前の問題に戻る、回答済みの問題の回答を編集する といったアクションを試す わからなかった問題、サービスについてメモしておく まず英語の問題文を確認する理由は、日本語の問題文は多少違和感のある表現がされている場合があるからです。その場合は原文 (English) でも確認したほうがよいです。これは本試験でも同様です。\n模擬試験の時点で英語の問題文に慣れておけば、本試験での読解にも役立つでしょう。言語の切り替えは何度でも実施できるので、安心して切り替えましょう。(ずっと英語になったりはしません)\nまた、フラグをつけたり前の問題に戻ったりといったアクションは本試験では必ず行うことになるでしょう。というのも、ほぼ確実に時間が余るので見直しの時間をとることができるからです。\n模擬試験の時点でそれらの操作に慣れておけば、本試験での操作もスムーズに行えます。\n問題を解いているときにわからなかった内容、理解が及んでいないと感じた内容についてはメモしておきます。\n問題の画面をスクショするという方法もありますが、場合によっては AWS 認定試験の NDA に抵触する可能性も無くもないので自己責任でお願いいたします。\nとは書きましたが、メインは問題を解くことなので、頑張って解きます。\n模擬試験の受験後はすぐに正答率がわかります。ただし、各問題の正解/不正解がわかるわけではなく、下記のように問題の分野別の正答率と全体の正答率がわかります。\n総合スコア: 76%\n1.0 Design Resilient Architectures: 100%\n2.0 Define Performant Architectures: 57%\n3.0 Specify Secure Applications and Architectures: 66%\n4.0 Design Cost-Optimized Architectures: 50%\n5.0 Define Operationally-Excellent Architectures: 100%\n模擬試験を解いた後は、わからなかった問題、サービスに関して再度内容を確認します。\n間違えた問題に関するサービスについて調べる (追い込み) 模擬試験のあとは、ひたすらサービスの概要について復習します。ただし、やみくもに各サービスの概要を頭に詰め込むのは非効率です。\n模擬試験、および本試験でも下記の内容が占める割合が (体感で) 非常に大きかったので、集中してやっておくとよいでしょう。\nネットワーク関連 (VPC, ELB, NAT ゲートウェイ などの構成方法) 各ストレージサービスの使いどころ (EBS, EFS, S3) S3 の各ストレージクラスの使いどころ (参照: ストレージクラス - Amazon S3 ｜AWS ) ファイル、S3 オブジェクト、ストレージの暗号化について (どこで(サーバ/ローカル)、何を使って(KMS, ローカルの鍵など)) 各サービスのスケーリングについて (EC2 Auto Scaling, そもそも勝手にスケーリングしてくれるのか) もちろん上記以外についても満遍なく出題されます。\n上で \u0026ldquo;ざっくり\u0026rdquo; とは書きましたが、各サービスの概要 (できること、用途) についてはしっかり把握して追う必要があります。そうすれば、本試験中でも焦らずにその場で考えることができます。\nというか、そういう形で準備するのがベストかもしれません。\n受験日当日 受験日当日 (前日) と受験中に気をつけることは下記の点です。\n体調を整える (一番大事) 身分証明書を忘れない (2 種類) \u0026ldquo;15 分前までに\u0026rdquo; とあるが、 30 分前くらいには会場へ わからなければとりあえずフラグをつけて次の問題へ進む まず、体調を整える。これが一番大事ですね。\n前日は胃に優しいものを食べて早めに寝る。たったこれだけです。私は前日の夜 23 時くらいに牛丼を食べてしまったために当日は腹痛との戦いでした。体調は万全の状態で臨みましょう。\n次に身分証明書です。\n2 種類の証明書が必要ですが、運転免許証とクレジットカード (裏面に署名済みのもの) で問題ありませんでした。\n次に会場への到着時間についてです。\n事前にくる案内では \u0026ldquo;15 分前までに\u0026rdquo; という記述がありましたが、 30 分前に行って正解でした。今回の受験会場である 銀座CBTS歌舞伎座テストセンター へは 10:00 にならないと入れません。(エレベーターが 10:00 以降でないとそのフロアに止まらない)\nということで 10:00 に行ったんですが、すぐに試験受付の列ができていました。もちろん遅く行ったからといって受験できなくなるわけではありませんが、さっさと受付を済ませて、トイレも済ませて、ゆっくり試験開始時間を待つには 30 分前くらいについていたほうが良さそうです。\n最後に問題の進め方です。\n本試験は 130 分間で 65 問を解きます。130 分には NDA への同意や受験後のアンケート時間も含まれますが、単純に計算して 1 問あたり 2 分の時間があります。問題によっては 2 分もしくはそれ以上の時間を要するようなものもありますが、問題文さえ読めば即答できるような問題も多々あります。 その結果、私の場合は 60 分で一通り解答し、その後 45 分くらいかけてしっかり見直しをし、十数分は時間を余らせて途中退出しました。\n時間はほぼ確実にあまるので、わからない問題にはフラグを付けて、あとで見直すようにしましょう。\n結果について 試験の合否は、受験終了後のアンケートに回答したあとすぐに表示されます。\n合格していると「おめでとうございます」的な文章が表示されます。ただし、正式な合格通知、証明書 (PDF) などについては 5 営業日以内に連絡がきます。私の場合は翌日には連絡が届いていました\u0008。\nまとめ AWS 認定ソリューションアーキテクトアソシエイトに合格しましたということで、それまでの過程について書きました。\n今回無事に合格できましたが、正直 試験中は合格は厳しいかなという思いでした。ほんとです。\n体調が芳しくなかったというのもありますが、明らかに問題慣れしてないなと感じました。私が試験対策として解いた問題は、サンプル問題 10 問と模擬試験の 25 問のみです。本試験の中にはそれら 35 問の中にあったものとほぼ同じ問題もありましたが、多くは初めて目にするような問題でした。\nそうなると、各サービスに対する理解を元にその場で考えて解いていく必要があるのですが、そうなる数が多くて大変でした。\nもし、より心理的に安心した状態で受けようと思うのであれば、いわゆる認定試験対策本などを利用して問題の出題形式にもっと慣れておく必要があるかなと感じました。\nともあれ合格は合格なので、これをゴールにせずに引き続き AWS に関する理解を深めていきたいと思います。また、今年中には他のアソシエイトレベルの試験にもチャレンジして、認定バッチの数を増やしたいですね。\nあと、これで今後は AWS の各種イベントで設置される可能性のある 認定者ラウンジ にも入ることができるのも嬉しいですね。\nソリューションアーキテクトアソシエイトを受験しようを思っている人の参考になれば幸いです。\nもし銀座CBTS歌舞伎座テストセンターで受験しようと思っている方は、合格したら 梅林 でトンカツ食べる！という思いで頑張りましょう。(私は混んでいたので諦めました)\n2年前に食べたミックス定食 ",
    "permalink": "https://michimani.net/post/aws-solution-architect-associate-exam/",
    "title": "AWS 認定ソリューションアーキテクトアソシエイトに合格しました"
  },
  {
    "contents": "S3 Batch Operations を使って S3 に保存された数万件の画像を一括でリサイズしてみた話です。\n目次 TL;DR やりたかったこと やったこと リサイズ処理用の Lambda 関数を実装する S3 インベントリレポートを作成する S3 Batch Operations のジョブを作成する S3 Batch Operations のジョブを実行する 料金には注意 まとめ TL;DR S3 Batch Operations めちゃくちゃ便利。ただし料金には注意。\nやりたかったこと S3 のとあるバケットに保存されている画像を一括でリサイズする リサイズの処理は Python 3.7 で実装する やったこと リサイズ処理用の Lambda 関数を実装する 「バッチオペレーション リサイズ」などでぐぐると、だいたい Node.js での実装例が出てきます。現時点で Node.js が書けないという言い訳をしながら、今回は Python でリサイズ処理を実装しました。ソースは GitHub に置いています。\nmichimani/resize-s3-image: This function resizes the image in Amazon S3. This function is supposed to be called by Amazon S3 Batch Operations. リサイズに関しては Python の画像処理ライブラリ Pillow を使ったので特に難しくはなかったです。が、一つ前の記事でも書いた通り、 Pillow には OS に依存する部分があるため、デプロイパッケージとして Lambda にアップロードする際に工夫が必要でした。\nローカルの PC (今回は macOS) でインストールした Pillow をそのままアップロードしたところ、[ERROR] Runtime.ImportModuleError: Unable to import module 'lambda_function': cannot import name '_imaging' from 'PIL' (/var/task/PIL/__init__.py) が発生し、実行できませんでした。\nLambda は Amazon Linux 上で実行されるため、Amazon Linux 上でインストールした Pillow が必要になります。ということで今回は Amazon Linux のコンテナイメージを使って Pillow ライブラリ群を準備しました。Docker の利用もほぼ初でしたが、なんとか無事にリサイズ処理実行までたどり着けました。\nS3 インベントリレポートを作成する まずは S3 Batch Operations の対象となる S3 オブジェクトの一覧を準備します。おそらく手作業でも作成できますが、今回は対象が数万件にも及ぶため S3 のインベントリレポート機能を使います。\n対象となるバケットの 管理 タブで インベントリ を選択します。\nインベントリ名\n任意の名前を入力します フィルター\nバケット内の特定のパスのみを対象にしたい場合にはここでプレフィックスを指定します。今回はバケット直下の blog_images というフォルダ内にあるオブジェクトを対象としたかったので、 blog_images と入力しました。/ は不要です。 送信先バケット\nインベントリレポートとして出力されるオブジェクトの保存場所です。任意の S3 バケットを指定します。 送信先プレフィックス インベントリレポートとして出力されるオブジェクトのプレフィックスです。入力しない場合は、 s3://送信先バケット/インベントリ名/ 以下にインベントリレポートのオブジェクトが生成されます。 頻度\nどれくらいの頻度でレポートを生成するか選択します。今回はとりあえず 1 回生成できれば良いので 日別 としておいて、レポートが作成されたのちにインベントリの設定を削除しました。 インベントの設定をしてから最初のレポートが作成されるまでには 最大 48 時間 かかります。これ以上経っても作成されない場合は何かしら設定が間違っている可能性があります。私の場合はフィルターの指定で末尾に / を付けていたため生成されませんでした\u0026hellip;。\nS3 Batch Operations のジョブを作成する S3 Batch Operations は S3 コンソールの左にある バッチオペレーション から設定します。ジョブは 新規作成 または 以前に実行したジョブを クローン する形で作成します。クローンした場合は諸々の入力欄をクローン元のジョブから引き継ぎます。\nリージョン は、今回は アジアパシフィック(東京) を選択します。\nマニフェスト とは、 Batch Operations の対象となるオブジェクトを指定するものです。ここで、事前に作成したインベントリレポートに含まれるオブジェクトをマニフェストとして指定します。\nマニフェストの指定方法としては、インベントリレポートとして生成される manifest.json または、 CSV を使用します。CSV については、インベントリレポートが生成されたバケット内に data/{ランダムな英数字}.csv.gz という名前で保存されているので、これを解凍して使用できます。今回は manifest.json を使用するので、 マニフェストオブジェクトへのパス で manifest.json へのパスを指定します。\nオペレーション としては画像ファイルのリサイズなので、 AWS Lambda を選択して、実行する関数、バージョンを指定します。実行する関数は、上で書いた関数です。\n追加オプション に関しては、オプションなので適宜設定内容を変更してください。\n完了レポート として出力されるのは、対象となる各オブジェクトに対して処理が成功したかどうかなどが含まれます。\nアクセス許可 については、この Batch Operations に付与するアクセス権のことです。具体的には、対象となるオブジェクトが保存されているバケットに対する Get* の権限と、完了レポートを送信するバケットへの Put* の権限です。また、 IAM ロールの信頼関係として、 batchoperations.s3.amazonaws.com を指定します。\nこれらの設定ついては、 IAM ロールのポリシーテンプレートと IAM 信頼ポリシーを表示 をクリックするとポリシー設定用の json が表示されるので、それをそのまま使用して IAM ポリシー、 IAM ロールを作成します。\n内容を確認してジョブを作成します。\nS3 Batch Operations のジョブを実行する ジョブを作成した時点では、まずは対象となるオブジェクトの合計が計算されます。\n合計が計算されると、ステータスが 確認待ち となります。確認待ちになったら、ジョブ ID をクリックしてジョブの詳細を表示します。\nそして、画面上部に表示されている 確認して実行 ボタンを押します。ここで初めてジョブが実行されます。画面上にも表示されていますが、確認待ち の状態で一定期間放置すると、ジョブは失敗扱いとなります。\n確認して実行 ボタンを押すと、すぐにジョブが始まります。ステータスとしては アクティブ → 完了中 → 完了 と遷移します。\n今回の対象は 57,568 件でしたが、実行ボタンを押してから完了になるまでにかかった時間は、 3 分程度でした。\n料金には注意 こんな便利な S3 Batch Operations ですが、 1 回の実行につき 0.25 USD の料金が発生します。対象オブジェクトの個数に関係無くです。\n短時間で処理が終わるからと言ってなんども実行するのはやめましょう。\nまとめ S3 Batch Operations を初めて使ってみた。 数万件のオブジェクトに対する処理が短時間で完了する。すごい。 料金には注意。 Lambda のデプロイパッケージには OS 依存に注意する。 ",
    "permalink": "https://michimani.net/post/aws-resize-images-in-s3-with-batch-operations/",
    "title": "S3 Batch Operations で S3 に保存された数万件の画像を一括でリサイズしてみた"
  },
  {
    "contents": "TL;DR Lamnbda のデプロイパッケージに含める Pillow のライブラリ群は、 Amazon Linux 2 でインストールしたものを使用する。\nPillow に限らず、Lambda 関数に外部パッケージ (ライブラリ) を含める場合は、 OS 依存の有無に注意が必要。\n何がしたかったのか やりたかったこと S3 に保存してある画像に対して、 S3 Batch Operations で Lambda を呼んでリサイズをする。\n割とよくあるオペレーションだと思うんですが、調べてみると Node.js でやってるパターンばかりで Python でやってるパターンが見つけられませんでした。なので、リサイズ処理用の Lambda 関数は Python で実装してみることにしました。\nランタイムは Python 3.7 にします。\nやったこと Python での画像処理には Pillow を使うと楽なので、今回も Pillow を使うことにします。\nLambda の Python 実行環境には Pillow は存在しないので、ローカルでインストールしたライブラリ群をデプロイパッケージとして zip ファイルにまとめて Lambda にアップロードする必要があります。\nPillow は pip でインストールします。\n$ pip3 install Pillow -t dist/ デプロイパッケージにはメインの python ファイルも含めます。\ndist ├── PIL ├── Pillow-6.0.0.dist-info └── lambda_function.py 上記のようなフォルダを zip 化して Lambda にアップロードします。\n起こったこと いざアップロードした関数を実行してみると、次のようなエラーが出ました。\n[ERROR] Runtime.ImportModuleError: Unable to import module \u0026#39;lambda_function\u0026#39;: cannot import name \u0026#39;_imaging\u0026#39; from \u0026#39;PIL\u0026#39; (/var/task/PIL/__init__.py) ローカル環境では起こっていなかったエラーです。\n解決するまでの流れ 色々試してみる 解決するまでに確認・試したことは次のようなことです。\nローカルと Lambda のランタイムの Python バージョンを合わせる Python 3.6 でやってみる これでは解決しませんでした。\n調べてみると、どうやら Pillow のライブラリ群にはインストールする OS に依存する部分があることがわかりました。ということは、 Lambda が実行される環境と同じ環境でインストールした Pillow が必要ということにあります。\nLambda の実行環境 AWS Lambda Runtimes - AWS Lambda 上記ページにもあるように、 Lambda の実行環境 (OS) は Amazon Linux です。なので、 Amazon Linux 上でインストールした Pillow が必要になります。\nAmazon Linux 上でインストールした Pillow の取得 シンプルというか、まず思い浮かべるのは Amazon Linux の EC2 インスタンスを立てて、そこでインストールした Pillow を使う方法です。\nただ、これだけのために EC2 インスタンスを立てるのは面倒で、且つ多少なりとも料金が発生してしまいます。\nここで使うのが Docker ですね。 ドッカードッカー言われてもう結構経ちますが、まだほとんど触ったことがなかったので良い機会になりました。\nAmazon Linux の docker イメージは dockerhub で公開されているので、それを使います。\namazonlinux - Docker Hub やりたいことは以下の通りです。\nPython 3.7 をインストールした Amazon Linux のコンテナを作成する Amazon Linux コンテナ内で、ホストと同期したディレクトリに Pillow をインストールする です。\nDockerfile と docker-compose.yml は次のような感じです。\nFROM amazonlinux:latest RUN yum install python3 -y RUN mkdir /home/deploy version: \u0026#39;2\u0026#39; services: app: build: . volumes: - \u0026#39;./deploy:/home/deploy\u0026#39; command: pip3 install -r /home/deploy/requirements.txt -t /home/deploy/dist これを下記のようなディレクトリ構成にして docker-compose up --build を実行すると、 dist ディレクトリに Amazon Linux 環境でインストールした Pillow のライブラリが保存されます。 (というか同期してるのでここにインストールされる)\n. ├── Dockerfile ├── deploy │ ├── dist │ └── requirements.txt └── docker-compose.yml あとは、ライブラリ群と Python ファイルを zip 化してアップロードすれば ok です。\n無事に Lambda で Pillow が使えた AWS Lambda で Pillow を使おうとしたらハマった話でした。なんとか無事に Pillow を使うことができました。\nそして S3 Batch Operations で数万件の画像が一瞬でリサイズされて、すげーってなってました。\n今後はデプロイパッケージに含める外部ライブラリについては、 OS 依存の有無に限らず Amazon Linux 環境でインストールしたものを使うようにしたいと思います。\n実際にできあがっものは GitHub に置いています。\nmichimani/resize-s3-image: This function resizes the image in Amazon S3. This function is supposed to be called by Amazon S3 Batch Operations. ",
    "permalink": "https://michimani.net/post/aws-use-pillow-in-lambda/",
    "title": "AWS Lambda で Pillow を使おうとしたらハマった"
  },
  {
    "contents": "みなさんトラッキングしてますか。\nこれまでスマートウォッチとしては Apple Watch を使っていたんですが、色々あって別のスマートウォッチを買うことになりました。そして買ったのが、 Fitbit のフィットネストラッカー Fitbit Inspire です。\nFitbit Inspire を買うことになった経緯 Apple Watch があるのになぜ？ そもそも Apple Watch 以外のスマートウォッチを買うことになった経緯としては、先月発売された Google のスマートフォン Pixel 3a を買ったからです。\nスマホは iPhone 4 時代からずっと iPhone を使っていたんですが、仕事で検証用端末として Pixel 3 を触ったときに良いなと思ってしまい\u0026hellip;ただ Pixel 3 は結構なお値段がするのでスルーしてました。そんなところに廉価版の Pixel 3a が発売されるということで、初の Android 端末として迎え入れることになりました。\nと、長々と書きましたが、 Android 端末でも諸々の通知を受けられるスマートウォッチが欲しい というのが、 Apple Watch 以外のスマートウォッチを買うことになった経緯です。\nなぜ Fitbit Inspire ？ Apple Watch 以外のスマートウォッチといっても選択肢が多すぎました。とりあえず次のような条件で探しました、\nスマホの通知を受け取れること 振動で通知してくれること 価格は 10,000 円くらい この条件だと Fitbit 、 GARMIN の各シリーズが候補にあがってきます。\n値段的には GARMIN の Vivosmart 3 が一番お手頃だったんですが、2年前くらいのモデルだったので、今年になってから発売された Fitbit Inspire \u0008にしました。\nちなみにこれくらいの価格帯だと、スマートウォッチというよりはフィットネストラッカー、いわゆる活動量計みたいなものがほとんどです。Fitbit Inspire も、公式ページには フィットネストラッカー と書かれています。\nFitbit Inspire \u0026amp;amp; Inspire HR | 健康づくり \u0026amp;amp; フィットネス用トラッカー Fitbit Inspire の概要 箱はこんな感じ。\n同梱物は、\nFitbit Inspire 本体 充電ドック 説明書 です。\n説明書は複数言語で書かれていて、日本語でも書かれています。ただセットアップ方法については触れられていないので、最初に見ても得られる情報はありません。\n充電ドックはマグネットで本体と接続できるので、 Apple Watch と同じですね。\nセットアップはスマホ経由で 上に書いた通り、説明書にはセットアップ方法が書かれていません。最初に本体の電源を入れると、画面上に \u0026lt;code\u0026gt;https://www.fitbit.com/jp/setup\u0026lt;/code\u0026gt; にアクセスしろとでるので、リンク先のページに従ってセットアップを進めます。\n必要な時間は 10 ~ 15 分 程度です。\nセットアップは iOS, Android, Windows, macOS で行うことができますが、今回は Pixel 3a との同期ということで、 Play Store から Fitbit アプリをインストールして、 Pixel 3a 端末でセットアップを進めます。\nまずは Pixel 3a と Fitbit Inspire を Bluetooth で接続します。\nその後は Fitbit アプリに従ってセットアップをしてきます。\nFitbit のモデルを選択。\nFitbit Inspire の画面上にコードが表示されるので、それをアプリ内で入力して接続を完了させます。\nそのあとは Fitbit Inspire のソフトウェアアップデートが始まります。これが 10 分弱くらい時間がかかります。\nただしアプリのセットアップはそのまま続けることができ、その間は次のような tips をアニメーションと説明文で紹介してくれます。\nFitbit Inspire の付け方 ベルトの交換方法 操作方法 メンテナンス方法 Fitbit Inspire の画面上でアップデートが完了すれば、セットアップ完了です。\n実際に使ってみて良かった点・悪かった点 では肝心のレビューに入りたいと思います。購入してから 2 週間くらい経つので、その間に感じたことを書いていきます。\n良かった点 バッテリーの持ちが良い 公式ページのスペック欄には 5日間連続使用可能 と書かれていますが、ほぼこの通りですね。バッテリー残量 100 % の状態から 4.5日くらい経過したところで残量が 20 % 切りました。\nApple Watch は基本的に毎日充電が必要になるので、充電回数が少ないのは良いですね。\n充電速度が速い バッテリーの持ちが良いと、逆にどのタイミングで充電するかが難しくなってきます。ただ、 Fitbit Inspire に関しては充電速度が非常にはやく、たとえばシャワーを浴びている 20 分程度の充電時間でも、少なくとも 2 日くらいは使用できるくらいに充電できます。\nなので、トラッキングが途切れても良い數十分を充電時間に充てるようにすれば、ほぼ 24/365 のトラッキングが可能です。\n睡眠トラッキングが正確 一日中トラッキングできるということは、睡眠時間もトラッキングすることができます。このトラッキング精度がなかなか高いです。\n就寝・起床時刻はほぼ体感と同じくらいなので、毎日の睡眠時間はしっかりと管理できます。\n悪かった点 スマホの通知が端末と Inspire の両方に来る Apple Watch の場合、 iPhone と接続している状態での通知は Apple Watch のみに届きます。しかし Fitbit Inspire の場合は、まず Android 端末に通知が来て、その 0.5 ~ 1 秒後くらいに Inspire のほうに通知が来ます。なので、例えば端末を 机の上に置いていたりすると、机が震えてから手首も震えるみたいな状態になります。\nこればっかりは 端末専用に作られたスマートウォッチではないため実現は難しいのかもしれません。ただ、不便ですね。\n歩数のトラッキング精度が低い Fitbit Inspire を使用している期間も引き続き Apple Watch も装着していたのですが、歩数に関して Apple Watch での計測値と大きく差が出ている日がありました。\nfitbitのinspireとApple Watch。歩数800も誤差出る？🤔 pic.twitter.com/2FkUxEuiWk\n\u0026mdash; よっしーCBR796RR (@michimani210) 2019年5月22日 さすがに5万歩はない。睡眠トラッカーくらいでしか使いみちがないかも。ちなみにApple Watchでは9,500歩。体感的にもそれくらい。 pic.twitter.com/InVkM49WTm\n\u0026mdash; よっしーCBR796RR (@michimani210) 2019年5月27日 通知内容の視認性が悪い Fitbit Inspire のディスプレイは縦長ですが、端末からの通知内容は横方向からスライドしてきます。なので、例えばメールの通知を受けた時は、ディスプレイ上に 2 ~ 3 文字ずつスライドしてくる感じです。通知自体を受け取ることは可能ですが、内容まで確認するのは難しいと感じました。\n現在の使い方 購入から 2 週間経って現在の Fitbit Inspire の使い方は、専ら 睡眠トラッキング です。\n日中は装着せず、帰ってきてなんやかんやしてそろそろ寝るかなとなった時点で装着してます。で、翌日起きて Apple Watch をつけるタイミングで外してます。\nじゃあ Pixel 3a からの通知はどうするんだ！という話ですが、Pixel のほうでも色々あって結局 iPhone + Apple Watch の生活に戻っています。 Pixel 3a についてはまた別で書きます。\nまとめ Fitbit のフィットネストラッカー Fitbit Inspire のレビューでした。\n私の結論としては、睡眠トラッカーとしてはすごく良いという感想です。価格的にも 10,000 円程度なので手は出しやすいと思います。\nちなみに Fitbit Inspure には上位モデルの Inspire HR があります。こちらは 心拍数トラッキングが可能だったり、水泳のデータ計測に対応していたりと、結構高機能になっています。お値段は 約 20,000 円と、Inspire のおよそ 2 倍です。\n高機能とはいえ 20,000 円か\u0026hellip;と正直思います。\n単純に睡眠時間をトラッキングしたいというだけであれば Inspire で十分かなと思いますね。\n",
    "permalink": "https://michimani.net/post/gadget-fitbit-inspire-review/",
    "title": "[レビュー] Fitbit のフィットネストラッカー Fitbit Inspire を 2 週間くらい使ってみた"
  },
  {
    "contents": "Amazon SES (Amazon Simple Email Service) は 企業や開発者のための、フレキシブルで可用性が高く、手頃な価格の E メール送受信プラットフォーム です。AWS で提供されているメール送受信サービスということで、他の AWS のサービスとの連携が容易です。\n今回は、 Amazon SES (以降、 SES) と AWS Lambda を使って、Gmail で受信したメールを Amazon SES 経由で Slack に通知する仕組みを実装してみました。\n概要 やりたいこと Gmail で特定の差出人から受信したメールを Slack に通知したい。 今回の前提条件 バージニア北部リージョン (us-east-1) を利用する。 Route 53 で管理しているドメインがある。 Slack のプランはフリープランである。 Slack の以下の情報を取得済みである。 Incoming WebHooks の Webhook URL Legacy tokens で生成された API Token 当初は Slack Apps にある Email を使おうと思っていましたが、この App はスタンダードプラン以上のプランでのみ利用可能ということが判明し、今回の仕組みを実装することになりました。\nEmail | Slack App ディレクトリ やること Route 53 で MX レコードを追加 SES で Rule Sets を作成 SES で受信したメール内容を Slack に通知する Lambda 関数を作成 SES の Actions に Lambda を追加 Gmail で転送フィルタを作成 構成としては下図のような形です。\nでは、順番にやっていきます。\n投稿日は古いですが、基本的に流れは下記の記事の通りです。 [[新機能]Amazon SES でメール受信が出来るようになりました！ ｜ DevelopersIO](https://dev.classmethod.jp/cloud/receiving-email-with-amazon-ses/) 1. Route 53 で MX レコードを追加 SES で利用するメールアドレスのドメインに対して、 MX レコードを追加します。今回は Route 53 で管理しているドメインのサブドメインに対して登録します。\n値に指定するのは、利用するリージョンによって異なります。今回はバージニア北部リージョンを利用するので、 10 inbound-smtp.us-east-1.amazonaws.com を指定します。他のリージョンでの値については公式ドキュメントを参照してください。\nリージョンと Amazon SES - Amazon Simple Email Service 2. SES で Rule Sets を作成 マネジメントコンソールを開いて、リージョンを バージニア北部 に変更し、 SES のコンソールを開きます。\n左のメニューの Email Receiving の Rule Sets から Create Rule ボタンを押して Rule Set を作成します。\nRecipient を追加する画面になるので、先ほど MX レコードを登録したドメインを入力して Add Recipient ボタンを押します。\n追加された行の Verify domain を押すとモーダルが表示されます。\nUse Route 53 を押すと Warning が出ますが、赤枠で囲った Email Receiving Record にチェックを入れ、 Create Record Sets を押します。\n続いて Actions の設定です。ここでは、 SES でメールを受信したあとにどんな Action をするかを設定します。あとで変更しますが、一旦は S3 への保存のみを設定します。\n次の画面では Rule の詳細設定をしますが、特に触らずにそのまま次へ進みます。\n最後に設定を確認して Create Rule を押します。\nこれらの手順が終わった後に、 SES のコンソールの左にある Domains で、追加したドメインが有効になっているかを確認します。\nVerification Status が verified になっていれば OK です。\n3. SES で受信したメール内容を Slack に通知する Lambda 関数を作成 ソースは GitHub に置いてるのでここには貼りませんが、やっていることは次のようなことです。\nS3 に保存されたメッセージを取得 body 部分を適切にデコード (base64 or quoted-printable) Slack へ通知 わざわざ S3 からメッセージデータを取得しているのは、 SES から Lambda に渡される Event の中にメールの本文データが含まれていないためです。 message id は含まれているので、それをもとに S3 からオブジェクトを取得します。\nまた、Slack への通知に際して、メール本文をタイムラインにポストすると圧迫感が凄いので、次のような形でポストしてます。\n差出人、日時、件名、宛先 のみをタイムラインにポスト そのポストのスレッドに本文をポスト 結果的にはこんな感じになります。\nmichimani/amazon-ses-to-slack: This is a AWS Lambda function for notifying receiving e-mail with Amazon SES to Slack. 4. SES の Actions に Lambda を追加 SES の Actions 設定で、S3 への保存の次の Action として、作成した Lambda (ここでは SEStoSlack という名前の関数) を指定します。\n5. Gmail で転送フィルタを作成 あとは Gmail の設定画面で、 SES 宛に転送するようなフィルタを作れば完了です。\nGmail で転送用アドレスを追加する際には確認コードが必要になりますが、そのメール自体も S3 に保存されるので、確認コードはそのファイルから取得する形になります。\n手順 4 で Lambda の設定が完了していれば、確認コードを含んだこのメールも Slack に通知されるので、そこで確認することができます。\nまとめ 以上、Gmail で受信したメールを Amazon SES 経由で Slack に通知する仕組みを作ってみた話でした。\nもちろん Amazon SES も料金は発生しますが、メールの受信に関しては 最初の 1,000 件までは 0 USD。それ以降は 1,000 件ごとに 0.10 USD となっています。 Slack のスタンダードプランは 1 ユーザあたり 850 円 (年払い) なので、仮に 1 ユーザだったとしてもその金額に達するにはメールを 約 78,272 (= 850 / 110 / 0.10 * 1000 + 1000) 通受信する必要があります ($1 = 110円 で計算)\nもしメールの転送がしたいためだけにスタンダードプランにすることがあれば (なさそうですが) Amazon SES + Lambda でのメール通知もありかもしれません。\n料金 - Amazon SES（Amazon Simple Email Service）| AWS ",
    "permalink": "https://michimani.net/post/aws-notifier-receiving-email-to-slack-with-amazon-ses/",
    "title": "Gmail で受信したメールを Amazon SES 経由で Slack に通知する"
  },
  {
    "contents": "先日作った Web アプリ ココイチ注文料金簡易カリキュレータ を PWA 化した話です。\nPWA・・・PWAとは、「Progressive Web Apps」の略称で、モバイル向けWebサイトをGooglePlayストアなどで見かけるスマートフォン向けアプリのように使える仕組みです。 ([PWAとは（Progressive Web Appsとは） | SEO用語集：意味/解説/SEO効果など [SEO HACKS]](https://www.seohacks.net/basic/terms/pwa/)) 作った経緯や使っているサービスなどについては一つ前の記事を見てください。\n概要 最初から PWA 対応したアプリケーションを作る場合は、 Vue CLI でプロジェクトを作る際に Progressive Web App (PWA) Support にチェックを入れておきます。\nそうすると、プロジェクトの作成と同時に必要なファイル (manifest.json, 各サイズのアイコンなど) が作成されます。\n今回は既存の Vue アプリケーションを PWA 化するので、 Progressive Web App (PWA) Support にチェックを入れたときと入れないときとの差分を補う形で対応しました。\nちなみに PWA 化することで出来るようになること (Push 通知やオフラインキャッシュなど) はいろいろあるのですが、今回の目的はとりあえず ホーム画面に追加してネイティブアプリっぽく使えるようにする ことです。\n手順 1. 必要なパッケージのインストール package.json に追記して、 PWA 化に必要なパッケージをインストールします。\n@@ -9,6 +9,7 @@ }, \u0026#34;dependencies\u0026#34;: { \u0026#34;core-js\u0026#34;: \u0026#34;^2.6.5\u0026#34;, + \u0026#34;register-service-worker\u0026#34;: \u0026#34;^1.6.2\u0026#34;, \u0026#34;vue\u0026#34;: \u0026#34;^2.6.10\u0026#34;, \u0026#34;vue-class-component\u0026#34;: \u0026#34;^7.0.2\u0026#34;, \u0026#34;vue-property-decorator\u0026#34;: \u0026#34;^8.1.0\u0026#34; @@ -16,6 +17,7 @@ \u0026#34;devDependencies\u0026#34;: { \u0026#34;@vue/cli-plugin-babel\u0026#34;: \u0026#34;^3.7.0\u0026#34;, \u0026#34;@vue/cli-plugin-eslint\u0026#34;: \u0026#34;^3.7.0\u0026#34;, + \u0026#34;@vue/cli-plugin-pwa\u0026#34;: \u0026#34;^3.7.0\u0026#34;, \u0026#34;@vue/cli-plugin-typescript\u0026#34;: \u0026#34;^3.7.0\u0026#34;, \u0026#34;@vue/cli-service\u0026#34;: \u0026#34;^3.7.0\u0026#34;, \u0026#34;@vue/eslint-config-prettier\u0026#34;: \u0026#34;^4.0.1\u0026#34;, $ npm install 2. Service Worker を登録する処理の追加 Service Worker を使うことによって Push 通知やオフラインキャッシュなど、よりネイティブアプリに近い動作を実現することが出来ます。 (もしかしたら ホーム画面への追加だけなら必要ない？ちょっとこの辺は)\nsrc の直下に下記のような registerServiceWorker.ts を作成します。\n/* eslint-disable no-console */ import { register } from \u0026#34;register-service-worker\u0026#34;; if (process.env.NODE_ENV === \u0026#34;production\u0026#34;) { register(`${process.env.BASE_URL}service-worker.js`, { ready() { console.log( \u0026#34;App is being served from cache by a service worker.\\n\u0026#34; + \u0026#34;For more details, visit https://goo.gl/AFskqB\u0026#34; ); }, registered() { console.log(\u0026#34;Service worker has been registered.\u0026#34;); }, cached() { console.log(\u0026#34;Content has been cached for offline use.\u0026#34;); }, updatefound() { console.log(\u0026#34;New content is downloading.\u0026#34;); }, updated() { console.log(\u0026#34;New content is available; please refresh.\u0026#34;); }, offline() { console.log( \u0026#34;No internet connection found. App is running in offline mode.\u0026#34; ); }, error(error) { console.error(\u0026#34;Error during service worker registration:\u0026#34;, error); } }); } そして src/main.ts 内で import します。\n@@ -1,5 +1,6 @@ import Vue from \u0026#34;vue\u0026#34;; import App from \u0026#34;./App.vue\u0026#34;; + import \u0026#34;./registerServiceWorker\u0026#34;; Vue.config.productionTip = false; 3. アイコンの作成 PWA 化するとホーム画面やスプラッシュ画像に表示されるアイコン画像が必要になるので、 public/img/icons 配下に下記のアイコン画像を準備します。\npublic/img/icons ├── android-chrome-192x192.png ├── android-chrome-512x512.png ├── apple-touch-icon-120x120.png ├── apple-touch-icon-152x152.png ├── apple-touch-icon-180x180.png ├── apple-touch-icon-60x60.png ├── apple-touch-icon-76x76.png ├── apple-touch-icon.png ├── favicon-16x16.png ├── favicon-32x32.png ├── msapplication-icon-144x144.png ├── mstile-150x150.png └── safari-pinned-tab.svg apple-touch-icon.png は 180px x 180px 、 safari-pinned-tab.svg は 16pt x 16pt 、 その他のアイコンはファイル名の通りのサイズ (px) で作成します。\n4. manifest.json の作成 今回の目的であるホーム画面へのアイコン追加を実現するためのファイルです。 public 直下に作成します。\n{ \u0026#34;name\u0026#34;: \u0026#34;ココイチ注文料金簡易カリキュレータ\u0026#34;, \u0026#34;short_name\u0026#34;: \u0026#34;coco1cal\u0026#34;, \u0026#34;icons\u0026#34;: [ { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/android-chrome-192x192.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;192x192\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/android-chrome-512x512.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;512x512\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/apple-touch-icon-60x60.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;60x60\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/apple-touch-icon-76x76.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;76x76\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/apple-touch-icon-120x120.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;120x120\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/apple-touch-icon-152x152.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;152x152\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; }, { \u0026#34;src\u0026#34;: \u0026#34;./img/icons/apple-touch-icon-180x180.png\u0026#34;, \u0026#34;sizes\u0026#34;: \u0026#34;180x180\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;image/png\u0026#34; } ], \u0026#34;start_url\u0026#34;: \u0026#34;./index.html\u0026#34;, \u0026#34;display\u0026#34;: \u0026#34;standalone\u0026#34;, \u0026#34;background_color\u0026#34;: \u0026#34;#FFFFFF\u0026#34;, \u0026#34;theme_color\u0026#34;: \u0026#34;#4e342e\u0026#34; } icons で最低限必要なのは android-chrome-192x192.png と android-chrome-512x512.png です。ただ、 iOS でもホーム画面への追加を実現したい場合は、その他のファイルも指定する必要があります。\nまた、 iOS のホーム画面アイコンは透過画像が使用できません。というか、使用はできるのですが、透過部分が黒で塗りつぶされます。\nなので、 Android 用アイコンには透過画像を使いたい場合、 iOS 用のアイコンには透過部分を含まない画像を用意したほうがよいです。\nbackground_color はスプラッシュ画像の背景色になります。\ntheme_color はツールバーの色になります。\nその他、 manifest.json に関する内容は The Web App Manifest | Web Fundamentals | Google Developers をご覧ください。\n5. index.html への追記 4 までの手順で PWA 化は完了しているのですが、追加で対応が必要な内容がありました。\nmanifest.json の theme_color が反映されない manifest.json の theme_color はツールバーの色になると書きましたが、ここを変更してもなぜか Vue.js デフォルトの #4DBA87 になってしまう問題がありました。 (Android 端末: Pixel 3a)\nPWA化したのはいいけど、テーマカラーが変わらないのはなぜ。manifest.json の値は変わってるのに。 pic.twitter.com/6kNVRyRrf3\n\u0026mdash; よっしーCBR789RR (@michimani210) May 22, 2019 調べてみると、どうやら Vue.js で作った PWA では manifest.json の theme_color が反映されないため、 head 内に直接メタタグで指定する必要があるらしいです。\nPWAのタブ色を変更する方法(Android)とVue.js使用時の注意点 | Cosnomi Blog ということで head 内に追記したところ、無事に反映されました。\n@@ -14,6 +14,7 @@ \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1.0\u0026#34;\u0026gt; + \u0026lt;meta name=\u0026#34;theme-color\u0026#34; content=\u0026#34;#4e342e\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;\u0026lt;%= BASE_URL %\u0026gt;favicon.ico\u0026#34;\u0026gt; iOS では manifest.json を読んでくれない PWA の仕様や manifest.json については Google が進めている内容で、 iOS Safari ではまだ未対応な部分も多々あるようです。\nそのため、 manifest.json の内容を head 内にメタタグで追記する必要があります。ただ、すべてを書くとなると面倒なので、 Google が用意ししている PWACompact を利用します。\nGoogleChromeLabs/pwacompat | GitHub 使い方は簡単で、 GitHub の readme にある通り head 内に下記の内容を追記するだけです。ただし、 readme と違う点は、 manifest.webmanifest ではなく manifest.json としているところです。\n@@ -5,6 +5,10 @@ \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;IE=edge\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width,initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;theme-color\u0026#34; content=\u0026#34;#4e342e\u0026#34;\u0026gt; + \u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;manifest.json\u0026#34; /\u0026gt; + \u0026lt;script async src=\u0026#34;https://cdn.jsdelivr.net/npm/pwacompat@2.0.8/pwacompat.min.js\u0026#34; + integrity=\u0026#34;sha384-uONtBTCBzHKF84F6XvyC8S0gL8HTkAPeCyBNvfLfsqHh+Kd6s/kaS4BdmNQ5ktp1\u0026#34; + crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;\u0026lt;%= BASE_URL %\u0026gt;favicon.ico\u0026#34;\u0026gt; これによって、 iOS 用に必要なメタタグが自動で挿入されるようになります。\nLighthouse で確認 Lighthouse でスコアを確認してみます。\n全体的に高いです。まあ、外部 API を使ったり複雑なデータの持ち方をしているわけではないので、スコアが高いのは当たり前かもしれません。\nPWA の項目について見てみます。\n全部オッケーっぽいですね。\nみんなでココイチへ行こう！ Vue.js で作ったアプリケーションを PWA 化してみた話でした。\niOS/Android のネイティブアプリを作るとなると、デベロッパー登録とかストアへの申請とかハードルがいくつかありますが、 PWA の技術を使えば Web アプリケーションをネイティブアプリっぽくできるので、すごいですね。 (語彙力)\n現状では iOS で対応が不完全だったりするので、今後そのあたりが統一されていけばスマホアプリ事情が変わっていくかもしれませんね。\nということで、PWA 化した ココイチ注文料金簡易カリキュレータ 、ぜひ使ってみてください。アクセスして「ホーム画面へ追加」するだけです。\nココイチ美味しいですよ🍛\n",
    "permalink": "https://michimani.net/post/programming-build-pwa-with-vuejs/",
    "title": "Vue.js で作った Web アプリを PWA 化してみました"
  },
  {
    "contents": "みなさん、ココイチでカレー食べてますか？\n私は一時期 週３くらいのペースで食べるくらいココイチのカレーが好きです。(最近は頻度が減りましたが、相変わらず好きです)\nココイチの醍醐味といえば、カスタマイズですが、慣れてないと注文方法がわかりにくかったりします。\nそんなココイチの注文をスムーズにできるようなツールを作りました。\n作ったもの ココイチ注文料金簡易カリキュレータ ココイチが好きすぎてココイチの料金簡易計算ツール作りました。https://t.co/EZbRBJhtWv\n\u0026mdash; よっしーCBR789RR (@michimani210) May 16, 2019 できること ココイチカレーの 4 要素 (ソースの種類, ライスの量, 辛さ, トッピング) を順番に選択して、料金の合計が計算できる トッピングについては ランダムで１つ, ランダムで２つ, ランダムで３つ, 完全ランダム で選択できる 全部おまかせ にすると、 4 要素すべてをランダムで選択できる 選択した結果の料金、明細を一画面で確認できる (トッピングが多い場合は一画面におさまらない場合があります) 以下、注意点です。\nあくまでもトッピングの料金計算のみです。サイドメニューの料金計算は出来ません。 辛さは ５辛まで選択できます。６辛以上も注文可能ですが、公式では 過去に５辛以上を感触したことがある方のみ とされているため。 各種料金、トッピングの種類は季節や店舗によって異なる場合があるので、正確な料金は店舗でご確認ください。 なぜ作ったのか 冒頭にも書きましたが、私自身がココイチ大好きなので、ランダムでトッピングを選んでくれるツールがあればいいなーと思って作りました。\n構想自体は去年の年末からあったんですが、怠慢により半年弱放置していました。\nそんな矢先に サイゼリヤ1000円ガチャ が公開されて、あーやられたなーと思ってすぐにリリースしました。サイゼリヤ1000円ガチャさん、おしりに火をつけてくれてありがとうございました。\nどんな技術で動いているのか アプリケーション 最近 Vue.js を触り始めたので、その練習も兼ねて フロントエンドは Vue.js を使って書いてます。\nサーバサイドはありません。フロントのみです。\nVue.js のプロジェクトは Vue CLI を使って簡単に作成できました。\n$ vue create coco1.app.michimani.net ? Please pick a preset: Manually select features ? Check the features needed for your project: Babel, TS, Linter ? Use class-style component syntax? Yes ? Use Babel alongside TypeScript for auto-detected polyfills? Yes ? Pick a linter / formatter config: Prettier ? Pick additional lint features: (Press \u0026lt;space\u0026gt; to select, \u0026lt;a\u0026gt; to toggle all, \u0026lt;i\u0026gt; to invert selection)Lint on save ? Where do you prefer placing config for Babel, PostCSS, ESLint, etc.? In dedicated config files 設定は、 TypeScript を使う以外は基本的にデフォルトのままです。\nアーキテクチャ 構成図は下図の通りです。\n下記 AWS のサービスを使った、いたってシンプルな構成です。\nRoute53 CloudFront S3 CodePipeline CodeCommit CodeBuild Lambda アプリケーションは静的ファイルのみなので、 Route53 + CloudFront + S3 でアクセスできる簡単設計です。\nデプロイは、CodeCommit への push をトリガーにして CodeBuild でビルド、S3 へデプロイ、完了後には Slack へ通知。というのを CodePipeline でやってます。\nあと、トッピングのデータを json ファイルで保持しているので、ココイチのメニューページをクロールする Lambda を定期的に実行して、できるだけ最新のデータで計算できるようにしています。\n困ったところ Route53 + CloudFront で独自ドメインでアクセスできるようにするところで少しだけ困りました。\nCloudFront にエイリアス設定した独自のドメインにアクセスしたら、Origin のバケット URL にリダイレクトされる。今までそんなことなかったのに、なんか設定の順番でも間違えた？\n\u0026mdash; よっしーCBR789RR (@michimani210) May 16, 2019 結果的に時間が解決してくれたんですが、 CloudFront で新たに Distribution を作ったときはステータスが Deployed になるまでおとなしく待つべきですね。Route53 での DNS 設定はそのあとにやったほうが良さそうです。\nVue.js については、データ管理を良しなにやってくれるのでビューの組み立てがめちゃくちゃ簡単だなという印象です。\nロジック自体は TypeScirpt で書けるので新たに覚えることはないですし、これくらいのシンプルなものであれば困るところは無かったです。\nもっと複雑なサービスになるとデータ管理とかをしっかり考えないとダメなんでしょうね\u0026hellip;。そうなると困る気がしてます。\nまとめ ココイチの注文料金を簡単に計算できるサービス 「 ココイチ注文料金簡易カリキュレータ 」 をリリースしたという話でした。\nぜひ使っていただいて、いろんなココイチのメニューを楽しんでください！\n",
    "permalink": "https://michimani.net/post/programming-about-coco1-price-calculate-tool/",
    "title": "注文方法がわかりにくいココイチの料金を簡単に計算できるツールを作りました"
  },
  {
    "contents": "Slack のフリープランでは閲覧・検索できるメッセージの上限数が 10,000 件となっており、それを超えると 10,000 件以上前のメッセージは閲覧・検索することができません。\nなので、 RSS の通知先チャンネルなど古いメッセージは削除してしまってもよい場合に、 Slack の API を使って該当するメッセージを一括で削除するのが目的です。\nやりたいこと 特定のチャンネルで、ポストされてから一週間以上経過するメッセージを削除する AWS Lambda で定期的に削除処理を実行する Runtime は Python 3系 処理の流れ Slack API で、チャンネルを指定してメッセージを取得する メッセージのタイムスタンプを見て、一週間以上経過しているメッセージを Slack API で削除する 以上です。\nSlack の API を利用するためには、 token が必要なので、 Legacy tokens から生成・取得します。xoxp-1111... みたいやなつです。\n使用するのは、チャンネル内のメッセージを取得する channels.history と、メッセージを削除する chat.delete です。\nchannels.history method | Slack - Slack API chat.delete method | Slack - Slack API それぞれの API は必須パラメータも少なく、使い方はとてもシンプルです。\nchannels.history の必須パラメータ token: Slack API の token です。 channel: メッセージを取得したいチャンネルの チャンネル ID です。チャンネル名ではありません。 chat.delete method token: Slack API の token です channel: 削除したいメッセージがポストされているチャンネルの チャンネル ID です。チャンネル名ではありません。 ts : 削除したいメッセージのタイムスタンプです。 channels.history の取得結果に含まれているのでそのまま使います。 チャンネル ID の調べ方 各 API で使用するチャンネル ID は、デスクトップ版の Slack アプリから確認します。\n左のチャンネルリストから チャンネル ID を調べたいチャンネル名を右クリックして、 copy link を選択します。(日本語化していると日本語になってます)\nコピーされたリンクは下記のようになっているので、最後のパス部分にあたる英数字がチャンネル ID になります。\nhttps://example.slack.com/messages/AAA123CCC ソース ソースは下記の通りで、 Gist に置いてます。\nスクリプト内では削除対象を一週間以上前としていますが、 TERM の値を変更すれば任意の期間を指定できます。\nAWS Lambda で実行する Gist の readme にも書いてますが、 Lambda で実行する場合は イベントとして次のような JSON を渡します。\n{ \u0026#34;target_ch_ids\u0026#34;: [ \u0026#34;CHANNEL_1\u0026#34;, \u0026#34;CHANNEL_2\u0026#34;, \u0026#34;CHANNEL_3\u0026#34; ] } CloudWatch Event で定期実行する場合も、入力の設定で上のような JSON テキストを指定します。\n",
    "permalink": "https://michimani.net/post/programming-delete-old-slack-messages/",
    "title": "Slack API で特定チャンネルの一定期間以上前のメッセージを削除する"
  },
  {
    "contents": "これを書いてる時点 (2019年4月) では最も新しいモデルである MacBook Pro 2018 のキーボードでチャタリングが発生してしまいました。\n結果としてキーボード部分の交換をしてもらったんですが、そこに至るまでの経緯を書いておきます。\nMacBook Pro のキーボード問題 MacBook Pro は 2016 年モデルからバタフライキーボードが採用されましたが、それ以降 2018 モデルまでに出たモデルにて、 押しても反応しない 、 1 回押しただけで複数回入力される といった不具合がありました。現在はそれらのモデルに対して MacBook および MacBook Pro キーボード修理プログラム として無償で修理対応してもらえます。対象は以下のモデル。\nMacBook (Retina, 12-­inch, Early 2015) MacBook (Retina, 12­-inch, Early 2016) MacBook (Retina, 12-­inch, 2017) MacBook Pro (13­-inch, 2016, Two Thunderbolt 3 Ports) MacBook Pro (13-­inch, 2017, Two Thunderbolt 3 Ports) MacBook Pro (13-­inch, 2016, Four Thunderbolt 3 Ports) MacBook Pro (13-­inch, 2017, Four Thunderbolt 3 Ports) MacBook Pro (15-­inch, 2016) MacBook Pro (15-­inch, 2017) もし上記のモデルを使っていてキーボードに何かしらの不具合が発生している場合は、 最初の小売販売日から 4 年 まではプログラムを受けることができます。\nMacBook Pro 2018 でも同様の症状が発生 2017 までのモデルで不具合が多かったバタフライキーボードですが、 2018 モデルでは改善が施され、不具合は発生しないとされていました。しかし、ネット上では 2018 モデルでも同じような症状が出ていると報告していいる人がいて、さらには Apple のサポートコミュニティでもそういった声が出ていました。\nMacBook Pro 15\u0026amp;quot; (2018) Keyboard \u0026amp;ldquo;t\u0026amp;rdquo; key repeating issue? で、運悪く自分の MacBook Pro 2018 でもキーボードの不具合が出てしまいました。\n具体的には、 space キーでチャタリングが発生していて、 1 回押しただけで 2 回分反応するという状態でした。どのキーで発生するかは特に決まっていないようなのですが、 space キーで発生するというが非常に厄介でした。\n日本語変換で変換対象が正しく選べない、半角スペースが連続で入る、とにかく不便でした。\nちなみに購入時期は 2018年10月 で、症状が出始めたのは 2019年1月頃。仕事で使っているわけではなく、ブログ書いたりちょっとプログラム書いたりするために使っている程度です。また、その間、外に持ち出して使うこともなく、物理的な衝撃はありませんでした。\n症状が発生した MacBook Pro MacBook Pro (13-inch, 2018) JIS キーボード とりあえず試したこと キーボードの清掃 まずはキーボードとその周辺の清掃です。普通にエアダスターで掃除します。\n→ 特に効果なし。\nSMC リセット SMC はシステム管理コントローラのことで、おもに電源まわりの処理をしている部分です。キーボードの信号に不具合がある可能性があるので、 SMC をリセットしてみます。\nリセット手順は Mac の SMC (システム管理コントローラ) をリセットする方法 を参照してください。\nMacBook Pro 2018 の場合は、システムを終了して、電源ボタンを 10 秒以上長押しして、数秒待って、再度電源を ON にする という流れです。\n→ 一時的に効果があった気になるが、しばらく (数時間) 使っていると再発した。\nNVRAM (PRAM) リセット NVRAM は、 Mac がすばやく起動したり動作するために設定情報などを記憶しておくメモリです。たぶんキャッシュと同じようなイメージなので、一度そのキャッシュをリセットしてみる感じです。\nリセット手順は Mac で NVRAM または PRAM をリセットする を参照してください。\nMacBook Pro 2018 だと途中でリンゴマークが出たりしますが、とりあえず手順どおりにやれば大丈夫です。\n→ 一時的に効果があった気になるが、しばらく (数時間) 使っていると再発した。\nセーフブートで起動してみる セーフブートは、 Mac の動作に必要な最小限の設定のみで起動します。システム以外のなにかが原因かどうかを切り分けるために、セーフブートで起動時にも症状が再現するかチェックします。\nセーフブートの起動で順については セーフモードを使って Mac の問題を切り分ける を参照してください。\n→ 症状は再現しなかった。ただ、操作時間が短いためあまり参考にならない？という感じ。\nちなみにこの 4 項目は、 Apple サポート へ電話で問い合わせた際にやってほしいと促される作業内容です。なので、サポートに連絡するのが面倒、時間がもったいない場合は、上記の項目を各自でチェックしてみるのが良さそうです。\nGenius Bar へ 結局解決には至らなかったので、 Genius Bar へ持っていくことにしました。Genius Bar を予約する際には、上で書いた項目を既にチェック済みであることを書いておくとスムーズに話が進みます。\nまた、持っていく直前にも症状が再現することを一応確認しておきます。\nGenius Bar に持っていくと、まずは症状の確認とこれまでの取った対処方法について聞かれます。その後実際にその場で症状が再現できるか確認するのですが\u0026hellip;こういうときに限って再現しないんですよね。辛い。\nこのままだと、とりあえず預かって、とりあえず部品交換して、とりあえずクリーンインストールしてお返しする感じになります と言われながらもひたすら再現を試みます。\n結局、数回は症状が再現できたので、キーボードまわりの部品交換をしてもらうことになりました。\n部分交換のみなのでデータはそのままですが、 修理後の総合チェックでなにか問題があればクリーンインストールするかもしれませんとのことでした。まあ、そのあたりは仕方ないですね。\n上にも書いたとおり、仕事で使ってるわけでもなく、代替の PC がないわけでもないのでその場でお預け。 1 週間 + α かかるとのことでした。\nちょうど一週間後に返ってきた ちょうど一週間後に修理完了メールが届いたので、さっそく受け取ってきました。\n修理・交換部品は以下のとおりです。\nBottom Case, Space Gray Top Case with Battery, Space Gray, JIS, Japanese Hardware Repair Labor FLAT RATE, RETAIL, TOP CASE MBP 13 下の 2 つは工賃的なものなので、部品としては上の 2 つです。\nメインのキーボード部分 (Top Case) と、Bottom Case も交換されたようです。\nとりあえず触った感じでは、キーのタッチ感が少し変わりました。変わらないはずなんですが。\n症状も今のところは出ていません。\n再発しないことを祈るのみです。\nMacBook Pro 2018 でキーボード不具合が起こったら 2018 モデルでは対策されたというキーボード問題ですが、一部のユーザには影響が出ているようです。\n1 年以内であれば製品保証対象なので、不具合を感じた場合は早めに Apple サポート、 Genius Bar へ行くのがよさそうです。\n",
    "permalink": "https://michimani.net/post/gadget-macbookpro2018-keyboard-problem/",
    "title": "MacBook Pro 2018 のキーボードでチャタリングが発生したので交換してもらった話"
  },
  {
    "contents": "3/29 (金) 〜 3/31 (日) まで 練馬区立区民・産業プラザ Coconeriホール で開催されていた PHPerKaigi 2019 に行ってきたので、そのレポートです。\n1 日目の レポートはこちらです。\n【3/31 (日)】 2日目 セッションレポート 1日目 と同様に、2日目に実施されたセッションのレポートを、セッションの概要と、一言感想くらいで書いていきます。\n10:25 #a マニュアルにない引数を与えるとどうなる？php-srcへのバグ報告をした時の話 引数のデータ型を間違えると NULL が返ってくる。 strlen(array()), mb_strlen(array()) では 返り値が違う mbstring 関数では違いがある mb_check_encoding() はバージョンによって、引数に配列を与えた場合の返り値が大きく違う PHP に対するバグレポート 簡潔に、英語で書く 再現方法を詳細に書く 今回の場合、引数に空の配列を与えるのか、空でない配列を与えるのか、など 多くの人が使う言語へのバグレポートは全世界に影響を与えているような感じで動揺する 私も GitHub で公開される OSS に PL を送ってマージされた経験がありますが、やっぱり多くの人に使われているものに貢献できるのは凄く嬉しさがあります。それが PHP のような利用者が非常に多い言語となると、その嬉しさはさらに大きいでしょうし、動揺もすると思います。このセッションではバグレポート時にチェックすべき項目、たとえば再現方法では「この場合では起こるけど、この場合では起こらない」みたいなことを漏れなく書く、中の人がわかりやすいように書くことに関して参考になる部分が多かったです。\nPHP に限らず何かしらのバグレポートをする際の参考にできると思います。\n10:50 #a PHPでURLルーティングをつくる Router とは パス部分のパース 正規表現 文字列探索 Router の I/O inpute path method routing definition output path method Routing map の生成 Path の階層は木構造 自作の Router https://github.com/bmf-san/ahi-router/blob/master/src/Router.php 正規表現を使わない 文字列を 1 文字ずつ探索していく 木構造のアルゴリズムの勉強が必要 Router を作るというのは想像したことがなかったので、とても面白かったです。Router って仕組みとしては URL の path とコントローラを結びつける っていう単純なイメージですが、その path の探索を効率よく行うためにはには木構造のアルゴリズムを理解する必要があったり、突き詰めると深いなと感じました。最近は、プログラムを書くに際してアルゴリズムとか数学的な知識も必要かなと思っていたところなので、やっぱりそういった内容の勉強は必要なんだと再確認しました。\n11:25 #a Hack HTTP Request and Response Interfaces HHVM and Hack Documentation Hack とは 魔改造されたPHP (昔) PHP のコードはほとんど動かない (今) まだちょっと動く 少しずつ機能削除されている PHP-FIG PSR-* Hack で PHP PHP の型と Hack の型は別物 hhi ファイル (型定義ファイル) 今後 HHVM で composer が使えなくなる Hack 専用のパッケージマネージャーができる 「Hack とはなんぞや？」ということでこのセッションを聴いてみました。聴いた結果、 なんか PHP っぽいけど PHP と対立してる言語 ということはわかりました。(あんまりわかってない)\nとりあえず言語の開発者を募集しているらしいので、気になる方は発表者の @ex_takezawa さんにコンタクトを取ってみてはどうでしょうか。PHP っぽいけど PHP じゃない、マイナーな言語ということで今後の楽しさがありそうだなと思いました。\n12:20 #a ランチセッション PHPerKaigi 2019 の Diamond Sponsor である 株式会社メルカリ の @hidenorigoto さんによるランチセッションでした。\nメルカリのバリューについての話から始まり、後半は 視点 と 視野 と 視座 の違いの話でした。視(点|野|座) の話は非常にわかりやすくて面白かったです。\nまた、最後にあった 『変化のある環境に身を置く、飛び込む』という話はとても大事なことだなと感じました。Q\u0026amp;A タイムであった「転職を考えるきっかけは？」といった趣旨の質問に対するこたえも凄く良くて、その時々で重要視することが変わる (家族との時間を大切にしたい、チャレンジングな環境に身を置きたい など) というのを聞いて なるほどなと思いました。\nちなみに 2 日目 は 無事にランチチケットをゲットできたので、美味しいお弁当をいただくことが出来ました。\n13:30 #b アンチパターンから学ぶ、RDBの正しい設計 他人の失敗を知る フレームワーク依存症 恩恵と制約 特に顕著な制約は ORM ORM は漏れのある抽象化 制約と規約 Active Record パターンは良くない (View と Model が 1対1) フレームワーク、 ORM は絶対悪ではない リポジトリパターン サービス、データ、リポジトリ リポジトリ経由でデータを取得する リポジトリは DB、キャッシュ、WebAPI からデータ取得する 知らないロック RDBMS の仕事はデータを守ること (正しく取得、正しく保存) データを保存するだけなら S3 でいい 暗黙的ロック INSERT \u0026hellip; SELECT 文 ギャップロック、ネクストキーロック キャッシュ中毒 劇的なパフォーマンス向上 システムの複雑度が上がる トラブルシューティングが難しくなる クエリキャッシュ マテリアライズド・ビュー アプリケーションキャッシュ テストやモニタリングが難しい 事故を防ぎにくい そのくせ、キャッシュが絡むトラブルはクリティカル ヒット率、更新頻度 最初からキャッシュありきの設計にしない まとめ データベースの寿命は、アプリケーションの寿命より長い 「失敗から学ぶRDBの正しい歩き方」を買おう 「すべては本に書いてあります！」 ということだったので、私も 失敗から学ぶRDBの正しい歩き方 (Software Design plus) を買おうと思います。(単純)\nとはいえ、セッションの内容だけでも凄く面白い内容で、 RDB を使う上でのアンチパターン、正しい設計方法についての意識改革になりました。特に印象的だったのは「できることからやろう」ということです。 DB まわりのリファクタリングには結構な労力が必要になるので、既存のサービスに手を加えるなら、まずはアプリケーション側でどうにかできないかを考えてみる ということです。\n14:15 #a PHP監視、サービスを守る為に行う不測の事態への努力 登壇スライド \u0026raquo; PHP監視サービスを守る為に行う不測の事態への努力 SLA 100% は幻想 What, Why, How 雰囲気で監視しない ユーザ目線での監視 ユーザの目的を把握する 死活監視 HTTP ステータスコード レイテンシ 独自で監視ツールを作る (あえてのアンチパターン) faultline 最速での復帰を支援 FictionBase faultline で監視できないミドルウェアの監視 昨年末辺りから一世を風靡した(?) 『入門 監視』 のポイントを抑えたセッションでした。私も 入門監視 は読んだので、その内容の復習ができたような感じで良かったです。入門監視 を読んでから実際に業務で携わっているサービスの監視方法も少し変更したりしているので、監視ツールについては今後もチェックして、どのツールで何が出来て何が出来ないのか、サービスごとに適したツールは何なのか、といったことを吟味できるようになれれば良いなと思いました。\nPHPer Challenge 今回の PHPerKaigi では、 PHPer Challenge というゲームが裏で (表で？) 同時に開催されていました。ゲームの概要としては、イベントのサイト、スポンサー各社のブログ、会場内の掲示物、ノベルティなどなどに隠された 「\u0026quot;#\u0026quot; からはじまる文字列」 = 「PHPer トークン」 をどれだけたくさん見つけられるか というものです。文字列をみつけたら、専用のサイトに送信することでポイントが貰えて、最終的に獲得したポイントで順位を競うというゲームでした。\nPHPer トークン が書かれている場所の例としては、上に書いたもの以外には、登壇者のスライド内や、突発的な LT で使用されたホワイトボードだったり、もういろんなところに存在してました。また、ちなみに各トークンは 100 〜 2000 pt で、最初に見つけた人から少しずつポイントが減少していく といったシステムだったようです。なので、同じトークンでも最初に見つけた人のほうがポイントが高いということです。\nまた「徳丸浩の挑戦状」と題して、徳丸先生が用意した 脆弱性を持ったアプリケーション の脆弱性を突くことで得られる PHPer トークンというのもありました。これに関しては Web アプリケーションの脆弱性の勉強にもなって、単なる宝探しゲームの域を超えたエンジニアっぽいゲームになってました。\nで、最終的に私の順位はどうだったかというと\u0008\u0026hellip;。\n狙ってなかったものの、順位はゾロ目になりました。ただしゾロ目賞みたいなのはなかったようです。\nちなみに 1 位の方のポイントは 54,700 だったようで、どこにそんなにトークンがあったのか\u0026hellip;っていう感じですね。\nただ、順位はともかくメインのセッション以外にもこういったお楽しみがあるイベントっていうのは、新鮮でとても楽しかったです。\n2日目 と PHPerKaigi のまとめ 以上、2日目のレポートでした。本当は 15時頃からの LT も聞きたかったんですが、やむを得ず離脱\u0026hellip;。あとから Twitter で #phperkaigi で流れてくるツイートを見ながら楽しそうだなーと、感じているところです。\n2日目もコアな話、意識改革につながる話を聞くことができて良かったのと、美味しいお弁当も食べることができたのが良かったですね。\nあとは、PHPer Challenge ですね。 1日目はほとんど会場内を探索できなかったので、2日目は少し探し回りました。結果的には上に書いた通り 33 位でしたが、ちょっとした宝探し感、徳丸先生の脆弱性を見つけるゲームについては楽しみながら脆弱性の勉強にもなるという、とても楽しいイベントでした。\nで、初参加となった PHPKaigi、言語単位のカンファレンス でしたが、一言で言うと 楽しかった に尽きます。他の参加者とのコミュニケーションはほとんど取れませんでしたが、 Twitter では少しフォロワーも増えたりして、また次回もあれば行きたいなっていう感じです。個人的に今年の目標としては、いろんなカンファレンスとかイベントに参加する っていうのを掲げているので、先月の JAWS DAYS に続いて今回の PHPerKaigi 2019 にも参加できたのは、よい経験になりました。\n最近は PHP ほとんど書いてないんですが、また久々に書いてみようかなと思うきっかけにもなりました。\n運営のみなさま、登壇者のみなさま、お疲れさまでした。楽しい時間をありがとうございました！\n",
    "permalink": "https://michimani.net/post/event-phperkaigi-2019-day2/",
    "title": "PHPerKaigi 2019 に行ってきました − 2日目 −"
  },
  {
    "contents": "3/29 (金) 〜 3/31 (日) まで 練馬区立区民・産業プラザ Coconeriホール で開催されていた PHPerKaigi 2019 に行ってきたので、そのレポートです。\nPHPerKaigi とは 公式サイトから引用させてもらいます。\nPHPerKaigi（ペチパーカイギ）は、PHPer、つまり、現在PHPを使用している方、過去にPHPを使用していた方、これからPHPを使いたいと思っている方、そしてPHPが大好きな方たちが、技術的なノウハウとPHP愛を共有するためのイベントです。\n歴戦の勇者みたいな方はもちろん、初心者の方にも、全てのPHPerが 楽しめるイベントです。\nPHPerKaigi 2019 私は新卒で入った会社で開発に携わっていたサービスで PHP が使われていたことで ペチパー となり、その後は個人でもアプリケーション作ったりして PHP を使ってます。その後の業務では Java とか C# とか Python とか JavaScript とかを使いつつ\u0026hellip; という感じなので、 PHP で開発している期間は 3 年くらいです。最近は Python と JavaScript (TypeScript) ばっかり触っているので、久々に PHP に触れようと思って参加しました。\n【3/30 (土)】 1日目 セッションレポート 1日目に実施されたセッションのレポートを、セッションの概要と、一言感想くらいで書いていきます。\n10:40 #b フレームワークを作りながらLaravelのアーキテクチャを学ぶ Laravel のアーキテクチャを理解するために、オレオレフレームワークを作る話\nパーフェクトPHP (PERFECT SERIES 3) に自作フレームワークの作り方が書いてある\nフレームワークを１から作る楽しさ\nPHPStorm があると便利\nLaravel のアーキテクチャを学ぶ\nLaravel のサービスコンテナ = Laravel が提供している DI コンテナ ファサード facade !== facade pattern サービスコンテナを経由してインスタンスメソッドを実行してる ファサードはどこでも利用できる分、複雑になりやすい 200X年のフレームワークとの違い すべてがファサードができている テストがやりやすい 手段ではなく本質を追い求める\n一回 MVC フレームワークを作ってみる そのあと Laravel のソースを見てみる これまで オレオレフレームワーク = 悪 だと思っていましたが、既存のフレームワークのアーキテクチャを学ぶという意味で、自分で一度フレームワークを作ってみるというのは大事だなと思いました。このセッションに限らず、他のセッションでもフレームワークを自作するという話は結構出ていたので、 Production として運用するかは別として、オレオレフレームワークを作るのは良いことかなと感じました。\n11:25 #b PHPerKaigi 2019 ゆるふわCI入門 よくばりセット 発表スライドは下記で公開されています。\nPHPerKaigi 2019で「PHPゆるふわCI入門」を発表しました (スライド付き) サンプルコード | zonuexe/phperkaigi-ci CI とはなにか\nGitHub composer PHPUnit git の tag, branch CI で何ができるか\nコードが正常に動いていることを確認し続けたい 自動テスト : PHPUnit, phpspec 自動テスト (受け入れテスト) : Codeception (古いテストがないコードには使いやすい) QA ツール : PHPStan, Psalm, Phan (最近のものは型までチェックしてくれる) 属人化を防ぐ 実行環境\nGitHub での PR 作成をトリガーにして実行される CI サービスが提供している OS 環境、または docker イメージを利用して実行する 特定用途の CI StyleCI : コードスタイル ゆるふわ CI\nゴール : Syntax Error になるコードを本番公開しない ピクシブの場合 ファイル数 6000 以上 差分だけをチェックして高速化 PHPUnit, PHPStan と 独自の linter node.js で並列実行している CI でできること・目的については他の言語でも同じですが、 PHP で使用するツールなどの具体例がたくさん出てきたので、「あ、そういうの使うんだー」っていう感じでした。今後また PHP で開発する機会があれば使ってみようと思います。\n13:30 #b PHPの現場 公開収録 PHPの現場 という Podcast の公開収録でした。正直、この Podcast は聞いたことがなかったんですが、言語に特化した内容でとても濃い話が繰り広げられていたので、今後聴くようにします。\nなお、このときのエピソードはすでに公開されています。\nPHPの現場 28. ファミコンで理解する DI（ytake） 14:45 #a モバイルアプリ向けAPI開発を通じて学んだこと モバイルアプリ向け API を Laravel で作った話 新規サービスは技術選定の連続 言語、インフラ、FW\u0026hellip; 一度決めるとあとから (ほぼ) 現行不可 要件定義 なぜ、誰のために、いつまでに 要件を満たすか、学習コスト、将来性、スケーラビリティ Lumen も候補にあったが、Laravel Passport を使いたかった 読書会 API 開発者とアプリエンジニアとの認識をあわせる (あくまで基礎レベル) 参考書 : Web API The Good Parts 設計・ドキュメントの残し方 API 設計には Swagger Spec API 設計が一番苦労した 設計の前にデザイナー／クライアント・API 設計者で認識を合わせる Laravel API 認証 Laravel Passport OAuth 2.0 のライブラリ Laravel 公式 既存システムに合わせるには、それ用のプロバイダを使ってオーバーライドする エラー時の API レスポンス すべてのパターンにおいて共通のレスポンスパターン Laravel の Responsable インターフェース すべての例外は App\\Exception\\Handler の render() を通るので、ここを触る API 開発をするうえで、デザイナー・クライアントサイドエンジニアとしっかり認識をあわせるというのは、使う言語に関係なく大事だなということがわかりました。Laravel で Web API を作るというのは過去にやったことがありましたが、当時はそのあたりをしっかり調整できていなかったなと反省しました。\n発表者の @hypermkt さんの声質が凄く良くて聞きやすかった印象です。\n15:30 #a 3ヶ月でphp5.5から7.2にバージョンアップした現在と今後の向き合い方 PHP 5.5, Codeigniter, AWS EC2 懸念点 タイトなスケジュール (3ヶ月) PHP バージョンアップのノウハウ不足 リソース不足 手順 公式サイトの移行手順に従って該当箇所の調査 docker 環境で検証 PHP 以外にもバージョンアップが必要なものがある Apache CentOS Codeignitor memcached ハマりポイント memcached は wget で最新版を入れる mcrypt の代わりに OpenSSL を使う (PHP7.2 では mcrypt は非推奨。OpenSSL のほうが処理が速い) mod_small_light のアーキテクチャ変更 count() CloudFront 導入による IP 問題 HTTP_X_FORWARDED を使う エラーハンドリングの仕様変更 意図せぬ不具合による修正・報告書作成にかかる工数の増加 開発体制 タスク管理：asana エラー検知：Sentry やるべきこととやらないことの選別 やること : 動くようにする やらないこと : コードのリファクタリング、不要な変更 バージョンアップの結果 速度改善 新しいアーキテクチャの導入ができた PHP 5 系の 公式サポート は 2018 年で終了していますが、 Zend Server や CentOS などの各ディストリビューション単位ではまだ 5 系のサポート期間が残っているものもあるので、今後も必要になってくる内容だと思いながら聞いていました。\n個人的には PHP のバージョンアップノウハウが全く無いので、今後アップデート作業をすることになった際には、このセッションで聞いた内容を参考にして、スムーズに進められるようにしたいです。\n1日目まとめ 初参加の PHPerKaigi、というか言語縛りのイベントには初めて参加しましたが、とても面白かったです。先月は JAWS DAYS に参加したんですが、そのときとはまた違った雰囲気がありました。もちろん規模的には JAWS DAYS のほうが大きくて人数も参加者数も多いわけですが、一つの言語に対するイベントということで、より具体的で詳細な内容のセッションが多かったなと感じました。\nあとは、T シャツがもらえたり、突発的な LT があったり、期間を通じてチャレンジゲームが開催されていたりして、トータルで楽しい印象を受けました。あまり書くと 2日目のレポートまとめに書くことがなくなるのでこれくらいにしておきます。\nPHPerKaigi 2019 Tシャツ メルカリウォーター\n",
    "permalink": "https://michimani.net/post/event-phperkaigi-2019-day1/",
    "title": "PHPerKaigi 2019 に行ってきました − 1日目 −"
  },
  {
    "contents": "Nintendo Switch の 大乱闘スマッシュブラザーズ SPECIAL (スマブラSP) のプレイ動画を iMovie で編集して書き出そうとしたときに、何をやってもエラーが出てしまい書き出しができませんでした。その対処方法についてです。\n前提 スマブラSP のプレイ動画は、ゲーム内で保存したリプレイデータを、ゲーム内の コレクション からムービーに変換することで取得することが出来ます。変換したムービーは Switch に挿した microSD カード内に保存され、 Nintendo/Alubum/Extra/ 以下に yyyy/mm/dd の形で日付ごとにディレクトリが作成され、その中に .mp4 形式で保存されています。\n今回は、その .mp4 データをローカルにコピーして、それを iMovie で読み込んで編集することを考えます。\n作業環境は下記の通りです。\nMacBook Pro 13-inch, 2018\nメモリ 16 GB 2.7 GHz Core i7 macOS Mojave 10.14.3 iMovie 10.1.10 やったこと・起こったこと 普通に iMovie で新規プロジェクトを作成して、前述の通りローカルにコピーした .mp4 形式のリプレイ動画を読み込みます。そして適当に編集します。 (ちなみに全く編集せずにそのまま書き出そうとした場合でも同じエラーとなりました。)\n編集後、いつも通りファイルとして共有する形で書き出します。\nすると、\n共有に失敗しました\n{プロジェクト名} の書き出しに失敗しました\nビデオレンダリングエラー : 10008 (iMovie エラー　10008 : renderVideoFrame に失敗しました)\nというようなエラーが出て書き出しできない現象が発生しました。実際のエラー表示は下記の通り。\n問題解決のためにやったこと 再起動とか iMovie の再起動、システムの再起動、プロジェクトの再作成を実施しましたが、効果なし。\niMovie 再インストール iMovie を再インストールしてみました。このとき、下記のライブラリファイルも削除しました。\niMovie Theater.theater iMovie ライブラリ.imovielibrary が、これも効果なし。\nQuickTime で mov 形式にしてみる 結果的にはこの方法で解決しました。\n編集したい .mp4 ファイルを、下記の手順で .mov 形式に変換します。\n編集したい動画ファイル (.mp4) を QuickTime で開く ファイル \u0026gt; 書き出す \u0026gt; 720p で書き出す あとは .mov ファイルを iMovie で読み込んで普通に編集してファイルとして共有すれば OK です。\nプロテクトでもかかってるの？ 今回の事象は私の端末だけでなく他の端末でも発生しており、同じ方法で解決しました。\nなので、おそらくゲーム内で作成された動画ファイルには何かしらプロテクトのようなものがかかっているのかもしれません。\n実際、iMovie エラー 10008 などでググってみてもあまり有益な解決策は出てこず、そもそも情報が古かったりして当てになりませんでした。もし同じような状況で困っている場合は、 QuickTime で .mov 形式にする方法を試してみてください。\n",
    "permalink": "https://michimani.net/post/other-imovie-10008-error/",
    "title": "スマブラSP のプレイ動画を iMovie で編集して書き出すときに 10008 エラーが出たときの対処方法"
  },
  {
    "contents": "普段使っているルービックキューブ GAN356 X と GAN354 M の GES 設定をメモしておきます。おそらく今後使っていく中で設定は変わっていくと思うので、20190313版 として残しておきます。\nちなみにこれを書いている時点での各ベストタイムは下記のとおりです。\nTime Best 00:24.18 Best mean of 3 00:29.46 Best mean of 5 00:30.81 GAN356 X 概要 GAN356 X に付属している GES は、強い順に 黃, 緑, 青, 紫 です。また、GMS の強度も変更可能で、透明: Storng, 黃: Medium, 緑: Weak, 白: Magnetless が選べます。\nそして GAN356 X の GES 設定の特徴としては、強さを 3 段階 (0.6, 0.8, 1.0) で設定できることです。細かい設定は出来ませんが、あらかじめ決められた強さに設定できることで、再現性が高く、他の人と設定を共有しやすいのが利点です。\n個人的おすすめ設定 GMS : 透明 (strong) GES : 黄 1.0 マグネットアシストの恩恵を十分に受けられる設定で、非常に軽い力で回すことが出来ます。 GES を一番強い 黃 にして、さらに強さも一番強い 1.0にすることで、安定感も出ます。この設定には非常に満足しているので、しばらく変えることはないです。\nGAN354 M 概要 GAN356 X と同様に、付属している GES は、強い順に 黃, 緑, 青, 紫 です。ただし、 GMS の強度は変更不可です。\nGAN354 M の GES 設定は、ネジの閉め具合によって無断階に調節が可能です。ただし、その分 すべての面の GES 設定を全く同じにすることが難しく、再現性が低いのが難点です。個人的には、まず最初にネジの頭が出るか出ないかくらい (0 とします) まで締めて、そこからどちら向きに何度回したか、で各面の締め具合を均等にして(いるつもりで)います。\n個人的おすすめ設定 GES : 紫 +1080° +1080° とは、締める方向に 3 周ということです。見た目の締め具合は下の写真のような感じです。\nGAN356 X と違って一番弱い GES を使っていますが、感覚的には同じような感じで回すことができています。ただ、若干安定感に欠ける印象があるので、 GAN354 M に関しては今後も適切な設定を探しながらいろいろ試していくことになると思います。\nその他 その他としては現状困っていることを書いておきます。\nスッテカーレス故に (?) めちゃくちゃ滑りやすい 乾燥している今の時期だと更に滑りやすい グリス塗ったほうがいいのだろうか デフォルトで塗られていたグリスは拭き取って、現状はグリスレスの状態 使っているうちに少しずつ摩耗している感じがある ",
    "permalink": "https://michimani.net/post/other-rubics-setting-gan356x-gan354m/",
    "title": "GAN356 X と GAN354 M の GES 設定などのメモ"
  },
  {
    "contents": "先日参加した AWS のハンズオンで DynamoDB を初めて触りました。そのときは Node.js で書かれたサンプルコードをコピペするだけで、データの読み/書きができました。 ということで、今回は Python でデータの読み/書きを実装してみたので、その時のメモです。\n目次 テーブル作成 データの登録 put_item() データの取得 get_item() まとめ テーブル作成 まずはテーブルを作成します。 今回は例として、欅坂46のメンバーブログの情報を保持するテーブルを作成してみます。あとでデータが取得しやすいように、 プライマリパーティションキー と プライマリソートキー は次のように設定します。\nプライマリパーティションキー : Authorcode (文字列) ・・・ ブログ記事を書いたメンバーのコード プライマリソートキー : Url (文字列) ・・・ ブログ記事のURL ブログの URL には自動採番された (であろう) 数値がパラメータとして付与されているので、 Url でソートすれば実質投稿順となります。\nデータの登録 put_item() 登録するデータは、次のような json を想定します。\n{ \u0026#34;Author\u0026#34;: \u0026#34;欅坂46二期生\u0026#34;, \u0026#34;Authorcode\u0026#34;: \u0026#34;1001\u0026#34;, \u0026#34;Entrydate\u0026#34;: 20190224, \u0026#34;Images\u0026#34;: [ \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c.jpg\u0026#34;, \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-01.jpg\u0026#34;, \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-02.jpg\u0026#34;, \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-03.jpg\u0026#34;, \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-04.jpg\u0026#34;, \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-05.jpg\u0026#34; ], \u0026#34;Title\u0026#34;: \u0026#34;初歌番組。武元唯衣です。\u0026#34;, \u0026#34;Url\u0026#34;: \u0026#34;http://www.keyakizaka46.com/s/k46o/diary/detail/19596?ima=0000\u0026amp;cd=member\u0026#34; } boto3 を使って DynamoDB にデータを登録するには put_item() を使います。\nimport boto3 table_name = \u0026#39;keyakizaka-blog\u0026#39; item = { \u0026#39;Author\u0026#39;: \u0026#39;欅坂46二期生\u0026#39;, \u0026#39;Authorcode\u0026#39;: \u0026#39;1001\u0026#39;, \u0026#39;Entrydate\u0026#39;: 20190224, \u0026#39;Images\u0026#39;: [ \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c.jpg\u0026#39;, \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-01.jpg\u0026#39;, \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-02.jpg\u0026#39;, \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-03.jpg\u0026#39;, \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-04.jpg\u0026#39;, \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-05.jpg\u0026#39; ], \u0026#39;Title\u0026#39;: \u0026#39;初歌番組。武元唯衣です。\u0026#39;, \u0026#39;Url\u0026#39;: \u0026#39;http://www.keyakizaka46.com/s/k46o/diary/detail/19596?ima=0000\u0026amp;cd=member\u0026#39; } dynamo = boto3.client(\u0026#39;dynamodb\u0026#39;) dynamo.put_item(TableName=table_name, Item=item) とすると登録できます。\u0026hellip;と思っていましたが、これでは出来ません。 リファレンスを読めば書いてるんですが、Item として指定する dict 型のオブジェクトの各項目に、それらの型が何なのかを、dict 型で指定する必要があります。例えば、上の例だと、 item として次のようなオブジェクトを定義する必要があります。\nitem = { \u0026#39;Author\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;欅坂46二期生\u0026#39;}, \u0026#39;Authorcode\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;1001\u0026#39;}, \u0026#39;Entrydate\u0026#39;: {\u0026#39;N\u0026#39;: \u0026#39;20190224\u0026#39;}, \u0026#39;Images\u0026#39;: {\u0026#39;L\u0026#39;: [ {\u0026#39;S\u0026#39;: \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c.jpg\u0026#39;}, {\u0026#39;S\u0026#39;: \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-01.jpg\u0026#39;}, {\u0026#39;S\u0026#39;: \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-02.jpg\u0026#39;}, {\u0026#39;S\u0026#39;: \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-03.jpg\u0026#39;}, {\u0026#39;S\u0026#39;: \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-04.jpg\u0026#39;}, {\u0026#39;S\u0026#39;: \u0026#39;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-05.jpg\u0026#39;} ]}, \u0026#39;Title\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;初歌番組。武元唯衣です。\u0026#39;}, \u0026#39;Url\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;http://www.keyakizaka46.com/s/k46o/diary/detail/19596?ima=0000\u0026amp;cd=member\u0026#39;} } 上に出てきているものでは\nS: 文字列 N: 数値 L: リスト のような感じです。リスト内の項目についても、それぞれ型の指定が必要です。\nまた、N で数値を指定する際にも、数値ではなく文字列で値を定義する必要があります。上の例では Entrydate の部分で 'Entrydate': {'N': 20190224}, ではなく 'Entrydate': {'N': '20190224'} とします。\nboto3 docs DynamoDB.Client.put_item データの取得 get_item() ここでいうデータの取得とは、単一のデータ取得です。 RDB の SELECT のような感じで複数データを取得する場合は query() を使いますが、それはまた次回。 単一のデータを取得する get_item() は、次のように使います。\nimport boto3 table_name = \u0026#39;keyakizaka-blog\u0026#39; dynamo = boto3.client(\u0026#39;dynamodb\u0026#39;) res = dynamo.get_item(TableName=table_name, Key={ \u0026#39;Authorcode\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;1001\u0026#39;}, \u0026#39;Url\u0026#39;: {\u0026#39;S\u0026#39;: \u0026#39;http://www.keyakizaka46.com/s/k46o/diary/detail/19596?ima=0000\u0026amp;cd=member\u0026#39;} }) データが存在していた場合は、下記のようなレスポンスが返ってきます。\n{ \u0026#34;Item\u0026#34;: { \u0026#34;Title\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;初歌番組。武元唯衣です。\u0026#34; }, \u0026#34;Authorcode\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;1001\u0026#34; }, \u0026#34;Author\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;欅坂46二期生\u0026#34; }, \u0026#34;Images\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;S\u0026#34;: \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c.jpg\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-01.jpg\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-02.jpg\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-03.jpg\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-04.jpg\u0026#34; }, { \u0026#34;S\u0026#34;: \u0026#34;http://cdn.keyakizaka46.com/images/14/91e/fd0a1b0dec088e270f545ce11bd3c-05.jpg\u0026#34; } ] }, \u0026#34;Entrydate\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;20190224\u0026#34; }, \u0026#34;Url\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;http://www.keyakizaka46.com/s/k46o/diary/detail/19596?ima=0000\u0026amp;cd=member\u0026#34; } }, \u0026#34;ResponseMetadata\u0026#34;: { \u0026#34;RequestId\u0026#34;: \u0026#34;CXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG\u0026#34;, \u0026#34;HTTPStatusCode\u0026#34;: 200, \u0026#34;HTTPHeaders\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;Server\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon, 03 Mar 2019 05:23:22 GMT\u0026#34;, \u0026#34;content-type\u0026#34;: \u0026#34;application/x-amz-json-1.0\u0026#34;, \u0026#34;content-length\u0026#34;: \u0026#34;776\u0026#34;, \u0026#34;connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;x-amzn-requestid\u0026#34;: \u0026#34;CXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG\u0026#34;, \u0026#34;x-amz-crc32\u0026#34;: \u0026#34;1759887117\u0026#34; }, \u0026#34;RetryAttempts\u0026#34;: 0 } } データが存在しなかった場合は、下記のようなレスポンスになるので、Item キーの存在チェックでデータの存在チェックができます。\n{ \u0026#34;ResponseMetadata\u0026#34;: { \u0026#34;RequestId\u0026#34;: \u0026#34;HXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG\u0026#34;, \u0026#34;HTTPStatusCode\u0026#34;: 200, \u0026#34;HTTPHeaders\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;Server\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon, 03 Mar 2019 05:25:51 GMT\u0026#34;, \u0026#34;content-type\u0026#34;: \u0026#34;application/x-amz-json-1.0\u0026#34;, \u0026#34;content-length\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;x-amzn-requestid\u0026#34;: \u0026#34;HXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG\u0026#34;, \u0026#34;x-amz-crc32\u0026#34;: \u0026#34;2745614147\u0026#34; }, \u0026#34;RetryAttempts\u0026#34;: 0 } } boto3 docs DynamoDB.Client.get_item まとめ 今回は boto3 の client.put_item() / client.get_item()　を使いました。 ただ、どうやら以下のようにすることで Item に普通の dict を渡すことができるようです。\ntable_name = \u0026#39;keyakizaka-blog\u0026#39; dynamo = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamo.Table(\u0026#39;name\u0026#39;) table.put_item(Item={ /* ... */ }) また、複数のデータを登録する際には、DynamoDB の BatchWriteItem オペレーションを利用するのが良さそうです。\n\u0026amp;ldquo;DynamoDB での項目の操作 - Amazon DynamoDB\u0026amp;rdquo; あと、今回プライマリソートキーには文字列を指定していますが、ここに指定するのは数値のほうが良い気もしています。 そのあたりも含めて、もうちょっと DynamoDB 触ってみたいと思います。\nこのほかにも、 boto3 を使って DynamoDB を操作してみた話を書いているので、参考にしてみてください。\n",
    "permalink": "https://michimani.net/post/aws-put-get-item-dynamodb-by-lambda-python/",
    "title": "Python (boto3) で DynamoDB を操作する - put_item()/get_item()"
  },
  {
    "contents": "AWS Loft Tokyo で開催された Hands-on for developers_Hajimete serverless_Tokyo_20190227 - 知っ得ハンズオンシリーズ はじめての Serverless に参加してきたので、そのレポート・メモです。\n目次 スケジュール 座学セッション 今回やるハンズオンの内容 Serverless とは AWS におけるサーバレスコンピューティング (と今回使うサービスの概要) ハンズオン Step1: Cognito, IAM を利用して S3 のバケット操作で権限による挙動の変化を確認する Step2: API Gateway, Lambda, DynamoDB を利用して REST インターフェースを構築する 全体の感想 スケジュール 18:00 - 18:30 受付 18:30 - 19:00 座学セッション ハンズオンの説明 Serverless とは AWS におけるサーバレスコンピューティングについて 19:00 - 20:30 ハンズオン (各自黙々とやる感じ) 20:30 - 21:00 質問など 座学セッション 最初は座学セッションということで、今回やるハンズオンの説明と、そもそも Serverless とは？というところから、AWS におけるサーバーレスコンピューティングについてのセッションでした。 私は、サーバーレスという言葉自体はもう既に何度も聞いたことがあるし、内容についてもふんわりとは分かっているという状態。また、実際に AWS Lambda で関数も作ったことがある。そんな人間なので、最初の座学セッションは良い復習になりました。\n以下、座学セッションのメモです。\n今回やるハンズオンの内容 Step1 Cognito, IAM を利用して S3 のバケット操作で権限による挙動の変化を確認する Step 2 API Gateway, Lambda, DynamoDB を利用して REST インターフェースを構築する Serverless とは サーバ管理は不要 利用状況に応じて自動で拡張 開発・実装において注意点あり 利用していないリソースの支払いは不要 可用性や対障害性は実装済み AWS におけるサーバレスコンピューティング (と今回使うサービスの概要) API Gateway や S3 へのオブジェクトプットなどのイベントがあったときにコンテナ環境が立ち上がる RDB への接続が苦手 コネクションポーリングが使えない だから DynamoDB (NoSQL) を使う 関数単位 細かいセキュリティ設定が可能 開発段階で適当にセキュリティを決めると、後々大変 今回は DynamoDB への書き込み・読み込み の 2 つの関数を作る Lambda の実行は 900 sec 以内 それより長くなる場合は Fargate 使う IAM AWS リソースに対するきめ細やかなアクセス制限が可能 IAM ユーザ : 人間に付与する管理権限 IAM ロール : AWS リソースに付与する管理権限 IAM ポリシー : ユーザ、ロールに付与する権限パラメータ (JSON) あくまでも AWS 内のリソースに対する権限 Amazon Cognito JavaScript などから起動 Cognito 経由でログインした場合に一時的に IAM ロールを付与する形 機能は大きく 2 つ (3 つあるが、1 つは現在非推奨 -\u0026gt; AppSync) User Pools : ID/PWD の管理サービス (認証) Identity Pool: User Pools のユーザに対して権限付与 (認可) Lambda 起動の遅い/速いで言語を選ぶ 数秒で終わる処理ならスクリプト言語、時間のかかる処理の場合はコンパイル言語 カスタムランタイムを利用する場合の注意点 実行ライブラリは個人で管理 API Gateway Web API の作成・保護・運用 リクエストしているユーザが Cognito で認証されているか、もチェック可能 ハンズオン ハンズオンは Step1 と Step 2 に分かれていましたが、それぞれ (一応) 独立しているのでどちらからやっても OK という形式でした。 ハンズオン用の資料.zip をダウンロードして、その中に含まれる PDF を見ながら、同じく zip 内に含まれるサンプルスクリプトやコピペ用テキストを利用して、各自黙々と進めていき、質問があれば適宜 SA さんに声をかける という形でした。\nStep1: Cognito, IAM を利用して S3 のバケット操作で権限による挙動の変化を確認する 実装したサンプルは次のような内容です。\nCognito で Sign up / Sign in する Sign up 時に付与したパラメータによって、適用する IAM ロールを変える 今回であれば、attr というカスタム属性が 1 のユーザにだけ、別の IAM ロールを割り当てる という形 Sign in したユーザで S3 のオブジェクトにアクセスする IAM ロールで S3 へのアクセス権限を絞る Sign in 時のパラメータによって、S3 オブジェクトへのアクセス可否を確認する 図にするとこんなイメージです。\nこれまでは Cognito ってどうやって使うんや？ っていう状態でしたが、今回のハンズオンで少しわかった気がしました。\nCognito は認証・認可基盤のサービスです。AWS では IAM も認証・認可基盤のサービスですが、その違いとしては、IAM は認証・認可する対象が AWS のリソース であるのに対して、Cognito は対象が クライアントアプリ という点です。クライアントアプリから認証を行うことで、その認証に適用されている IAM ロールの権限をもって、AWS リソースにアクセスできる という感じです。\nどうでもいいですが、 Cognito は **ｺｸﾞﾆｰﾄ** って読むんですね...。勝手に **ｺｸﾞﾅｲﾄ** だと思ってました。 ちょっと取っ掛かりにくいサービスではありますが、とりあえず Cognito で何が出来るのか はわかりました。\nStep2: API Gateway, Lambda, DynamoDB を利用して REST インターフェースを構築する 実装したサンプルは次のような内容です。\nDynamoDB に対してデータの Read/Write をする Lambda 関数をそれぞれ作成 API Gateway で GET/POST メソッドに対して、それぞれ Read/Write の Lambda 関数が呼ばれるようにする ブラウザから GET のエンドポイントを直接叩いて DynamoDB のデータが取得できるか確認する cURL、Postman などを使って POST のエンドポイントにアクセスして、DynamoDB にデータが登録できるか確認する 図にするとこんなイメージです。\nAPI Gateway については、過去に使ってみようとしたものの、コンソール上の操作がよくわからずに中途半端なところで挫折した という経緯があります..。たが、今回は前述したとおりハンズオンの資料が非常に丁寧だったので、順を追って操作することができました。\nちなみに、ハンズオンの資料内では API Gateway で生成されたエンドポイントを直接叩くところまでしか書いてありませんでしたが、資料内の js ファイルを少し修正すれば、Ajax で実行できるようになっていたようです。\nハンズオンの時間が少し余ったのでそのあたりを試していたのですが、詰まったポイントがあったので書いておきます。\nハンズオンの資料に用意されている html (localhost) から Ajax で API を実行しようとすると、CORS エラーで実行できません。なので、API Gateway で CORS を有効にする必要があります。\nまた、有効化したあとは再度デプロイしないと反映されません。そこもハマったポイントでした。\nこれで localhost からも Ajax で実行できるようになりました。今回はサンプルということで Access-Control-Allow-Origin を \u0026quot;*\u0026quot; にしてますが、アクセス元がわかっているなら明示的に指定したほうがよさそうです。\nStep2 のハンズオンでは、 API Gateway の他に DynamoDB も使いました。こちらも初めて使いました。 思っていたよりも簡単に使えるのと、なにより料金が安い！個人で使うレベルであれば無料枠内でおさまります。一方、RDS は結構な料金がかかるので、なかなか個人では試せないという印象です。 NoSQL についてはこれまではあまり触ってこなかったんですが、更新の少ないシンプルな REST API では、RDB より断然 NoSQL だなとあらためて感じました。 RDB と NoSQL では用途がぜんぜん違うので単純な比較は難しいですが、これまでは簡単な API ですら RDB を使おうとしていたので、その考え方が変わったのは良かったです。\n全体の感想 AWS Loft で開催されるハンズオン、というかハンズオン自体はじめての参加でしたが、以上に充実した時間でした。ハンズオン用の資料も丁寧に作られていて、基本的に手順通りに操作していれば最低限の目的のものは完成するという状態でした。 今回であれば、途中で IAM ポリシーの作成や S3 バケットへの CORS 設定などの手順もありましたが、それらの設定のために必要な文字列、JSON データもハンズオン資料にすべて含まれているため、スムーズに進めることが出来ました。\nかと言って、単純なコピペ作業で終わるのではなく、提供された資料に含まれるサンプルコードも、後に自分で何か実装するときには役に立つものだと思いました。今回は Web 上から Cognito で認証したり、S3 バケット内のリストを取得したりファイルをアップロードしたり、また、Ajax で API Gateway のエンドポイントにアクセスしたりする内容でしたが、それらの操作を実現する JavaScript の処理 (.js ファイル) はハンズオン資料にすべて含まれています。なので、これらを見ることで自分で実装するときの参考にできるというわけです。\nこんなにも充実したハンズオンが、なんと無料で受けられるなんて驚きですよね。 先週は JAWS DAYS で色んな話を聞いてきましたが、やはり自分で手を動かしてみないとわからない部分は多いと思うので、ハンズオン形式で実際にサービスを触るというのは凄く良いなと思いました。\nちなみに、同じ内容のハンズオンが 3/26 (火) 18:30-20:30 に開催されるようなので、Cognito, API Gateway, DynamoDB などのサービスを実際に触ってみたい方は参加してみてはいかがでしょうか。\nAWS Loft Tokyo にて開催されるイベント・セミナー一覧 API Gateway 完全に理解した。\n\u0026mdash; michimani (@_michimani_) February 27, 2019 理解したとは言っていない。\n",
    "permalink": "https://michimani.net/post/aws-handson-hajimete-serverless/",
    "title": "知っ得ハンズオンシリーズ はじめての Serverless＠AWS Loft Tokyo に行ってきました"
  },
  {
    "contents": "TOC 五反田メッセで開催された JAWS DAYS 2019 に参加してきたので、そのレポートです。 JAWS DAYS は今年で 7 回目らしいのですが、今回が初参加でした。なので、各セッションの詳細レポートというよりは、 JAWS DAYS とは？？ みたいな内容になってます。\nJAWS DAYS とは？ Amazon Web Service (AWS) のユーザーグループである JWAS-UG (AWS Use Group - Japan) が主催、アマゾンウェブサービスジャパンが後援となって開催される JAWS-UG 最大のイベントです。 冒頭にも書きましたが今回で 7 回目の開催で、毎年 AWS に関する幅広いテーマの様々なセッションが開かれているイベントです。\nAbout | JAWS DAYS 2019 なぜ参加したのか 私自身、 AWS を触り始めて約 1 年半くらいになります。その間、AWSome Day 2018 東京、Developers.IO 2018、AWS Start-up ゼミ＠AWS Loft Tokyo といった AWS に関するセミナー/イベントに参加してきました。(ここでいう参加とは、登壇とかではなく単にセッションを聞きに行っただけです) で、今年は AWS 関係に限らずそういった勉強会的なものにもう少し積極的に参加していこうということで、今回の JAWS DAYS 2019 にも参加することにしました。\n参加方法 Doorkeeper というイベント管理サービスから参加登録しました。 料金は 1,000 円で、事前支払い または 当日支払い が選べました。ただ、当日の受付は非常に混雑していたので、できる限り事前支払いのほうがよいでしょう。\n参加登録すると QR コードが発行されるので、当日はその QR コードを受付で読み取ってもらう形になります。 この QR コードですが、 iOS であれば Wallet アプリに追加できるので、わざわざ印刷していったり、Doorkeeper のサイトにアクセスして表示させたりする必要がなく、とても便利でした。\nJAWS DAYS はお祭りだった こういうセミナーとかって、ちょっと殺伐としてるというか、ピリッとした雰囲気があるんじゃないかと勝手に不安になっていたんですが、 JAWS DAYS は全くそんなことはありませんでした。 事前にちらっと聞いていた通り、すごく賑やかでお祭りのような雰囲気でした。\n特に企業サポーターやサークルサポーターがブースを出しているトラック (セッションエリア) はかなり賑わっていました。ただ、今回はセッションを聞くことに注力しすぎて、ゆっくり見て回れなかったのが失敗ですね。 ただ、 Twitter で流れてきた JAWS どら焼きはしっかりいただきました。\n当日は みなさん #jawsdays のハッシュタグを付けて色々ツイートしていたので、このタグのツイートを見れば大体の雰囲気はわかると思います。\nセッションの内容も様々 今回はトラック (セッションブース) が A 〜 J の 11 トラックに分かれていて、各回 1 〜 3 名の登壇者によるセッションが行われていました。 個人的には、最近の業務に関わる内容や自分に足りてないところの内容に関わるセッションを聞こうということで、主に コンテナ (Docker/Kubernetes)、サーバレス関係のセッションを聞いてました。他にどんなセッションがあったかは タイムテーブル をご覧ください。\nちなみに、前日にふと思いついて タイムテーブル内の特定のセッションに色を付ける bookmarklet を作った んですが、まあ、それなりに活躍しました。実際に行こうと思っていたセッションが人気すぎて聞けなかったりして、予定していたセッションとは違うセッションを聞くこともありましたが、どれも面白い内容で退屈することはありませんでした。\nただ、やっぱり今の自分では話についていけない内容も多々あったので、そのあたりは今後少しずつ勉強していきたいと思います。まあ、そういう足りない部分を明確にするきっかけにはなりますね。\nあと、今回の反省としては、セッション聴講用のレシーバをしっかり確保しておくということです。 各トラックは一応パーティションで仕切られていますが、完全に仕切られているわけではないため他のトラックの音も聞こえてきます。そういう状態のため、登壇者のマイクの音量はかなり絞られていて、その代わりに、マイクの音声は配布されるレシーバで聞くことができるようになっています。なので、レシーバなしの場合は登壇者のすぐ近くの席に座っていないと、かなり聞きづらい状態でした。\n後半につれて参加者の人数も増えていき (なんと来場者数 1900 人!!)、如何にして登壇者の近くを確保するかという感じでした。ここも反省点ですね。\nお弁当が美味しい お昼の時間帯にはお弁当が配布されました。今回は以下の 4 種類のお弁当が配布されることになっていました。\n海老チリ弁当 豪華一口カツと鮮魚の照焼き弁当 油淋鶏弁当 ヒレエビ弁当 当初の予定では 豪華一口カツと鮮魚の照焼き弁当 を狙っていたんですが、お昼の時間帯には参加者全員がお弁当配布の列に並ぶことになるので、会場内は大混雑。結果的に一番近くで配布されていた ヒレエビ弁当 を食べました。 ただ、このお弁当の美味しいこと。\n量も味も大満足でしたね。 ちなみに、お弁当以外にも、野外にあるキッチンカーで豚丼やラーメンも提供されていました。\n懇親会は行ったほうがいい 全セッションが終わったあとは懇親会があったんですが、これは絶対に行くべきでしたね\u0026hellip;。あとで Twitter を見て思いました。 こういうイベントは自分の知識を増やすためという目的もありますが、やっぱり他の人との関わりを作るということも大事だということですね。せっかく地方からこんな 空気の悪い 便利でイベントもたくさんある都会に移り住んでいるのに、そのメリットを生かさないと損ですよね。\nというのが最後の反省点。\nまとめ 初めて JAWS DAYS に参加してきたレポートでした。 色々と反省点はありますが、それ以上に面白かった・楽しかったという印象が強いです。たった一日でしたが、本当に色々な話を聞くことができたので、かなり濃い一日でした。また来年も開催されれば参加したいですし、JAWS DAYS 以外のイベントにも参加してみようと思いました。\n運営のみなさま、登壇者のみなさま、参加されたみなさま、お疲れさまでした。\n",
    "permalink": "https://michimani.net/post/aws-jaws-days-2019-report/",
    "title": "JAWS DAYS 2019＠TOC 五反田メッセ に行ってきました"
  },
  {
    "contents": "JAWS DAYS 2019 が明日に迫っています。\n私自身、初の JAWS DAYS ということで、別に登壇するわけでもないのですが何故か緊張しています。 というのも、セッションごとにトラックを移動するのがうまくいくか、これが不安要素です。\nよく参加されてるみなさんは、自分が参加しようとしているセッションをどのように管理されているのでしょうか。\n個人的には、タイムテーブルに色でも付いていればいいのにな、と思ってます。\nということで、タイトルにもあるように、 JAWS DAYS のタイムテーブルの特定のセッションをハイライトする bookmarklet を作ってみました。\n使い方 下のテキストエリアに、ハイライトしたいセッションの URL を改行区切りで入力します。例えば、下記のような形。\nhttps://jawsdays2019.jaws-ug.jp/session/1527/ https://jawsdays2019.jaws-ug.jp/session/1270/ すると、その下のテキストエリアに、入力したセッションをハイライトする bookmarklet が出力されるので、その bookmarklet を JAWS DAYS 2019 のタイムテーブルページ で使用します。\nbookmarklet 自体の使い方についてはぐぐってください。\nこんな感じでハイライトされます。\n一応 Chrome (72.0.3626.96) と Safari (12.0.3) で動作は確認していますが、動かなかったらすみません。\n",
    "permalink": "https://michimani.net/post/development-create-timetable-color-bookmarklet-jd2019/",
    "title": "JAWS DAYS 2019 のタイムテーブルの特定のセッションをハイライトする bookmarklet を作った"
  },
  {
    "contents": "Lambda を VPC 内で実行する場合、外部ネットワークと接続するためには NAT ゲートウェイを使用する必要があるのですが、 NAT ゲートウェイの利用には 約 4,000 〜 5,000 円の費用がかかるため、なんとかして利用しない方向で構築できないかと色々試してみました。\nやりたいこと やりたいのは、 EC2 に対して、 IP 制限されている FTP ポートへのヘルスチェック (FTP ログイン試行) を実施して、正常にアクセス出来なかった場合に Slack で通知する 、ということです。\n対象の EC2 と同じ VPC 内で実行する Lambda では、次のようなスクリプトを実行するとします。(ランタイム: Python 3.6)\nimport json from ftplib import FTP from urllib.request import Request, urlopen host = \u0026#39;EC2 のプライベートIP\u0026#39; user = \u0026#39;ftp-user\u0026#39; password = \u0026#39;ftp-user-pass\u0026#39; webhook_url = \u0026#39;slack-webhook-url\u0026#39; def lambda_handler(event, context): try: ftp = FTP(host=host, user=user, passwd=password, timeout=10) ftp.quit() except Exception as e: try: print(e) headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} data = bytes(json.dumps({\u0026#39;text\u0026#39;: str(e)}), \u0026#39;utf-8\u0026#39;) request = Request(webhook_url, data=data, headers=headers) urlopen(request) except Exception as e2: print(e2) 結論 試したことはあとに書くとして、まず結論から。\nどうしても Lambda を使いたい場合 どうしても Lambda を使って実現するためには、次のような構成でやるしかないようです。\n対象の EC2 と同じ VPC 内で ヘルスチェック用の Lambda を実行して、 Slack への通知 (外部ネットワークとの接続) のために NAT ゲートウェイを設置する、という構成です。\nま、これが一般的な構成になるんでしょうね。\n構成の構築手順 上図の構成を構築する手順としては、以下のとおりです。(既に VPC とパブリックなサブネットは存在するものとします)\nプライベートサブネットを作成 NAT ゲートウェイを作成 NAT ゲートウェイ用のルートテーブルを作成 作成したサブネット内で Lambda を実行 1. プライベートサブネットを作成 プライベートサブネットと言っても、普通のサブネットです。VPC 内に新たに作成します。\n2. NAT ゲートウェイを作成 マネジメントコンソールで VPC の画面左から NAT ゲートウェイを選択し、作成します。\nNAT ゲートウェイには EIP が必要になりますが、その場で作成できます。\n3. NAT ゲートウェイ用のルートテーブルを作成 VPC 内に、新たにルートテーブルを作成します。\nDestination 0.0.0.0/0 の Target として、作成した NAT ゲートウェイを指定します。\n4. 作成したサブネット内で Lambda を実行 Lambda のネットワーク設定で、 VPC と、作成したサブネットを指定します。\nLambda を使わない そもそも論になりますが、ヘルスチェックスクリプトを、どこか固定 IP を持っているサーバにおいて、そこで実行するという方法です。\n固定の IP があれば、その IP をセキュリティグループで許可すればいいだけです。\n既に別でサーバを持っている場合はこの方法が一番簡単で一番コストも低くなるでしょう。\nやってみたこと 以下、 NAT ゲートウェイを使わずになんとか出来ないか色々試してみた内容です。\nLambda を パブリックサブネット内で実行してみる ※Amazon SNS と Slack のアイコンは無視してください\n一見するとこの構成で Lambda は外部ネットワークに接続できそうですが、できません。\n通常、パブリックインターネットを通じてアクセスできないように、リソースは Amazon Virtual Private Cloud (Amazon VPC) 内に作成します。\nAmazon VPC 内のリソースにアクセスできるように Lambda 関数を構成する とあるように、 Lambda はパブリックサブネット内から外には出られません。\nなので、EC2 へのヘルスチェックは可能ですが、その結果を Slack へ投げる (Webhook を叩く) ところでタイムアウトしてしまいます。\nSlack への通知を SNS 経由にしてみる Lambda から、同じ AWS サービスである Amazon SNS を使って Slack へポストするという方法です。\n構成は上図のままで、 Lambda のスクリプトを下記のように変更します。\nimport boto3 import json from ftplib import FTP from urllib.request import Request, urlopen host = \u0026#39;EC2 のプライベートIP\u0026#39; user = \u0026#39;ftp-user\u0026#39; password = \u0026#39;ftp-user-pass\u0026#39; sns_topic_arn = \u0026#39;SNS-TOPIC-ARN\u0026#39; def lambda_handler(event, context): try: ftp = FTP(host=host, user=user, passwd=password, timeout=10) ftp.quit() except Exception as e: try: print(e) puclish_sns_topic(\u0026#39;SNS publish.\u0026#39;, str(e)) except Exception as e2: print(e2) def puclish_sns_topic(subject, message): sns_client = boto3.client(\u0026#39;sns\u0026#39;) sns_client.publish( TopicArn=sns_topic_arn, Message=message, Subject=subject ) また、 SNS から実行する Slack へのポスト用 Lambda を作成します。\nimport json from urllib.request import Request, urlopen webhook_url = \u0026#39;slack-webhook-url\u0026#39; def lambda_handler(event, context): try: message = str(event[\u0026#39;Records\u0026#39;][0][\u0026#39;Sns\u0026#39;][\u0026#39;Message\u0026#39;]) headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} data = bytes(json.dumps({\u0026#39;text\u0026#39;: message}), \u0026#39;utf-8\u0026#39;) request = Request(webhook_url, data=data, headers=headers) urlopen(request) except Exception as e: print(e) 結果としては、 Lambda から SNS の API を実行するところでタイムアウトになります。\n同じ AWS のサービスとは言え、ネットワークは別物です。当たり前のことですが、雰囲気でイケると閃いたのでやってみました。\nNAT ゲートウェイを設置する ということで、やはり NAT ゲートウェイは必要であるということから、実際に設置してみた構成がこれです。\nこうすることで、 Lambda から SNS への Publish が可能になり、無事に Slack への通知もできるようになりました。\n\u0026hellip; いや、NAT ゲートウェイ使うなら SNS 経由で Slack へ投稿する必要無いな。ということで、冒頭に書いた最初のスクリプトで、やりたいことが実現できるようになりました。\nまとめ 色々やってみたようで、最終的には公式ドキュメントに載っているレベルの結論しか得られませんでした。\nただ、個人的には色々試して納得できたので良かったです。ネットワークまわりの構成って面倒くさいイメージがありますが、 AWS だとポチポチしていくだけなので簡単ですね。とは言えネットワークまわりの知識はほとんどないので、単に VPC だサブネットだと言っても奥が深いなと感じました。\n",
    "permalink": "https://michimani.net/post/aws-run-lambda-in-vpc/",
    "title": "Lambda を VPC 内で実行して外部ネットワークと接続する (ために色々やってみた)"
  },
  {
    "contents": "起こった事象 Homebrew を 使って macOS にインストールしていた PHP が、何の前触れもなく使えなくなっていました。\n$ php -v dyld: Library not loaded: /usr/local/opt/icu4c/lib/libicui18n.62.dylib Referenced from: /usr/local/opt/php@7.1/bin/php Reason: image not found [1] 8697 abort php -v 環境 MacBook Pro (15-inch, 2016) macOS Sierra 10.12.6 Homebrew 2.0.0 やったこと そもそもインストールされているのか $ brew list | grep php php@7.1 されています。消えたわけではなさそうです。\n再インストールする とりあえず再インストールしてみます。\n$ brew reinstall php@7.1 ==\u0026gt; Reinstalling php@7.1 ... ... 再インストールしたところ、無事に使えるようになりました。\n$ php -v PHP 7.1.25 (cli) (built: Dec 7 2018 08:24:41) ( NTS ) Copyright (c) 1997-2018 The PHP Group Zend Engine v3.1.0, Copyright (c) 1998-2018 Zend Technologies with Zend OPcache v7.1.25, Copyright (c) 1999-2018, by Zend Technologie ",
    "permalink": "https://michimani.net/post/development-brew-php-image-not-foud-error/",
    "title": "Homebrew を使って macOS にインストールした PHP が使えなくなった"
  },
  {
    "contents": "やりたいこと Qiita の記事編集画面ではタグを入力するテキストボックスがあります。あそこは半角スペース区切りで文字を入力すると、その区切りごとに文字の背景色がついて、どんなタグが設定されるのか判別しやすくなります。\nこの UI はタグ付けだけではなく検索窓とかにも使えそうなので、今回はこの UI を再現してみます。\nやってみた ということで、やってみた結果です。下のテキストボックスに文字を入力すると、半角スペース区切りで背景色がつきます。\n\u0026ldquo;背景色\u0026rdquo; と言いながら、実は被せている このテキストボックスの HTML と CSS は次のようになっています。\n\u0026lt;div id=\u0026#34;overlay-sample\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;overlay-textbox-area\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;overlay-area\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; autocomplete=\u0026#34;off\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; #overlay-sample { margin: 30px; font-family: inherit; } #overlay-area { color: transparent; letter-spacing: .02em; margin: 2px; overflow-wrap: break-word; overflow: hidden; padding: 13px 12px; position: absolute; white-space: pre-wrap; width: 100%; font-size: 16px !important; } #overlay-area span.overlay { background-color: #ffcc80; border-radius: 2px; box-shadow: 0 0 0 1px #ffa726; padding-bottom: 1px; padding-top: 1px; } #overlay-textbox-area input[type=text] { font-family: inherit; font-size: 16px !important; letter-spacing: .02em; outline: 0px; padding: 6px 12px; position: relative; width: 100%; } 色々と細かいスタイルを設定していますが、肝心なのは次の 2 点です。\n背景色は背景ではなくオーバーレイ テキストボックスとオーバーレイ要素のフォント・サイズを合わせる 1. 背景色は背景ではなくオーバーレイ 背景色と言いながらも、実際は背景色がついた要素を上から被せています。上にかぶせているのが #overlay-area span.overlay で指定されている要素です。例えば Hello World!! こんにちは 世界 と入力したときの HTML は次のようになっています。\n\u0026lt;div id=\u0026#34;overlay-sample\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;overlay-textbox-area\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;overlay-area\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;overlay\u0026#34;\u0026gt;Hello\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;overlay\u0026#34;\u0026gt;World!!\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;overlay\u0026#34;\u0026gt;こんにちは\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;overlay\u0026#34;\u0026gt;世界\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; autocomplete=\u0026#34;off\u0026#34; value=\u0026#34;Hello World!! こんにちは 世界\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; テキストボックスの上に span.overlay が乗るようにしています。 span.overlay の背景色は好きな色にします。そして、その親要素である #overlay-area には、透過されるように color: transparent; を指定します。\n2. テキストボックスとオーバーレイ要素のフォント・サイズを合わせる これが一番のポイントですが、テキストボックスとオーバーレイ要素のフォントとサイズは一致させておく必要があります。また、文字幅を指定する letter-spacing についても、同じスタイルを適用する必要があります。\n当たり前といえばそれまでですが、テキストボックスに対しては別のスタイルが適用されたりすることもあるので、明示的に同じスタイルが適用されるようにします。上の例では font-family: inherit; でスタイルを継承しています。\nテキストの入力・変更は JS で監視 文字入力イベントについては JavaScript で監視します。実際には、テキストエリア内でのキーダウンをトリガーとして、入力された文字を半角スペースで分割 → span 要素を追加 しています。\nconst textElem = document.querySelector(\u0026#39;#overlay-textbox-area input[type=text]\u0026#39;); textElem.addEventListener(\u0026#39;keyup\u0026#39;, (event) =\u0026gt; { renderOverLay(event.target.value); }); function renderOverLay(text) { const overlayArea = document.getElementById(\u0026#39;overlay-area\u0026#39;); const charPartsList = text.split(\u0026#39; \u0026#39;); let spanSource = \u0026#39;\u0026#39;; for (let i = 0; i \u0026lt; charPartsList.length; i++) { if (spanSource !== \u0026#39;\u0026#39; \u0026amp;\u0026amp; !spanSource.match(/ $/)) { spanSource += \u0026#39; \u0026#39;; } if (charPartsList[i] === \u0026#39;\u0026#39;) { spanSource += \u0026#39; \u0026#39;; } else { spanSource += `\u0026lt;span class=\u0026#34;overlay\u0026#34;\u0026gt;${charPartsList[i]}\u0026lt;/span\u0026gt;`; } } overlayArea.innerHTML = spanSource; } このとき、DOM 要素を追加する要素 (今回であれば .overlay-area ) に対して white-space: pre-wrap; を適用しておく必要があります。\nまとめ ソースは gist に置いてます。\nmichimani/textbox-overlay-by-whitespace.html ",
    "permalink": "https://michimani.net/post/programming-textbox-overlay-css-style-by-whitespace/",
    "title": "テキストボックスで半角スペース区切りの文字を判別しやすくする"
  },
  {
    "contents": "CSS だけでアニメーションを描画することができるということで、前回は単位円を描く点のアニメーションを作成しました。\n今回はその続きというか、応用というか、正弦曲線を描く点のアニメーションを作成してみます。\n正弦曲線 その名の通り、正弦関数によって表される曲線で、式にすると\n$${y} = \\sin({x})$$\nで表される曲線のことです。\n描いてみた 正弦曲線は周期を持つ曲線なので、今回は以下の条件で表される曲線を描く点のアニメーションを作成してみます。\n$${y} = \\sin({x}),　-\\pi \\leq {x} \\leq \\pi$$\n※以下、正弦曲線 \"風\" というイメージでご覧ください html, css まず、次のような HTML 要素と、それに対するスタイルを用意しておきます。\n\u0026lt;div id=\u0026#34;graph-area\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;dot\u0026#34; class=\u0026#34;sinx\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;hr id=\u0026#34;x-axis\u0026#34; class=\u0026#34;axis\u0026#34;\u0026gt;\u0026lt;/hr\u0026gt; \u0026lt;hr id=\u0026#34;y-axis\u0026#34; class=\u0026#34;axis\u0026#34;\u0026gt;\u0026lt;/hr\u0026gt; \u0026lt;/div\u0026gt; #graph-area { position: relative; height: 320px; margin: 50px 0px 50px calc(50% - 160px); width: 320px; } .axis { border: 0.5px solid #959595; margin: 0px; position: absolute; width: 100%; top: 160px; } #x-axis { } #y-axis { transform: rotate(90deg); } #dot { border-radius: 50%; height: 10.5px; left: -46px; position: absolute; top: 155px; width: 10.5px; z-index: 100; background-color: #000000; } #dot.sinx { animation-duration: 1500ms; animation-fill-mode: forwards; animation-iteration-count: infinite; animation-name: drawSin; } 前回と同様に、アニメーションとして drawSin という名前で keyframe を定義します。\n@keyframes drawSin { 0% { left: 29px; top: 155.0px; } 2.0% { left: 34px; top: 174.499px; } 4.0% { left: 39px; top: 193.708px; } 6.0% { left: 44px; top: 212.339px; } /* 途中省略 */ 97.0% { left: 274px; top: 127.216px; } 99.0% { left: 279px; top: 146.626px; } 100.0% { left: 281px; top: 155px; } } これまた前回と同様に、上の keyframe は下記スクリプトで生成しました。(100% の部分に関してはちょっと手で修正しました)\nimport math ACCURACY = 50 BASE_LEFT = 155 BASE_TOP = 155 RADIUS = 160 PER = round(360 / ACCURACY, 0) percent = 0 rad = 0 style = \u0026#39;@keyframes drawSin {\\n\u0026#39; print(\u0026#39;per:\u0026#39;, PER) # -180 \u0026lt;= rad \u0026lt;= 180 while (percent \u0026lt;= 100): tmp_rad = rad - 180 print(tmp_rad) x = math.radians(tmp_rad) print(\u0026#39;sin({}) = {}\u0026#39;.format(x, math.sin(x))) top = round(BASE_TOP - math.sin(x) * RADIUS, 3) left = round(BASE_LEFT + x * (RADIUS / 4)) style += \u0026#39;\u0026#39;\u0026#39; {}% {{ left: {}px; top: {}px; }}\\n\u0026#39;\u0026#39;\u0026#39;.format(percent, left, top,) rad += PER percent = round((rad / 360) * 100, 0) print(percent, \u0026#39;%\u0026#39;) style += \u0026#39;}\u0026#39; keyframe_css = \u0026#39;./keyframe.css\u0026#39; with open(keyframe_css, mode=\u0026#39;w\u0026#39;) as f: f.write(style) 今回は見た目の都合上、 x 軸の値に対して y 軸の値の 1/4 の値としています。(left = round(BASE_LEFT + x * (RADIUS / 4))　の部分)\nなので、見た目的には下記のような式に近い形になっています。\n$${y} = \\sin\\left(4{x}\\right),　-\\frac{\\pi}{4} \\leq {x} \\leq \\frac{\\pi}{4}$$\nひとこと CSS だけで正弦曲線を描いてみた話でした。\nCSS というよりは三角関数の話がメインみたいなところもありますが、計算で各状態での座標の位置さえ決まれば、 CSS だけでも複雑なアニメーションが描けることがわかりました。\n",
    "permalink": "https://michimani.net/post/programming-draw-sin-graph-using-css/",
    "title": "CSS だけで正弦曲線を描いてみる"
  },
  {
    "contents": "このブログは CloudFront + S3 で運用しています。アクセス解析に関しては Google Analytics を使っていますが、せっかくなので CloudFront が出力しているログを Amazon Athena を使って解析してみようと思います。\n目次 CloudFront でログ出力を有効にする Athena で CloudFront のログ解析 Athena で CloudFront のログ解析用のテーブルを作成する Athena で CloudFront のログを解析する Athena の料金 まとめ CloudFront でログ出力を有効にする そもそもログが出力されていなかったら意味がありません。\nCloudFront の Distribution Settings でログ出力が有効になっていることを確認します。なっていなかったら、適当な S3 バケットを作ってそこに出力されるようにしておきます。\nAthena で CloudFront のログ解析 Athena で CloudFront のログを解析するためには、解析に使用するテーブルを作成する必要があります。テーブルを作成すれば、そこに対して慣れ親しんだ SQL を発行することによってログの解析ができるようになります。\nAthena で CloudFront のログ解析用のテーブルを作成する テーブルを作成するための CREATE 文を作成します。\n必要なフィールドは、 CloudFront のログ、または 公式ドキュメントを確認します。\nアクセスログの設定および使用 - Amazon CloudFront 今回は CloudFront のログから必要なフィールドを確認してみます。ちなみにCloudFront の Distribution には Web と RTMP がありますが、今回は Web の場合についてです。\nCloudFront のログは一定時間ごとに gzip 圧縮された状態で出力されます。それぞれのログファイルの 2 行目にフィールドが半角スペース区切りで出力されています。\nこのままではフィールド名として使用できない名前・文字が含まれているので、それらの変更と見た目の問題も考慮して適宜変更します。具体的には、\ndate, time は予約語なので、先頭に request_ を付ける -, ( を _ に置換 ) を削除 すべて小文字にする という処理をします。\nこれでフィールド名が確定したので、下記のような CREATE 文を作成します。\nCREATE EXTERNAL TABLE IF NOT EXISTS michimani_cf_logs ( request_date DATE, request_time STRING, x_edge_location STRING, sc_bytes INT, c_ip STRING, cs_method STRING, cs_host STRING, cs_uri_stem STRING, sc_status STRING, cs_referer STRING, cs_user_agent STRING, cs_uri_query STRING, cs_cookie STRING, x_edge_result_type STRING, x_edge_request_id STRING, x_host_header STRING, cs_protocol STRING, cs_bytes INT, time_taken DECIMAL(8,3), x_forwarded_for STRING, ssl_protocol STRING, ssl_cipher STRING, x_edge_response_result_type STRING, cs_protocol_version STRING, fle_status STRING, fle_encrypted_fields STRING ) ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;\\t\u0026#39;, \u0026#39;input.regex\u0026#39; = \u0026#39;\\t\u0026#39; ) LOCATION \u0026#39;s3://S3_BACKET_NAME/michimani.net-cf-log/\u0026#39; TBLPROPERTIES (\u0026#39;has_encrypted_data\u0026#39;=\u0026#39;false\u0026#39;); ここで変更が必要なのは LOCATION 部分です。ここは、 CloudFront のログが出力される S3 バケット名を指定します。あとテーブル名も適宜指定します。\nCREATE 文の実行は、Athena のコンソール画面で テキストエリアに上記 SQL をコピペして Run query ボタンを押すだけです。\nAthena で CloudFront のログを解析する テーブルが作成できれば、あとは普通に SQL 文で解析ができるようになります。\nSELECT request_date, request_time, cs_method, cs_uri_stem FROM michimani_cf_logs WHERE cs_method = \u0026#39;GET\u0026#39; AND (cs_uri_stem LIKE \u0026#39;%/\u0026#39; OR cs_uri_stem LIKE \u0026#39;%index.html\u0026#39;) AND sc_status = \u0026#39;200\u0026#39; AND request_date = DATE \u0026#39;2019-01-28\u0026#39; AND request_time \u0026gt; \u0026#39;04:10:00\u0026#39; AND request_time \u0026lt; \u0026#39;04:20:00\u0026#39; ORDER BY request_time DESC limit 10; Athena を利用せずに解析しようとすると、すべてのログファイルを unzip してゴニョゴニョする必要があるので、非常に面倒です。それがこんなに簡単に出来てしまうのは凄い以外の感想がないです。\nAthena の料金 気になるのはお値段です。\nAthena の利用用金は、発行したクエリの対象となるデータ(sキャンされたデータ)のサイズによって決まり、そのデータサイズ 1TB ごとに 5 USD の料金が発生します。\nスキャンされたデータサイズは、クエリ実行画面で確認することができます。例えば上の実行結果では、クエリのテキストエリアの下に Run time: 3.29 seconds, Data scanned: 822.59 KB と出力されています。この 822.59 KB というのがスキャンされたデータサイズとなります。 バイト数はメガバイト単位で切り上げられるため、この場合は 10 MB として計算されます。\nAthena に関して発生する料金はクエリ実行に対してのみですが、その他に、もちろんですが S3 にログを保持しておくための料金は発生します。\n料金 - Amazon Athena | AWS まとめ 以上、 Amazon Athena を使って CloudFront のログを解析してみた話でした。\nAthena って凄く難しいというか、使い所がよくわかっていませんでしたが、こんなにも簡単に使えるのは驚きでした。本当にスタンダードな、初歩的な使い方ではありますが、とりあえず触ってみると親近感が湧きますね。\n",
    "permalink": "https://michimani.net/post/aws-analyse-cloudfront-log-using-athena/",
    "title": "CloudFront のログを Athena を使って解析してみる"
  },
  {
    "contents": "最近フロントエンドを触ることが多くなってきたので、 CSS でアニメーションを描画する方法について調べてみました。\nCSS でアニメーションを描画 一度だけ実行されるアニメーション まず、クリックするごとに形が変わる要素を作ってみました。\nクリックイベントに関しては JavaScript を使ってるものの、アニメーション自体は CSS のみ。以下、ソース。\nまず HTML はこれだけ。\n\u0026lt;div id=\u0026#34;css-test-object\u0026#34; class=\u0026#34;default\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 続いて CSS。\ndiv#css-test-object { width: 100px; height: 100px; margin: 20px 0px 20px calc(50% - 50px); box-shadow: 0 24px 38px 3px rgba(0,0,0,0.14), 0 9px 46px 8px rgba(0,0,0,0.12), 0 11px 15px -7px rgba(0,0,0,0.2); } div#css-test-object.default { animation-duration: 500ms; animation-fill-mode: forwards; animation-name: toDefault; } div#css-test-object.circle { animation-duration: 500ms; animation-fill-mode: forwards; animation-name: toCircle; } div#css-test-object.square { animation-duration: 500ms; animation-fill-mode: both; animation-name: toSquare; } @keyframes toDefault { from { background-color: #01579b; border-radius: 25%; } to { background-color: #ffffff; border-radius: 0%; } } @keyframes toCircle { from { background-color: #ffffff; border-radius: 0%; } to { background-color: #f57f17; border-radius: 50%; } } @keyframes toSquare { from { background-color: #f57f17; border-radius: 50%; } to { background-color: #01579b; border-radius: 25%; } } #css-test-object に対して 3 種類の class を準備して、それぞれのクラスがロードされた時に実行されるアニメーションを @keyframes で定義してます。例えば .circle の場合、\ndiv#css-test-object.circle { animation-duration: 500ms; animation-fill-mode: forwards; animation-name: toCircle; } @keyframes toCircle { from { background-color: #ffffff; border-radius: 0%; } to { background-color: #f57f17; border-radius: 50%; } } まず、 div#css-test-object.circle に対するスタイルの指定について。\nanimation-duration\nアニメーションの開始から終了までにかかる時間を指定しています。今回の場合、 500 ms つまり 0.5 秒かけて形を変化させています。\nanimation-fill-mode\nアニメーションの実行前後に、指定したスタイルを適用するかを指定しています。 none, forwards, backwards, both から指定します。デフォルトは none で、アニメーション実行時以外はそのスタイルが適用されません。forwards だと、最後のフレーム (つまり @keyframes の to) で指定したスタイルが保持されます。 その他については下記参照。\nanimation-fill-mode | MDN animation-name\n実行するアニメーションのキーフレームを示す @keyframes 名前を指定しています。\n続いて、アニメーションの中身 toCircle について。\nfrom\n開始時のスタイルを指定しています。\nto\n終了時のスタイルを指定しています。\n今回の場合 toCircle アニメーションでは、背景色が #ffffff から #f57f17 に、要素の角丸が 0% から 50% に変化します。\nちなみにクリックイベントについては下記のような JavaScirpt で実装。\nconst mainObj = document.getElementById(\u0026#39;css-test-object\u0026#39;); mainObj.addEventListener(\u0026#39;click\u0026#39;, (event) =\u0026gt; { changeObject(event); }); function changeObject(event) { if (event.target.classList.contains(\u0026#39;circle\u0026#39;)) { event.target.classList.remove(\u0026#39;circle\u0026#39;); event.target.classList.add(\u0026#39;square\u0026#39;); } else if (event.target.classList.contains(\u0026#39;square\u0026#39;)) { event.target.classList.remove(\u0026#39;square\u0026#39;); event.target.classList.add(\u0026#39;default\u0026#39;); } else { event.target.classList.remove(\u0026#39;default\u0026#39;); event.target.classList.add(\u0026#39;circle\u0026#39;); } } 繰り返し実行されるアニメーション ドットを動かしてみる。\n以下の HTML に対して、\n\u0026lt;div class=\u0026#34;dot-area\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;dot round\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 以下の CSS を適用してます。\ndiv.dot-area { position: relative; width: 100px; height: 100px; margin: 20px 0px 20px calc(50% - 50px); } div.dot { position: absolute; width: 10px; height: 10px; background-color: #000000; border-radius: 50%; left: 50px; top: 50px; box-shadow: 0 24px 38px 3px rgba(0,0,0,0.14), 0 9px 46px 8px rgba(0,0,0,0.12), 0 11px 15px -7px rgba(0,0,class0.2); } div.dot.round { animation-duration: 3000ms; animation-fill-mode: forwards; animation-iteration-count: infinite; animation-name: round; } div.dot.round-smooth { animation-duration: 3000ms; animation-fill-mode: forwards; animation-iteration-count: infinite; animation-name: roundSmooth; } @keyframes round { 0% { left: 100.0px; top: 50.0px; } 10.0% { left: 65.451px; top: 2.447px; } 20.0% { left: 9.549px; top: 20.611px; } 30.0% { left: 9.549px; top: 79.389px; } 40.0% { left: 65.451px; top: 97.553px; } 50.0% { left: 100.0px; top: 50.0px; } 60.0% { left: 65.451px; top: 2.447px; } 70.0% { left: 9.549px; top: 20.611px; } 80.0% { left: 9.549px; top: 79.389px; } 90.0% { left: 65.451px; top: 97.553px; } 100.0% { left: 100.0px; top: 50.0px; } } アニメーションを繰り返し実行するためには animation-iteration-count に infinite を指定します。ここを具体的な数値にすれば、その回数だけ繰り返されます。\nkeyframe について先程と違うのは、 from と to の代わりに、割合 (%) でスタイルを指定している点です。\nアニメーションの開始時を 0% 、終了時を 100% として、途中のスタイルを適宜指定します。\n本当は単位円を描画したかったんですが、上記の記述では動きがカクカクしています。なので、より細かくスタイルを指定してみます。\n@keyframes roundSmooth { 0% { left: 100.0px; top: 50.0px; } 1.0% { left: 99.606px; top: 43.733px; } 2.0% { left: 98.429px; top: 37.566px; } 3.0% { left: 96.489px; top: 31.594px; } 4.0% { left: 93.815px; top: 25.912px; } /* 略 */ 98.0% { left: 98.429px; top: 62.434px; } 99.0% { left: 99.606px; top: 56.267px; } 100.0% { left: 100.0px; top: 50.0px; } } こうすると、なめらかになります。\nちなみに、上のスタイルは下記のスクリプトで作ってます。三角関数知っててよかったー(棒)\nimport math ACCURACY = 100 BASE_LEFT = 50 BASE_TOP = 50 RADIUS = 50 PER = 360 / ACCURACY percent = 0 rad = 0 style = \u0026#39;@keyframes roundSmooth {\\n\u0026#39; while (percent \u0026lt;= 100): left = round(BASE_LEFT + math.cos(math.radians(rad)) * RADIUS, 3) top = round(BASE_TOP - math.sin(math.radians(rad)) * RADIUS, 3) style += \u0026#39;\u0026#39;\u0026#39; {}% {{ left: {}px; top: {}px; }}\\n\u0026#39;\u0026#39;\u0026#39;.format(percent, left, top,) rad += PER percent = round((rad / 360) * RADIUS, 0) style += \u0026#39;}\u0026#39; keyframe_css = \u0026#39;./keyframe.css\u0026#39; with open(keyframe_css, mode=\u0026#39;w\u0026#39;) as f: f.write(style) ACCURACY で精度、 RADIUS で単位円の半径 (px)、 BASE_LEFT と BASE_TOP で基準となる位置を指定します。\n以上、CSS でアニメーションを描画してみた話でした。\nとりあえず基本的な部分はわかったので、より複雑なアニメーションにもチャレンジできそうです。\n",
    "permalink": "https://michimani.net/post/programming-show-animation-using-css/",
    "title": "CSS だけで単位円を描くアニメーションを作る"
  },
  {
    "contents": "AWS CLI のコマンドを Tab キーで補完することができると知ったので、実際にやってみました。内容は参考記事とほぼ同じで、シェルだけが違います。今回は zsh で AWS CLI コマンドの補完を有効にする方法のメモです。\nAWS CLI のインストール方法によっては多少変わる部分があるとのことですが、自分の場合はその部分とは別の部分で躓きました。\nAWS CLIのコマンドを補完してみる（Tabキー万歳） ｜ DevelopersIO コマンド補完 - AWS Command Line Interface 前提 AWS CLI がインストールされている\nAWS Command Line Interface をインストールする - AWS Command Line Interface 自分は バンドルされたインストーラを使用して インストールしていたようです。(後に躓いた原因)\nやること AWS コンプリータ の場所を確認する .zshrc で AWS コンプリータ を読み込むようにする 1. AWS コンプリータ の場所を確認する AWS CLI コマンドの補完をしてくれる AWS コンプリータ がある場所を確認します。\n\u0026gt;\u0026gt;\u0026gt; which aws_completer aws_completer not found はい。いきなりおかしいですね。AWS CLI がインストールされていれば存在しているはずです。\n\u0026gt;\u0026gt;\u0026gt; which aws /usr/local/bin/aws 当たり前ですが aws コマンドは存在しています。\nなんとなく /usr/local/bin/ にありそうな気はするので、一応確認してみます。\n\u0026gt;\u0026gt;\u0026gt; ls -al /usr/local/bin/ | grep aws lrwxr-xr-x 1 root admin 22 12 22 12:13 aws -\u0026gt; /usr/local/aws/bin/aws どうやら aws の実態は /usr/local/aws/bin/aws にあるようです。なので、 /usr/local/aws/bin/ 以下を確認してみます。\n\u0026gt;\u0026gt;\u0026gt; ls -al /usr/local/aws/bin/ | grep aws -rwxr-xr-x 1 root wheel 824 12 22 12:13 aws -rwxr-xr-x 1 root wheel 1432 12 22 07:23 aws.cmd -rwxr-xr-x 1 root wheel 204 12 22 07:23 aws_bash_completer -rwxr-xr-x 1 root wheel 1145 12 22 12:13 aws_completer -rwxr-xr-x 1 root wheel 1807 12 22 07:23 aws_zsh_completer.sh ありました。\nバンドルされたインストーラを使用して AWS CLI をインストール した場合はこのような状態になっているようです。(どうやってインストールしたか忘れてた)\n無事に AWS コンプリータ の場所がわかりました。\nただ、 aws_completer にPATH が通っていないので通しておきます。というか、 aws と同様にシンボリックリンクを作成しておきます。\n\u0026gt;\u0026gt;\u0026gt; ln -s /usr/local/aws/bin/aws_completer /usr/local/bin/aws_completer \u0026gt;\u0026gt;\u0026gt; which aws_completer /usr/local/bin/aws_completer 2. .zshrc で AWS コンプリータ を読み込むようにする AWS コンプリータ を .zshrc で読み込むようにします。\nsource /usr/local/aws/bin/aws_zsh_completer.sh 上の 1 行を .zshrc に追記して、再読込します。\n\u0026gt;\u0026gt;\u0026gt; source ~/.zshrc これで AWS CLI コマンドの補完が有効になりました。試しに aws codecommit まで打って Tab キーを押してみると、次のように候補が出てきます。\n\u0026gt;\u0026gt;\u0026gt; aws codecommit batch-get-repositories delete-file get-comments-for-pull-request get-repository post-comment-for-pull-request update-pull-request-description create-branch delete-repository get-commit get-repository-triggers post-comment-reply update-pull-request-status create-pull-request describe-pull-request-events get-differences list-branches put-file update-pull-request-title create-repository get-blob get-file list-pull-requests put-repository-triggers update-repository-description credential-helper get-branch get-folder list-repositories test-repository-triggers update-repository-name delete-branch get-comment get-merge-conflicts merge-pull-request-by-fast-forward update-comment delete-comment-content get-comments-for-compared-commit get-pull-request post-comment-for-compared-commit update-default-branch これで色々と捗りそうです。\n",
    "permalink": "https://michimani.net/post/aws-cli-command-completion-at-zsh/",
    "title": "zsh で AWS CLI コマンドの補完を有効にしようとしたらちょっとだけ躓いた"
  },
  {
    "contents": "今年の 2 月に、2 年間ほど利用していた楽天モバイルから LINEモバイルに変更しました。\nLINEモバイルと言えば、本田翼の CM が最高に可愛いですね。\nということで、LINEモバイルに変更してからまだ 10 ヶ月ほどですが、楽天モバイルとの諸々の比較をしていきます。同じ LINEモバイルでも、この 10 ヶ月の間に docomo 回線から Softbank 回線に変更してそれぞれ 5 ヶ月ずつくらいの利用しているので、その辺の違いについても書きたいと思います。\nどんな使い方をしているのか 諸々を比較する前に、前提条件として 通話・モバイルデータ通信の使用状況について書いておきます。\n通話 電話はほとんどしません。\n店の予約などに使用するくらいで、毎月の通話合計時間は 0 〜 5 分程度です。\nモバイルデータ通信 以下、今月 (26日まで) の利用状況です。\n基本的にモバイルデータ通信を利用しているのは、平日は通勤時間帯のみで、用途としてはネットニュースやブログ記事などを見る程度で、ゲームや動画視聴はほとんどありません。休日は主に Google Map と SNS の利用で、こちらもゲームや動画視聴はありません。\n上のスクショからもわかるように、通信量の大部分を SNS (Twitter, Instagram) が占めています。後述しますが、 LINEモバイルではこれらの通信量がカウントされないので、だいぶ助かってます。\n楽天モバイル と LINEモバイル の比較 料金 まず、料金の比較です。\n楽天モバイル、 LINEモバイル どちらも 通話 SIM で、毎月のデータ通信量が 3 GB のプラン (楽天モバイルは 3.1 GB) を利用しています。 (していました)\nまた、どちらも かけ放題などの各種オプションは利用していません。\nプラン名 料金 (通話 SIM, 税別) 通話料 楽天モバイル 3.1 GB プラン 1,600 円 10円/30秒 LINEモバイル コミュニケーションフリープラン 3 GB 1,690 円 10円/30秒 月額費用としては楽天モバイルのほうが月額 90 円安いので、年間で 1,080 円安いことになります。\nその他のプラン\n楽天モバイル LINEモバイル ただし、 LINE モバイルのコミュニケーションフリープランでは、下記のアプリ利用時の通信量がカウントされません。\nLINE Twitter Facebook Instagram 上のスクショからもわかるように、私の使い方では Twitter, Instagram を利用することが多いため、これらの通信量がカウントされないのは 90 円以上の価値があると思っています。\n実際、 iPhone 上でのデータ通信量は 4.2 GB (およそ 4,200 MB) となっていますが、 LINE モバイル上でカウントされているデータ通信量は 1,232 MB となっています。今月が特別少ないわけではなく、毎月これくらいの値になってます。\nなので、同じ 3 GB であっても、 LINE モバイルであれば用途によってはその半分も利用しない (ことになっている) ということになります。\n実際、楽天モバイルのときは毎月の 3.1 GB をほぼほぼ使い切ってしまい、1ヶ月だけ 5 GB プランにしたりして過ごしていました。\nLINE モバイルにしてからは、毎月 1 〜 1.5 GB ほど余りが出るので、それを次の月に繰り越していくことで毎月付与される 3 GB をまるっと繰り越せる状態になっています。なので、実質 毎月 6 GB 分くらいは利用できる状態です。\nこんな感じで、最近では毎月 前の月の繰越分だけでまかなえている状態です。\n通信速度、通信エリア 肝心の通信速度、通信エリアについてです。\nそれぞれの具体的なスピードとかはしっかり測定してこなかったので、次の 5 つの場所について、それぞれの通信状況、位置情報の正確さがどんな感じだったかを書いていきます。文字だけだとあれなので、独断と偏見を含んだ形で各項目 1 〜 5 で点数付けしてみることにします。\n① 屋内 : 俗に言う屋内\n② 屋外 : 俗に言う屋外\n③ 電車内 (地上) : 地上を走ってる電車の車内\n④ 電車内 (地下) : 地下を走ってる電車の車内\n⑤ 郊外 (ちょっとした山間部) : 箱根とか奥多摩とか、そのあたりを想像してください\n楽天モバイル (docomo 回線) ①: 2\n②: 2\n③: 1\n④: 1\n⑤: 2\nとにかく全体的に電波が悪かった印象です。\n屋内・屋外問わず速度には不満ばかりで、位置情報の取得にも時間がかかったり正確でなかったりしていました。電車の車内では、地上・地下問わず更に電波が悪く、通常のニュースサイトですら表示に時間がかかっていました。位置情報をほとんど掴まないので、当時流行っていた Pokemon Go も全然楽しめませんでした。\n当時は WiMAX のポケット Wi-Fi を持っていたので、それ経由での通信が主な手段でした。WiMAX の電波も良いとは言えなかったので、だいぶストレスがありました。\nちなみに利用していた期間は 2016年4月 〜 2018年2月 の間ですが、最後の数カ月は若干速度もエリアも改善した気がしていました。とはいえ、気の所為レベルです。\nLINEモバイル (docomo 回線) ①: 3\n②: 3\n③: 3\n④: 2\n⑤: 3\n同じ docomo 回線ではありましたが、楽天モバイルの頃と比べると格段に良くなった印象がありました。\n屋内・屋外問わず特に不満はないですが、さすがにアプリのアップデートをやろうとすると結構な時間がかかっていました。まあ、基本的には Wi-Fi つないでるときにやりますが、どうしても今アップデートしたい！というときには不便でした。\nまた、電車の車内 (地下) では遅いと感じることが多かったです。\n位置情報に関しては、地上であれば割とすぐに拾ってくれます。地下では誤差があったり、取得に時間がかかったりしますが、まったく拾わないわけではないので問題ない感じです。Pokemon Go も普通にできます。\nLINEモバイル (Softbank 回線) ①: 4\n②: 4\n③: 4\n④: 3\n⑤: 3\nほぼ不満ありません。\n電車内 (地下) でたまに遅いときがありますが (特に帰宅ラッシュ時間帯) 、ストレスを感じるほどではないです。100MB くらいのアプリであればインストール・アップデートも待たされる感はありません。\n位置情報に関しては docomo 回線にくらべて、取得までの時間が早くなったと感じます。Pokemon Go でも起動時の位置情報取得にかかる時間が短くなってます。\nLINE モバイルの Softbank 回線は今年の 7 月くらいから提供が開始されたばかりでユーザ数も少ないから快適に使えているという可能性はありますが、現状では特に不満はありません。\nもちろんこの前の大規模障害では影響を受けましたが、まあ他のキャリアで起こった場合はその回線がダメになるので、そのあたりは特にマイナス要素とは思っていません。\nちなみに、通話品質につては 3 パターンとも特に差はなく普通に会話するのに支障ないレベルの品質です。\nまとめ 楽天モバイルと LINEモバイルの諸々をざっくりと比較してみました。\n結論としては、 LINEモバイルの Softbank 回線が一番快適です、ということですね。\nまあ、通信速度や品質に関しては場所・時間によって大きく差が出る部分なのでこのレビューの内容がすべて正しいとは言えませんが、主要 3 キャリアから格安 SIM への乗り換えを検討している場合の参考になれば幸いです。\n",
    "permalink": "https://michimani.net/post/gadget-line-mobile-review/",
    "title": "スマホのメイン回線を楽天モバイルから LINEモバイルに変更して 10 ヶ月くらいたったので使用感をざっくり比較してみます"
  },
  {
    "contents": "CloudWatch でアラームの設定をして、SNS 経由で Slack に通知する際のメッセージを、見やすい形に整形する Lambda を作ってみました。\n経緯 CloudWatch Alarm を Slack に通知すること自体は非常に簡単で、下記の方法で通知させることが可能です。\nSNS トピックを作成 作成したトピックのサブスクリプションに、HTTPS のプロトコルで Slack の Incoming Webhooks のエンドポイントを指定 CloudWatch Alarm の通知先に、作成した SNS トピックを指定 ただ、これだけでは CloudWatch Alarm から送られてくる JSON の文字列がそのままメッセージ本文として通知されるので、非常に見づらいです。\nこれを良い感じに整形します。\nやったこと 方法としては、SNS のサブスクリプションで直接 Incoming Webhooks のエンドポイントを指定するのではなく、 Lambda 関数を指定します。そして、その Lambda 関数内でメッセージの整形・ Slack への通知を実行します。\nで、作ったのは次のような Lambda 関数。\nmichimani/sns_to_slack.py | GitHubGist import boto3 import json import logging import os from base64 import b64decode from urllib.error import URLError, HTTPError from urllib.request import Request, urlopen SLACK_CHANNEL = os.environ[\u0026#39;slackChannel\u0026#39;] ENCRYPTED_HOOK_URL = os.environ[\u0026#39;kmsEncryptedHookUrl\u0026#39;] HOOK_URL = boto3.client(\u0026#39;kms\u0026#39;).decrypt(CiphertextBlob=b64decode(ENCRYPTED_HOOK_URL))[\u0026#39;Plaintext\u0026#39;].decode(\u0026#39;utf-8\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34;Lambda handler.\u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#39;Event: \u0026#39; + str(event)) message = json.loads(event[\u0026#39;Records\u0026#39;][0][\u0026#39;Sns\u0026#39;][\u0026#39;Message\u0026#39;]) logger.info(\u0026#39;Message: \u0026#39; + str(message)) alarm_name = message[\u0026#39;Trigger\u0026#39;][\u0026#39;MetricName\u0026#39;] new_state = message[\u0026#39;NewStateValue\u0026#39;] reason = message[\u0026#39;NewStateReason\u0026#39;] state_color = \u0026#39;#00FF00\u0026#39; if new_state != \u0026#39;OK\u0026#39;: state_color = \u0026#39;#FF0000\u0026#39; slack_message = { \u0026#39;channel\u0026#39;: SLACK_CHANNEL, \u0026#39;icon_emoji\u0026#39;: \u0026#39;:cloudwatch-%s:\u0026#39; % (new_state.lower()), \u0026#39;attachments\u0026#39;: [ { \u0026#39;color\u0026#39;: state_color, \u0026#39;fields\u0026#39;: [ { \u0026#39;value\u0026#39;: \u0026#34;*%s* state is now *%s*\\n```\\n%s\\n```\u0026#34; % (alarm_name, new_state, reason) } ] } ] } req = Request(HOOK_URL, json.dumps(slack_message).encode(\u0026#39;utf-8\u0026#39;)) try: response = urlopen(req) response.read() logger.info(\u0026#39;Message posted to %s\u0026#39;, slack_message[\u0026#39;channel\u0026#39;]) except HTTPError as e: logger.error(\u0026#39;Request failed: %d %s\u0026#39;, e.code, e.reason) except URLError as e: logger.error(\u0026#39;Server connection failed: %s\u0026#39;, e.reason) 'icon_emoji': ':cloudwatch-%s:' % (new_state.lower()) の部分では通知用のアイコンをカスタム絵文字で指定しています。\n事前に :cloudwatch-ok: と :cloudwatch-alarm: という名前でカスタム絵文字を作成しておくと、より それっぽさが出ます。不要な場合はこの 1 行を削除すれば ok です。\n実際は次のような形で通知が届きます。\nとりあえず OK と ALARM の場合のみを考慮したパターンになってますが、適宜 INSUFFICIENT_DATA の状態についても色とかメッセージを変えることもできそうです。\n",
    "permalink": "https://michimani.net/post/aws-notice-cloudwatch-alarm-to-slack/",
    "title": "CloudWatch Alarm を SNS 経由で Slack に通知するときのメッセージ内容を良い感じにしてみた"
  },
  {
    "contents": "Adobe Fonts (旧 Typekit) を利用して Web フォントを適用する際に、ページの描画と Web フォントが適用に若干のタイムラグが発生し、画面がちらつくことがあります。\nこれは、 Web フォントが適用されるまでの僅かな時間にシステムフォントによってページ内のテキストが描画されるからです。\n僅かな時間とは言え、フォントによっては幅や高さが異なりテキストが表示されるエリアの見た目にも関わってくる部分なので、できればこのチラツキはなくしたいです。\nちなみにこのチラツキのことを FOUT (Flash Of Unstyled Text) と呼ぶみたいです。\nやりたいこと Web フォントの適用が完了してからページを描画すれば、チラツキはなくなります。なので、最初はページ全体を非表示にしておいて、 Web フォントがロードできた タイミングでページ全体を表示すれば OK です。\nただし、仮に Web フォントのロードが完了しなかった場合は いつまでたってもページが表示されないので、その場合については考慮する必要があります。\nこの Web フォントがロードできた ことをどうやって検出するかですが、 Adobe Fonts では非常に簡単な方法でそのタイミングを知らせてくれています。\nAdobe Fonts の Web フォントでは、 Web フォントがロードできたタイミングで、 html タグに wf-active という class を追加してくれています。(厳密には他にも class を追加しています)\nなので、この class の有無で Web フォントがロードできたかどうかを判定することができます。\nやること CSS の記述 class によって Web フォントのロード完了を検出できるので、下記のようなスタイルを適用させます。\nhtml { visibility: hidden; } html.wf-active, html.loading-delay { visibility: visible; } これによって、 class に wf-active を持つ html のみ表示されるようになります。ただし Web フォントのロードが完了しない = html タグに class wf-active が追加されないと、ページが表示されません。\nhtml.loading-delay については、その対策として使用します。\nJavaScript の記述 Web フォントのロードが完了しない場合に備えて、次のような処理を実装します。\nsetTimeout(function() { if (document.getElementsByTagName(\u0026#34;html\u0026#34;)[0].classList.contains(\u0026#39;wf-active\u0026#39;) != true) { document.getElementsByTagName(\u0026#34;html\u0026#34;)[0].classList.add(\u0026#39;loading-delay\u0026#39;); } }, 3000); これによって、 3000 ms (3秒) 経っても class wf-active が追加されていない場合は、代わりに class loading-delay を追加するようにしています。その結果、遅くとも 3 秒後にはページの内容が表示されるようになります。\nAdobe Fonts の Web フォント用スクリプトでは、 scriptTimeout の値として 3000 がデフォルトで指定されているので、ここでの指定も 3000 でよいかと思います。が、このあたりは適宜変更で。\n結果 このブログでも Adobe Fonts で Web フォントを適用させていたのですが、チラツキがあり見づらい状態でした。が、上記方法でチラツキを無くすことができました。\nちなみにこの方法は Google が提供している Web フォントでもそのまま利用可能なようです。 (同じ方法で Web フォントデータをロードしているらしいです)\nWeb フォントのサービスには他にも色々ありますが、サービスによっては独自に JavaScript の API でロード完了イベントを取得できたりする場合もあるようなので、この方法が使えない場合はそれを使う方法も考えられそうです。\nAdobe Fonts Google Fonts TypeSquare FONTPLUS 参考 Webフォント読み込み時の「再描画ちらつき」をなくす方法 – 時にはWEBの話っ！ Webフォントによるちらつきをなくす方法（ネット低速環境も考慮） - Qiita ",
    "permalink": "https://michimani.net/post/development-remove-web-fonts-fout/",
    "title": "Adobe Fonts で Web フォントを適用するときに発生するチラツキを無くす"
  },
  {
    "contents": "AWS を個人の勉強用で使っていたり、開発環境で使っていたりする場合、少しでもランニングコストを抑えるために休日や夜間にはインスタンスを停止したいときがあります。これを自動でやります。 この手の方法は既に色々出回っていますが、自分なりにしっくりくるものができたので書いておきます。\n※追記 AWS CDK を使ってこれらの環境を構築する方法についても書きました。\n概要 特定のタグを持つインスタンスのみを対象に、自動起動・停止を実行します。\n使うサービスは、以下の2つです。\nCloudWatch Events Lambda CloudWatch は色々監視するために使うイメージがありますが、その中にある CloudWatch Events は、cron のような感じで決められたスケジュールに従って処理を実行できます。\nやること Lambda で自動起動・停止のスクリプトを書く CloudWatch Events で起動・停止のイベントスケジュールを設定する 1. Lambda で自動起動・停止のスクリプトを書く ランタイム : Python 3.6 特定のタグと値を持つインスタンスを対象に、自動起動・停止を実行します。\nimport json import boto3 import traceback def lambda_handler(event, context): try: region = event[\u0026#39;Region\u0026#39;] action = event[\u0026#39;Action\u0026#39;] client = boto3.client(\u0026#39;ec2\u0026#39;, region) responce = client.describe_instances(Filters=[{\u0026#39;Name\u0026#39;: \u0026#39;tag:AutoStartStop\u0026#39;, \u0026#34;Values\u0026#34;: [\u0026#39;TRUE\u0026#39;]}]) target_instans_ids = [] for reservation in responce[\u0026#39;Reservations\u0026#39;]: for instance in reservation[\u0026#39;Instances\u0026#39;]: target_instans_ids.append(instance[\u0026#39;InstanceId\u0026#39;]) print(target_instans_ids) if not target_instans_ids: print(\u0026#39;There are no instances subject to automatic start / stop.\u0026#39;) else: if action == \u0026#39;start\u0026#39;: client.start_instances(InstanceIds=target_instans_ids) print(\u0026#39;started instances.\u0026#39;) elif action == \u0026#39;stop\u0026#39;: client.stop_instances(InstanceIds=target_instans_ids) print(\u0026#39;stopped instances.\u0026#39;) else: print(\u0026#39;Invalid action.\u0026#39;) return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;message\u0026#34;: \u0026#39;Finished automatic start / stop EC2 instances process. [Region: {}, Action: {}]\u0026#39;.format(event[\u0026#39;Region\u0026#39;], event[\u0026#39;Action\u0026#39;]) } except: print(traceback.format_exc()) return { \u0026#34;statusCode\u0026#34;: 500, \u0026#34;message\u0026#34;: \u0026#39;An error occured at automatic start / stop EC2 instances process.\u0026#39; } 次の部分で、特定のタグを持つ EC2 インスタンスのリストを取得しています。\nresponce = client.describe_instances(Filters=[{\u0026#39;Name\u0026#39;: \u0026#39;tag:AutoStartStop\u0026#39;, \u0026#34;Values\u0026#34;: [\u0026#39;TRUE\u0026#39;]}]) 今回は AutoStartStop というタグ名で TRUE という値を持っているインスタンスを取得しています。\nスクリプト内に出てくる event は CloudWatch Events から この Lambda 関数を実行するときに渡すパラメータです。\nパラメータは JSON 文字列で指定できるので、対象となるリージョンと起動/停止のアクションを受け取るようにしています。\nソースは gist に置いています。\nmichimani/start-stop-ec2-python | GitHub このスクリプトの実行に必要なロール この関数を実行するためには、以下のようなロールを使います。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 重要なのは EC2 に関する権限です。 リストを取得する権限 (ec2:DescribeInstances) と、インスタンスを起動・停止する権限 (ec2:StartInstances, ec2:StopInstances) が必要です。\n2. CloudWatch Events で起動・停止のイベントスケジュールを設定する CloudWatch \u0026gt; イベント ルール \u0026gt; ルールの作成 と進んで、スケジュールを設定します。\nこのように設定すると、平日 10:00 に自動起動が実行されることになります。 停止の場合は、ターゲットへの入力 (JSON文字列) を {\u0026quot;Region\u0026quot;: \u0026quot;ap-northeast-1\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;stop\u0026quot;} とすればよいです。\nひとつ注意が必要なのは、 Cron 式で指定する日時はのタイムゾーンは UTC という点です。\nあとは、自動起動・停止したいインスタンスに特定のタグを付ければ完成です。\nまとめ タグベースにしておけば、自動起動・停止の対象にしたいインスタンスが増えても管理しやすいです。\n最近では RDS も起動・停止に対応したようなので、これをベースにして作ってみようと思います。\n",
    "permalink": "https://michimani.net/post/aws-auto-start-stop-ec2/",
    "title": "CloudWatch Events + Lambda で 特定のタグを持つ EC2 インスタンスを自動で起動・停止させる"
  },
  {
    "contents": "2018 年は今まで以上にアウトプットをしていこうと考えてスタートしたわけですが、実際どんな感じだったのか振り返ってみたいと思います。\nQiita からの脱出 2017 年は、 Qiita で技術的なことを少しずつ書くようになっていました。 2018 年もその調子でどんどん書いていこうと思っていましたが、去年の丁度いまごろ、 Qiita を運営する Increments がエイチームに買収されました。\nIncrements 株式会社の株式取得（子会社化）に関するお知らせ | Ateam Inc. Qiita のサービス自体は特に変わらないということで、少しモチベが下がりながらも引き続き Qiita に記事を書いていた。ただ、だんだんとポエムっぽい記事が占める割合が増えていき、最近では他のユーザの揚げ足を取るような内容の記事がたくさん出てきてランキング上位を占めるというプチ事件もありました。\nそのことを受けて、技術的なこと (を含めた色々) を書く場所を自分で作ろう！ということで、このブログを作ったわけです。\nちなみに 2018 年に入ってから Qiita で書いた記事数は 17 で、それ以前のものを含めて 40 の記事はほぼすべてこのブログに移行しています。\n書いた記事数 内容は置いといて、書いた記事の数は次のとおりです。(2018/12/13 時点)\nドメイン 記事数 michimani.net 27 qiita.com/michimani 17 ※このブログに移行済み michinoeki-mania.com 46 ※バイク関係のブログ dev-michimani.hatenablog.com 10 ※雑記用で作った。今はもう無い 合計 100 ちょうど 100！\n昨年はバイクブログだけで 90 以上書いてましたが、まあ今年はそれ以外のこともアウトプットしようと思っていたので、結果オーライかなという感じ。\nOSS 関連 アウトプットとは少し違うかもしれませんが、 今年は初めて GitHub で公開されているリポジトリにプルリクエストを送る ということをやりました。そして、嬉しいことにそのプルリクは master にマージされることになりました。\nごく簡単な内容ではありましたが、 OSS のコミット履歴に自分の名前が残るのは嬉しいことだと感じました。\nissue の作成、コメントのやり取り、プルリクの作成はすべて英語でのやりとりでしたが、案外なんとかなるもんだなと感じたので、来年もチャレンジしていきたいです。\nちなみにプルリクが採用されたのは、 Hugo のテーマである indigo のリポジトリです。\nAngeloStavrow/indigo | GitHub セミナー関連 これまたアウトプットとは少し違うかもしれませんが、今年は技術的なセミナーにも参加してみました。\n技術的なセミナーについては「参加しただけでわかった気になってしまう」というマイナスの意見もありますが、個人的には参加することで自分のやる気に繋がる感じがしました。\n調べてみると、関東では毎週のように色々なセミナーが開催されているようなので、関東にいることのメリットとして来年はもっといろんなセミナーに参加してみようと思いました。\n【2018年版】ITエンジニア向けオススメイベント・勉強会サイト7選 | DevelopersIO あと、技術的なことではないですが、来年はルービックキューブのオフ会的なものにも参加してみようと思っています。\n2019 年はどうするか 既に各項目で書いているので詳しくは書きませんが、とりあえず 外に出る ことを目標にしたいと思います。\nブログでのアウトプットもそうですが、セミナー参加、コミュニティへの参加などで色んな人と情報共有していきたいと考えてます。\nそれに加えて、 AWS 関連の認定も取得に向けても頑張りたいですね。\n",
    "permalink": "https://michimani.net/post/other-retrospect-in-2018/",
    "title": "2018 年の自分のアウトプット事情について振り返ってみる"
  },
  {
    "contents": "起こっていた現象 zsh でなにかしらのコマンドを実行して出力される結果の最終行が表示されない という現象が起こっていました。\n環境 macOS 10.12.6 zsh 5.6.2 (x86_64-apple-darwin16.7.0) zsh のフレームワークとして sorin-ionescu/prezto を使用して、テーマを pure にしていました。\n現象の詳細 1 行だけの出力をしようとすると、\n~ ❯❯❯ echo -n hello world ~ ❯❯❯ という感じで、何も表示されません。実際には一瞬だけ表示されてすぐに消えるという状態でした。\n他のパターンとして、まず下記のように適当なテキストファイルを用意します。\nline 1 line 2 line 3 このファイルの最終行の末尾には改行コードが無い状態です。\nこのファイルを cat で出力してみると、\n~ ❯❯❯ cat test.txt line 1 line 2 ~ ❯❯❯ という感じで最終行が出力されません。\npure はプロンプトが 2 段で表示されるテーマなので、それが悪さしている感じでした。(多分)\nやったこと 結論としては、下記の 2 つのオプションを .zshrc に追記しました。\nsetopt prompt_cr setopt prompt_sp それぞれのオプションについては 16 Options (zsh) | THE Z SHELL の 16.2.8 Prompting に詳しく書いてありますが、簡単に内容を書いておきます。\nprompt_cr\nプロンプトを表示する直前にキャリッジリターンを出力します。 prompt_sp\nprompt_cr オプションによって表示されなかった行を保持するようにします。 上記の 2 つのオプションはデフォルトでオンになっていると書かれていますが、明示的に書かないと機能しませんでした。これは prezto が悪いのか pure が悪いのか\u0026hellip;。\nこの対応策については prezto と pure の issue に関連するものがいくつかあり、そこからたどり着きました。\nsorin-ionescu/prezto | Some prompts remove last line from command output #1557 sorin-ionescu/prezto | Ensure external prompts also set the prompt_sp option #1425 sindresorhus/pure | Add back the prompt_sp option for zsh \u0026amp;gt;= 5.4.1 #338 ひとまずこれで改行を含まない最終行も表示されるようになりました。\n",
    "permalink": "https://michimani.net/post/develop-zsh-prompt-remove-last-line/",
    "title": "zsh で出力結果の最終行が表示されない問題の解決方法"
  },
  {
    "contents": "CloudWatch スケージュールで定期的に Lambda を実行する場合、稀に同時実行されてしまう場合があるようです。\nそれを防ぐために、S3 にロックファイルを作成することで制御してみようと思います。\n同時実行数での制御 Lambda には 同時実行数 という設定項目があり、同時実行の予約数を制限することができます。\nじゃあ、この値を 1 にすればそれで解決するのでは？ と思ったんですが、どうやら 1 に設定した場合でも、関数の実行方法やユースケースによっては同時実行されてしまうケースがあるようです。\n参考 Lambdaの同時実行数設定にて多重起動防止を行えるか確かめてみました S3 にロックファイルを設置して制御する Lambda の同時実行を防止する方法としては、次のような方法が見つけられます。\nDB に実行状態を保持する 実行中であることがわかるファイルを設置する 今回は後者の方法で同時実行を制御してみます。\nS3 バケットの作成 ロックファイルを設置するバケットを作成します。今回は lambda-test というバケットとします。\n関数の準備 ランタイムは Python 3.6 とします。\n必要な処理としては、ロックファイルの 作成、削除、存在有無を確認 の3 種類です。次のように実装します。\nimport boto3 BACKET = \u0026#39;lambda-test\u0026#39; LOCK_FILE = \u0026#39;running_lock\u0026#39; def create_lock_file(): print(\u0026#39;create lock file.\u0026#39;) s3 = boto3.resource(\u0026#39;s3\u0026#39;) s3obj = s3.Object(BACKET, LOCK_FILE) s3obj.put(Body=\u0026#39;running...\u0026#39;) def delete_lock_file(): print(\u0026#39;delete lock file.\u0026#39;) s3 = boto3.resource(\u0026#39;s3\u0026#39;) s3obj = s3.Object(BACKET, LOCK_FILE) s3obj.delete() def is_running(): s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.ObjectSummary(bucket_name=BACKET, key=LOCK_FILE).load() except Exception as e: return False return True def lambda_handler(event, context): if is_running() is True: print(\u0026#39;Process is running.\u0026#39;) else: create_lock_file() # do something ... delete_lock_file() 今回、S3 のバケット内に対象のオブジェクトが存在するかどうかを ObjectSummary() でチェックしていますが、他にもチェック方法はたくさんあるようです。\ncheck if a key exists in a bucket in s3 using boto3 | Stack Overflow とりあえずこれで同時実行は防げそうです。\n",
    "permalink": "https://michimani.net/post/aws-lambda-single-process/",
    "title": "Lambda の同時起動を S3 に置いたロックファイルで制御する"
  },
  {
    "contents": "namespace を持つ XML を JavaScript でパースするときのメモです。\n前回 概要 XML のパースには、下記のように evaluate を使用してパースします。\nvar xpathResult = document.evaluate( xpathExpression, contextNode, namespaceResolver, resultType, result ); namespace を持つ XML をパースするには、 namespaceResolver 部分に、 namespace の情報を渡す必要があります。渡す値としては、デフォルトの Namespace Resolver を使う場合と、独自に作成した Namespace Resolver を使う場合があります。\nちなみに、 namespace を持たない XML の場合は null を渡します。\nデフォルトの Namespace Resolver を使う場合 あらかじめ Namespace Resolver を生成しておきます。\nvar nsResolver = document.createNSResolver( contextNode.ownerDocument == null ? contextNode.documentElement : contextNode.ownerDocument.documentElement ); 生成した Namespace Resolver を、 evaluate に渡します。\nvar xpathResult = document.evaluate( xpathExpression, contextNode, nsResolver, resultType, result ); 独自に作成した Namespace Resolver を使う場合 次のような XML をパースすることを考えます。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;cities version=\u0026#34;1.0.0\u0026#34; xmlns=\u0026#34;https://michimani.net/2018/namespace\u0026#34;\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;1\u0026#34;\u0026gt;札幌\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;2\u0026#34;\u0026gt;函館\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;3\u0026#34;\u0026gt;小樽\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;4\u0026#34;\u0026gt;室蘭\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;2\u0026#34; city=\u0026#34;1\u0026#34;\u0026gt;弘前\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;2\u0026#34; city=\u0026#34;2\u0026#34;\u0026gt;青森\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;2\u0026#34; city=\u0026#34;3\u0026#34;\u0026gt;八戸\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;3\u0026#34; city=\u0026#34;1\u0026#34;\u0026gt;盛岡\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;3\u0026#34; city=\u0026#34;2\u0026#34;\u0026gt;釜石\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;3\u0026#34; city=\u0026#34;3\u0026#34;\u0026gt;宮古\u0026lt;/city\u0026gt; \u0026lt;/cities\u0026gt; あらかじめ次のような関数を定義しておきます。\nfunction customNsResolver() { return \u0026#39;https://michimani.net/2018/namespace\u0026#39;; } そして、 evaluate でパースする際に namespaceResolver として、定義した関数を渡します。\nvar xpathResult = document.evaluate( xpathExpression, contextNode, customNsResolver, resultType, result ); namespace を複数使いたい場合 上のパターンでは使用する namespace は 1 つでしたが、複数使用したい場合は下記のように関数を定義します。\nfunction customNsResolver(prefix) { var ns = { \u0026#39;ns1\u0026#39;: \u0026#39;https://michimani.net/2018/namespace1\u0026#39;, \u0026#39;ns2\u0026#39;: \u0026#39;https://michimani.net/2018/namespace2\u0026#39; }; return ns[prefix] || null; } prefix の値は、指定する XPath の文字列内で指定します。\n例えば、 ns1 の namespace を使用する場合は下記のような XPath を指定します。\nlet xpath = \u0026#39;//ns1:city[@pref=\u0026#39;2\u0026#39; and @city=\u0026#39;3\u0026#39;]\u0026#39;; 参考 Introduction to using XPath in JavaScript | MDN web doc ",
    "permalink": "https://michimani.net/post/programming-javascript-parse-xml-with-namespace/",
    "title": "namespace を持つ XML を JavaScript でパースする"
  },
  {
    "contents": "環境 macOS Python3.6.2 (pyenv) 手順 python-language-server をインストール $ pip install python-language-server pyls のパスを確認しておきます。\n$ which pyls /Users/hoge/.pyenv/shims/pyls Sublime Text に LSP をインストール Package Control で LSP と入力してインストール。\n少し時間がかかります。\nGithub | tomv564/LSP Sublime Text で LSP の設定 Sublime Text \u0026gt; Preferences \u0026gt; Package Settings \u0026gt; LSP \u0026gt; Settings と進んで、下記のように設定。\n{ \u0026#34;clients\u0026#34;: { \u0026#34;pyls\u0026#34;: { \u0026#34;command\u0026#34;: [ \u0026#34;/Users/hoge/.pyenv/shims/pyls\u0026#34; ], \u0026#34;enabled\u0026#34;: true } } } command のリストに指定するパスは、 pyls のパスです。\nSublime Text を再起動 再起動して完了です。\n",
    "permalink": "https://michimani.net/post/development-sublimetext-pyls/",
    "title": "Sublime Text 3 で pyls を使って Python の入力補完をする"
  },
  {
    "contents": " 前にも同じ内容で書いた んですが、スマートな方法ではなかったので考え直しました。\n前回は jQuery を使っていましたが、今回は使わない方法で実装してます。\n前提 前回と同様に、クリックした要素のリンク先を iframe で挿入して、挿入したページの高さを取得する、ということをやります。\n検証用の構造は下記のような構成になっています。\n. ├── hogehoge-1 │ └── index.html ├── hogehoge-2 │ └── index.html ├── hogehoge-3 │ └── index.html ├── index.html └── test.js 各ファイルの内容は次の通り。\nindex.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;GET IFLAME HEIGHT\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div id=\u0026#34;content-area\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a class=\u0026#34;iframe-btn\u0026#34; href=\u0026#34;./hogehoge-1/index.html\u0026#34;\u0026gt;hogehoge 1\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a class=\u0026#34;iframe-btn\u0026#34; href=\u0026#34;./hogehoge-2/index.html\u0026#34;\u0026gt;hogehoge 2\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a class=\u0026#34;iframe-btn\u0026#34; href=\u0026#34;./hogehoge-3/index.html\u0026#34;\u0026gt;hogehoge 3\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;script src=\u0026#34;./test.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; test.js const btnElems = document.querySelectorAll(\u0026#39;.iframe-btn\u0026#39;); for (let i = 0; i \u0026lt; btnElems.length; i++) { btnElems[i].addEventListener(\u0026#39;click\u0026#39;, (event) =\u0026gt; { event.preventDefault(); insertIframe(event); }); } function insertIframe(btnElem) { const iframeArea = document.getElementById(\u0026#39;content-area\u0026#39;); iframeArea.innerHTML = \u0026#39;\u0026#39;; let insert = new Promise((resolve) =\u0026gt; { let iframe = `\u0026lt;iframe frameborder=\u0026#34;no\u0026#34; src=\u0026#34;${btnElem.target.href}\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;`; console.log(\u0026#39;insert iframe.\u0026#39;); iframeArea.innerHTML = iframe; }); insert.then(getIframeHeight()); } function getIframeHeight() { let iframe = document.querySelector(\u0026#39;#content-area iframe\u0026#39;); if (iframe != null) { iframe.contentWindow.addEventListener(\u0026#39;load\u0026#39;, (event) =\u0026gt; { console.log(document.querySelector(\u0026#39;#content-area iframe\u0026#39;).contentWindow.document.body.scrollHeight); }); } else { console.log(\u0026#39;iframe does not exists.\u0026#39;); } } hogehoge-*/index.html hogehoge-1 は高さ 100px 、hogehoge-2 は高さ 200px 、 hogehoge-3 は高さ 300px になってます。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;GET IFLAME HEIGHT\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div style=\u0026#34;height: 100px;\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;hogehoge-1 100px\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 前回から変わったところ まず、冒頭にも書きましたが jQuery を使わないようにしました。最近はできるだけ生の JavaScript の記述で実装するようにしています。\niframe 内のページの高さを取得するためには、当たり前ですが iframe の描画が終わっている必要があります。\n前回は setTimeout を使って 200 ミリ秒待つ みたいなことをしてましたが、完全ではありません。\n今回は、iframe の挿入と高さの取得の処理順を Promise を使って制御するようにしました。\nまた、 iframe 内のページの高さ取得自体は、そのページが load されたタイミングで取得するようにしました。\n結果 上記の構成で、ルートの index.html にて、 hogehoge-1 → hogehoge-2 → hogehoge-3 の順にボタンをクリックすると、コンソールには下記のように出力されます。\ninsert iframe. test.js:15 100 test.js:26 insert iframe. test.js:15 200 test.js:26 insert iframe. test.js:15 300 test.js:26 動的に埋め込んだ iframe の高さを取得できています。\n前回よりはスマートになったと思います。\n",
    "permalink": "https://michimani.net/post/javascript-get-iframe-height-2nd/",
    "title": "【続】動的に埋め込んだiframeの高さを取得する"
  },
  {
    "contents": "このブログをスタートして約 1 ヶ月が経ちました。静的ファイルのストレージ、デプロイ、DNS など、運用に関わる部分に AWS のサービスを色々と使っているので、 1 ヶ月間でかかった費用について振り返ってみます。\n使っているサービス まずは、使っているサービスとその用途についてのおさらいです。\nS3\nブログ自体は静的サイトジェネレータの Hugo を使用して作成しているので、 Hugo で作った静的ファイルを S3 に置いています。\nCloudFront\nS3 のバケットに対して distribution を設定して、 Web 上で閲覧出来るようにしています。\nS3 だけでも Web ホスティングは可能ですが、SSL 化するために CloudFront を利用しています。\nRoute 53\n独自ドメインで運用するために、 DNS として Route 53 を利用しています。\nLambda\nクライアントからの / で終わるリクエストを CloudFront -\u0026gt; S3 間で /index.html に変更するために利用しています。(Lambda@Edge)\n参考： できた！S3 オリジンへの直接アクセス制限と、インデックスドキュメント機能を共存させる方法 ｜ DevelopersIO また、S3 への自動デプロイの結果を Slack に通知するためにも利用しています。\nCodeCommit\nコード (記事) をバージョン管理するために、 Git のホスティング先として利用しています。\nコミットするのは自分だけです。\nCodeBuild\nCodeCommit へ push された内容をビルドして、 S3 に配置するために利用しています。\nCodePipeline\nCodeCommit への push から CodeBuild でのビルド・デプロイ、 Lambda での通知 までのフローを管理するために利用しています。\nかかった費用 では、実際に 1 ヶ月 (厳密には 25 日間) でどれくらいの費用がかかったかというと、$1.1、日本円にして、約 124 円 ($1 = 113 円) です。税込みにしても 134 円くらいですね。(まだ請求は確定していませんが)\nまあ、そもそもアクセスが殆どなかった (25 日間でたったの 400 PV!) ということもありますが、缶ジュース 1 本くらいの費用で済みそうです。\n参考にならないかもしれませんが、一応各サービスごとに発生している費用を見てみます。\nS3 ($0.06) Amazon Simple Storage Service APN1-Requests-Tier1 ($0.05) PUT, COPY, LIST のリクエスト数に対する料金です。\n料金体型は、1,000 リクエストあたり 0.0047 USD 。\nAmazon Simple Storage Service APN1-Requests-Tier2 ($0.00) GET およびその他のリクエスト数に対する料金です。\n料金体型は、 10,000 リクエストあたり 0.0037 USD 。\nAmazon Simple Storage Service APN1-TimedStorage-ByteHrs ($0.01) これは利用しているストレージ容量に対する料金です。\n料金体系は、最初の 50 TB までは、 1 GB あたり $0.025 。\nCloudFront ($0.04) CloudFront は各リージョンごとに料金明細が見れますが、今回は JAPAN と US についてに見てみます。(他のリージョンにもリクエストはありましたが、すべて $0.00 でした。)\nJAPAN - Amazon CloudFront JP-Requests-Tier1 ($0.00) HTTP でのリクエストに対する料金です。\n料金体系は、 10,000 リクエストあたり 0.0090 USD 。\nJAPAN - Amazon CloudFront JP-Requests-Tier2-HTTPS ($0.01) HTTPS でのリクエストに対する料金です。\n料金体系は、 10,000 リクエストあたり 0.012 USD 。\nJAPAN - Bandwidth ($0.02) インターネットへのデータ転送 (アウト) に対する料金です。\n料金体系は、10 TB までは 1 GB あたり 0.114 USD 。\nUS - Amazon CloudFront JP-Requests-Tier1 ($0.00) HTTP でのリクエストに対する料金です。\n料金体系は、 10,000 リクエストあたり 0.0075 USD 。\nUS - Amazon CloudFront JP-Requests-Tier2-HTTPS ($0.00) HTTPS でのリクエストに対する料金です。\n料金体系は、 10,000 リクエストあたり 0.01 USD 。\nUS - Bandwidth ($0.01) インターネットへのデータ転送 (アウト) に対する料金です。\n料金体系は、10 TB までは 1 GB あたり 0.085 USD 。\nRoute 53 ($0.50) Amazon Route 53 DNS-Queries ($0.00) 発行された DNS クエリ数に対する料金です。\n最初の 10 億クエリまでは、 1,000,000 クエリあたり 0.40 USD 。\nAmazon Route 53 HostedZone ($0.50) 登録している Hosted Zone の数に対する料金です。\n最初の 25 ゾーンまでは、 ゾーン 1 つあたり 0.05 USD 。\nLambda ($0.00) Lambda は、リクエスト と 演習時間 それぞれに料金が発生します。また、通常の実行と Lambda@Edge での実行で料金体系が異なります。\n通常の実行 リクエスト 最初の 1,000,000 リクエストは無料で、以降は 1,000,000 リクエストあたり 0.20 USD 。\n演習時間 400,000 GB-秒 は無料で、以降は 1 GB-秒あたり 0.00001667 USD 。\nLambda@Edge リクエスト 1,000,000 リクエストあたり 0.60 USD 。\n演習時間 128MB-秒 ごとに 0.00000625125 USD\nLambda に関しては料金体系がちょっと分かりづらいので、公式の料金ページにある例で確認してください。\nAWS Lambda 料金 CodeCommit ($0.00) CodeCommit に関しては、使用しているユーザが自分だけということもあり、今後も料金が発生する可能性はないと考えています。\nCodeBuild ($0.00) ビルドに使用するコンピューティングインスタンスが build.general1.small であれば、最初の 100 分までは無料で、以降 1 分あたり 0.005 USD です。\nHugo のビルドは非常に早いため、ページ数 200 くらいでビルド・S3 へのデプロイにかかる時間は 30 〜 40 秒くらいとなっています。\nCodePipeline ($0.00) パイプライン 1 つあたり 1 USD ですが、30 日以内に作成したパイプラインについては料金が発生しません。また、毎月 1 つのアクティブなパイプラインについては無料で利用できます。\nまとめ AWS のサービスを色々使ってブログを運用した 1 ヶ月間でかかった費用について、各サービスの料金発生状況を見てみました。\nアクセス数が極端に少ないので、S3 や CloudFront ではほとんど料金が発生していませんでした。\nまた、運用面における Code 兄弟については、今後も料金が発生することはなさそうです。仮にたくさん記事を書いてビルド時間が伸びたとしても、微々たる費用でしょう。\n今回見てきた中で固定の費用というのは Route 53 の 費用だけです。あとの費用については変動しますが、まあ、このアクセス雨が続く限りは毎月缶コーヒーくらいの費用で済みそうです。\n嬉しいの悲しいのか微妙なところですが。\nただ、独自ドメイン、 SSL という 2 点を考慮して他のブログプラットフォームと比較する場合、アクセス数によってはこっちのほうがお安く済むのではと思いました。\n",
    "permalink": "https://michimani.net/post/aws-monthly-invoice-cloudfront-s3/",
    "title": "CloudFront + S3 でブログを運用し始めて 1 ヶ月経つので、かかった費用について振り返ってみた"
  },
  {
    "contents": "これまで iOS の Lightroom CC は、iPhone デフォルトのカメラで撮った写真を現像 (といってもプリセットを当てるだけですが) するために使っていました。が、どうやら高度なカメラ機能もあるらしいというのを最近知ったので、実際に使ってみました。\n今回使用した端末は、 iPhone X です。\nLightroom CC カメラの起動方法 Lightroom CC のカメラを起動する方法は、次の 3 パターンです。\nLightroom CC のアイコンを押し込む (3D touch) Lightroom CC の Widget を追加する Lightroom CC のアプリを起動して、写真取り込みの横にあるカメラマークを押す 3 番目に限っては、なぜ今まで気が付かなかったのか謎です。\n3D touch または Widget では カメラ, 自分撮り, 直近の写真 を選択できるので、 カメラ を選ぶと Lightroom のカメラが起動します。\nレンズ補正機能がすごい カメラを起動すると、こんな感じで何やら色々と細かく設定できるようになっています。\n今回、特にすごいと感じたのが、 レンズ補正 の機能です。\n上の画像では、シャッターボタンの上にある レンズのマーク を押すことで、望遠 と 広角補正 を切り替えることができます。\n具体的にどう変わるのかは実際に撮った写真で見ると明らかです。\nどちらも同じ場所から撮った写真で、ズームは使用していません。\n広角補正で撮った写真 望遠で撮った写真 どうでしょうか。\n特に望遠では、iPhone デフォルトのカメラでここまで寄ろうと思ってズームするともっとノイズが出たり画質が悪くなってしまいますが、全くそんなことになりません。\n広角補正もいい感じに効いてる気がします。\nカメラの起動スピードは、デフォルトにカメラに比べると若干遅い気はしますが、そこまで気になりません。\nむしろ、撮ったあとに読み込む必要がなく、そのまま現像に進めることを考えるとデフォルトのカメラで撮るよりも時短できます。\nAUTO になっている部分をタップすると プロフェッショナル と HDR を選択することができます。\nプロフェッショナル では、露光量、ホワイトバランス、マニュアルフォーカス を自由に設定することができるので、少しこだわった撮り方もできそうです。\nとりあえず今回はレンズ補正機能だけでも結構すごいと感じたので、他の機能はおいおい試していくことにします。\n他に撮った写真 広角 望遠 ",
    "permalink": "https://michimani.net/post/gadget-lightroom-camera/",
    "title": "【iOS】Lightroom CC にカメラ機能があるのを今更知ったので使ってみた"
  },
  {
    "contents": "本日開催されていた Adobe MAX Japan にて、 Adobe のオリジナルフォント 貂明朝 (てんみんちょう) の新しいバージョンが発表されました。干支などの絵文字がカラー化されたということなので、さっそく試してみました。\n前準備 まずは Creative Cloud のデスクトップアプリから、貂明朝をアクティブにします。\n以前からアクティブにしていた人は、一旦アクティベートをオフにして、再度アクティベートすることで新しいバージョンが使用できるようになるとのことです。\nちなみに Adobe のフォントサービス Adobe Fonts は、先月の Adobe MAX で発表されたサービスです。といっても名称が Typekit から変わっただけですが、料金が Creative Cloud に結合されたり、すべてのフォントが Web フォントとして使えるようになったり、Web フォントとして配信できる PV 数に制限がなくなったりと、サービス名称変更だけでなく大幅なアップデートがされました。\nもちろん Creative Cloud の一番安いプランである フォトプラン でもすべてのフォントが使用できます。最高ですね！ Adobe は神にでもなるのでしょうか\u0026hellip;\nCotEditor でカラー絵文字を出してみた ということで、さっそく新しいバージョンで対応されたカラー絵文字を出してみました。\n新しくなった貂明朝で早速カラー絵文字を出してみた。#貂明朝 #AdobeMAXJapan #AdobeFonts pic.twitter.com/eqGCNz1XoF\n\u0026mdash; michimani (@_michimani_) November 20, 2018 干支以外にも下記のようなオリジナル絵文字もあります。\nもちろん、 Web フォントにも対応しています。貂明朝を Web フォントとして使用する場合は、 Adobe Fonts の Web プロジェクトに貂明朝を追加して、下記のようなスタイルを適用します。\n.ten-mincho { font-family: ten-mincho, serif; font-weight: 400; font-style: normal; } また、今回のバージョン 2 から使用できるようになったカラー絵文字を使用する場合は、追加で下記のようなスタイルを適用して、 OpenType の拡張機能を利用します。\n-moz-font-feature-settings: \u0026#34;ss02\u0026#34;; -webkit-font-feature-settings: \u0026#34;ss02\u0026#34;; font-feature-settings: \u0026#34;ss02\u0026#34;; \u0026hellip; と公式サイトには書いてあったのですが、環境 (ブラウザ ) によってはうまく適用されないようです。。。\n一応モノクロ絵文字としては出ているようなので、スタイルの指定方法が間違っているのかもしれません。もし詳しい方がいたらコメントください(_ _)\nということで、速報的に Adobe のオリジナルフォント 貂明朝 の新バージョンを試してみたという話でした。\n参考 貂明朝：バージョン 1.000 からのアップデートに関する要点 アドビ、可愛く妖しい無料フォント「貂明朝」のv2を公開 ～干支のカラー絵文字を追加 ",
    "permalink": "https://michimani.net/post/other-new-ten-mincho/",
    "title": "Adobe のオリジナルフォント「貂明朝」の NEW バージョンでカラー絵文字を出してみた"
  },
  {
    "contents": "XML をクライアントサイドで操作したい場面があったので、 JavaScript で XML を扱う方法について調べてみました。\nJavaScript で XPath を使用する - JavaScript 基本的には上のリファレンスの通りです。\nやりたいこと 今回の対象とするのは次のような XML です。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;cities version=\u0026#34;1.0.0\u0026#34;\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;1\u0026#34;\u0026gt;札幌\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;2\u0026#34;\u0026gt;函館\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;3\u0026#34;\u0026gt;小樽\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;1\u0026#34; city=\u0026#34;4\u0026#34;\u0026gt;室蘭\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;2\u0026#34; city=\u0026#34;1\u0026#34;\u0026gt;弘前\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;2\u0026#34; city=\u0026#34;2\u0026#34;\u0026gt;青森\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;2\u0026#34; city=\u0026#34;3\u0026#34;\u0026gt;八戸\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;3\u0026#34; city=\u0026#34;1\u0026#34;\u0026gt;盛岡\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;3\u0026#34; city=\u0026#34;2\u0026#34;\u0026gt;釜石\u0026lt;/city\u0026gt; \u0026lt;city pref=\u0026#34;3\u0026#34; city=\u0026#34;3\u0026#34;\u0026gt;宮古\u0026lt;/city\u0026gt; \u0026lt;/cities\u0026gt; この XML から、 pref と city の値をもとに、都市の名前を取る ということをやりたいです。\n実装 上の XML が cities.xml として、やりたいことを JavaScript で実現するためのコードです。\nconst xhr = new XMLHttpRequest(); xhr.addEventListener(\u0026#39;load\u0026#39;, () =\u0026gt; { var citiesXml = xhr.responseXML; }); xhr.addEventListener(\u0026#39;error\u0026#39;, () =\u0026gt; { console.error(\u0026#39;Failed to load xml.\u0026#39;); }); xhr.open(\u0026#39;GET\u0026#39;, \u0026#39;/path/to/cities.xml\u0026#39;); xhr.send(); function getCityName(prefId, cityId) { return citiesXml.evaluate(`//city[@pref=\u0026#39;${prefId}\u0026#39; and @city=\u0026#39;${cityId}\u0026#39;]`, citiesXml, null, XPathResult.STRING_TYPE, null).stringValue; } console.log(getCityName(1,4)); // =\u0026gt; 室蘭 詳細 document の evaluate() を使います。\nevaluate は var xpathResult = document.evaluate( xpathExpression, contextNode, namespaceResolver, resultType, result ); のような形で使います。\n引数は次の通り。\nxpathExpression: XPath の文字列を指定します。 contextNode: 対象となる document を指定します。 namespaceResolver: 対象の document が名前空間を持っている場合に指定します。持っていなければ null を指定します。 resultType: 返却されるべき XPathResult のタイプを指定します。上の例では文字列が返されるべきなので STRING_TYPE を指定していますが、一般的には ANY_TYPE を指定します。実際にどのタイプが返却されたかは、 XPathResult オブジェクトの resultType の値で確認できます。ノードが返却される場合は、 UNORDERED_NODE_ITERATOR_TYPE になります。 XPathResult Defined Constants result: 既存の XPathResult を指定した場合は、そのオブジェクトを再利用します。 null を指定した場合は、新たに XPathResult オブジェクトが作成されます。 XPath の指定方法として、今回は 2 つの属性の条件を指定しています。\n単独の属性指定であれば city[@pref='${prefId}'] ですが、複数になる場合は and でつなぎます。\nひとこと 無事に JavaScript で XPath を使って XML をパースすることができました。\n今回は XPathResult が STRING_TYPE の例でしたが、他のタイプ、特に UNORDERED_NODE_ITERATOR_TYPE での挙動についてはまだよくわかっていないので、別途 色々試してみようと思います。\n",
    "permalink": "https://michimani.net/post/javascript-parse-xml/",
    "title": "JavaScript で XPath を使って XML を Parse する"
  },
  {
    "contents": "前に GAN354 M を買いました という記事を書いてざっくりレビューしましたが、今回はその続きです。実際に結構な回数を回してみて感じたことを書きます。\nファーストインプレッション セカンドインプレッション 今回はセカンドインプレッションということで、実際に回しているうちに気づいたことを書いていきます。\nタイムは確実に上がる まずタイムについてですが、確実にあがりました。\n以前の平均タイムは 50 ~ 55 秒くらいで、たまにもたつくと 1 分を超えることも結構ありました。\nただ、 GAN354 M を使い始めてからの平均タイムは 40 ~ 50 秒くらいで、 1 分を超えることはほとんどありません。\n要因としては\n動きが滑らかである 安定感がある (ポップの心配がない) この 2 点が挙げられると思います。\nまず動きが滑らかなので、途中で引っかかることによるタイムロスがなくなります。\nまた、安定感がありポップする心配が殆ど無いので、多少強引に回すこともできます。その結果、タイムの向上につながっています。\nマグネットアシストは弱い GAN354 M にはマグネットアシストが付いていますが、正直アシストは弱く、そこまで恩恵は感じません。\nTribox のレビュー でもマグネットアシストは弱めとの判定でしたが、まさにその通りです。\n実際、他のマグネットアシスト付きキューブを使用したことがないので比較ができませんが、個人的にはもう少し強くてもよいかなと感じます。\nとはいってもマグネットアシストがあるのと無いのとではぜんぜん違うので、力は弱くてもアシストがあるのは心強いです。\n個人的にしっくりきている GES の設定 GAN354 M には、次の 4 種類の GES が付属しています。\nYellow S9 Green G7 Blue G8 Purple G9 この中では Yellow が一番強く、 Purple が一番弱い反発力になってます。\n個人的には Purple を使って、締め具合をこれくらいにするのがちょうどいいです。\n初心者にもオススメ 私の平均タイムは上にも書いてあるとおり 40 ~ 50 秒なので、こういった高性能キューブを使う人達には足元にも及ばないタイムです。\nただ、 GAN354 M に関しては、使うだけでタイムが上がりますし、回転も非常にスムーズでどんどん回したくなるキューブです。どんどん回すことで、さらなるタイムの向上にもつながるので、初心者にもオススメしたいキューブです。\nとりあえずもっと練習して、じわじわと平均タイムを上げていきたいと思います。\n最近、Twitter では 今日の 1 回 と題してソルブの様子をアップしています。\n今日の１回。\nコンスタントに40秒台が出るようになってきたけど、まだまだ最初の一面が遅い。#rubikscube BGMは #欅坂46 の アンビバレント でした。 pic.twitter.com/hH1mbq6OTO\n\u0026mdash; michimani (@_michimani_) November 19, 2018 アドバイスが有ればコメントとかリプで貰えると嬉しいです。フォローもお願いします！\n",
    "permalink": "https://michimani.net/post/other-rubics-review-gan354m-2/",
    "title": "GAN354 M を使い始めて3週間くらい経ったので、あらためてレビューしてみます"
  },
  {
    "contents": "これまで技術的なことは Qiita に書いていたのですが、アウトプットする場所がバラバラだと色々面倒だと思ったので、 Qiita の記事をこのブログ (Hugo) に持ってきました。\n今回はその方法についてです。\nやりたいこと Qiita の記事から Hugo 用の markdown を作る Qiita 記事内の画像も移行する なるべく手は加えたくない (全く加えないのは無理でした) やったこと (概要) Qiita API で記事の情報を取得 取得した記事内の画像を保存 取得した記事を Hugo 用に加工 やったこと (詳細) 1. Qiita API で記事の情報を取得 まず、Qiita から記事の内容を取得します。\n幸いにも Qiita には、投稿や編集、記事一覧取得などの API が公開されているので、それを使います。\nその前に、 Qiita のマイページからアクセストークンを発行しておきます。\nスコープについては、 read_qiita だけで OK です。\n使う API は \u0026lt;code\u0026gt;GET /api/v2/authenticated_user/items\u0026lt;/code\u0026gt; です。\n公式ページにもありますが、パラメータは下記の通りです。\npage ページ番号 (1から100まで) Example: 1 Type: string Pattern: /^[0-9]+$/ per_page 1ページあたりに含まれる要素数 (1から100まで) Example: 20 Type: string Pattern: /^[0-9]+$/ 指定しない場合は、どちらも Example の値が使用されます。\n今回、移行の対象となる Qiita の記事数は 41 だったので、 per_page に 100 を指定してコールします。\nこれまた公式の例のままですが、下記のようなレスポンスが返ってきます。\n[ { \u0026#34;rendered_body\u0026#34;: \u0026#34;\u0026lt;h1\u0026gt;Example\u0026lt;/h1\u0026gt;\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;# Example\u0026#34;, \u0026#34;coediting\u0026#34;: false, \u0026#34;comments_count\u0026#34;: 100, \u0026#34;created_at\u0026#34;: \u0026#34;2000-01-01T00:00:00+00:00\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;created_at\u0026#34;: \u0026#34;2000-01-01T00:00:00+00:00\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Dev\u0026#34;, \u0026#34;private\u0026#34;: false, \u0026#34;updated_at\u0026#34;: \u0026#34;2000-01-01T00:00:00+00:00\u0026#34;, \u0026#34;url_name\u0026#34;: \u0026#34;dev\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;4bd431809afb1bb99e4f\u0026#34;, \u0026#34;likes_count\u0026#34;: 100, \u0026#34;private\u0026#34;: false, \u0026#34;reactions_count\u0026#34;: 100, \u0026#34;tags\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Ruby\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;0.0.1\u0026#34; ] } ], \u0026#34;title\u0026#34;: \u0026#34;Example title\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2000-01-01T00:00:00+00:00\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://qiita.com/yaotti/items/4bd431809afb1bb99e4f\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Hello, world.\u0026#34;, \u0026#34;facebook_id\u0026#34;: \u0026#34;yaotti\u0026#34;, \u0026#34;followees_count\u0026#34;: 100, \u0026#34;followers_count\u0026#34;: 200, \u0026#34;github_login_name\u0026#34;: \u0026#34;yaotti\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;yaotti\u0026#34;, \u0026#34;items_count\u0026#34;: 300, \u0026#34;linkedin_id\u0026#34;: \u0026#34;yaotti\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Tokyo, Japan\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Hiroshige Umino\u0026#34;, \u0026#34;organization\u0026#34;: \u0026#34;Increments Inc\u0026#34;, \u0026#34;permanent_id\u0026#34;: 1, \u0026#34;profile_image_url\u0026#34;: \u0026#34;https://si0.twimg.com/profile_images/2309761038/1ijg13pfs0dg84sk2y0h_normal.jpeg\u0026#34;, \u0026#34;twitter_screen_name\u0026#34;: \u0026#34;yaotti\u0026#34;, \u0026#34;website_url\u0026#34;: \u0026#34;http://yaotti.hatenablog.com\u0026#34; }, \u0026#34;page_views_count\u0026#34;: 100 }, { //... } ] これをもとに、 Hugo 用の markdown ファイルを生成します。\n2. 取得した記事内の画像を保存 加工する前に、記事内の画像データを保存します。移行後には Qiita の記事は削除する予定なので。\nQiita 記事内の画像ファイルは、下記のような形になっています。\n\u0026lt;img width=\u0026#34;\\d+\u0026#34; alt=\u0026#34;.+\u0026#34; src=\u0026#34;.+\u0026#34;\u0026gt; src にはフルパスが入っているので、そこから画像を保存してきます。\nまた、 alt にはアップロードしたときのファイル名が入っているようなので、この部分も変更するようにします。\n保存する際のファイル名は、 src の値の最後の / 以降 とすると、重複なく保存できます。\n新たな alt 属性として、保存時のファイル名を設定するようにします。\n後に紹介するスクリプト内の話になりますが、 元の src と 新しい src、元の alt と 新しい alt の情報を保持しておいて、次の加工ステップでそれぞれ置換します。\n3. 取得した記事を Hugo 用に加工 加工と言っても、ほとんどすることはありません。\nなぜなら、 Qiita API で取得した状態で記事の内容は markdown になっているからです。\nなので、あとは記事のタイトルとかカテゴリとかの情報を先頭に付与するだけです。\n具体的には、下記のような情報を追加します。\n--- title: \u0026#34;{title}\u0026#34; date: {date} draft: false author: [\u0026#34;michimani\u0026#34;] categories: {tags} tags: {tags} archives: [\u0026#34;201\u0026#34;, \u0026#34;201/\u0026#34;] eyecatch: \u0026#34;/images/og/eyecatch-default.jpg\u0026#34; ogimage: \u0026#34;/images/og/eyecatch-default.png\u0026#34; --- API から返却される json は記事情報のリストになっているので、各記事情報 (post) に対して\n{title} : post.title {date} : post.date {tags} : post.tags のリストをゴニョゴニョする で取得します。\nで、記事本文は post.body なので、この値に対して、画像ファイルのパス、alt 属性を置換します。\nあとは、記事のタイトルをファイル名として使用できるように適宜加工して、 {title}.md という形で保存すれば完成です。\nQiita から Hugo 用の markdown を生成するやつ ということで、上で説明したことをやってくれるスクリプトを作りました。\nGithub michimani/qiita-to-hugo Python3.6 での動作を想定してますが、標準ライブラリのみなので、他のバージョンでも動くと思います。試してませんが。\nこのごろ何かと話題の Qiita ですが、 API がしっかり用意されているので、他ブログへの移行や、他ブログからの移行も比較的やりやすい気がします。\n",
    "permalink": "https://michimani.net/post/blog-qiita-to-hugo/",
    "title": "Qiita に投稿していた記事を Hugo に移行してみた"
  },
  {
    "contents": "Hugo で作ったブログに Disqus を使ってコメント欄を設置しましたでの、その方法についてです。\nやること Disqus でユーザ登録 Disqus でコメント機能を使用するサイトの登録 Hugo でコメント欄を表示するように設定 1. Disqus でユーザ登録 ユーザ登録はメールアドレスでの登録のほか、Facebook Twitter, Google それぞれのアカウントを使った登録も可能です。\nSign-up to comment on Disqus 2. Disqus でコメント機能を使用するサイトの登録 ユーザ登録が完了したら、 Disqus の画面上でサイトの登録をします。\n右上の歯車アイコンから Add Disqus To Site を選択します。\n次の画面では色々書かれていますが、一番下にある GET STARTED で次に進みます。\n次の画面では、 I want to install Disqus on my site を選択します。\nインストールするサイトの情報を入力します。\n次の画面では、　Disqus のプランを選択します。\nとりあえずコメント機能を使うだけであれば Basic で問題ないです。\n次に、インストールするサイトのプラットフォームを選択します。\n今回は Hugo で作ったサイトなので、一番下の install manually \u0026hellip; を押して次へ進みます。\nJavaScript のコードや設置方法について書かれている画面が表示されますが、特にやることはなく、コードもコピーする必要はありません。\nそのまま Configure Disqus へ進みます。\nConfigure Disqus の画面では、コメント欄の見た目や、インストールするサイトの URL 、コメントポリシーに関する情報を入力します。\n必須なのは Website Name と Website URL です。\nこれで設定は完了です。が、後の Hugo での設定で必要な情報を取得しておきます。\n設定完了後に表示されたページから Configure your site\u0026rsquo;s community settings を押して、サイトの詳細な設定画面を表示します。\nそこから、 Shortname の文字列をコピーしておきます。\n以上で Disqus での作業は終了です。\n3. Hugo でコメント欄を表示するように設定 Hugo 側では、\nconfig.toml への追記 記事テンプレートファイルを編集 の対応をします。\nconfig.toml への追記 先ほどコピーした Shortname を config.toml に追記します。追記する場所は、 title や theme と同じ階層です。\ndisqusShortname = \u0026#34;michimani-net\u0026#34; 記事テンプレートファイルを編集 Disqus によるコメント欄を表示するために、個別記事用のテンプレートファイルを編集します。 使用しているテーマによって異なるかもしれませんが、私の場合は Indigo を使っているので、 themes/indigo/layouts/_default/single.html を編集します。\n\u0026lt;/p\u0026gt; {{ end }} \u0026lt;/footer\u0026gt; + {{ template \u0026#34;_internal/disqus.html\u0026#34; . }} \u0026lt;/article\u0026gt; {{ partial \u0026#34;footer.html\u0026#34; . }} 今回追記している {{ template \u0026quot;_internal/disqus.html\u0026quot; . }} ですが、これは Hugo にあらかじめ用意されている Disqus 用のテンプレートなので、 Disqus のコメント欄を表示したい場所にこの記述をそのまま記述すれば OK です。\nまとめ 以上で Hugo で作ったブログにコメント機能を追加することができました。\nなお、ローカルでの確認中は Disqus comments not available by default when the website is previewed locally. と表示され、コメント欄は表示されません。\nDisqus での操作は全て英語ですが、難しい内容ではないので特に詰まることはなかったです。Hugo 自体に Disqus 用のテンプレートが用意されていることもあり、導入は簡単でした。\nこのブログではツッコミどころが多々あるかと思いますので、コメントで訂正、ご指摘いただければ幸いです。\n",
    "permalink": "https://michimani.net/post/blog-install-disqus-to-hugo/",
    "title": "Hugo で作ったブログに Disqus を使ってコメント機能を追加する"
  },
  {
    "contents": " 前回 ルービックキューブの大会について調べてみましたが、それに関連して「競技用キューブ」なるものがあることがわかりました。今回は、その競技用のキューブの中でも評価が高い「GAN354 M」を買ってみたので、そのレビューです。\nちなみに私の平均タイムは 55 秒くらいで、まだまだ駆け出しです。\n競技用キューブとは ルービックキューブには様々な種類があります。その中でも競技用キューブと呼ばれるものは、一般に売られているものよりも動きが滑らかだったり、回転させるときの抵抗を調整したりすることができます。\nまた、中にはマグネットが仕込まれていて、回転をアシストしてくれるものもあります。\nルービックキューブのタイム短縮には、手順を覚えるというのはもちろんですが、如何にキューブの回転におけるロスを無くすか ということも重要です。\n競技用キューブを使うことで、そのロスを減らしてタイム短縮に大きく前進するわけです。\n評価が高い競技用キューブ GAN354 M 3x3x3 に限らずルービックキューブや他のパズル類をたくさん扱っているオンラインショップ tribox 。そこで最新の競技用キューブについての特典付きレビューがあり、その中でもトップ評価だったのが GAN354 M というキューブです。\nGAN354 M Stickerless 正式な名称は　GAN354 M Stickerless です。\nGAN354 M Stickerless 商品ページ GAN は、制作しているメーカー GANCUBE を、 3 は、3x3x3 を、 54 はキューブの一辺の長さ 54mm をそれぞれ表しています。\n同じ GANCUBE のキューブには、他に GAN356 R Stickerless や、 GAN356 Air SM などがあります。\n競技者によるレビューでは、特に 柔軟性 において非常に高い評価となっています。\n特徴 マグネットアシスト まずは、マグネットが内蔵されているということです。\nこれがあることにより、キューブが中途半端な場所で止まることを防いで、ソルビングの速度向上に貢献します。\nただし、 GAN354 M のマグネットアシストは 強すぎない という評価で、実際まわしてみても、マグネットの強さはあまり感じませんでした。まあ、それでもアシストされてる感は多少あって、まさにちょうどいい感じです。\n硬さ調整 メガハウスのスピードキューブにも調整機構はついていましたが、それよりも高度な調整ができるようになっています。\nルービックキューブの硬さ調整といっても何をどのように調整するか、あまりピンとこないと思います。\nキューブの各面にあるセンターキューブを開けると、 GES と呼ばれるパーツがついており、ここを緩めたり締めたりすることで調整します。この GES 自体にも硬さがあり、 GAN354 M にもデフォルト + 3 種類が付属しています。\nこの部分を緩めると、キューブが回転途中の状態であっても縦方向、横方向に回しやすくなるというメリットがあります。が、その分ポップして (キューブが散らばって) しまうことも多くなります。\n逆に締めると中途半端な状態での回転はできなくなりますが、その分安定感が増すのでガンガン回すことができます。\n一般的なキューブとの比較 これまではずっと一般的なキューブを使っていたので、それらとの比較です。まずは見た目。\n左から、メガハウスのルービックキューブ、メガハウスのスピードキューブ、そして今回買った GAN354 M です。\n色・配色 まずパット見で分かるのは、色と配色ですね。\nGAN354 M はステッカーレスで、パーツ本体に色がついています。それも蛍光色っぽい鮮やかな色です。\nまた、各面の配色も微妙に違っています。\nメガハウスのキューブと比べて、 GAN354 M では、青と黃の配置が逆になっています。\n大きさ メガハウスのルービックキューブは、どちらもサイズが 57 mm 、一方 GAN354 M は 54 mm なので、その差は 3 mm です。\nたった 3 mm ですが、持った瞬間、というか見た目からもう大きさがぜんぜん違うのがわかります。\n最初は大きさの差に戸惑いましたが、数時間触っていれば慣れます。\n回転の滑らかさ これはかなり差が出てます。\nメガハウスのスピードキューブよりも相当滑らかに動きます。\nマグネットアシストの恩恵もあり、かなりヌルヌル動くので慣れるまでは回しにくいなと感じました。が、慣れれば非常に強い味方です。\nタイムは早くなったのか ここまで GAN354 M の特徴、一般的なキューブとの違いを書いてきましたが、結局のところタイムが縮まったのかという点が一番気になるところです。\n冒頭にも書きましたが、私の平均タイムは 55 秒くらいでしたが、 GAN354 M を使った結果、平均タイムは 50 秒弱 に、 約 5 秒縮まりました。\nまた、これまでは 40 秒台はたまに出るくらいだったのが、 GAN354 M にしてからは 40 秒台が出る回数が格段に増えました。\n競技用キューブというとちょっと敷居は高く感じますが、あとから競技用に変えると慣れるまで時間がかかると思うので、最初から競技用キューブを使うのも全然ありだと思いました。\n今後も少しずつ慣れていって、タイム向上目指して頑張りたいと思います。\n追加でレビューを書きました。\nGES の設定についても書いてます。\n",
    "permalink": "https://michimani.net/post/other-rubics-gan354m/",
    "title": "高評価の競技用キューブ GAN354 M を買ってみた"
  },
  {
    "contents": "個人的に使っている Web サービスや規模の小さいサービスでは git pull でリリースしている場合もあるかと思います。わざわざ ssh でログインして git pull するのは面倒なので、 CodePipeline と Systems Manager Run Command を使って自動化してみます。\nSystems Manager Run Command とは AWS Systems Manager Run Command では、マネージドインスタンスの設定をリモートかつ安全に管理することができます。マネージドインスタンスは、Systems Manager のために設定されたハイブリッド環境の Amazon EC2 インスタンスまたはオンプレミスコンピュータです。\nAWS Systems Manager Run Command とありますが、今回は、 EC2 内でコマンドを実行する という使い方をします。\nやること 対象の EC2 インスタンスに SSM エージェントをインストールする 対象の EC2 インスタンスに、必要な IAM ロールを割り当てる Run Command を実行するための Lambda 関数を実装する CodePipeline でデプロイのパイプラインを作成する 1. 対象の EC2 インスタンスに SSM エージェントをインストールする 今回の対象は EC2 インスタンスで、 OS は Amazon Linux です。\n下記の手順で SSM エージェントをインストールします。\n一時ディレクトリを作成、移動\n$ mkdir /tmp/ssm $ cd /tmp/ssm SSM エージェントをインストール\n$ sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm SSM エージェントのステータスを確認\n$ sudo status amazon-ssm-agent 起動していない場合は、下記コマンドで起動します。\n$ sudo start amazon-ssm-agent 他の環境、OS の場合については公式リファレンスを確認してください。\nSSM エージェント をインストールし設定する 2. 対象の EC2 インスタンスに、必要な IAM ロールを割り当てる 対象となる EC2 インスタンスには、 AmazonEC2RoleforSSM ポリシーをアタッチしたロールを割り当てます。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:DescribeAssociation\u0026#34;, \u0026#34;ssm:GetDeployablePatchSnapshotForInstance\u0026#34;, \u0026#34;ssm:GetDocument\u0026#34;, \u0026#34;ssm:GetManifest\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ListInstanceAssociations\u0026#34;, \u0026#34;ssm:PutInventory\u0026#34;, \u0026#34;ssm:PutComplianceItems\u0026#34;, \u0026#34;ssm:PutConfigurePackageResult\u0026#34;, \u0026#34;ssm:UpdateAssociationStatus\u0026#34;, \u0026#34;ssm:UpdateInstanceAssociationStatus\u0026#34;, \u0026#34;ssm:UpdateInstanceInformation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ssmmessages:CreateControlChannel\u0026#34;, \u0026#34;ssmmessages:CreateDataChannel\u0026#34;, \u0026#34;ssmmessages:OpenControlChannel\u0026#34;, \u0026#34;ssmmessages:OpenDataChannel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2messages:AcknowledgeMessage\u0026#34;, \u0026#34;ec2messages:DeleteMessage\u0026#34;, \u0026#34;ec2messages:FailMessage\u0026#34;, \u0026#34;ec2messages:GetEndpoint\u0026#34;, \u0026#34;ec2messages:GetMessages\u0026#34;, \u0026#34;ec2messages:SendReply\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeInstanceStatus\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ds:CreateComputer\u0026#34;, \u0026#34;ds:DescribeDirectories\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetEncryptionConfiguration\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 3. Run Command を実行するための Lambda 関数を実装する ランタイム : Python 3.6\n実行ロール : 下記のようなポリシーを作成して、ロールにアタッチします\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codepipeline:ListPipelineExecutions\u0026#34;, \u0026#34;codepipeline:PutJobFailureResult\u0026#34;, \u0026#34;codepipeline:PutJobSuccessResult\u0026#34;, \u0026#34;ssm:*\u0026#34;, \u0026#34;ec2:*\u0026#34;, \u0026#34;codepipeline:GetPipelineExecution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; } ] } 実際のソースは下記のとおりです。\nimport boto3 import json import logging import os import traceback from base64 import b64decode from urllib.request import Request, urlopen from urllib.error import URLError, HTTPError from datetime import datetime SLACK_CHANNEL = \u0026#39;channel_name\u0026#39; TARGET_INSTANCE_ID = \u0026#39;{対象の EC2 インスタンスのインスタンス ID}\u0026#39; PIPELINE_NAME = \u0026#39;laravel-deploy-pl\u0026#39; HOOK_URL = \u0026#39;https://hooks.slack.com/services/*********\u0026#39; pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): job_id = event[\u0026#34;CodePipeline.job\u0026#34;][\u0026#34;id\u0026#34;] try: ec2 = boto3.client(\u0026#39;ec2\u0026#39;) ssm = boto3.client(\u0026#39;ssm\u0026#39;) job_id = event[\u0026#34;CodePipeline.job\u0026#34;][\u0026#34;id\u0026#34;] target_content_info = get_content_info() if target_content_info[\u0026#39;can_deploy\u0026#39;] != True: print(\u0026#39;Nothing to do.\u0026#39;) put_job_failer(job_id, \u0026#39;This pipeline Not in progress.\u0026#39;) return {\u0026#39;status\u0026#39;: 200} cmd_res = ssm.send_command( InstanceIds = [TARGET_INSTANCE_ID], DocumentName = \u0026#34;AWS-RunShellScript\u0026#34;, Parameters = { \u0026#34;commands\u0026#34;: [ \u0026#34;cd /var/www/laravel-test\u0026#34;, \u0026#34;git pull origin master:master\u0026#34;, \u0026#34;/usr/local/bin/composer install\u0026#34;, \u0026#34;php artisan migrate\u0026#34;, \u0026#34;php artisan cache:clear\u0026#34;, \u0026#34;php artisan view:clear\u0026#34; ], \u0026#34;executionTimeout\u0026#34;: [\u0026#34;3600\u0026#34;] }, ) if cmd_res[\u0026#39;ResponseMetadata\u0026#39;][\u0026#39;HTTPStatusCode\u0026#39;] != 200: print(\u0026#39;Failed to execute ssm.send_command().\u0026#39;) put_job_failer(job_id, \u0026#39;Some errors occered in execution ssm.send_command().\u0026#39;) return {\u0026#39;status\u0026#39;: 200} put_job_success(job_id, target_content_info[\u0026#39;message_for_slack\u0026#39;]) except Exception: print(traceback.format_exc()) put_job_failer(job_id, traceback.format_exc()) return {\u0026#39;status\u0026#39;: 500} \u0026#34;\u0026#34;\u0026#34; for CodePipeline. To get target content info in current execution pipeline. \u0026#34;\u0026#34;\u0026#34; def get_content_info(): try: exe_pipelines = pipeline.list_pipeline_executions(pipelineName=PIPELINE_NAME, maxResults=1) exe_pipeline = exe_pipelines[\u0026#39;pipelineExecutionSummaries\u0026#39;][0] exe_id = exe_pipeline[\u0026#39;pipelineExecutionId\u0026#39;] exe_detail = pipeline.get_pipeline_execution(pipelineName=PIPELINE_NAME, pipelineExecutionId=exe_id) print(exe_detail) exe_status = exe_pipeline[\u0026#39;status\u0026#39;] git_summary = { \u0026#39;revision_id\u0026#39;: exe_pipeline[\u0026#39;sourceRevisions\u0026#39;][0][\u0026#39;revisionId\u0026#39;], \u0026#39;commit_message\u0026#39; : exe_pipeline[\u0026#39;sourceRevisions\u0026#39;][0][\u0026#39;revisionSummary\u0026#39;], \u0026#39;revision_url\u0026#39; : exe_pipeline[\u0026#39;sourceRevisions\u0026#39;][0][\u0026#39;revisionUrl\u0026#39;], \u0026#39;message_for_slack\u0026#39; : \u0026#34;Commit \u0026lt;{}|{}\u0026gt; \\n{}\u0026#34;.format( exe_pipeline[\u0026#39;sourceRevisions\u0026#39;][0][\u0026#39;revisionUrl\u0026#39;], exe_pipeline[\u0026#39;sourceRevisions\u0026#39;][0][\u0026#39;revisionId\u0026#39;], exe_pipeline[\u0026#39;sourceRevisions\u0026#39;][0][\u0026#39;revisionSummary\u0026#39;] ) } if exe_status == \u0026#39;InProgress\u0026#39;: git_summary.update({\u0026#39;can_deploy\u0026#39;: True}) else: git_summary.update({\u0026#39;can_deploy\u0026#39;: False}) print(\u0026#39;Current pipeline has finished with status {}\u0026#39;.format(exe_status)) return git_summary except Exception as e: raise e \u0026#34;\u0026#34;\u0026#34; for CodePipeline. To finish lambda function with status \u0026#34;SUCCESS\u0026#34;. \u0026#34;\u0026#34;\u0026#34; def put_job_success(job_id, message): try: pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) pipeline.put_job_success_result( jobId=job_id, currentRevision={ \u0026#39;revision\u0026#39;: \u0026#39;revision_string\u0026#39;, \u0026#39;changeIdentifier\u0026#39;: \u0026#39;changeIdentifier_string\u0026#39; } ) post_to_slack(message) except Exception: print(traceback.format_exc()) post_to_slack(traceback.format_exc(), False) \u0026#34;\u0026#34;\u0026#34; for CodePipeline. To finish lambda function with status \u0026#34;FAILER\u0026#34;. \u0026#34;\u0026#34;\u0026#34; def put_job_failer(job_id, message): try: pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) pipeline.put_job_failure_result( jobId=job_id, failureDetails={ \u0026#39;type\u0026#39;: \u0026#39;JobFailed\u0026#39;, \u0026#39;message\u0026#39;: message } ) post_to_slack(message, False) except Exception: print(traceback.format_exc()) post_to_slack(traceback.format_exc(), False) \u0026#34;\u0026#34;\u0026#34; for notification. To post message with attachments to Slack. \u0026#34;\u0026#34;\u0026#34; def post_to_slack(message, is_ok=True): status = \u0026#39;ok\u0026#39; state_color = \u0026#39;#00FF00\u0026#39; icon_emoji = \u0026#39;:codepipeline:\u0026#39; author_name = \u0026#39;CodePipeline\u0026#39; if is_ok != True: status = \u0026#39;ng\u0026#39; state_color = \u0026#39;#FF0000\u0026#39; title = \u0026#39;deploy test [{}]\u0026#39;.format(status) slack_message = { \u0026#39;channel\u0026#39;: SLACK_CHANNEL, \u0026#39;icon_emoji\u0026#39;: icon_emoji, \u0026#39;attachments\u0026#39;: [ { \u0026#39;author_name\u0026#39;: author_name, \u0026#39;title\u0026#39;: title, \u0026#39;color\u0026#39;: state_color, \u0026#39;fields\u0026#39;: [ { \u0026#39;value\u0026#39;: message } ] } ] } req = Request(HOOK_URL, json.dumps(slack_message).encode(\u0026#39;utf-8\u0026#39;)) try: response = urlopen(req) response.read() logger.info(\u0026#34;Message posted to %s\u0026#34;, slack_message[\u0026#39;channel\u0026#39;]) except HTTPError as e: logger.error(\u0026#34;Request failed: %d %s\u0026#34;, e.code, e.reason) except URLError as e: logger.error(\u0026#34;Server connection failed: %s\u0026#34;, e.reason) 長々と書いてありますが、 Run Command を使用しているのは下記の部分です。\ncmd_res = ssm.send_command( InstanceIds = [TARGET_INSTANCE_ID], DocumentName = \u0026#34;AWS-RunShellScript\u0026#34;, Parameters = { \u0026#34;commands\u0026#34;: [ \u0026#34;cd /var/www/laravel-test\u0026#34;, \u0026#34;git pull origin master:master\u0026#34;, \u0026#34;/usr/local/bin/composer install\u0026#34;, \u0026#34;php artisan migrate\u0026#34;, \u0026#34;php artisan cache:clear\u0026#34;, \u0026#34;php artisan view:clear\u0026#34; ], \u0026#34;executionTimeout\u0026#34;: [\u0026#34;3600\u0026#34;] }, ) 対象のインスタンス ID のリストと、実行したいコマンドをリストで指定します。\n今回は対象が Laravel のプロジェクトなので、 git pull したあとに DB のマイグレーション、キャッシュの削除を実行しています。\n実行結果は Slack に通知するようにしていて、実際の通知はこんな感じになります。\nちなみに Slack のカスタマイズアイコンとして :codepipeline: の名前で CodePipeline のアイコンを登録しています。(それっぽくしたかった)\n4. CodePipeline でデプロイのパイプラインを作成する 最後に、ローカルからの git push を検知して、上記の Lambda を実行するためのフローを CodePipeline で作成します。\n構成は下図のとおりです。\nSource まず　Srouce ステージですが、ここは git リポジトリの情報を設定します。\n今回はアクションプロバイダに Github を指定、監視するブランチを mater で指定しました。\nStaging 名前は Staging となっていますが、ここは適宜変更してください。\nやっている内容は、上で実装した Lambda 関数を呼んでいるだけです。\n注意点 新たにパイプラインを作成するときには、Build または Deploy のステージを必ず入れる必要があり、その設定画面では Lambda を選択することが出来ません。\nなので、一旦ダミーのビルド/デプロイを CodeBuild/CodeDeploy で作成しておいて、それを指定しておきます。\nその後、不要なステージを削除して、 Lambda 呼び出しのステージを追加する という手順になります。\nまとめ 以上、Systems Manager Run Command を使って EC2 に web サービスをデプロイする方法でした。\n外から任意のコマンドを実行できるということは、これまた何でも出来てしまうと思うので、例えば yum パッケージのアップデートとかも自動で出来るかなと思いました。\n",
    "permalink": "https://michimani.net/post/aws-deploy-using-run-command/",
    "title": "AWS Systems Manager Run Command を使ってデプロイしてみる"
  },
  {
    "contents": "CloudFront + S3 で構築したページに Basic 認証をかけたい場面が出てきたので調べてみると、特に難しいことをする必要なく実装できそうでした。今回はその作業メモです。\n概要 冒頭にも書きましたが、　CloudFront + S3 で構築して公開しているページの、特定のパスに対して Basic 認証をかけます。 今回は、このブログの /about/ に Basic 認証をかけると想定します。\nやること Lambda で Basic 認証用のスクリプトを作成 CloudFront で /about/* のパス宛に Behavior を作成する これは CloudFront の Lambda@Edge という機能を使っています。\nLambda@Edge -ユーザーに近いロケーションでコードを実行- 1. Lambda で Basic 認証用のスクリプトを作成 このスクリプトは バージニア北部 (us-east-1) に作成する必要があるので、マネジメントコンソールの右上から、リージョンを変更しておきます。\nLambda は下記の要領で作成します。\n名前: CFBasicAuthenticattion (任意) ランタイム : Node.js 8.10 ロール : 1 つ以上のテンプレートから新しいロールを作成します。 ロール名 : lambda_edge_exection (任意) ポリシーテンプレート : 基本的な Lambda@Edge のアクセス権限 スクリプトは lmakarov/lambda-basic-auth.js を使います。\n\u0026#39;use strict\u0026#39;; exports.handler = (event, context, callback) =\u0026gt; { // Get request and request headers const request = event.Records[0].cf.request; const headers = request.headers; // Configure authentication const authUser = \u0026#39;user\u0026#39;; const authPass = \u0026#39;pass\u0026#39;; // Construct the Basic Auth string const authString = \u0026#39;Basic \u0026#39; + new Buffer(authUser + \u0026#39;:\u0026#39; + authPass).toString(\u0026#39;base64\u0026#39;); // Require Basic authentication if (typeof headers.authorization == \u0026#39;undefined\u0026#39; || headers.authorization[0].value != authString) { const body = \u0026#39;Unauthorized\u0026#39;; const response = { status: \u0026#39;401\u0026#39;, statusDescription: \u0026#39;Unauthorized\u0026#39;, body: body, headers: { \u0026#39;www-authenticate\u0026#39;: [{key: \u0026#39;WWW-Authenticate\u0026#39;, value:\u0026#39;Basic\u0026#39;}] }, }; callback(null, response); } // Continue request processing if authentication passed callback(null, request); }; 下記の部分を、Basic 認証のユーザ・パスワードに書き換えます。\n// Configure authentication const authUser = \u0026#39;user\u0026#39;; const authPass = \u0026#39;pass\u0026#39;; 右上の [保存] を押して、その横にある [アクション] から 新しいバージョンを発行 します。\nすると、右上の ARN が末尾にバージョン番号がついたものになるので、コピーしておきます。\n2. CloudFront で /about/* のパス宛に Behavior を作成する CloudFront のコンソールで該当の Distribution を選択して、 Behaviors のタブから [Create Behavior] を押して新しい Behavior を作ります。\nPath Pattern には、Basic 認証をかけたいパスを入力します。今回は /about/ 以下が対象なので、 /about/* と入力します。\n途中の設定は適宜必要な値に設定して、一番下にある Lambda Function Associations で、リクエスト時の挙動を設定します。\nCloudFront Event は、ユーザがアクセスしたときなので Viewer Request を選択、 ARN には、先程コピーした Lambda の ARN (arn:aws:lambda...) を入力します。\nこの内容で Behavior を作成したら、対象となるパスに対して Invalidation を発行して、設定は完了です。\n今回は Lambda@Edge で Basic 認証を実装しましたが、 Lambda が呼べるということは何でもできるということなので、色々使いみちがありそうです。\n",
    "permalink": "https://michimani.net/post/aws-cloudfront-basic-auth/",
    "title": "CloudFront + S3 で構築したページに Basic 認証をかける"
  },
  {
    "contents": "ルービックキューブには国内外で大会が開催されていて、たまにニュースやバラエティーでその映像を見ることがあります。\nでも実際にどこでやっているのかとか、どうやったら参加できるのかとか、全くわからなかったので調べてみました。\n主催は日本ルービックキューブ協会 国内で開催される大会は、有志のメンバーで構成される \u0026lt;strong\u0026gt;日本ルービックキューブ協会 (JRCA)\u0026lt;/strong\u0026gt; が主催しています。\n大会は全国各地で開催されており、大会以外にも記録会や講習会、定例会 (オフ会) なども定期的に開かれているようです。\nJRCA が主催となって開催される大会は \u0026lt;strong\u0026gt;World Cube Association\u0026lt;/strong\u0026gt; の規約に則っているため、そこで出た記録は国内公式記録であり、世界公式記録として残ります。\n大会のルール とりあえず短い時間で完成させる ということは間違いないのですが、それ以外の詳細についても調べてみました。\n使用するパズル 大会で使用するパズル (キューブ) については、次のような決まりがあります。\n使用するパズルは競技者本人が準備する パズルは色付きで、すべての面が違う色でなければならない 各面で凹凸や素材の違いがあってはならない 全てのブランドのパズルやパズルのパーツを使うことができる ざっくりとは上記のような決まりですが、後述する種目によって制限される項目もあります。\n例えば、 各面で凹凸や素材の違いがあってはならない とありますが、通常の 3x3x3 のような種目では、ロゴシールを 1 つの面の中央のパーツに貼ることができるとされています。しかし、目隠し種目に関してはロゴシールを貼ることが出来ないとされています。\n競技種目 全てではないですが、次のような種目があります。\n3x3x3 2x2x2 4x4x4 5x5x5 3x3x3 片手 3x3x3 足 単にマスの数が異なるだけでなく、片手とか足とか目隠しとか\u0026hellip;様々な種目があるようです。\n記録の計測 各種目について、他のスポーツ競技と同様に １回戦、２回戦、決勝 といった形でラウンドごとに競技を行います。\n各ステージごとに、競技者は 5 回パズルを解いて、そのスピードを競います。\nタイムの算出方法としては、次の 3 種類があります。\nAverage of 5\n5 回パズルを解いて、最高と最低記録を除いた残り 3 回の平均を記録とする Best of X (X は 1, 2, または 3)\nX 回パズルを解いて、それらの中で一番良いものを記録とする Mean of 3\n3 回パズルを解いて、それらの平均を記録とする その他の規則 上記以外にも次のような細かい規則がしっかりと決められています。\nパズルはコンピュータによりランダムに生成されたスクランブル手順を用いてスクランブルされなければならない ※スクランブル とは、競技前にパズルをバラバラにすること 競技エリアは禁煙でなければならない 参加を希望する者は誰でも参加できること 3 つ目に挙げたように、参加したい人は誰でも参加することができます。\nただし、 制限時間（大会毎に告知されています）より早く完成できるのであれば という前置きが付きます。\nちなみに、 10月7日に開催された広島大会 では、3x3x3 の制限時間は 10分 でした。\n※詳細については WCA 大会規則 なぜ急にルービックキューブの話？ そもそもなぜこんなことを調べたかというと、私自身ルービックキューブを軽くかじっているからです。\nベストタイムは 35 秒くらいで、だいたい 45〜55 秒あたりをウロウロしているレベルです。\n過去の大会の記録を見てみると、このくらいの記録では下から5番目くらいの記録でした。。。 とは言っても圧倒的に遅いわけではないので、参加してみても面白いかなーと思いました。\n",
    "permalink": "https://michimani.net/post/other-about-solving-rubiks-cube/",
    "title": "ルービックキューブの大会について調べてみた"
  },
  {
    "contents": "技術的な考え方まとめのようなページで何度か名前を見たことがあった本「ライト、ついてますか ―問題発見の人間学」を読んでみました。\n最近では信用できるか怪しいですが、 Amazon での評価もそこそこ高かったです。\nタイトル 本のタイトルは「ライト、ついてますか ―問題発見の人間学」となっていますが、もともとは外国の本で、オリジナルのタイトルは「ARE YOUR LIGHT ON? How to figure out what the plobrem really is.」 です。\n著者は、Donald C. Gause と Gerald M. Weinberg の 2 名。日本語訳は 木村 泉 さんです。\nなぜこの本を読もうと思ったか 冒頭にも書きましたが、「新人エンジニアに読ませたい」とか「エンジニアとしての考え方を養う」みたいな触れ込みでこの本が紹介されていることが多かったからです。\n私自身 もう新人とは言えない年にはなってますが、これまでこういう類の本は一切読んでこなかったので、とりあえずよく紹介されているから読んでみようと思ったわけです。\n概要 タイトルにもありますが、問題とは何なのか ということについて書かれています。\n具体的には、次の 6 部構成で書かれています。\n何が問題か？ 問題とは何なのか？ 問題とは本当のところ何か？ それは誰の問題か？ それはどこからきたか？ われわれはそれを本当に解きたいか？ それぞれの部にはいくつかの小さなエピソードをもとに、部のタイトルとなっていることについて説明されています。\nそして、その説明を要約するような、名言のようなフレーズがいくつも出てくる という内容です。\n印象に残ったところ 概要のところでも書いたように、この本には重要なフレーズや名言っぽいことがたくさん書かれています。なので、それらの中から印象に残っているところをいくつか紹介します。\n問題を抱えているのはだれか？\nあなたの問題の本質は何ですか？\nこれは第 1 部の最初に出てくる言葉です。\n何かしら問題として出てきたときに、いきなりそれをとこうとするのではなく、 その問題を抱えているのは誰なのか をまず考えようということです。\nまた、抱えているのが誰なのかがわかれば、その人に対して あなたの問題の本質はなんですか？ と聞いてみようということです。\nその問題によって 誰が、どんなふうに 困っているのかがわかれば対応方法も具体的に出てきやすいなと、改めて思いました。\nキミの問題定義を\n外国人や盲人や子供について\n試してみよう。またキミ自身が\n外国人や盲人や子供になってみよう\nこれは第 3 部の 不適合を見落とす話 に出ていくるフレーズです。\nこの章では 新しい視点は必ず新しい不適合を作り出す という言葉もあり、何かに対して問題を定義する際に、さまざまな立場に立って考えることでより多くの問題の定義を見つけることができる という内容です。\nグローバルなシステムやサービスでは、このようにさまざまな視点に立って問題を考えることがより重要になりそうです。\n問題をどう変えたら\n解答を変えることができるだろうか？\nこれは第 3 部の うまいレベルに着陸する話 に出てくるフレーズです。\n章のタイトルからもわかるように、問題に対して、如何に解答する側の都合のいいようにもっていくか という内容です。\n問題によっては馬鹿正直に解こうとすると大変な目に遭うようなものだったり、そもそも解くことができない問題もあります。しかし、問題を抱えている人は何かしらの解答を求めているので、それに応えるためにどうするか。\nじゃあ、解答しやすいように問題を変えてしまおう という力技ですね。ただ、こういう考えも必要だなとは思いました。\nまとめ いくつか本の中のフレーズを引用しましたが、ちょっとした違和感が伝わるでしょうか。\n冒頭に書きましたが、この本は外国で書かれた本を日本語訳されています。また、各部ではその部の概要を説明する内容のエピソードがあるとも書きましたが、これらのエピソードは現地の環境での話です。\nこれらのことから、正直細かい内容は頭に入ってきづらいです。\n特に日本語訳については不自然な部分が多く、文章としても読みづらいところが多々ありました。\nとは言っても、フレーズに関してはニュアンスで理解はできて、いろいろな視点での考え方が身につきそうな内容なので、本としては面白いです。\n特に問題解決に関してはエンジニアが一生付き合っていかなければいけないものなので、新人エンジニアにおすすめしたい本として紹介される理由はわかった気がします。\nでも、やっぱり日本語訳が残念ですね。それに尽きる思います。\n",
    "permalink": "https://michimani.net/post/book-review-light-on/",
    "title": "「ライト、ついてますか ―問題発見の人間学」を読んでみた"
  },
  {
    "contents": "今月 目黒にオープンした AWS Loft Tokyo。そこで開かれたセミナー「AWS Start-upゼミ::デモ祭り」に行ってきました。AWS Loft に行くのも初めてだったのでそのあたりのことも触れます。\nAWS Loft Tokyo とは \u0026ldquo;AWS Loft は、AWS を利用中のスタートアップおよびデベロッパーのための施設です。 リラックスした雰囲気のコ・ワーキングスペースや毎週開催される様々なセミナー、AWS のテクノロジーに熟達した AWS のソリューションアーキテクトやサポートエンジニアへいつでも技術的な相談が可能な「Ask An Expert カウンター」もご用意しております。\u0026rdquo;\nとあるように、最近話題のコワーキングスペースです。しかし他のコワーキングスペースと違って、利用できるのは AWS アカウントを持っている人 で、その利用料金はなんと 無料 です。 ただ、営業時間が 「平日（月〜金） 10:00 〜 18:00」となっています。\n今回はセミナーでの訪問ということで、営業時間後の 18 時過ぎに入りました。 日中は賑わっているという話を聞いていましたが、さすがに営業時間語だったので人はまばら。というか、セミナー参加者しかいませんでした。\nエレベータのドアはダンボール仕様。 天井からは植物。 Amazon の端末が置かれてます。 フロアは広いスペースがメインで、カウンター、テーブル、ソファなど様々な場所で作業ができるようになっています。 奥の方には個室もありました。\nAWS Start-upゼミ AWS Loft のオープン当日から、営業時間後を中心に様々なセミナーが開催されています。 今回参加したのは\n[AWS Start-upゼミ::デモ祭り] よくある課題を一気に解説 \u0026amp; デモ！ 〜御社の技術レベルがアップする 2018 秋期講習〜\nです。 AWS Start-up ゼミシリーズとして過去に 2017 秋期講習 、 2018 春期講習 が開催されており、今回はそれらの内容をアップデートしながら、個々の課題についてデモをやりまくる という内容でした。 今回が初めての参加でしたが、ほとんどは初参加の人たちだったようです。\n内容 トピックとしては下記のような内容でした。\nログの扱い デプロイ周りちゃんとやりたい コンテナを使いたい 運用監視をちゃんとしたい コスト下げたい WebSocket でリアルタイムになにかやりたい セキュリティについて 本当は 15 項目くらい準備されていたようですが、時間と内容を完全に見誤った (担当者談) らしく、かなり詰め込んでこれだけの内容になりました。 スピーカーは AWS ソリューションアーキテクト (SA) の方 2 名でした。 どれも具体的な運用方法、ハマりがちなところなどを説明していただきましたが、正直自分の理解力が追いつかず、新たに知ることが多かったです。\nなので、濃い内容はおそらく後日 公式に公開されるので、各トピックについてざっくりと書いておきたいと思います。\n1. ログの扱い とりあえず S3 にためる。話はそれから。 Kinesis Data Firehose を使えばストリーミングデータを S3 とかに簡単に投げられる S3 に入れてしまえば、あとは Athena や Redshift といったデータレイクで解析 2. デプロイ周りちゃんとやりたい キーワードは ステートレス デプロイは push 型ではなく pull 型 3. コンテナを使いたい The Twelve-Factor App 必読 とりあえずやるなら Fargate がシンプル 4. 運用監視をちゃんとしたい 各サービスのモニタリング項目を把握 何がどうなるとやばいか 通知、リカバリアクションを決める 5. コスト下げたい 構成をクラウドに最適化 ← ここでもステートレス スポットインスタンス、リザーブドの活用 (スピード感と費用のバランス) 違う視点でアーキテクチャを考える (サーバレス構成にしてみる) 6. WebSocket でリアルタイムになにかやりたい AWS AppSync (マネージドな GlaphQL のようなもの) を活用する ALB と WebSocket サーバをつなぐ (スケーリングがしやすくなる) 接続が切れたらリトライすればいい というポリシーで実装する 7. セキュリティについて 予測 → 防御 → 検知 → 対応 まず最低限 Well-Architected Framework でリスクの洗い出し 業界・業種に応じた遵守事項に従う (GDPR など) ざっくりとはこのような内容でした。 Docker についてはもっと自分で触らないとやばいなと、取り残されてる感を感じました。\nまとめ ゼミの内容については、個人的には結構難しい内容ではありましたが、AWS のサービスを使った各トピックの実例や構築パターンを知ることができたので良かったです。 AWS Loft に関しては、とりあえず平日の昼間に来てコワーキングスペースとして作業したいですね。自分の顔のラテアートも飲みたいですし。\nセミナーについてはまだまだ開催予定とのことなので、興味のある方は行ってみてはどうでしょうか。\nAWS Loft Tokyo にて開催されるイベント・セミナー一覧 ちなみに今回はうまい棒 (AWS ver.) が食べ放題でした。 ",
    "permalink": "https://michimani.net/post/aws-loft-autumn-seminar/",
    "title": "AWS Loft Tokyo で開催された AWS Start-upゼミ に行ってきました"
  },
  {
    "contents": "Hugo で作ったブログに、カテゴリ一覧 と タグ一覧 のページを作ったので、その方法についてです。\nついでにヘッダーメニューにも各一覧ページへのリンクを設置します。\nやること config.toml に追記 terms.html を作成 ヘッダーにカテゴリ一覧・タグ一覧へのリンクを追加 1. config.toml に追記 カテゴリ一覧、タグ一覧を生成するためには config.toml に次の設定を追記します。\n[taxonomies] tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; 2. terms.html を作成 使用しているテーマのディレクトリ内にある layouts/_default に terms.html というファイルを作成します。\n今回は Indigo を使用しているので themes/indigo/layouts/_default 内に次のような内容の terms.html を作成します。\n{{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h2\u0026gt;{{ .Title }}\u0026lt;/h2\u0026gt; {{ if eq .Title \u0026#34;Tags\u0026#34;}} {{ if lt 0 (len .Site.Taxonomies.tags) }} \u0026lt;ul\u0026gt; {{ range .Site.Taxonomies.tags.ByCount }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ $.Site.BaseURL | relURL }}tags/{{ .Name | urlize }}\u0026#34;\u0026gt;{{ .Name }}\u0026lt;span\u0026gt;({{ .Count }})\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ end }} {{ else if eq .Title \u0026#34;Categories\u0026#34;}} {{ if lt 0 (len .Site.Taxonomies.categories) }} \u0026lt;ul\u0026gt; {{ range .Site.Taxonomies.categories.ByCount }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ $.Site.BaseURL | relURL }}categories/{{ .Name | urlize }}\u0026#34;\u0026gt;{{ .Name }}\u0026lt;span\u0026gt;({{ .Count }})\u0026lt;/span\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ end }} {{ end }} {{ partial \u0026#34;footer.html\u0026#34; . }} このレイアウトは /categories/ または /tags/ にアクセスした際に使用されます。\nTitle の値でカテゴリかタグかを判定してるのが変な感じですが、とりあえずコレで両方を出し分けられます。\n3. ヘッダーにカテゴリ一覧・タグ一覧へのリンクを追加 ヘッダーにカテゴリ・タグそれぞれの一覧ページへのリンクを追加します。\nIndigo の場合、テーマディレクトリ内のlayouts/partials/pagenav.html を次のように編集します。\n\u0026lt;nav\u0026gt; {{ $currentPage := . }} \u0026lt;div id=\u0026#34;page-nav\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;page-nav-item\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.BaseURL }}\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; {{ range .Site.Menus.main }} \u0026lt;div class=\u0026#34;page-nav-item\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt; {{ .Pre }} \u0026lt;span\u0026gt;{{ .Name }}\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; {{ end }} + \u0026lt;div class=\u0026#34;page-nav-item\u0026#34;\u0026gt; + \u0026lt;a href=\u0026#34;/categories/\u0026#34;\u0026gt;Categories\u0026lt;/a\u0026gt; + \u0026lt;/div\u0026gt; + \u0026lt;div class=\u0026#34;page-nav-item\u0026#34;\u0026gt; + \u0026lt;a href=\u0026#34;/tags/\u0026#34;\u0026gt;Tags\u0026lt;/a\u0026gt; + \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/nav\u0026gt; 以上で、カテゴリ・タグの一覧ページが作成できました。\n手順は非常に簡単ですが、各カテゴリ・タグに該当する記事数の表示までしてくれるのは良いですね。\n",
    "permalink": "https://michimani.net/post/category-tag-list/",
    "title": "[Hugo] カテゴリ一覧、タグ一覧のページを作成する"
  },
  {
    "contents": "Gatsby と同様に、CodeCommit に push したら CodeBuild でビルドして S3 にデプロイして公開される、というところまでを自動化します。\n概要 基本的には Gatsby の場合 と同じなので、異なる部分について書いていきます。\nやること S3 のバケットポリシーを変更 CloudFront の Origin Domain Name を変更 CodeBuild の環境、 Buildspec を変更 1. S3 のバケットポリシーを変更 まず、S3 のバケットポリシーを下記のように変更します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AddPerm\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::{バケット名}/*\u0026#34; } ] } Gatsby と Hugo の大きな違いとして、生成されるサイトがシングルページかどうか という点があります。\nCloudFront で Default Root Object に index.html を指定しておけば、https://michimani.net/ でのアクセスで https://michimani.net/index.html が表示されます。\nGatsby ではここから画面遷移しないのでこれで問題なかったのですが、Hugo では https://michimani.net/post/some-post/ のような個別のページへ遷移することになります。この場合、https://michimani.net/post/some-post/index.html が表示されるかと思いきや、Access Denied と言われてしまいます。\nこれを回避するために、バケットポリシーを上記のように変更します。\n2. CloudFront の Origin Domain Name を変更 上記の問題解決に関連して、CloudFront の Origin Domain Name を変更します。\n新しい値としては、S3 の Static website hosting で確認できるエンドポイントの http:// を除いた部分 です。\n3. CodeBuild の環境、 Buildspec を変更 Hugo は Go 言語で作られているので、 CodeBuild の環境 (イメージ) を aws/codebuild/golang:1.10 に変更します。\nまた、ビルド時のコマンドも変わるため、 Buildspec を下記のように変更します。\nversion: 0.2 phases: install: commands: - curl -Ls https://github.com/gohugoio/hugo/releases/download/v0.49/hugo_0.49_Linux-64bit.tar.gz -o /tmp/hugo.tar.gz - tar xf /tmp/hugo.tar.gz -C /tmp - mv /tmp/hugo /usr/bin/hugo - rm -rf /tmp/hugo* build: commands: - hugo post_build: commands: - aws s3 sync \u0026#34;public/\u0026#34; \u0026#34;s3://{バケット名}\u0026#34; --delete --acl \u0026#34;public-read\u0026#34; - aws cloudfront create-invalidation --distribution-id {ディストリビューションのID} --paths \u0026#34;/*\u0026#34; また、Gatby の時には CodePipeline で Algolia のインデックスを削除するアクションを入れていましたが、これも削除します。\n以上で Hugo の自動デプロイ環境が整いました。\npush からデプロイまでのパイプライン全体の所要時間としては、Gatsby のときは 6〜7 分でしたが、Hugo では 3〜4 分 になりました。\nこのあたりにも Hugo ビルド速度の恩恵を感じられました。\n",
    "permalink": "https://michimani.net/post/deploy-hugo/",
    "title": "Hugo で作ったサイトを CloudFront + S3 で公開する"
  },
  {
    "contents": " 前回 までで Gatsby の設定やデプロイ方法について書いてきましたが、色々と問題が出てきたため Hugo に移行しました。\n移行といっても、記事の数が少なかったためほぼゼロからのスタートです。\n今回は Gatsby の問題点と、Hugo のインストールについて書きます。\nGatsby の問題点・不満点 Gatsby というか、使っていた starter starter-personal-blog の問題が大きいです。\nstarter-personal-blog では Algolia というサービスを用いたブログ内検索機能があります。\nビルド時に Algolia にインデックスを生成して、ブログ内の検索をかなりの早さで実行できるという部分がとても良かったです。\nが、問題として、インデックスに登録できるブログ記事の文字数、正確には記事の Markdown ファイルの容量に制限があり、その制限を超えているとビルドが失敗してしまうというものです。制限は下記の通り。\n無償版 : 10 KB まで 有償版 : 20 KB まで つまり、ちょっと長い文章を書こうとするとエラーになり、ビルドが失敗してしまうということです。\n有償版は最低でも $35/月 ということで結構いい値段がします。\nじゃあ、自前でブログ内検索を実装すればいいのでは？とも思いましたが、React の知識が足りないためカスタマイズも出来ず\u0026hellip;。\nビルドの時間にも少し不満があったので、ビルドが爆速だと噂の Hugo に移行することにしました。\nHugo とは Hugo は、Go で作られている静的サイトジェネレータです。\nローカルでの表示確認やビルドが爆速であること、テーマの数が豊富なところが売りとなっています。\nインストール インストールも簡単です。\nmacOS では Homebrew に hugo の cli があるので、それをインストールします。\n$ brew install hugo ... ... $ hugo version Hugo Static Site Generator v0.49.2/extended darwin/amd64 BuildDate: unknown 公式サイトの手順 に従ってインストールを進めます。\n$ hugo new site michimani.net テーマを追加します。今回は indigo を使います。\n$ cd michimani.net $ git init $ git submodule add https://github.com/AngeloStavrow/indigo.git themes/indigo $ echo \u0026#39;theme = \u0026#34;indigo\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml ブログの設定 設定は、直下にある config.toml を適宜編集します。\nbaseURL = \u0026#34;https://michimani.net/\u0026#34; languageCode = \u0026#34;ja-JP\u0026#34; copyright = \u0026#34;Copyright © 2018, michimani\u0026#34; title = \u0026#34;michimani.net\u0026#34; ここで注意が必要なのが、baseURL の値です。末尾には / を付ける必要があります。\nレイアウトの修正 baseURL との兼ね合いで一部 asset へのリンクが正しく生成されない部分があるので、その部分を修正しておきます。\n対象のファイルは themes/indigo/layouts/404.html と themes/indigo/layouts/partials/header.html ですが、修正方法は同じです。\n\u0026lt;div id=\u0026#34;sitelogo\u0026#34;\u0026gt; - \u0026lt;a class=\u0026#34;glyph\u0026#34; alt=\u0026#34;Home\u0026#34; href=\u0026#34;{{ .Site.BaseURL }}\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;{{ .Site.BaseURL }}/images/site-logo.svg\u0026#34; alt=\u0026#34;Site Logo\u0026#34; height=\u0026#34;64px\u0026#34; width=\u0026#34;64px\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; + \u0026lt;a class=\u0026#34;glyph\u0026#34; alt=\u0026#34;Home\u0026#34; href=\u0026#34;{{ .Site.BaseURL }}\u0026#34;\u0026gt;\u0026lt;img src=\u0026#34;{{ .Site.BaseURL }}images/site-logo.svg\u0026#34; alt=\u0026#34;Site Logo\u0026#34; height=\u0026#34;64px\u0026#34; width=\u0026#34;64px\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; 記事作成・ビルド 記事作成 ブログ記事は下記コマンドで新規作成します。\n$ hugo new post/new-post-name.md 上記の場合、 https://michimani.net/post/new-post-name/ にアクセスして表示される記事が作成されます。\n記事内で使用する画像類は static/images の中に置いて、 /images/***.png の形で挿入します。\nローカルで確認 ローカルでの確認は下記コマンドを実行します。\nhugo server -D | EN +------------------+----+ Pages | 36 Paginator pages | 0 Non-page files | 0 Static files | 66 Processed images | 0 Aliases | 14 Sitemaps | 1 Cleaned | 0 Total in 40 ms Watching for changes in /Users/hoge/michimani.net/{content,data,layouts,static,themes} Watching for config changes in /Users/hoge/michimani.net/config.toml Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop http://localhost:1313/ にアクセスして確認します。\nこのコマンドを実行した時点で hugo の売りである速さを感じることができます。Gatsby ではローカルサーバの起動まで数秒がかかりましたが、hugo では、出力されているとおり 40 ms です。\nビルド ビルドは下記のコマンドです。\n$ hugo | EN +------------------+----+ Pages | 36 Paginator pages | 0 Non-page files | 0 Static files | 66 Processed images | 0 Aliases | 14 Sitemaps | 1 Cleaned | 0 Total in 74 ms こちらも一瞬で public ディレクトリが生成されます。\nまとめ 以上、Go 言語でできている静的サイトジェネレータ hugo のインストールからビルドまででした。\nGatsby のときと同様に デプロイまでの自動化については次回書きたいと思います。\nと言っても、Gatsby のときに構築したフローを少し変更するだけで済むので、新たに組み直す必要はありませんでした。では。\n",
    "permalink": "https://michimani.net/post/install-hugo/",
    "title": "静的サイトジェネレータを Gatsby から Hugo に移行した"
  },
  {
    "contents": " 前回 まで、実際に公開するところまでできました。\n公開するにはビルド後の public ディレクトリを S3 にアップロードする必要がありますが、そのあたりも自動化したいところです。\n今回は、AWS のサービスを使って自動化を実現します。\n使う AWS のサービス 使うのは次のサービスです。\nCodeCommit : ソース管理に使用します。 CodePipeline : ソースの変更からビルド、デプロイまでのフローを管理します。 CodeBuild : ソースのビルド、デプロイを実行します。 Lambda : Algolia のインデックスを操作します。 自動デプロイの流れ 自動デプロイの流れは次のようになります。\n記事を作成して CodeCommit に作成したリポジトリに push Lambda で Algolia のインデックスを削除 CodeBuild でビルド、S3 へのソースの配置 Lambda で実行結果を通知 これらの流れを CodePipeline で繋げます。\nCodePipeline の設定の最終形は下図のようになります。\n流れをひとつずつみていきます。\n1. 記事を作成して CodeCommit に作成したリポジトリに push これは特筆することはないですね。記事ファイルを追加して commit → push するだけです。\n2. Lambda で Algolia のインデックスを削除 これですが、Algolia へのインデックス登録は、npm run build つまり gatdby build を実行した際に登録されます。\nその際、同じページであっても オブジェクトID が異なると別ページとして登録されることになります。このオブジェクトIDはビルド時の環境によって決まるもので、今回のように 仮想環境でビルドを実行する場合はビルドのたびに値が変わり、結果的にインデックスの重複がどんどん増えてしまいます。\nこれを回避するために、ビルド前に既存のインデックスを全て削除します。\nとしては、Lambda で下記のようなスクリプトを作って Algolia の API を実行します。\nfrom algoliasearch import algoliasearch import boto3 def lambda_handler(event, context): try: ALGOLIA_APP_ID = \u0026#39;your_algolia_app_id\u0026#39; ALGOLIA_ADMIN_API_KEY = \u0026#39;your_algolia_admin_api_key\u0026#39; ALGOLIA_INDEX_NAME = \u0026#39;your_algolia_index_name\u0026#39; algolia = algoliasearch.Client(ALGOLIA_APP_ID, ALGOLIA_ADMIN_API_KEY) index = algolia.init_index(ALGOLIA_INDEX_NAME) res = index.clear_index() job_id = event[\u0026#34;CodePipeline.job\u0026#34;][\u0026#34;id\u0026#34;] put_job_success(job_id) except Exception as e: put_job_failer(job_id, e.reason) return res def put_job_success(job_id): pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) pipeline.put_job_success_result( jobId=job_id, currentRevision={ \u0026#39;revision\u0026#39;: \u0026#39;revision_string\u0026#39;, \u0026#39;changeIdentifier\u0026#39;: \u0026#39;changeIdentifier_string\u0026#39; } ) def put_job_failer(job_id, message): pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) pipeline.put_job_failure_result( jobId=job_id, failureDetails={ \u0026#39;type\u0026#39;: \u0026#39;JobFailed\u0026#39;, \u0026#39;message\u0026#39;: message } ) ここで注意が必要なのが、 algoliasearch というライブラリが Lambda のランタイム上には存在しないということです。\nこれを解決するためには、ローカル環境で algoliasearch をローカル (グローバルではなく という意味で) にインストールして、上記のスクリプトと含めて zip 化して Lambda にアップロードします。下記のサイトを参考にしました。\n【AWS】Lambdaでpipしたいと思ったときにすべきこと 3. CodeBuild でビルド、S3 へのソースの配置 続いて、CodeBuild でビルドして、そのまま S3 への配置まで行います。\nCodeBuild のコンソール画面からビルドプロジェクトを作成します。\nプロジェクト名 任意に入力します。\nソース ソースプロバイダに CodeCommit を指定して、対象のリポジトリを指定します。\n環境 マネージド型イメージで OS は Ubuntu、ランタイムは aws/codebuild/nodejs:10.1.0 を選択します。\nサービスロールでは、下記のような権限を持つロールを作成して選択します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-northeast-1:12345678XXXX:log-group:/aws/codebuild/{ビルドプロジェクト名}\u0026#34;, \u0026#34;arn:aws:logs:ap-northeast-1:12345678XXXX:log-group:/aws/codebuild/{ビルドプロジェクト名}:*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::codepipeline-ap-northeast-1-*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:codecommit:ap-northeast-1:12345678XXXX:{リポジトリ名}\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;codecommit:GitPull\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::{対象のS3バケット名}/*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34; ] } ] } Buildspec ビルドに関する設定です。\nリポジトリの直下に buildspec.yml を配置しても良いのですが、今回はビルドプロジェクト内で指定します。内容は下記のような形です。\nversion: 0.2 phases: install: commands: - touch .npmignore pre_build: commands: - npm install build: commands: - echo GOOGLE_ANALYTICS_ID=UA-123456789-0 \u0026gt; .env - echo ALGOLIA_APP_ID=your_algolia_app_id \u0026gt;\u0026gt; .env - echo ALGOLIA_SEARCH_ONLY_API_KEY=your_algolia_search_only_api_key \u0026gt;\u0026gt; .env - echo ALGOLIA_ADMIN_API_KEY=your_algolia_admin_api_key \u0026gt;\u0026gt; .env - echo ALGOLIA_INDEX_NAME=your_algolia_index_name \u0026gt;\u0026gt; .env - echo FB_APP_ID= \u0026gt;\u0026gt; .env - npm run build post_build: commands: - aws s3 sync \u0026#34;public/\u0026#34; \u0026#34;s3://{対象のバケット名}\u0026#34; --delete --acl \u0026#34;public-read\u0026#34; artifacts: base-directory: public files: - \u0026#34;**/*\u0026#34; build の部分では、.env ファイルを作成しています。\nAPI キーなどについては直接書いてますが、本来であれば環境変数として埋め込むのが正解だと思います。\n4. Lambda で実行結果を通知 最後に、結果を通知します。\n今回はとりあえず終わったことがわかればいいので内容は適当ですが、下記のような形で Slack に通知します。\nimport boto3 import json import logging from urllib.request import Request, urlopen from urllib.error import URLError, HTTPError SLACK_CHANNEL = \u0026#39;channel_name\u0026#39; HOOK_URL = \u0026#39;https://hooks.slack.com/services/XXXXX...\u0026#39; logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): post_to_slack(json.dumps(event, indent=2)) job_id = event[\u0026#34;CodePipeline.job\u0026#34;][\u0026#34;id\u0026#34;] put_job_success(job_id) return {\u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;} def put_job_success(job_id): pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) pipeline.put_job_success_result( jobId=job_id, currentRevision={ \u0026#39;revision\u0026#39;: \u0026#39;revision_string\u0026#39;, \u0026#39;changeIdentifier\u0026#39;: \u0026#39;changeIdentifier_string\u0026#39; } ) def put_job_failer(job_id, message): pipeline = boto3.client(\u0026#39;codepipeline\u0026#39;) pipeline.put_job_failure_result( jobId=job_id, failureDetails={ \u0026#39;type\u0026#39;: \u0026#39;JobFailed\u0026#39;, \u0026#39;message\u0026#39;: message } ) post_to_slack(message) def post_to_slack(message): slack_message = { \u0026#39;channel\u0026#39;: SLACK_CHANNEL, \u0026#39;attachments\u0026#39;: [ { \u0026#39;fields\u0026#39;: [ { \u0026#39;value\u0026#39;: \u0026#34;Finished deploying. \\n```\\n{}\\n```\u0026#34;.format(message) } ] } ] } req = Request(HOOK_URL, json.dumps(slack_message).encode(\u0026#39;utf-8\u0026#39;)) try: response = urlopen(req) response.read() logger.info(\u0026#34;Message posted to %s\u0026#34;, slack_message[\u0026#39;channel\u0026#39;]) except HTTPError as e: logger.error(\u0026#34;Request failed: %d %s\u0026#34;, e.code, e.reason) except URLError as e: logger.error(\u0026#34;Server connection failed: %s\u0026#34;, e.reason) ハマったところ CodePipeline から Lambda を呼ぶときは、その Lambda 内で必ず put_job_success_result() または put_job_failure_result() を実行する必要があります。\nこれをしないと、処理が CodePipeline に返ってこずに、次のアクションに移らず、タイムアウトで失敗していまいます。\n以上で、自動デプロイのフローが完成しました。\n今回はすべて AWS のサービスを使用しましたが、やはり親和性が高いので設定もコンソール上でほとんどできてしまいました。\nひとまずこれで環境が整った感じなので、これから色々と書き進めていきたいと思います。\n",
    "permalink": "https://michimani.net/post/deploying-gatsby/",
    "title": "Gatsby で静的サイトを作ってみた - 自動デプロイ編 -"
  },
  {
    "contents": "Mac でスクリーンキャプチャを撮ると、スクリーンショット 2018-10-22_22.27.08.png のような感じでファイル名に半角スペースが含まれてしまいます。 スクリーンショット の部分はコマンドで変更できます (無しにもできる) が、その部分と日付の間には半角スペースが入ってしまいます。\nコマンドラインでファイルを扱う際には半角スペースがあると面倒なので、どうにかして半角スペースを無くしたい、というのが今回の目的です。 とりあえず、半角スペースをアンダースコア _ に変換することをゴールにします。\n前準備 スクリーンキャプチャの保存先を適当なフォルダに設定します。\n$ defaults write com.apple.screencapture location /Users/hoge/Pictures/_ScreenCaptureTmp $ killall SystemUIServer 今回は /Users/hoge/Pictures/_ScreenCaptureTmp とします。(TMP_SC_DIR) スクリンキャプチャはこのフォルダに作成されます。半角スペースをファイル名に含んだ状態で。\nそして、本当にスクリーンキャプチャを保存したいディレクトリを用意します。 今回は /Users/hoge/Dropbox/200_photos/001_SC_MBP とします。(COMP_SC_DIR)\nどうするか (概要) 以下を実現するスクリプトを作ります (今回は Python で作りました) TMP_SC_DIR 内のファイルのファイル名に含まれる半角スペースをアンダースコアに置換する ファイル名を置換したファイルを COMP_SC_DIR に移動する スクリーンキャプチャを撮るたびに 1 のスクリプトを実行する どうするか (詳細) 1 を実現するスクリプトを作成 import glob import os TMP_SC_DIR = \u0026#39;/Users/hoge/Pictures/_ScreenCaptureTmp/\u0026#39; COMP_SC_DIR = \u0026#39;/Users/hoge/Dropbox/200_photos/001_SC_MBP/\u0026#39; i = 1 tmp_sc_list = glob.glob(TMP_SC_DIR + \u0026#39;*.png\u0026#39;) if len(tmp_sc_list) == 0: print(\u0026#39;no screen capture.\u0026#39;) else: for file in tmp_sc_list: os.rename( file, file.replace(TMP_SC_DIR, COMP_SC_DIR).replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;)) i += 1 print( \u0026#39;{} screen capture(s) rename and move, completely.\u0026#39; .format(str(len(tmp_sc_list)))) これを適当な場所に置きます。 今回は /Users/hoge/Projects/system/rename_and_move_screen_capture.py とします。\nAutomator でフォルダアクションを作成 Automator は Mac での様々な操作を自動化してくれるアプリケーションで、デフォルトで入っています。 今回はその Automator で\n1.TMP_SC_DIR を監視する 2. ファイルが置かれたら rename_and_move_screen_capture.py を実行する 3. 実行結果を通知する\nというフローを実行するフォルダアクションを作成します。\n設定についてはキャプチャのとおりです。\n変数の値を設定 は、スクリプトを実行したときに出力しているメッセージを変数に設定して、通知の際に表示させるために入れています。\nで、これを適当な名前で保存してスクリーンキャプチャを撮って数秒後に通知がきます。\nこれでスクリーンキャプチャの半角スペース問題はなんとかなりました。\n",
    "permalink": "https://michimani.net/post/develop-handle-whitespace-in-screen-captcha-in-mac/",
    "title": "Mac で撮ったスクリーンキャプチャのファイル名に含まれる半角スペースを Automator でどうにかする"
  },
  {
    "contents": "今回は、作ったサイトを実際に公開するまでの話です。\nGatsbyJS は静的サイトジェネレータなので、gatsby build コマンドを実行して生成される public ディレクトリを、どこかしらのサーバに置けばページが公開されます。\n今回は AWS の CloudFront と S3 を使って公開したいと思います。\nAWS で使うサービス 今回 AWS の中でも、下記のサービスを使います。\nRoute 53\nドメイン (michimani.net) の管理 CloudFront\nCDN。SSL 対応のために使う Certificate Manager\nSSL 証明書の管理 S3\nファイルの設置場所 ただ公開するだけであれば S3 だけでいいのですが、SSL 対応、独自ドメイン使用のために他の 3 を使用します。\nドメインを Route 53 で管理する 今回使うドメイン michimani.net を Route 53 で管理できるようにします。\nこのドメインは Route 53 で取得したものではなく、お名前.com で取得したものなので、お名前.com のドメイン Navi でも設定が必要になります。\nドメインを Route 53 に登録する マネジメントコンソールで Route 53 のページから Hosted zones を選択し、Create Hosted Zone を押して、Domain Name に ドメイン名を入力して Create を押します。\nすると、作成したゾーンの詳細画面で 4 つの NS レコードを確認するとができます。\nお名前.com のドメイン Navi でネームサーバを登録する 生成された NS レコードを、お名前.com のドメイン Navi \u0026gt; ネームサーバ設定 から登録します。\nS3バケットの作成、CroudFront の設定 このあとについては下記のような手順で進めていきます。\nCertificate Manager で SSL 証明書を発行 S3 にバケット作成 CloudFront で Distribution 作成 Route 53 で DNS 設定 ただ、この手の方法については既に様々なところに知見が転がっており、その手順に従って操作すればうまくいきます。 私の場合、下記サイトを参考に設定を行いました。\nCloudFront で S3 静的ウェブサイトホスティングを SSL/TLS に対応させる ビルド時の注意 ホスティング先の準備が整えば、あとはコンテンツを配置するだけです。\nビルドするには下記のコマンドを実行します。\n$ gatsby build すると、プロジェクト直下に public ディレクトリが生成されるので、その中身を S3 にアップロードします。\nただ、上記コマンドを実行する際には、プロジェクト直下に下記のような環境設定ファイル .env を置いておく必要があります。\nGOOGLE_ANALYTICS_ID= ALGOLIA_APP_ID= ALGOLIA_SEARCH_ONLY_API_KEY= ALGOLIA_ADMIN_API_KEY= ALGOLIA_INDEX_NAME= FB_APP_ID= 項目の通り、Google Analytics や Algolia についての設定項目となっています。\nこれらのサービスの利用の有無にかかわらず、この .env がビルド時にエラーとなるため、値が空の状態で設置しておきます。\n※Algolia の設定については、 starter-personal-blog の公式ページに手順が載っています。\nSetup Algolia account for your GatsbyJS blog 以上でサイトの公開までたどり着きました。\n次回は CodeCommit + CodePipeline + CodeBuild で、記事作成からデプロイまでの自動化について書きます。\n",
    "permalink": "https://michimani.net/post/hosting-gatsby/",
    "title": "Gatsby で静的サイトを作ってみた - ホスティング編 -"
  },
  {
    "contents": " 前回 は Gatsby を \u0026lt;strong\u0026gt;starter-personal-blog\u0026lt;/strong\u0026gt; という starter を使ってインストールしました。\n今回はその starter-personal-blog の設定についてです。どこをどう変えればいいのかっていう。\nブログ全体の設定 ブログのタイトルや説明、書いてる人の情報 (SNS アカウントなど) の設定です。\n触るファイルは content/meta/config.js です。\nタイトル - siteTitle: \u0026#34;PersonalBlog - a blog starter for GatsbyJS\u0026#34;, // \u0026lt;title\u0026gt; - shortSiteTitle: \u0026#34;PersonalBlog GatsbyJS Starter\u0026#34;, // \u0026lt;title\u0026gt; ending for posts and pages - siteDescription: \u0026#34;PersonalBlog is a GatsbyJS starter.\u0026#34;, + siteTitle: \u0026#34;michimani.net - a simple blog\u0026#34;, // \u0026lt;title\u0026gt; + shortSiteTitle: \u0026#34;michimani.net\u0026#34;, // \u0026lt;title\u0026gt; ending for posts and pages + siteDescription: \u0026#34;michimani.net is a simple blog.\u0026#34;, siteTitle :html の title タグの値になります\nshortSiteTitle : フッターなどに表示されます\nsiteDescription : フッターなどに表示されます\nURL、言語 - siteUrl: \u0026#34;https://gatsby-starter-personal-blog.greglobinski.com\u0026#34;, + siteUrl: \u0026#34;https://michimani.net\u0026#34;, - siteLanguage: \u0026#34;en\u0026#34;, + siteLanguage: \u0026#34;ja\u0026#34;, 書いてる人の情報 名前、説明、アイコン - infoTitle: \u0026#34;greg lobinski\u0026#34;, - infoTitleNote: \u0026#34;personal blog\u0026#34;, + infoTitle: \u0026#34;michimani\u0026#34;, + infoTitleNote: \u0026#34;web developer, motorcycle rider, ...\u0026#34;, アイコンは src/images/jpg/avatar.jpg を好きな画像に変えます。サイズは正方形で用意しますが、 100px x 100px あれば十分です。\nメールアドレス 後述しますが、starter-personal-blog には独自の問い合わせフォーム機能を持っています。ただしこれは netlify というホスティング(?)サービスを使用する場合です。\n今回は CloudFront + S3 で運用するので、特に意味はありません。\n- contactEmail: \u0026#34;john@doe.com\u0026#34;, + contactEmail: \u0026#34;michimani210[@]gmail.com\u0026#34;, SNS サイドバーのプロフィール欄に表示される情報です。\n今回 Facebook は無しとするので削除してます。\n- { name: \u0026#34;github\u0026#34;, url: \u0026#34;https://github.com/greglobinski\u0026#34; }, - { name: \u0026#34;twitter\u0026#34;, url: \u0026#34;https://twitter.com/greglobinski\u0026#34; }, - { name: \u0026#34;facebook\u0026#34;, url: \u0026#34;http://facebook.com/greglobinski\u0026#34; } + { name: \u0026#34;github\u0026#34;, url: \u0026#34;https://github.com/michimani\u0026#34; }, + { name: \u0026#34;twitter\u0026#34;, url: \u0026#34;https://twitter.com/_michimani_\u0026#34; } テンプレートファイル 続いて、各テンプレートファイルの変更です。\n変更を加えるのは下記のファイルです。\ncontent/parts/author.md : 記事下に表示されます content/parts/footnote.md : フッターに表示されます content/parts/info.md : サイドバーのプロフィールに表示されます その他のカスタム 上で書きましたが、今回は starter-personal-blog 独自の問い合わせフォームを使いません。なので、 src/pages/contact.js を編集します。\n\u0026lt;Article\u0026gt; \u0026lt;PageHeader title=\u0026#34;Contact\u0026#34; /\u0026gt; \u0026lt;Content\u0026gt; - Feel free to contact me by email: \u0026lt;Obfuscate email={config.contactEmail} /\u0026gt; or use the - form below. + お問い合わせは、下記メールアドレス または Twitter へのリプライ／DM でお願いいたします。\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt; + mail: {config.contactEmail}　※[a] は @ に変換してください \u0026lt;br /\u0026gt; + twitter: @{config.authorTwitterAccount} \u0026lt;/Content\u0026gt; - \u0026lt;Form /\u0026gt; \u0026lt;/Article\u0026gt; \u0026lt;Form /\u0026gt; を削除することで、独自の問い合わせフォームを非表示にしてます。\n以上でブログの設定は完了です。あとはひたすらブログを書いていくだけです。\nとは言っても、公開するためには AWS の CloudFront + S3 を使います。また、ソース管理には CodeCommit、CI/CD には CodePipeline と CodeBuild を使います。\n次回以降はそのあたりについて書きます。では。\n",
    "permalink": "https://michimani.net/post/setting-gatsby-starter-personal-blog/",
    "title": "Gatsby で静的サイトを作ってみた - starter-personal-blog の設定編 -"
  },
  {
    "contents": "WordPress、はてなブログ、Qiita などで色々書いてきましたが、色んなものを触ってみようということで、Gatsby で静的サイト (このブログ) を作ってみました。\nまずは Gatsby プロジェクトを作成して表示できるところまでやってみます。\n1. gatsby-cli インストール $ npm install gatsby-cli --global 2. python 2.7 の実行環境準備 Gatsby プロジェクトを作成して npm install するときに python 2.7 が必要になるので、実行環境を整えます。\npyenv を使います。\n$ pyenv install 2.7.14 Installing Python-2.7.14... ERROR: The Python zlib extension was not compiled. Missing the zlib? Please consult to the Wiki page to fix the problem. https://github.com/yyuu/pyenv/wiki/Common-build-problems BUILD FAILED ビルドに失敗したようです。コンソールに出力されているページを参考に、下記コマンドでインストールします。\n$ CFLAGS=\u0026#34;-I$(xcrun --show-sdk-path)/usr/include\u0026#34; pyenv install -v 2.7.14 ... ... ... Installed Python-2.7.14 to /Users/hoge/.pyenv/versions/2.7.14 無事にインストールできました。\npyenv を使うとディレクトリ単位で Python のバージョンを切り替えられます。\nローカル環境では基本的に 3 系を使うので、Gatsby プロジェクト用にディレクトリを作り、そこだけ 2.7.14 にします。\n$ mkdir gatsby_project $ cd gatsby_project $ pyenv versions system 2.7.14 * 3.6.4 (set by /Users/hoge/.pyenv/version) $ pyenv local 2.7.14 $ python -V Python 2.7.14 3. Gatsby プロジェクトを作成 ブログとして使うため、ブログ用の starter を使います。\nstarter とは、あらかじめスタイルが整えられた、サイトのテンプレートのようなものです。\n今回は \u0026lt;strong\u0026gt;gatsby-starter-personal-blog\u0026lt;/strong\u0026gt; という starter を使います。\n$ gatsby new michimani.net https://github.com/greglobinski/gatsby-starter-personal-blog.git info Creating new site from git: https://github.com/greglobinski/gatsby-starter-personal-blog.git ... ... ... ローカル環境で実行してみます。\n$ cd michimani.net $ gatsby develop success delete html and css files from previous builds — 0.018 s success open and validate gatsby-config — 0.018 s success copy gatsby files — 0.029 s success onPreBootstrap — 2.056 s success source and transform nodes — 0.589 s success building schema — 0.678 s success createLayouts — 0.012 s success createPages — 0.107 s success createPagesStatefully — 0.028 s success onPreExtractQueries — 0.005 s success update schema — 0.151 s success extract queries from components — 0.195 s success run graphql queries — 4.106 s success write out page data — 0.023 s success write out redirect data — 0.001 s Generating image thumbnails [==============================] 391/391 11.8 secs 100% info bootstrap finished - 21.465 s success onPostBootstrap — 0.002 s DONE Compiled successfully in 12914ms 21:40:02 You can now view gatsby-starter-personal-blog in the browser. http://localhost:8000/ View GraphiQL, an in-browser IDE, to explore your site\u0026#39;s data and schema http://localhost:8000/___graphql Note that the development build is not optimized. To create a production build, use gatsby build ブラウザで http://localhost:8000/ にアクセスして確認します。\nこのあとは とりあえずローカル環境でページが表示できるようになりました。 このあとは、starter の設定ファイルの変更や、実際に記事を追加していきますが、それは次回に。\n",
    "permalink": "https://michimani.net/post/gatsby-install/",
    "title": "Gatsby で静的サイトを作ってみた - インストール編 -"
  },
  {
    "contents": "Sublime Text 3 で TypeScript の開発環境を整理したときのメモです。\n「Sublime Text TypeScript」でググると T3S というプラグインを入れる記事がヒットしますが、このプラグインはもう何年もメンテンナスされていません。 README にも以下のように書かれています。\nSTATUS : Not maintained Due to a lack of time, the lack of will to continue working on it and the fact that i\u0026rsquo;ve changed my code editor, this plugin will no longer be maintained. I\u0026rsquo;ve created this plugin mainly so i could code with typescript on sublime text, it served me well and i hope it helped others too, luckily there\u0026rsquo;s other options right now for people using sublimetext :\n. ArcticTypescript a fork of TS3 with lots of change and work to tackle performance issues : https://github.com/Phaiax/ArcticTypescript . And microsoft official sublimetext typescript plugin : https://github.com/Microsoft/TypeScript-Sublime-Plugin Thx to all the people who contributed to this plugin\nSee you space cowboys\n上記の文にもあるように、このプラグインの代わりに Microsoft/TypeScript-Sublime-Plugin を使うことにします。\n前提 node がインストールされていること プラグインのインストール Sublime Text の Package Manager からではなく、Github からリポジトリを clone する方法でインストールします。\nSublime Text の Package ディレクトリに移動 Sublime Text の Package ディレクトリは、Mac だと /Users/hogehoge/Library/Application Support/Sublime Text 3/Packages のようにな場所にあります。 わからない場合は、Sublime Text を起動して Preferences \u0026gt; Browse Packages ... を選択すると Package ディレクトリの場所がわかります。\n$ cd /Users/hogehoge/Library/Application\\ Support/Sublime\\ Text\\ 3/Packages Microsoft/TypeScript-Sublime-Plugin を clone README にある通り、clone コマンドを叩きます。\n$ git clone --depth 1 https://github.com/Microsoft/TypeScript-Sublime-Plugin.git TypeScript node の PATH 設定 (※環境によっては不要) このプラグインを使うためには node がインストールされている必要がありますが、インストール方法によっては作業が必要になります。 まずは node のパスを確認します。\n$ which node /Users/hogehoge/.nodebrew/current/bin/node ここで、 /usr/local/bin だった場合は 作業不要 です。\nnodebrew を使って node をインストールしていると、上のようなパスになっているので、追加で作業が必要になります。 というのも、プラグインは /usr/local/bin を見に行くようになっているからです。\nnode の PATH 設定は、 Preferences \u0026gt; Setting (⌘ + , でも可) で Preferences.sublime-setting - User を開きます。 設定は json 形式で記述するようになっているので、以下の設定を追記します。\n{ \u0026#34;node_path\u0026#34;: \u0026#34;/Users/hogehoge/.nodebrew/current/bin/node\u0026#34; } Tips and Known Issues ? Microsoft/TypeScript-Sublime-Plugin Wiki ? GitHub の 7 を参照\nSublime Text を再起動 あとは Sublime Text を再起動すれば完了です。 Syntax Highlight や 補完、エラー表示、ビルドなどが行えるようになります。\n",
    "permalink": "https://michimani.net/post/develop-coding-typescript-using-sublimetext3/",
    "title": "Sublime Text 3 で TypeScript の開発環境を整える"
  },
  {
    "contents": "さくらのレンタルサーバー (スタンダードプラン) で Python3 系 を使えるようにしたときのメモです。\n以前に Amazon Linux で Python3 系を使えるようにしたので同じ方法で出来ると思ったんですが、ちょっとだけ違いました。\nやりたいこと さくらのレンタルサーバー (スタンダードプラン) で Python3 系 を使えるようにします。 python3 コマンドではなく python コマンドで使えるようにします。\nまた、前提としてシェルを bash に変更しています。 さくらレンタルサーバーのデフォルトでは csh になっています。\nログインシェルを bash に変更する場合は、下記コマンドを実行して再ログインします。\n$ chsh -s /usr/local/bin/bash やること Amazon Linux のときと同様に、pyenv を使います。\nただし、今回は pyenv に加えて pyenv-virtualenv も必要です。\n手順 1. pyenv インストール $ git clone https://github.com/yyuu/pyenv.git ~/.pyenv 2. pyenv-virtualenv インストール $ git clone https://github.com/yyuu/pyenv-virtualenv.git ~/.pyenv/plugins/pyenv-virtualenv 3. パス設定 ~/.bashrc に下記内容を追加します。\nexport PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34; export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34; export TMPDIR=\u0026#34;$HOME/tmp\u0026#34; export PYTHON_PATH=./ eval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; .bashrc を再読込します。\n$ source ~/.bashrc 4. pyenv で欲しいバージョンをインストール・設定 ここからは普通に pyenv を使います。\nインストール可能なバージョンを確認。\n$ pyenv install --list Available versions: 2.1.3 2.2.3 . . . 3.6.5 3.6.6 3.6.6rc1 3.7.0 3.7-dev . . . 任意のバージョンをインストール。\n$ pyenv install 3.6.6 Downloading Python-3.6.6.tgz... -\u0026gt; https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz Installing Python-3.6.6... Installed Python-3.6.6 to /home/hoge/.pyenv/versions/3.6.6 デフォルトで使うバージョンに設定。\n$ pyenv versions * system (set by /home/hoge/.pyenv/version) 3.6.6 $ pyenv global 3.6.6 $ python -V Python 3.6.6 以上です。\n",
    "permalink": "https://michimani.net/post/sakura-how-to-use-python3/",
    "title": "さくらレンタルサーバーで Python3 系を使う"
  },
  {
    "contents": "やりたいこと PHP のアプリケーションで外部ライブラリ等を使用する場合、最近は composer を使ってインストールすることがほとんどです。 自分で作ったライブラリ (的なもの) も composer を使ってインストールできるようにしたい。 ということで、そのためにやったことのメモです。\n前提 GitHub に公開しているリポジトリを対象とする 今回は、GitHub で公開している text-to-color というリポジトリを、composer でインストールできるようにします。\nちなみに text-to-color は、文字列からカラーコードを生成する、ただそれだけのスクリプトです。\n手順 1. composer.json ファイルを作成する リポジトリの直下に composer.json ファイルを作ります。\n$ composer init 上記コマンドを実行すると、パッケージ名称や作成者などの情報を対話的に入力することで、composer.json ファイルを生成できます。もちろん手作業で作っても問題ありません。 以下のような json ファイルが生成されます。\n{ \u0026#34;name\u0026#34;: \u0026#34;michimani/text-to-color\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;text-to-color is a simple php class for generating color code from text.\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;require\u0026#34;: { \u0026#34;php\u0026#34;: \u0026#34;\u0026gt;= 5.6.0\u0026#34; }, \u0026#34;autoload\u0026#34;: { \u0026#34;psr-4\u0026#34;: { \u0026#34;Michimani\\\\TextToColor\\\\\u0026#34;: \u0026#34;src/\u0026#34; } } } autoload を記述すると、インストールするアプリケーションの autoload.php に自動で追加されるようにできます。 その下の psr-4 は、このライブラリで namespace を利用している場合の記述です。\nその他、composer.json に関する公式ドキュメントは こちら composer.json が作成できたら、リポジトリに push しておきます。\n2. Packagist にリポジトリを登録する Packagist とは、composer でインストールできる PHP パッケージが集約された場所です。 ここへの登録は必須ではありませんが、登録しておくと、より簡単にインストールができるようになります。\n登録方法は簡単で、まずは Packagist のアカウント作成からです。GitHub アカウントで作成できます。\n右上の Submit から、GitHub のリポジトリの URL を入力します。\nこのとき、リポジトリに composer.json が無い場合は登録できません。\nこれで Packagist への登録は完了です。 しかし、下記のようなアラートが出ていると思います。\nこれは、GitHub へ更新があったときに Packagist にもその更新を反映するために手動で操作をしなければならないという内容です。 自動アップデートの設定は以下のとおりです。\nPackagist のマイページで API Token を取得します (Your API Token と書かれているところです) GitHub のリポジトリ画面から Settings \u0026gt; Integrations \u0026amp; services 進みます Add Service から Packagist を選択します User に Packagist のユーザ名、Token に API Token を入力して、Active にチェックを入れて Add service ボタンを押します 追加したサービスの詳細画面にある Test service ボタンを押して Okay, the test payload is on its way. と表示されれば設定完了です 3. インストールしてみる 実際に composer を使ってインストールする前に、パッケージの情報が取得できるか確認します。\n$ composer search michimani/text-to-color michimani/text-to-color text-to-color is a simple php class for generating color code from text. 正しく登録でいていれば、パッケージ名と説明が出力されます。\nあとは、インストールしたいプロジェクトで下記コマンドを実行すればインストールできます。\n$ composer require michimani/text-to-color ",
    "permalink": "https://michimani.net/post/php-install-own-package-using-composer/",
    "title": "PHP パッケージを composer でインストールできるようにする"
  },
  {
    "contents": "Amazon EFS 自体は 2016年から一般に利用できるようになっていますが、もうすぐ (2018年7月) 東京リージョンでも利用可能になるようです。 正直、国内にいると他リージョンでのみ利用可能なサービスについては疎くなるので、この機会に EFS についてざっくりと調べてみました。\nEFS とは Amazon Elastic File System (Amazon EFS) はシンプルで、スケーラブル、伸縮自在なファイルストレージを、AWS クラウドサービスとオンプレミスリソース用に提供します。\n(中略)\nAmazon EFS は、高可用性と耐久性を実現するよう設計されたリージョンでのサービスで、ウェブ配信やコンテンツ管理、エンタープライズアプリケーション、メディアおよびエンターテインメントの処理ワークフロー、ホームディレクトリ、データベースバックアップ、開発者ツール、コンテナストレージやビッグデータおよび分析アプリケーションなど幅広い多様なユースケースでパフォーマンスを発揮します。\nAmazon EFS - 概要 とあります。\n簡単に言うと、共有ファイルストレージ のことです。\nEBS, S3 との違いは？ AWS のサービス内には、ストレージサービスが他にもあります。EBS と S3 です。\nこれらは東京リージョン開設時から利用可能で、AWS の基本的なアーキテクチャには不可欠なサービスです。\n新たに東京リージョンで利用可能になる EFS と、EBS, S3 との違い・使い分けについて簡単にまとめます。\n※詳細な違いについては When to Choose Amazon EFS を参照してください。\nPer-operation latency (レイテンシー) EFS\n低い 整合性が担保されている EBS\nかなり低い 整合性が担保されている S3\n低い 整合性が担保されている CloudFront と結合できる Data Availability/Durability (データ可用性/耐久性) EFS\nリージョン内の複数のアベイラビリティーゾーンにデータとメタデータを保存 EBS\n単一のアベイラビリティーゾーン内で冗長性 S3\nリージョン内の複数のアベイラビリティーゾーンにデータとメタデータを保存 Access (アクセス) EFS\n複数のアベイラビリティゾーン、数千の EC2 インスタンスまたはオンプレミスサーバ EBS\n単一のアベイラビリティゾーン内の、ひとつの EC2 インスタンス S3\nWeb 上からの数百万の接続 Use Cases (ユースケース) EFS\nWeb サービス コンテンツ アプリケーション EBS\nブートボリューム NoSQL データベース、データハウス S3\nWeb コンテンツ、メディア バックアップ 大規模データの分析 EFS の利用で何が変わる？ 一番 おぉ！ と思うのは、EC2 インスタンスをスケーリングするときのことを考えた場合です。\n高負荷時にスケールアウトする場合、元となる AMI から新しい EC2 インスタンスを起動すると思います。\nある WEB サービスを運用するにあたって、何かしらバージョンアップ等を実施した場合、デプロイするときにマスターの EC2 インスタンスはもちろんですが、スケール時に使用する AMI の更新も必要になります。\nしかし、アプリケーションの置き場所を EFS にすると、デプロイはそこだけを対象にすればよくなり、スケール時には EC2 インスタンスのみの起動を考えて、アプリケーション自体のバージョンを気にする必要がなくなります。\nデプロイやスケールのアーキテクチャがよりシンプルな形になりそうな気がしています。\n",
    "permalink": "https://michimani.net/post/aws-efs-tokyo-available/",
    "title": "Amazon EFS が東京リージョンでも利用可能になると聞いて"
  },
  {
    "contents": "Amazon CloudWatch ではあらかじめいくつかのメトリクスが用意されており、それらに関しては特に追加の作業がなくてもモニタリングを開始できます。\nまた、CloudWatch Agent を使用すれば、デフォルトには無いメトリクス (Cdisk 使用率など) も計測できるようになります。\nCloudWatch Agent で disk 使用率をモニタリングする方法はこちらで。\n今回は、EC2 インスタンス内で動いている Apache の起動状態をモニタリングする方法についてのメモです。\nこの方法を使えば、Apache だけでなく、EC2 内で動いている各プロセスの起動状態をモニタリングできると思います。\n前提 Amazon CLI がインストール済みで、aws コマンドが使える EC2 インスタンスに CloudWatch を触るためのロールがアタッチされている\nロールの内容については 以前の記事 で設定した CloudWatch Agent のロールが付与されていれば十分です モニタリングまでの流れ Apacheのプロセスをモニタリングするまでの流れは次のようになります。\n起動中の httpd のプロセス数を数える aws cloudwatch put-metrics-data コマンドで、数えたプロセス数を CloudWatch に送信する 以上です。\nこれをセルスクリプトで作って cron で定期的に実行すれば、CloudWatch でモニタリング出来ます。\n1. 起動中の httpd のプロセス数を数える プロセス数については、下記のコマンドで取得します。\n$ ps awux | grep -v grep | grep -w httpd | wc -l 6 Apache が起動しているかどうかが分かればいいので、この値をチェックして、0 なら 0 、1 以上 なら 1 を、CloudWatch に送信するようにします。\n2. aws cloudwatch put-metrics-data コマンドで、数えたプロセス数を CloudWatch に送信する aws cloudwatch put-metrics-data は、その名の通り CloudWatch にメトリクスデータを put するコマンドです。実際のコマンドは下記のようになります。\naws cloudwatch put-metric-data --metric-name process/httpd --namespace MyMetrics --value 1 --region ap-northeast-1 --unit Count --dimensions \u0026#34;InstanceId=xxxxxyyyyyzzzzz,ProcessName=httpd\u0026#34; 色々オプションが付いていますが、最低限これくらいは というオプションです。特に --dimensions については CloudWatch のコンソール上でどういう形で見たいか、によって変わってくるので適宜変更してください。\nプロセスモニタリング用シェルスクリプト作りました 上の 1 と 2 を実行するシェルスクリプトを書きました。\nmichimani/put-process-status.sh 実行時にプロセス名 (Apache なら httpd) を渡すことで、そのプロセスが起動しているかをチェックできます。\ncron でこのスクリプトを定期的に実行すれば、 CloudWatch でモニタリングできます。あとはその値によって、SNS でアラートを飛ばせば OK です。\n## Check Apache status */5 * * * * sh /path/to/put-custom-metrics.sh httpd CloudWatch のアラームはデフォルトで最小 5 分間隔でしか値のチェックが出来ません。なので、データの送信も 5 分間隔で良いと思います。その分、タイムラグが出てしまいますが。\n必要であれば高解像度アラームを利用してより細かい間隔でチェックする事はできます。\nまた、このスクリプトでは 1 または 0 を送信しているので、アラームを作成する場合は 5 分間の 最低値 が 1 未満かどうか で設定すると良さそうです。\n",
    "permalink": "https://michimani.net/post/aws-monitoring-apache-status-with-cloudwatch/",
    "title": "Amazon CloudWatch で Apache の起動状態をモニタリングする"
  },
  {
    "contents": " bootstrap-datepicker を使うと、bootstrap を使用したサイトに良い感じの日付入力フォームを簡単に作ることができます。 ただ、そのまま利用したときに、 スマホ表示での UI が微妙 なのでその対応方法です。\nフォームの書き方については、下記サイトを参考にしています。\n簡単！カレンダーから日付入力 Bootstrap Datepicker の使い方と解説 一般的な書き方 フォーム部分\n\u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;col-sm-3 control-label\u0026#34;\u0026gt;日付\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-9 form-inline\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group date\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; placeholder=\u0026#34;2018-05-11\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group-addon\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-calendar\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; js部分\n$(document).ready(function(){ $(\u0026#39;.date\u0026#39;).datepicker({ todayBtn: \u0026#39;linked\u0026#39;, format: \u0026#34;yyyy-mm-dd\u0026#34;, autoclose: true, todayHighlight: true, daysOfWeekHighlighted: \u0026#34;0,6\u0026#34;, }); }); この書き方では、\nテキストエリアにフォーカスがあたった時 カレンダーアイコンをクリックした時 に、日付選択カレンダーが表示されます。 PC だとこれでも特に問題はないのですが、スマホ (今回の対象は iOS Safari) で操作する際に使いにくいです。 具体的には、テキストエリアにフォーカスを当てた時に、日付選択カレンダーが表示され、さらに端末のキーボードも表示されます。これが使いにくい理由です。\nそこで、テキストエリアにフォーカス時には日付選択カレンダーは表示せず、カレンダーアイコンを押した時のみ日付選択カレンダーを表示する を実現したいわけです。\n対応後 フォーム部分\n\u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;col-sm-3 control-label\u0026#34;\u0026gt;日付\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;col-sm-9 form-inline\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control\u0026#34; name=\u0026#34;real_date\u0026#34; placeholder=\u0026#34;2018-05-11\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group-addon\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-calendar date\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; class=\u0026#34;dummy-date\u0026#34; data-real=\u0026#34;real_date\u0026#34;\u0026gt; \u0026lt;/i\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; js部分\n$(document).ready(function(){ $(\u0026#39;.date\u0026#39;).datepicker({ todayBtn: \u0026#39;linked\u0026#39;, format: \u0026#34;yyyy-mm-dd\u0026#34;, autoclose: true, todayHighlight: true, daysOfWeekHighlighted: \u0026#34;0,6\u0026#34;, }); $(\u0026#39;.dummy-date\u0026#39;).change(function(){ $(`input[name=${$(this).data(\u0026#39;real\u0026#39;)}]`).val($(this).val()); }); }); カレンダーから選択された場合は一旦 hidden に値を逃して、そのイベントを拾ってテキストエリアに値をセットしています。\nデモ ここ で上の両方を試せます。 スマホで表示してもらうと操作性の違いがわかりやすいと思います。\n",
    "permalink": "https://michimani.net/post/javascript-bootstrap-datepicker-ui-for-smart-phone/",
    "title": "bootstrap-datepicker の UI がスマホ表示だと微妙"
  },
  {
    "contents": "いつもどおり yum update をしたら Apache が起動しなくなったので、そのときの対処メモです。\n経緯 OS が Amazon Linux の EC2 インスタンスにログインした時に、yum のアップデートがあると sudo yum update を実行してください 的なメッセージが出ます。\n特に理由がなければこれを実行して、yum でインストールされているもののアップデートを実施します。 今回も、これまで通り sudo yum update を実行しました。\nアップデート項目には、Linuxカーネル、httpd24など、これまでにもあったような内容だったので、y を押してアップデートを実行、問題なく完了しました。\n起こったこと アップデート後、そのインスタンス上で動いている Webアプリケーションが落ちていました。\nやったこと Apache のステータス確認 $ sudo service httpd status httpd is stopped 止まっていました。\nApache の起動・再起動 $ sudo service httpd start Stopping httpd: [FAILED] 起動できません。\n$ sudo service httpd restart Stopping httpd: [FAILED] Starting httpd: [FAILED] 再起動も出来ません。\nerror_log の確認 $ sudo less /etc/httpd/logs/error_log すると以下のようなエラーが出ていました。(一部ぼかしてます)\n[Tue May 09 22:10:56.170735 2018] [mpm_prefork:notice] [pid 13163] AH00169: caught SIGTERM, shutting down [Tue May 09 22:10:56.292700 2018] [ssl:emerg] [pid 31332] AH02572: Failed to configure at least one certificate and key for 13.XXX.YYY.ZZ:80 [Tue May 09 22:10:56.292775 2018] [ssl:emerg] [pid 31332] SSL Library Error: error:140A80B1:SSL routines:SSL_CTX_check_private_key:no certificate assigned [Tue May 09 22:10:56.292780 2018] [ssl:emerg] [pid 31332] AH02312: Fatal error initialising mod_ssl, exiting. ググってみると、SSL まわりの設定が原因とのこと。\nhttpd.conf の修正 ここ によると、/etc/httpd/conf/httpd.conf のバーチャルホストの記述に問題があるとのことだったので、それを修正。\n修正前\n\u0026lt;VirtualHost *:443\u0026gt; DocumentRoot /var/www/hogehoge/public ServerName hogehoge.com \u0026lt;/VirtualHost\u0026gt; 修正後\n\u0026lt;VirtualHost *:443\u0026gt; DocumentRoot /var/www/hogehoge/public ServerName hogehoge.com SSLEngine on SSLCertificateFile /etc/pki/tls/certs/hogehoge.crt SSLCertificateKeyFile /etc/pki/tls/private/hogehoge.key \u0026lt;/VirtualHost\u0026gt; 証明書の場所を明示しないとダメだったようです。\n",
    "permalink": "https://michimani.net/post/aws-ec2-apache-stopping-after-yum-update/",
    "title": "Amazon Linuxでyum update したらApacheが停止して起動もできなくなった"
  },
  {
    "contents": "Webページを作っている時に、要素を環状に配置したい場面があったので、配置するための座標を生成するスクリプトを作ってみました。\nここで言う 座標 とは web ページ上での座標です。つまり、 画面の左上が (0, 0) 画面左上から 右に 10px 、下に 50px 行った場所は (10, 50) という感じです。\nsatellites-generator | github 使い方 オブジェクトを生成して、 generateCoordinate() を呼ぶと、各要素の座標を持った配列が得られます。\nパラメータは以下。 generateCoordinate(number, radius, base, options = {})\n型 説明 例 number number 配置する要素の数 6 radius number 中心からの距離 (円の半径) 30 base json 中心の座標 {x: 50, y: 200} ※オプションについては github の README をご覧ください。\n例\nvar sG = new SatellitesGenerator(); let coordinates = sG.generateCoordinate(6, 30, {x: 50, y:200}, {isReverse: true}); console.log(coordinates); 結果\n[ { \u0026#34;x\u0026#34;: 50, \u0026#34;y\u0026#34;: 170 }, { \u0026#34;x\u0026#34;: 24.019237886466847, \u0026#34;y\u0026#34;: 185 }, { \u0026#34;x\u0026#34;: 24.019237886466836, \u0026#34;y\u0026#34;: 215 }, { \u0026#34;x\u0026#34;: 49.99999999999999, \u0026#34;y\u0026#34;: 230 }, { \u0026#34;x\u0026#34;: 75.98076211353316, \u0026#34;y\u0026#34;: 215 }, { \u0026#34;x\u0026#34;: 75.98076211353316, \u0026#34;y\u0026#34;: 185 } ] あとは、x , y を、各要素の left , top にそれぞれ指定すると、環状に並びます。\n関数の中身 三角関数の計算をしているだけです。\ngenerateCoordinate(number, radius, base, option = {}) { const coordinates = new Array(); try { const per = 360 / number; let start = Math.PI/2; if (typeof(option.startPos) != \u0026#39;undefined\u0026#39;) { switch (option.startPos) { case \u0026#39;right\u0026#39;: start = 0; break; case \u0026#39;left\u0026#39;: start = Math.PI; break; case \u0026#39;bottom\u0026#39;: start = Math.PI*3/4; break; } } let isReverse = false; if (typeof(option.isReverse) != \u0026#39;undefined\u0026#39; \u0026amp;\u0026amp; typeof(option.isReverse) == \u0026#39;boolean\u0026#39; \u0026amp;\u0026amp; option.isReverse == true) { isReverse = true; } for (let i = 0; i \u0026lt; number; i++) { let rad = 360 - (per * i); if (isReverse) { rad = per * i; } coordinates.push({ x: base.x + radius * Math.cos(Math.PI*rad/180 + start), y: base.y + -1 * radius * Math.sin(Math.PI*rad/180 + start) }); } } catch (e) { console.error(e); } finally { return coordinates; } } ",
    "permalink": "https://michimani.net/post/javascript-put-element-as-a-circle/",
    "title": "要素を環状に配置するための座標を生成する"
  },
  {
    "contents": "動的に埋め込んだ iframe の高さを取得しようとしたときのメモです。\nやりたかったこと 以下のように動的に iframe を埋め込んで、iframe の高さを埋め込んだページの高さに合わせたかった。\n\u0026lt;div id=\u0026#34;content-area\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a class=\u0026#34;iframe-btn\u0026#34; href=\u0026#34;/hogehoge-1\u0026#34;\u0026gt;hogehoge 1\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a class=\u0026#34;iframe-btn\u0026#34; href=\u0026#34;/hogehoge-2\u0026#34;\u0026gt;hogehoge 2\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a class=\u0026#34;iframe-btn\u0026#34; href=\u0026#34;/hogehoge-3\u0026#34;\u0026gt;hogehoge 3\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; という html を用意して、各リンクをクリックした時にそのリンク先を iframe で埋め込むというもの。\n$(\u0026#39;.iframe-btn\u0026#39;).click(function(e){ e.preventDefault(); let iframe = `\u0026lt;iframe frameborder=\u0026#34;no\u0026#34; src=\u0026#34;${$(this).attr(\u0026#39;href\u0026#39;)}\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;`; $(\u0026#39;#content-area\u0026#39;).html(iframe); }); このままの実装では iframe にスクロールバーが出てしまいます。\nあらかじめ css で #content-area に高さを指定した場合、 iframe 内のページによってはスクロールバーが出たり、下に空白が出たりします。\nやったこと iframe を埋め込む js 部分を下記のようにしました。\nlet iframe = `\u0026lt;iframe frameborder=\u0026#34;no\u0026#34; src=\u0026#34;${$(this).attr(\u0026#39;href\u0026#39;)}\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt;`; $(\u0026#39;#content-area\u0026#39;).css(\u0026#39;visibility\u0026#39;, \u0026#39;hidden\u0026#39;).html(iframe); setTimeout(function(){ let iframeHeight = $(\u0026#39;#content-area iframe\u0026#39;).get(0).contentWindow.document.body.scrollHeight; $(\u0026#39;#content-area\u0026#39;).css(\u0026#39;height\u0026#39;, `${iframeHeight + 20}px`).css(\u0026#39;visibility\u0026#39;, \u0026#39;\u0026#39;); }, 200); 200 ミリ秒後に iframe の高さを取得して、 #content-area の高さにセットしています。(+20 は調整分)\n今回は iframe で埋め込むページが単純な静的ページなので 200ミリ秒 で大丈夫ですが、確実とは言えません。\nイメージとしてはここを\n$(\u0026#39;#content-area iframe\u0026#39;).load(function(){ ... }); のようにしたいところですが、これでは取得できませんでした。\nとりあえず静的ページに対してはこれで大丈夫そう\u0026hellip;？\n",
    "permalink": "https://michimani.net/post/javascript-get-iframe-height/",
    "title": "動的に埋め込んだiframeの高さを取得する"
  },
  {
    "contents": "ClowdWatchにはあらかじめEC2用のメトリクス(チェックできる値)が用意されていますが、残念ながらディスク使用率のメトリクスはありません。EC2のディスク使用率をCloudWatchで監視するには、EC2インスタンスにCloudWatchエージェントをインストールする必要があります。CloudWatchエージェントを介してEC2の情報をCloudWatchで取得するような感じです。その手順です。\n※基本的にAWS公式の手順書通りです。\nやること EC2インスタンスにCloudWatchエージェントをインストール\nCloudWatchエージェント用のIAMロール作成 インスタンスにIAMロールをアタッチ CloudWatchエージェントをインストール CloudWatchエージェントの設定ファイルを作成\nCloudWatchエージェントを起動\n1. EC2インスタンスにCloudWatchエージェントをインストール 1. CloudWatchエージェント用のIAMロール作成 CloudWatchエージェントがEC2の情報にアクセスできるように、IAMロールを作成します。\n余談ですが、AWSのサービスを扱う上で ロール は非常に重要な要素です。AWSomeDayに行ったときにAWSの人が説明していたフレーズでわかりやすかったのは、 ロール = 権限が付いたヘルメット という考え方です。余談ですが。\nまずポリシーを作成します。ポリシーは、ロールに付与する権限の情報 のようなイメージです。 マネジメントコンソールで IAM \u0026gt; ポリシー \u0026gt; ポリシーの作成 と進みます。\njson型式で下記のように指定します。\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchAgentServerPolicy\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;ec2:DescribeTags\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 次に進んで名前の入力と、内容を確認します。\nこれでポリシーが作成できました。\n続いて、ロールを作成します。 マネジメントコンソールで IAM \u0026gt; ロール \u0026gt; ロールの作成 と進みます。\nサービスとユースケースの選択では、EC2 の EC2 Role for Simple System Manager を選択します。\n選択したユースケースにデフォルトで付与されているポリシーが表示されるので、そのまま進みます。\n適当に名前を付けて保存します。\n続いて、いま作成したロールに、先ほど作成したポリシーをアタッチします。\nロールの一覧から作成したロールを選択して、ポリシーのアタッチ ボタンを押します。\n検索窓で先ほど作成したポリシーを検索・選択して、アタッチします。\n以上でロールの作成が完了しました。\n2. インスタンスにIAMロールをアタッチ 作成したロールを、監視したいインスタンスにアタッチします。\nマネジメントコンソールでEC2一覧を表示し、該当のインスタンスを選択してアクションから インスタンスの設定 \u0026gt; IAMロールの割り当て/置換 と進みます。\n作成したロールをアタッチします。\n3. CloudWatchエージェントをインストール $ mkdir tmp/AmazonCloudWatchAgent $ cd tmp/AmazonCloudWatchAgent $ wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip $ unzip AmazonCloudWatchAgent.zip $ sudo ./install.sh 2. CloudWatchエージェントの設定ファイルを作成 作成方法は色々ありますが、ウィザードで作成してみます。\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard コマンドを叩くと対話形式で色々と選択していきます。選ぶ時は数字キーを使います。 ============================================================= = Welcome to the AWS CloudWatch Agent Configuration Manager = ============================================================= On which OS are you planning to use the agent? 1. linux 2. windows default choice: [1]: 1 Trying to fetch the default region based on ec2 metadata... Are you using EC2 or On-Premises hosts? 1. EC2 2. On-Premises default choice: [1]: 1 Do you want to monitor any host metrics? e.g. CPU, memory, etc. 1. yes 2. no default choice: [1]: 1 Do you want to monitor cpu metrics per core? Additional CloudWatch charges may apply. 1. yes 2. no default choice: [1]: 2 Do you want to add ec2 dimensions (ImageId, InstanceId, InstanceType, AutoScalingGroupName) into all of your metrics if the info is available? 1. yes 2. no default choice: [1]: 2 Would you like to collect your metrics at high resolution (sub-minute resolution)? This enables sub-minute resolution for all metrics, but you can customize for specific metrics in the output json file. 1. 1s 2. 10s 3. 30s 4. 60s default choice: [4]: 4 Which default metrics config do you want? 1. Basic 2. Standard 3. Advanced 4. None default choice: [1]: 2 Current config as follows: { \u0026#34;metrics\u0026#34;: { \u0026#34;metrics_collected\u0026#34;: { \u0026#34;cpu\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;cpu_usage_idle\u0026#34;, \u0026#34;cpu_usage_iowait\u0026#34;, \u0026#34;cpu_usage_user\u0026#34;, \u0026#34;cpu_usage_system\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;totalcpu\u0026#34;: false }, \u0026#34;disk\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;used_percent\u0026#34;, \u0026#34;inodes_free\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;diskio\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;io_time\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 }, \u0026#34;swap\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;swap_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 } } } } Are you satisfied with the above config? Note: it can be manually customized after the wizard completes to add additional items. 1. yes 2. no default choice: [1]: 1 Do you have any existing CloudWatch Log Agent (http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html) configuration file to import for migration? 1. yes 2. no default choice: [2]: 2 Do you want to monitor any log files? 1. yes 2. no default choice: [1]: 2 Saved config file to /opt/aws/amazon-cloudwatch-agent/bin/config.json successfully. Current config as follows: { \u0026#34;metrics\u0026#34;: { \u0026#34;metrics_collected\u0026#34;: { \u0026#34;cpu\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;cpu_usage_idle\u0026#34;, \u0026#34;cpu_usage_iowait\u0026#34;, \u0026#34;cpu_usage_user\u0026#34;, \u0026#34;cpu_usage_system\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;totalcpu\u0026#34;: false }, \u0026#34;disk\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;used_percent\u0026#34;, \u0026#34;inodes_free\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;diskio\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;io_time\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 }, \u0026#34;swap\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;swap_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 } } } } Please check the above content of the config. The config file is also located at /opt/aws/amazon-cloudwatch-agent/bin/config.json. Edit it manually if needed. Do you want to store the config in the SSM parameter store? 1. yes 2. no default choice: [1]: 2 Program exits now. 作成された設定ファイルは直接viなどで開いて修正することも可能です。 設定ファイルを修正した場合は、CloudWatchエージェントを再起動する必要があります。\n3. CloudWatchエージェントの開始 設定ファイルを作成したら、エージェントを起動する必要があります。 コマンドを叩いて起動します。\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s ステータスの確認\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -m ec2 -a status { \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;starttime\u0026#34;: \u0026#34;2018-03-01T08:25:58+0000\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.208.0\u0026#34; } CloudWatchでメトリクスの確認 エージェントを開始すると、あとは勝手にエージェントがCloudWatchにデータを送ってくれます。 マネジメントコンソールでCloudWatchの メトリクス を確認してみると、CWAgent という名前空間が新たにできています。\nその中の device, fstype, host, path には、EC2インスタンス内のファイルシステムとそのパスが一覧で表示されており、その中の メトリクス名 disk_used_percent でストレージの使用率をモニタリングできます。\nあとはこのメトリクスに対してアラームを設定してSNSで通知すればOKです。\nSNSでの通知については前に書いた方法と同じです。\n",
    "permalink": "https://michimani.net/post/aws-monitoring-disk-usage-of-ec2/",
    "title": "Amazon CloudWatchでEC2のディスク使用率を監視する"
  },
  {
    "contents": "お名前.comで取得したドメインでメールを送受信(主に受信)したいと思ってEC2にpostfixをインストールしたのでその時のメモです。 これから出てくる example.com を、設定するドメインに置き換えて読んで下さい。\n環境 EC2 AmazonLinux 設定に際して、次の値が必要になるので、先にAWSのマネジメントコンソールで調べておきます。\n対象のEC2インスタンスののIPアドレス 対象のEC2インスタンスが属しているVPCのIPアドレス (例：10.0.0.0/16) 事前準備 お名前.com ドメインNavi まずDNSの設定をします。\nお名前.comのドメインNaviから、以下のDNSレコードを設定します。\nホスト名 TYPE VALUE 優先 example.com A EC2のIPアドレス mail.example.com A EC2のIPアドレス example.com MX mail.example.com 10 TTLは初期値(3600)のままでいいです。\nDNSの反映には時間がかかるので、一番最初にやっておきます。\n事前準備 AWSマネジメントコンソール 続いて、AWSのマネジメントコンソールから、EC2のセキュリティグループの設定を変更します。\n対象のEC2に設定されているセキュリティグループのインバウンドルールに、下記を追加します。\nタイプ ポート範囲 ソース IMAP 143 ::0/, 0.0.0.0/0 SMTP 25 ::0/, 0.0.0.0/0 POP3 110 ::0/, 0.0.0.0/0 postfixのインストールと設定ファイルの変更 ここからはSSHでEC2インスタンスにログインして操作します。\npostfixをインストールします。\n$ sudo yum install -y postfix このあと以下の3つの設定ファイルを書き換えるので、念のためオリジナルをコピーしておきます。\n/etc/postfix/main.cf /etc/postfix/master.cf /etc/sasl2/smtpd.conf $ sudo cp /etc/postfix/main.cf /etc/postfix/main.cf.org $ sudo cp /etc/postfix/master.cf /etc/postfix/master.cf.org $ sudo cp /etc/postfix/smtpd.cf /etc/postfix/smtpd.cf.org $ diff /etc/postfix/main.cf.org /etc/postfix/main.cf 76c76 \u0026lt; #myhostname = virtual.domain.tld --- \u0026gt; myhostname = mail.example.com 83c83 \u0026lt; #mydomain = domain.tld --- \u0026gt; mydomain = example.com 99c99 \u0026lt; #myorigin = $mydomain --- \u0026gt; myorigin = $mydomain 113c113 \u0026lt; #inet_interfaces = all --- \u0026gt; inet_interfaces = all 116c116 \u0026lt; inet_interfaces = localhost --- \u0026gt; #inet_interfaces = localhost 166c166 \u0026lt; #mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain, --- \u0026gt; mydestination = $myhostname, localhost.$mydomain, localhost, $mydomain 264c264 \u0026lt; #mynetworks = 168.100.189.0/28, 127.0.0.0/8 --- \u0026gt; mynetworks = 10.0.0.0/16, 127.0.0.0/8 ## 10.0.0.0/16 は事前にメモしておいたVPCのIPアドレス 419c419 \u0026lt; #home_mailbox = Maildir/ --- \u0026gt; home_mailbox = Maildir/ 676a677,684 \u0026gt; \u0026gt; ## add setting \u0026gt; smtpd_sasl_auth_enable = yes \u0026gt; smtpd_sasl_local_domain = $myhostname \u0026gt; smtpd_recipient_restrictions = \u0026gt; permit_mynetworks \u0026gt; permit_sasl_authenticated \u0026gt; reject_unauth_destination $ diff /etc/postfix/master.cf.org /etc/postfix/master.cf 12c12 \u0026lt; #submission inet n - n - - smtpd --- \u0026gt; submission inet n - n - - smtpd 14,15c14,15 \u0026lt; # -o smtpd_sasl_auth_enable=yes \u0026lt; # -o smtpd_client_restrictions=permit_sasl_authenticated,reject --- \u0026gt; -o smtpd_sasl_auth_enable=yes \u0026gt; -o smtpd_client_restrictions=permit_sasl_authenticated,reject $ diff /etc/sasl2/smtpd.conf.org /etc/sasl2/smtpd.conf 1c1 \u0026lt; pwcheck_method: saslauthd --- \u0026gt; pwcheck_method: auxprop sendmailの停止、postfixとsaslauthdの起動設定 デフォルトでは sendmail が起動しているので、停止します。また、自動起動の設定も解除します。\n$ sudo service sendmail stop Shutting down sendmail: [ OK ] $ sudo chkconfig sendmail off 続いて、MTA(Mail Transfer Agent)を sendmail から postfix に変更します。\n$ sudo alternatives --config mta There are 2 programs which provide \u0026#39;mta\u0026#39;. Selection Command ----------------------------------------------- + 1 /usr/sbin/sendmail.sendmail 2 /usr/sbin/sendmail.postfix Enter to keep the current selection[+], or type selection number: ;2 を入力してEnterキーを押します postfix を起動します。自動起動設定もします。\n$ sudo chkconfig --add postfix $ sudo chkconfig postfix on $ sudo service postfix start Starting postfix: [ OK ] SALS認証のための saslauthd を起動します。自動起動設定もします。\n$ sudo chkconfig saslauthd on $ sudo service saslauthd start Starting saslauthd: [ OK ] ユーザの作成 ユーザとは、hogehoge@example.com というメールアドレスを作ったときの hogehoge にあたる部分です。\nユーザの作成、パスワード設定\n$ sudo useradd hogehoge $ sudo passwd hogehoge Changing password for user hogehoge. New password: Retype new password: passwd: all authentication tokens updated successfully. 作ったユーザになってメールディレクトリを作成します。\n$ su - hogehoge Password: Last login: Thu Jan 24 23:04:06 JST 2018 on pts/0 $ mkdir Maildir これで受信設定は完了です。\nhogehoge@example.com 宛にメールを送ると作成した Maildir の下に new ディレクトリが作成され、そこに 1234567890.abcdefghABCD123456.ip-XX-XXX-XXX-XXX といったファイルが生成されます。\nこれがメールの内容になりますが、件名や本文はbase64エンコードされているので、読むにはbase64デコードする必要があります。\n",
    "permalink": "https://michimani.net/post/aws-receive-email-with-mail-server-on-ec2/",
    "title": "EC2にメールサーバを立ててお名前.comで取得したドメインでメールを受信できるようにするまで"
  },
  {
    "contents": "サーバ監視ってややこしそうな印象でしたが、AWSを使うと簡単でした。 タイトルの通り、CloudWatchでEC2のCPU使用率を監視してSNSで通知した時のメモです。\nここでいう SNS は、Social Networking Service ではなく、AWSのサービスの一つである Simple Notification Service のことです。\n前提 EC2インスタンスが稼働中であること（事前に監視対象のインスタンスIDをメモしておく） SNS（Simple Notification Service）でトピック・サブスクリプション作成 トピックとは 通知のチャンネルのようなもの トピックの中（下？）に通知先を色々設定できる（HTTP、Email、AWS Lambda\u0026hellip;） 作成時に トピック名（半角英数字とハイフン、アンダースコアで256文字まで）と、表示名（10文字以内）を入力 サブスクリプションとは トピックの中（下？）に作成する通知先のこと エンドポイントとして、HTTPならURL、Emailならメールアドレスを指定する Emailを指定すると、設定したアドレス宛に承認用URLが届くので、本文内のURLで承認する SNSの料金設定 メール送信であれば、毎月1,000件までは無料 それ以降は、100,000件あたり2ドル ※他のサブスクリプションについてはSNS料金ページを参照してください https://aws.amazon.com/jp/sns/pricing/ 作成手順 トピック作成 作成したトピックに対してサブスクリプション作成 プロトコルを Email エンドポイントに 通知先のメールアドレス 設定したメールアドレス宛に届いた承認メール内のURLにアクセスして、承認 件名は AWS Notification - Subscription Confirmation CloudWatchでアラーム作成 アラームとは ◯◯が▲▲以上になったら□□する といった設定のこと 以下の3つの状態がある アラーム：設定した条件に合致している状態 不足：データが不足していて設定した条件が判定できない状態 OK：設定した条件に合致していない状態 CloudWatchの料金 無料 最大50個のメトリクス、3つのダッシュボード、モニタリングは5分間隔 ※詳細はCloudWatch料金ページを参照してください https://aws.amazon.com/jp/cloudwatch/pricing/ 作成手順 ※スクショはありません\nメトリクスの選択 で EC2 を選択し、右横のテキストボックスに対象のインスタンスIDを入力して Enter 下の一覧から CPUUtilization (CPUUtilization) の行にチェックを入れ、次へ 名前、説明 を入力 しきい値、アラームのプレビュー の下にあるテキストボックス、ラジオボタン、セレクトボックスを適宜設定する\n以下、「5分間の使用率平均が70%を超えた場合にトリガー」という条件の場合の設定例\nが：≧ 70\n期間：1 中 1データポイント\n間隔：5分間\n統計：スタンダード　平均 アクション の項目で、アラームが次の時：状態：警告 に、作成したSNSのトピックを指定する\nアラームの状態が、アラーム に変わった時に通知が来る アラームが次の時：状態：OK も設定すると、アラームの状態が OK に変わったときにも通知が来る 注意点 間隔を 1分 とかに設定することもできますが、無料で使用する場合はモニタリングの最小間隔が 5分 なので、アラームの状態が頻繁に 不足 になります。 ",
    "permalink": "https://michimani.net/post/aws-monitoring-cpu-of-ec2-and-notice-with-sns/",
    "title": "AmazonCloudWatchでEC2のCPU使用率を監視してSNSで通知する"
  },
  {
    "contents": "牛に何かを喋らせる cowsay コマンドがありますが、-f オプションを使用することで牛以外にも様々なモノに喋らせることができます。 それを、ランダムで実行できるようにしました。\nmichimani/rand_cowsay.php cowsayがインストールされているサーバ等で、上記のphpを実行するだけです。実行ごとに喋るキャラクターがランダムで変わります。\nちなみに、Macにcowsayをインストールするには、下記コマンドを実行します。\n$ brew install cowsay 実行してみます。\n$ php rand_cowsay.php test ______ \u0026lt; test \u0026gt; ------ \\ ___-------___ \\ _-~~ ~~-_ \\ _-~ /~-_ /^\\__/^\\ /~ \\ / \\ /| O|| O| / \\_______________/ \\ | |___||__| / / \\ \\ | \\ / / \\ \\ | (_______) /______/ \\_________ \\ | / / \\ / \\ \\ \\^\\\\ \\ / \\ / \\ || \\______________/ _-_ //\\__// \\ ||------_-~~-_ ------------- \\ --/~ ~\\ || __/ ~-----||====/~ |==================| |/~~~~~ (_(__/ ./ / \\_\\ \\. (_(___/ \\_____)_) by turtle 亀。\n$ php rand_cowsay.php test ______ \u0026lt; test \u0026gt; ------ \\ . . . \\ . . . ` , \\ .; . : .\u0026#39; : : : . \\ i..`: i` i.i.,i i . \\ `,--.|i |i|ii|ii|i: UooU\\.\u0026#39;@@@@@@`.||\u0026#39; \\__/(@@@@@@@@@@)\u0026#39; (@@@@@@@@) `YY~~~~YY\u0026#39; || || by flaming-sheep 燃えている羊。。。\n-t オプションで、cowthink コマンドになります。\n$ php rand_cowsay.php -t test ______ ( test ) ------ o ^ /^ o / \\ // \\ o |\\___/| / \\// .\\ o /O O \\__ / // | \\ \\ *----* / / \\/_/ // | \\ \\ \\ | @___@` \\/_ // | \\ \\ \\/\\ \\ 0/0/| \\/_ // | \\ \\ \\ \\ 0/0/0/0/| \\/// | \\ \\ | | 0/0/0/0/0/_|_ / ( // | \\ _\\ | / 0/0/0/0/0/0/`/,_ _ _/ ) ; -. | _ _\\.-~ / / ,-} _ *-.|.-~-. .~ ~ \\ \\__/ `/\\ / ~-. _ .-~ / \\____(oo) *. } { / ( (--) .----~-.\\ \\-` .~ //__\\\\ \\__ Ack! ///.----..\u0026lt; \\ _ -~ // \\\\ ///-._ _ _ _ _ _ _{^ - - - - ~ by dragon-and-cow 以上。\n",
    "permalink": "https://michimani.net/post/php-random-exec-cowsay/",
    "title": "【PHP】cowsay -f をランダムで実行する"
  },
  {
    "contents": "Laravel5.1をLaravel5.5にアップグレードして composer installを実行したときに、以下のエラーが出ました。\nPHP Fatal error: Uncaught Error: Class name must be a valid object or a string in /var/www/hogehoge/vendor/laravel/framework/src/Illuminate/Foundation/Exceptions/Handler.php:139 Stack trace: #0 [internal function]: Illuminate\\Foundation\\Exceptions\\Handler-\u0026gt;Illuminate\\Foundation\\Exceptions\\{closure}(0, \u0026#39;Symfony\\\\Compone...\u0026#39;) #1 /var/www/hogehoge/bootstrap/cache/compiled.php(6213): call_user_func(Object(Closure), 0, \u0026#39;Symfony\\\\Compone...\u0026#39;) #2 /var/www/hogehoge/vendor/laravel/framework/src/Illuminate/Foundation/Exceptions/Handler.php(140): Illuminate\\Support\\Arr::first(Array, Object(Closure)) #3 /var/www/hogehoge/vendor/laravel/framework/src/Illuminate/Foundation/Exceptions/Handler.php(97): Illuminate\\Foundation\\Exceptions\\Handler-\u0026gt;shouldntReport(Object(ReflectionException)) #4 /var/www/hogehoge/app/Exceptions/Handler.php(34): Illuminate\\Foundation\\Exceptions\\Handler-\u0026gt;report(Object(ReflectionException)) #5 /var/www/hogehoge/vendor/laravel/framework/src/Illuminate/Foundation/Console/Kernel.php(3 in /var/www/hogehoge/vendor/laravel/framework/src/Illuminate/Foundation/Exceptions/Handler.php on line 139 Script @php artisan package:discover handling the post-autoload-dump event returned with error code 255 エラーメッセージでググってみると、stackoverflowに解決方法がありました。\nComposer Update with Laravel 5.4 \u0026amp;amp; PHP 7 \u0026amp;ldquo;Class Name must be a valid object or a string\u0026amp;rdquo; - Stack Overflow 書かれている通り、bootstrap/cache/compiled.php を削除して再度 composer install を実行すると、エラーは出ませんでした。\ncompiled.php はプロジェクトの実行効率を上げるファイルですが、Laravel5.4へのアップグレードドキュメントに下記の記載がありました。\nコンパイル済みサービスファイルの削除 存在している場合は、bootstrap/cache/compiled.phpファイルを削除してください。フレームワークで使用しなくなりました。\nアップグレードガイド 5.4 Laravel しっかりドキュメントに書いてありました。\n",
    "permalink": "https://michimani.net/post/php-update-laravel-5.1-to-5.5-error-workaround/",
    "title": "【PHP】Laravel5.1からLaravel5.5にアップグレードしてcomposer installしたときにPHP Fatal errorが出たときの対処方法"
  },
  {
    "contents": "以前にも少し書きましたが、それ以外にも色々と修正した部分があったので、以前に書いた内容も含めてあらためて書きます。\n※環境によっては下記内容以外にも修正が必要な場合があります。詳しくはLaravel公式のアップグレードガイドを参照してください。\nコア部分、設定関係 composer.json の書き換え @@ -5,15 +5,23 @@ \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;project\u0026#34;, \u0026#34;require\u0026#34;: { - \u0026#34;php\u0026#34;: \u0026#34;\u0026gt;=5.5.9\u0026#34;, - \u0026#34;laravel/framework\u0026#34;: \u0026#34;5.1.*\u0026#34;, + \u0026#34;php\u0026#34;: \u0026#34;\u0026gt;=7.0.0\u0026#34;, + \u0026#34;fideloper/proxy\u0026#34;: \u0026#34;~3.3\u0026#34;, + \u0026#34;laravel/framework\u0026#34;: \u0026#34;5.5.*\u0026#34;, + \u0026#34;laravel/tinker\u0026#34;: \u0026#34;~1.0\u0026#34;, \u0026#34;doctrine/dbal\u0026#34;: \u0026#34;^2.5\u0026#34; }, \u0026#34;require-dev\u0026#34;: { + \u0026#34;filp/whoops\u0026#34;: \u0026#34;~2.0\u0026#34;, \u0026#34;fzaninotto/faker\u0026#34;: \u0026#34;~1.4\u0026#34;, \u0026#34;mockery/mockery\u0026#34;: \u0026#34;0.9.*\u0026#34;, - \u0026#34;phpunit/phpunit\u0026#34;: \u0026#34;~4.0\u0026#34;, - \u0026#34;phpspec/phpspec\u0026#34;: \u0026#34;~2.1\u0026#34; + \u0026#34;phpunit/phpunit\u0026#34;: \u0026#34;~6.0\u0026#34; + }, + \u0026#34;extra\u0026#34;: { + \u0026#34;laravel\u0026#34;: { + \u0026#34;dont-discover\u0026#34;: [ + ] + } }, \u0026#34;autoload\u0026#34;: { \u0026#34;classmap\u0026#34;: [ @@ -31,18 +39,14 @@ }, \u0026#34;scripts\u0026#34;: { \u0026#34;post-root-package-install\u0026#34;: [ - \u0026#34;php -r \\\u0026#34;copy(\u0026#39;.env.example\u0026#39;, \u0026#39;.env\u0026#39;);\\\u0026#34;\u0026#34; + \u0026#34;@php -r \\\u0026#34;file_exists(\u0026#39;.env\u0026#39;) || copy(\u0026#39;.env.example\u0026#39;, \u0026#39;.env\u0026#39;);\\\u0026#34;\u0026#34; ], \u0026#34;post-create-project-cmd\u0026#34;: [ - \u0026#34;php artisan key:generate\u0026#34; - ], - \u0026#34;post-install-cmd\u0026#34;: [ - \u0026#34;Illuminate\\\\Foundation\\\\ComposerScripts::postInstall\u0026#34;, - \u0026#34;php artisan optimize\u0026#34; + \u0026#34;@php artisan key:generate\u0026#34; ], - \u0026#34;post-update-cmd\u0026#34;: [ - \u0026#34;Illuminate\\\\Foundation\\\\ComposerScripts::postUpdate\u0026#34;, - \u0026#34;php artisan optimize\u0026#34; + \u0026#34;post-autoload-dump\u0026#34;: [ + \u0026#34;Illuminate\\\\Foundation\\\\ComposerScripts::postAutoloadDump\u0026#34;, + \u0026#34;@php artisan package:discover\u0026#34; ] }, \u0026#34;config\u0026#34;: { フィルタ を ミドルウェアに書き換え Controllerのコンストラクタなどで $this-\u0026gt;beforeFilter() といった形でフィルタを使用している場合は、ミドルウェアへの書き換えが必要です。\nbefore\n\u0026lt;?php public function __construct() { parent::__construct(); $this-\u0026gt;beforeFilter(\u0026#34;@hogehoge\u0026#34;); } public function hogehoge() { // 処理 } after\n\u0026lt;?php public function __construct() { parent::__construct(); $this-\u0026gt;middleware(function ($request, $next) { // 処理 return $next($request); }); } Kernel.php の書き換えと、ミドルウェアの追加 app/Http/Kernel.phpでミドルウェアを設定していましたが、書き方が変更になったのと、新たにミドルウェアが追加されているのでその対応です。\n@@ -9,16 +9,38 @@ class Kernel extends HttpKernel /** * The application\u0026#39;s global HTTP middleware stack. * + * These middleware are run during every request to your application. + * * @var array */ protected $middleware = [ \\Illuminate\\Foundation\\Http\\Middleware\\CheckForMaintenanceMode::class, - \\App\\Http\\Middleware\\EncryptCookies::class, - \\Illuminate\\Cookie\\Middleware\\AddQueuedCookiesToResponse::class, - \\Illuminate\\Session\\Middleware\\StartSession::class, - \\Illuminate\\View\\Middleware\\ShareErrorsFromSession::class, - \\App\\Http\\Middleware\\VerifyCsrfToken::class, + \\Illuminate\\Foundation\\Http\\Middleware\\ValidatePostSize::class, + \\App\\Http\\Middleware\\TrimStrings::class, + // \\Illuminate\\Foundation\\Http\\Middleware\\ConvertEmptyStringsToNull::class, + \\App\\Http\\Middleware\\TrustProxies::class, + ]; + + /** + * The application\u0026#39;s route middleware groups. + * + * @var array + */ + protected $middlewareGroups = [ + \u0026#39;web\u0026#39; =\u0026gt; [ + \\App\\Http\\Middleware\\EncryptCookies::class, + \\Illuminate\\Cookie\\Middleware\\AddQueuedCookiesToResponse::class, + \\Illuminate\\Session\\Middleware\\StartSession::class, + // \\Illuminate\\Session\\Middleware\\AuthenticateSession::class, + \\Illuminate\\View\\Middleware\\ShareErrorsFromSession::class, + \\App\\Http\\Middleware\\VerifyCsrfToken::class, + \\Illuminate\\Routing\\Middleware\\SubstituteBindings::class, + ], + \u0026#39;api\u0026#39; =\u0026gt; [ + \u0026#39;throttle:60,1\u0026#39;, + \u0026#39;bindings\u0026#39;, + ], ]; /** @@ -27,8 +49,11 @@ class Kernel extends HttpKernel * @var array */ protected $routeMiddleware = [ - \u0026#39;auth\u0026#39; =\u0026gt; \\App\\Http\\Middleware\\Authenticate::class, + \u0026#39;auth\u0026#39; =\u0026gt; \\Illuminate\\Auth\\Middleware\\Authenticate::class, \u0026#39;auth.basic\u0026#39; =\u0026gt; \\Illuminate\\Auth\\Middleware\\AuthenticateWithBasicAuth::class, + \u0026#39;bindings\u0026#39; =\u0026gt; \\Illuminate\\Routing\\Middleware\\SubstituteBindings::class, + \u0026#39;can\u0026#39; =\u0026gt; \\Illuminate\\Auth\\Middleware\\Authorize::class, \u0026#39;guest\u0026#39; =\u0026gt; \\App\\Http\\Middleware\\RedirectIfAuthenticated::class, + \u0026#39;throttle\u0026#39; =\u0026gt; \\Illuminate\\Routing\\Middleware\\ThrottleRequests::class, ]; } ここで、新たに追加されている ConvertEmptyStringsToNull ですが、これが少し曲者です。\nこのミドルウェアの処理は、formで送信される値が空白(\u0026quot;\u0026quot;)のものは、null として扱う というものです。この結果、今まで空白でpostされてDBの nullableでない カラムに空白が保存されていたようなところで、dbエラーが発生してしまいます。\n対処方法としては、この ConvertEmptyStringsToNull をコメントアウトするか、対象のカラムを nullable にするか、です。\n更に、新たに下記のファイルを追加します。\napp/Http/Middleware/TrimStrings.php app/Http/Middleware/TrustProxies.php これらはGithubからそのまま持ってきます。\nhttps://github.com/laravel/laravel EventServiceProvider.php と RouteServiceProvider.php の書き換え それぞれ boot() の引数が変更されているので修正します。\napp/Providers/EventServiceProvider.php\n@@ -24,9 +24,9 @@ class EventServiceProvider extends ServiceProvider * @param \\Illuminate\\Contracts\\Events\\Dispatcher $events * @return void */ - public function boot(DispatcherContract $events) + public function boot() { - parent::boot($events); + parent::boot(); // } app/Providers/RouteServiceProvider.php\nRouteServiceProvider.phpについてはboot()以外にも変わっている部分があります。こちらも Github のソースを見ながら修正します。\n@@ -1,44 +1,63 @@ \u0026lt;?php - namespace App\\Providers; - -use Illuminate\\Routing\\Router; +use Illuminate\\Support\\Facades\\Route; use Illuminate\\Foundation\\Support\\Providers\\RouteServiceProvider as ServiceProvider; - class RouteServiceProvider extends ServiceProvider { /** - * This namespace is applied to the controller routes in your routes file. + * This namespace is applied to your controller routes. * * In addition, it is set as the URL generator\u0026#39;s root namespace. * * @var string */ protected $namespace = \u0026#39;App\\Http\\Controllers\u0026#39;; - /** * Define your route model bindings, pattern filters, etc. * - * @param \\Illuminate\\Routing\\Router $router * @return void */ - public function boot(Router $router) + public function boot() { // - - parent::boot($router); + parent::boot(); } - /** * Define the routes for the application. * - * @param \\Illuminate\\Routing\\Router $router * @return void */ - public function map(Router $router) + public function map() { - $router-\u0026gt;group([\u0026#39;namespace\u0026#39; =\u0026gt; $this-\u0026gt;namespace], function ($router) { - require app_path(\u0026#39;Http/routes.php\u0026#39;); - }); + $this-\u0026gt;mapApiRoutes(); + $this-\u0026gt;mapWebRoutes(); + // + } + /** + * Define the \u0026#34;web\u0026#34; routes for the application. + * + * These routes all receive session state, CSRF protection, etc. + * + * @return void + */ + protected function mapWebRoutes() + { + Route::middleware(\u0026#39;web\u0026#39;) + -\u0026gt;namespace($this-\u0026gt;namespace) + -\u0026gt;group(base_path(\u0026#39;routes/web.php\u0026#39;)); + } + /** + * Define the \u0026#34;api\u0026#34; routes for the application. + * + * These routes are typically stateless. + * + * @return void + */ + protected function mapApiRoutes() + { + Route::prefix(\u0026#39;api\u0026#39;) + -\u0026gt;middleware(\u0026#39;api\u0026#39;) + -\u0026gt;namespace($this-\u0026gt;namespace) + -\u0026gt;group(base_path(\u0026#39;routes/api.php\u0026#39;)); } ルーティングファイルの変更 Laravel5.1では app/Http/routes.php にルーティングの設定を書いていましたが、5.5では以下の2つのファイルで設定します。\nroutes/api.php routes/web.php とりあえず今までの app/Http/routes.php を routes/web.php にリネームします。\nroutes/api.php については Github から取得します。\napp/config/app.php の書き換え @@ -2,6 +2,8 @@ return [ + \u0026#39;env\u0026#39; =\u0026gt; env(\u0026#39;APP_ENV\u0026#39;, \u0026#39;production\u0026#39;), + /* |-------------------------------------------------------------------------- | Application Debug Mode @@ -120,7 +122,6 @@ return [ Illuminate\\Bus\\BusServiceProvider::class, Illuminate\\Cache\\CacheServiceProvider::class, Illuminate\\Foundation\\Providers\\ConsoleSupportServiceProvider::class, - Illuminate\\Routing\\ControllerServiceProvider::class, Illuminate\\Cookie\\CookieServiceProvider::class, Illuminate\\Database\\DatabaseServiceProvider::class, Illuminate\\Encryption\\EncryptionServiceProvider::class, app/config/auth.php の書き換え デフォルトから特に編集していなければ、 Github の内容をそのままコピペすればOKです。\n@@ -1,67 +1,95 @@ - - \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;eloquent\u0026#39;, - + \u0026#39;defaults\u0026#39; =\u0026gt; [ + \u0026#39;guard\u0026#39; =\u0026gt; \u0026#39;web\u0026#39;, + \u0026#39;passwords\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, + ], - - \u0026#39;model\u0026#39; =\u0026gt; App\\User::class, - + \u0026#39;guards\u0026#39; =\u0026gt; [ + \u0026#39;web\u0026#39; =\u0026gt; [ + \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;session\u0026#39;, + \u0026#39;provider\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, + ], + \u0026#39;api\u0026#39; =\u0026gt; [ + \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;token\u0026#39;, + \u0026#39;provider\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, + ], + ], - - \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, - + \u0026#39;providers\u0026#39; =\u0026gt; [ + \u0026#39;users\u0026#39; =\u0026gt; [ + \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;eloquent\u0026#39;, + \u0026#39;model\u0026#39; =\u0026gt; App\\User::class, + ], + // \u0026#39;users\u0026#39; =\u0026gt; [ + // \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;database\u0026#39;, + // \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, + // ], + ], - - \u0026#39;password\u0026#39; =\u0026gt; [ - \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;emails.password\u0026#39;, - \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;password_resets\u0026#39;, - \u0026#39;expire\u0026#39; =\u0026gt; 60, + \u0026#39;passwords\u0026#39; =\u0026gt; [ + \u0026#39;users\u0026#39; =\u0026gt; [ + \u0026#39;provider\u0026#39; =\u0026gt; \u0026#39;users\u0026#39;, + \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;auth.emails.password\u0026#39;, + \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;password_resets\u0026#39;, + \u0026#39;expire\u0026#39; =\u0026gt; 60, + ], ], sessionsテーブルにカラム追加 セッションをDBに保持する設定にしている場合、sessionsテーブルにカラムの追加が必要です。\n$ php artisan make:migration alter_add_sessions \u0026lt;?php use Illuminate\\Support\\Facades\\Schema; use Illuminate\\Database\\Schema\\Blueprint; use Illuminate\\Database\\Migrations\\Migration; class AlterAddSession extends Migration { /** * Run the migrations. * * @return void */ public function up() { Schema::table(\u0026#39;sessions\u0026#39;, function($t){ $t-\u0026gt;integer(\u0026#39;user_id\u0026#39;)-\u0026gt;nullable(); $t-\u0026gt;integer(\u0026#39;ip_address\u0026#39;)-\u0026gt;nullable(); $t-\u0026gt;string(\u0026#39;user_agent\u0026#39;); }); } /** * Reverse the migrations. * * @return void */ public function down() { Schema::table(\u0026#39;sessions\u0026#39;, function($t){ $t-\u0026gt;dropColumn([\u0026#39;user_id\u0026#39;, \u0026#39;ip_address\u0026#39;, \u0026#39;user_agent\u0026#39;]); }); } } アプリケーションレベルの細かい変更 クエリビルダーの lists() lists() は廃止されたため、pluck() に変更が必要です。使用方法、返り値は一緒です。\nURL生成の url() ルートパスを得るために url() としていたところは url('') と変更が必要です。\n\\URL::Schema() 関数名が Schema から　Scheme に変更になっています。\n強制的にHTTPSにする処理を実装していると使用しているかと思いますが、開発環境が非SSL環境だと本番に反映してからでないと気付けない場合があります。\nPHPのバージョンについて Laravel5.5ではPHP7.0以上が必須となっていますが、これは合っているようで間違っています。\n2018年1月時点での最新のLaravelのバージョンは 5.5.31 となっていますが、このバージョンを使用する場合には PHP7.1 が必須となります。\n5.5.31で composer install を実行すると、下記のようなエラーとなります。\n$ composer install Loading composer repositories with package information Installing dependencies (including require-dev) from lock file Your requirements could not be resolved to an installable set of packages. Problem 1 - Installation request for doctrine/annotations v1.6.0 -\u0026gt; satisfiable by doctrine/annotations[v1.6.0]. - doctrine/annotations v1.6.0 requires php ^7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 2 - Installation request for doctrine/cache v1.7.1 -\u0026gt; satisfiable by doctrine/cache[v1.7.1]. - doctrine/cache v1.7.1 requires php ~7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 3 - Installation request for doctrine/collections v1.5.0 -\u0026gt; satisfiable by doctrine/collections[v1.5.0]. - doctrine/collections v1.5.0 requires php ^7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 4 - Installation request for doctrine/common v2.8.1 -\u0026gt; satisfiable by doctrine/common[v2.8.1]. - doctrine/common v2.8.1 requires php ~7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 5 - Installation request for doctrine/dbal v2.6.3 -\u0026gt; satisfiable by doctrine/dbal[v2.6.3]. - doctrine/dbal v2.6.3 requires php ^7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 6 - Installation request for doctrine/inflector v1.3.0 -\u0026gt; satisfiable by doctrine/inflector[v1.3.0]. - doctrine/inflector v1.3.0 requires php ^7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 7 - Installation request for symfony/css-selector v4.0.3 -\u0026gt; satisfiable by symfony/css-selector[v4.0.3]. - symfony/css-selector v4.0.3 requires php ^7.1.3 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 8 - Installation request for symfony/event-dispatcher v4.0.3 -\u0026gt; satisfiable by symfony/event-dispatcher[v4.0.3]. - symfony/event-dispatcher v4.0.3 requires php ^7.1.3 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 9 - Installation request for doctrine/instantiator 1.1.0 -\u0026gt; satisfiable by doctrine/instantiator[1.1.0]. - doctrine/instantiator 1.1.0 requires php ^7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. Problem 10 - doctrine/inflector v1.3.0 requires php ^7.1 -\u0026gt; your PHP version (7.0.25) does not satisfy that requirement. - laravel/framework v5.5.31 requires doctrine/inflector ~1.1 -\u0026gt; satisfiable by doctrine/inflector[v1.3.0]. - Installation request for laravel/framework v5.5.31 -\u0026gt; satisfiable by laravel/framework[v5.5.31]. Laravelのコアで使用している doctorine がPHP7.1必須のようです。\nhttp://www.doctrine-project.org/2017/07/25/php-7.1-requirement-and-composer.html 対策としては、PHPのバージョンを7.1に上げるのが一番良い。が、簡単には上げられない環境もあるので、その場合は、composer.jsonでPHPのバージョンを指定すればいい(らしい)。\n参考： doctrine/dbal:2.6 でPHP7.1が必要になった時の対策 まとめ とりあえずこれで2020年までは気持ち的には安心ですが、今後は細かく対応を進めていって、今回みたいな一気にアップグレードをする必要がにないようにしたいです。\n",
    "permalink": "https://michimani.net/post/php-update-laravel-5.1-to-5.5-note/",
    "title": "Laravel5.1からLaravel5.5にアップグレードしたときに修正した部分まとめ"
  },
  {
    "contents": "Laravel プロジェクトで作成した API で JSON を返却する際に、ヘッダーに余分なものが付いていたので、その対応方法です。\n今回jsonを返却するときの要件として、\nステータスコードを指定したい Cache-Control は no-cacheのみ指定したい Set-Cookie は付与したくない というのがあります。\nLaravelの response ファザードを使用してjsonを返す場合 \u0026lt;?php public function test_json($response_code = 200) { $res = [\u0026#39;foo\u0026#39; =\u0026gt; \u0026#39;bar\u0026#39;]; $json = json_encode($res); return response($json, $response_code) -\u0026gt;header(\u0026#39;Cache-Control\u0026#39;, \u0026#39;no-cache\u0026#39;) -\u0026gt;header(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/json\u0026#39;) -\u0026gt;header(\u0026#39;Content-Length\u0026#39;, strlen($json)); } test_json(400); curlでtest_json()を実行するurlを叩いた結果が下記。\n$ curl -v http://localhost/path/to/test_json \u0026gt; GET /path/to/test_json HTTP/1.1 \u0026gt; Host: localhost \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 400 Bad Request \u0026lt; Date: Wed, 27 Dec 2017 04:19:00 GMT \u0026lt; Server: Apache/2.2.31 (Unix) mod_wsgi/3.5 Python/2.7.13 PHP/7.1.1 mod_ssl/2.2.31 OpenSSL/1.0.2j DAV/2 mod_fastcgi/2.4.6 mod_perl/2.0.9 Perl/v5.24.0 \u0026lt; X-Powered-By: PHP/7.1.1 \u0026lt; Cache-Control: no-cache, private \u0026lt; Content-Length: 13 \u0026lt; Set-Cookie: XSRF-TOKEN=eyJpd....RhMTUifQ%3D%3D; expires=Wed, 27-Dec-2017 04:54:07 GMT; Max-Age=7200; path=/ \u0026lt; Set-Cookie: 91bb0181f7d...908eb59101d6226=eyJ....ODgyIn0%3D; expires=Thu, 28-Dec-2017 02:54:07 GMT; Max-Age=86400; path=/; HttpOnly \u0026lt; Connection: close \u0026lt; Content-Type: application/json \u0026lt; * Closing connection 0 {\u0026#34;foo\u0026#34;:\u0026#34;bar\u0026#34;} 一見問題なさそうですが、まず Cache-Control に private が付いている。ぐぐってみると別に問題なさそうですが、気持ち悪いです。 あと、Set-Cookie で不要なものが付いている。これに関してはミドルウェアでごちゃごちゃやると付与しないように出来るみたいです。今回はやりません。\n純粋にPHPの関数だけでjsonを返却する場合 \u0026lt;?php public function test_json($response_code = 200) { $res = [\u0026#39;foo\u0026#39; =\u0026gt; \u0026#39;bar\u0026#39;]; $json = json_encode($res); http_response_code($response_code); header(\u0026#39;Cache-Control: no-cache\u0026#39;); header(\u0026#39;Content-Type: application/json\u0026#39;); header(\u0026#39;Content-Length: \u0026#39;.strlen($json)); echo $json; } test_json(400); urlを叩きます。\n$ curl -v http://localhost/path/to/test_json \u0026gt; GET /path/to/test_json HTTP/1.1 \u0026gt; Host: localhost \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Wed, 27 Dec 2017 04:30:58 GMT \u0026lt; Server: Apache/2.2.31 (Unix) mod_wsgi/3.5 Python/2.7.13 PHP/7.1.1 mod_ssl/2.2.31 OpenSSL/1.0.2j DAV/2 mod_fastcgi/2.4.6 mod_perl/2.0.9 Perl/v5.24.0 \u0026lt; X-Powered-By: PHP/7.1.1 \u0026lt; Cache-Control: no-cache, no-cache \u0026lt; Set-Cookie: XSRF-TOKEN=eyJpd....RhMTUifQ%3D%3D; expires=Wed, 27-Dec-2017 04:54:07 GMT; Max-Age=7200; path=/ \u0026lt; Set-Cookie: 91bb0181f7d...908eb59101d6226=eyJ....ODgyIn0%3D; expires=Thu, 28-Dec-2017 02:54:07 GMT; Max-Age=86400; path=/; HttpOnly \u0026lt; Content-Length: 13 \u0026lt; Content-Type: text/html; charset=UTF-8 \u0026lt; * Connection #0 to host localhost left intact {\u0026#34;foo\u0026#34;:\u0026#34;bar\u0026#34;} HTTP/1.1 200 OK？ Cache-Control: no-cache, no-cache？？ Content-Type: text/html？？？ Set-Cookie残ったまま？？？？\n色々とおかしいです。 挙動的には、echo したあとにLaravel側の何かしらの処理が動いている感じです。多分。\nしばらく悩んだ末の解決策 \u0026lt;?php public function test_json($response_code = 200) { $res = [\u0026#39;foo\u0026#39; =\u0026gt; \u0026#39;bar\u0026#39;]; $json = json_encode($res); http_response_code($response_code); header(\u0026#39;Cache-Control: no-cache\u0026#39;); header(\u0026#39;Content-Type: application/json\u0026#39;); header(\u0026#39;Content-Length: \u0026#39;.strlen($json)); echo $json; exit(); } test_json(400); echo したあとに exit() するだけです。\n$ curl -v http://localhost/path/to/test_json \u0026gt; GET /path/to/test_json HTTP/1.1 \u0026gt; Host: localhost \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 400 Bad Request \u0026lt; Date: Wed, 27 Dec 2017 04:44:56 GMT \u0026lt; Server: Apache/2.2.31 (Unix) mod_wsgi/3.5 Python/2.7.13 PHP/7.1.1 mod_ssl/2.2.31 OpenSSL/1.0.2j DAV/2 mod_fastcgi/2.4.6 mod_perl/2.0.9 Perl/v5.24.0 \u0026lt; X-Powered-By: PHP/7.1.1 \u0026lt; Cache-Control: no-cache \u0026lt; Content-Length: 13 \u0026lt; Connection: close \u0026lt; Content-Type: application/json \u0026lt; * Connection 0 {\u0026#34;foo\u0026#34;:\u0026#34;bar\u0026#34;} 長時間悩んだときの解決策って、だいたいあっさりしてます。\n",
    "permalink": "https://michimani.net/post/php-return-json-responce-in-laravel-project/",
    "title": "Laravelでjsonを返却するときにContent-Typeがapplication/jsonにならない、ヘッダー情報に不要なものが付与される時の対応"
  },
  {
    "contents": "Laravel5.1で作ったアプリケーションをAmazonLinux上で動かしていて、cronでバッチ処理の設定をしたいと思いました。が、すこし詰まる部分があったのでそのときのメモです。\nLaravelでコマンドを作成するにはターミナルで下記コマンドを実行します。\n$ php artisan make:console hoge_batch --command=\u0026#34;hoge_batch\u0026#34; Console command created successfully. コマンド実行クラスが生成されるので、そこに処理を記述します。\n実行する時は下記で実行できます。\n$ php artisan hoge_batch [InvalidArgumentException] Command \u0026#34;hoge_batch\u0026#34; is not defined. ここで 最初の罠 です。この段階では動きません。\napp/Console/Commands/Kernel.php に追記する必要があります。\n\u0026lt;?php protected $commands = [ Commands\\hoge_batch::class, ]; じゃあこれをAmazonLinux上でcron設定して実行させます。\nまずは date コマンドでサーバのタイムゾーンがJSTになっているか確認です。これがJSTになっていないと、いつまでたっても実行されない、なんてことになります。\n$ date Tue Dec 26 12:30:33 JST 2017 cronの設定です。とりあえずテストもかねて数分後を設定します。\n$ crontab -e 35 12 * * * php artisan /var/www/path/to/app/artisan hoge_batch \u0026hellip;実行されません。タイムゾーンはしっかりしているのに。\nここで 2つ目の罠 。 サーバのタイムゾーン設定を変更した時は、 crond を再起動する必要があります。\nおそらくインスタンス起動後にタイムゾーンなど諸々設定はしたものの、crond を再起動していなかったようです。\n$ sudo service crond restart 再び実行してみると、コマンド自体は実行されているものの、実行途中で止まっている？ようです。\ncrontab -l でcron設定を見直してみると、/var/spool/mail/ec2-user にメールが来てるよ、というメッセージが出ていました。\n$ tail /var/spool/mail/ec2-user #6 in /var/www/path/to/app/bootstrap/cache/compiled.php on line 13440 PHP Fatal error: Uncaught UnexpectedValueException: The stream or file \u0026#34;/var/www/path/to/app/storage/logs/laravel-2017-12-26.log\u0026#34; could not be opened: failed to open stream: Permission denied in /var/www/path/to/app/bootstrap/cache/compiled.php:13440 これが 最後の罠 。 どうやらログファイルに書き込み権限が無いようです。\ncronをapacheユーザで実行するように設定します。\nとりあえず crontab -e でもともとの設定は削除しておきます。消さないとエラーが出続けます。\n$ sudo -u apache -e これで再度cronを設定すると、やっと動きました。\n",
    "permalink": "https://michimani.net/post/aws-run-laravel-command-with-cron/",
    "title": "Amazon Linuxでcron設定してLaravelのコマンドを実行するときに色々とハマった"
  },
  {
    "contents": "一年以上前に同じようなことを書いてますが、その続編です。\n今みると、このままでは動かないようなソースで残念です。\nやることは変わってなくて、諸々考慮したバージョンです。 ソースは GitHub に置いています。 michimani/get-all-image 直したところ コマンドラインで使えるようにした 前回は関数として作っていたので、その関数を呼ぶスクリプト書かないと使えませんでした。 今回はコマンドラインで使えるようにしたので ターミナルで\n$ php get_all_image.php http://example.com を叩くだけです。\nMacなら買ってきた状態でphpコマンドが叩けるので、上のリンクからダウンロードしてすぐ使えます。Windowsのことはちょっとわかりません。\n引数のチェックを入れた 前回は引数として受け取るURLのチェックとかは一切していませんでした。\n今回はコマンドライン引数としてURLを受け取る様になっているので、そのチェックをしています。\n\u0026lt;?php if (!isset($argv[1])) { throw new Exception(\u0026#39;The first argument is required for url.\u0026#39;); } $url = $argv[1]; if (!preg_match(\u0026#39;/^(http|https):\\/\\//\u0026#39;, $url)) { throw new Exception(\u0026#39;Invalid url.\u0026#39;); } $header = @get_headers($url); if (!preg_match(\u0026#39;/^HTTP\\/.*\\s+200\\s/i\u0026#39;, $header[0])) { throw new Exception(\u0026#39;Target page does not found.\u0026#39;); } 画像リンクが http(s) から始まっていないパターンの対応 画像リンクには src=\u0026quot;http://~\u0026quot; のパターンと src=\u0026quot;/img/~\u0026quot; みたいにドメイン以下のパスが書いてあるパターンがあるので、その対応です。\n\u0026lt;?php $base_tmp = explode(\u0026#39;/\u0026#39;, $url); $base = sprintf(\u0026#39;%s/%s/%s\u0026#39;, $base_tmp[0], $base_tmp[1], $base_tmp[2]); //... if (!preg_match(\u0026#39;/^https?:\\/\\//\u0026#39;, $img_url)) { $img_url = sprintf(\u0026#39;%s/%s\u0026#39;, $base, $img_url); } ",
    "permalink": "https://michimani.net/post/php-save-all-image-in-a-page-2/",
    "title": "続・PHPでWebページ上の画像ファイルをすべて保存する"
  },
  {
    "contents": "12月13日にAmazon Linux 2 発表されました。\n文字通り既存のAmazon Linuxの後継となるOSです。\nAmazon Linux 2 のご紹介 Amazon Linux 2に関する 公式のQ\u0026amp;amp;Aページ は既に存在しています。 が、まだ日本語化されていません。(2017/12/18時点)\n日本語ページも準備されていましたので、より正確な日本語訳については公式ページをご確認ください。(2018/10/15)\n今回はその中から抜粋・翻訳した内容をメモしておきます。\n概要 Q. Amazon Linux 2とは何ですか？ Amazon Linux 2は、近代的なアプリケーション環境にLinuxコミュニティの最新機能を提供し、長期的なサポートを提供する次世代Amazon Linuxオペレーティングシステムです。 Amazon Linux 2は、Amazonマシンイメージ（AMI）およびコンテナイメージフォーマットに加えて、オンプレミス開発およびテストのための仮想マシンイメージとして利用できるため、ローカル開発環境からアプリケーションを簡単に開発、テスト、および認証することができます。\nAmazon Linuxとの違いとしては、EC2上だけではなく、ローカル環境やオンプレ環境でも利用できるようになったようです。\nDocker環境では、Dockerコンテナイメージとして、VirtualBoxやVMware上では仮想化マシンイメージとして利用できます。\nEC2上での利用に関しては、これまでと同じで、EC2インスタンス作成時にAmazon Linux 2のAMIを選択します。\n利用できるインスタンスタイプにも制限はなく、最新のバージョンのインスタンスタイプがすべてサポートされています。\nサポート関係 Q. Amazon Linux 2の長期サポートには何が含まれていますか？ Amazon Linux 2 LTSビルドの長期サポートには、次の2つの主要コンポーネントが含まれます。\nAWSは、Amazon Linux 2のセキュリティアップデートとバグ修正を5年間提供します。 AWSは、ユーザー空間アプリケーションバイナリインターフェイス（ABI）とアプリケーションプログラミングインターフェイス（API）の互換性を5年間維持します。 サポートは5年間となっています。\nただし、現在(2017/12/18)提供されている Amazon Linux 2 (2017.12) は LTS Candidate 、つまり、まだ 長期サポートの 候補版 となっていて、利用者からのフィードバックを随時受け付けている状態です。\n正式なLTSバージョンは まもなく登場 となっています。\nQ. AWSは現在のバージョンのAmazon Linuxをサポートしますか？ はい、既存のアプリケーションの中断を避け、Amazon Linux 2への移行を容易にするために、AWSは、最終LTSビルドが発表されてからAmazon Linux 2017.09 AMIおよびコンテナイメージの定期的なセキュリティアップデートを2年間提供します。また、AWSプレミアムサポートやAmazon Linuxディスカッションフォーラムなどの既存のサポートチャネルをすべて使用して、サポートリクエストを引き続き提出することもできます。\n既存のAmazon Linuxとの互換性、移行方法 Q. Amazon Linux 2の可用性により、Amazon LinuxのAMI（2017.09）の既存のバージョンに変更はありますか？ Amazon Linux 2のリリースにより、2017.09 Amazon LinuxのAMIリリースとコンテナ・イメージが現在の世代のAmazon Linuxの最終リリースとなることを発表します。 AWSは、Amazon Linux 2の新しいバージョンのみを提供します。\nQ. Amazon Linux 2は、Amazon Linuxの既存のバージョンと下位互換性がありますか？ systemdのようなAmazon Linux 2に新しいコンポーネントが含まれているため、現在のバージョンのAmazon Linuxで動作するアプリケーションでは、Amazon Linux 2上で実行するために追加の変更が必要になることがあります。\nQ.既存のバージョンのAmazon LinuxをAmazon Linux 2にインプレースアップグレードすることはできますか？ いいえ。既存のAmazon LinuxイメージからAmazon Linux 2へのインプレースアップグレードはサポートされていません。移行する前に、まずAmazon Linux 2の新規インストールでアプリケーションをテストすることをお勧めします。\n既存のAmazon Linuxからの移行に関しては、何かポチっとすれば終わりとはいかないようです。\n別インスタンスでAmazon Linux 2を起動して、そこで環境を作って移行する必要があります。\nまとめ 既存のAmazon Linuxについてはしばらくサポートが続きますが、セキュリティ的にもメンタル的にも、LTSバージョンが出たら早い段階で移行したいところです。\n",
    "permalink": "https://michimani.net/post/aws-compare-amazonlinux-2-amazonlinux/",
    "title": "Amazon Linux 2 の登場と、Amazon Linux との違い、移行方法などをQ\u0026Aから抜粋したメモ"
  },
  {
    "contents": "MAMPでは、設定からドキュメントルートを変更できます。が、複数プロジェクトを切り替える場合に毎回設定を変更して再起動するのは面倒です。\nなので、ポートでバーチャルホストを設定して、各ポートにプロジェクトを割り振るという方法で対応します。\n環境 macOS Sierra 10.12.6 MAMP 4.1.1 前提 以下のようなプロジェクト(ドキュメントルート)があるとします。\nproject_a (/Users/hogehoge/Projects/project_a/html) project_b (/Users/hogehoge/Projects/project_b/html) project_c (/Users/hogehoge/Projects/project_c/html) それぞれ、ドキュメントルート配下に次のようなhtmlファイルが置いてあるとします。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;PROJECT A\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;プロジェクト A です\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; MAMPの設定ファイル変更 変更するファイルは次の2つ\n/Applications/MAMP/conf/apache/httpd.conf /Applications/MAMP/conf/apache/extra/httpd-vhosts.conf /Applications/MAMP/conf/apache/httpd.conf は、下記部分を有効にします。\n# Virtual hosts - #Include /Applications/MAMP/conf/apache/extra/httpd-vhosts.conf + Include /Applications/MAMP/conf/apache/extra/httpd-vhosts.conf /Applications/MAMP/conf/apache/extra/httpd-vhosts.conf には下記の記述を末尾に追加します。\nListen 8001 \u0026lt;VirtualHost *:8001\u0026gt; DocumentRoot \u0026#34;/Users/hogehoge/Projects/project_a/html\u0026#34; \u0026lt;Directory \u0026#34;/Users/hogehoge/Projects/project_a/html\u0026#34;\u0026gt; AllowOverride All \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Listen 8002 \u0026lt;VirtualHost *:8002\u0026gt; DocumentRoot \u0026#34;/Users/hogehoge/Projects/project_b/html\u0026#34; \u0026lt;Directory \u0026#34;/Users/hogehoge/Projects/project_b/html\u0026#34;\u0026gt; AllowOverride All \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Listen 8003 \u0026lt;VirtualHost *:8003\u0026gt; DocumentRoot \u0026#34;/Users/hogehoge/Projects/project_c/html\u0026#34; \u0026lt;Directory \u0026#34;/Users/hogehoge/Projects/project_c/html\u0026#34;\u0026gt; AllowOverride All \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; アクセスしてみる 設定ファイルを変更したらMAMPでサーバを再起動して、それぞれのポートにアクセスします。\nlocalhost:8001 localhost:8002 localhost:8003 さらにhostsでの切り替えを試してみた(無理でした) ポートでの切り替えができるようになったものの、localhost:8001 ではどのプロジェクトなのかわからないので、hostsで下記のような設定をしてみました。\n127.0.0.1:8001 local.project_a.com 127.0.0.1:8002 local.project_b.com 127.0.0.1:8003 local.project_c.com が、うまくいきません。 調べたところ、hostsで解決できるのはIPアドレスまでで、ポートの値までは解決できないようです。\n",
    "permalink": "https://michimani.net/post/develop-using-vertual-host-with-mamp/",
    "title": "MAMPでバーチャルホストを設定したときのメモ"
  },
  {
    "contents": "普段の開発や作業中にターミナルで叩いているコマンドのメモです。 プログラミング初心者の方々（自分もまだまだ初心者ですが）のお役に立てば幸いです。 普段使っている便利なコマンドがあれば教えてください。\nGit関係 gitコマンドのエイリアス作成 長いgitコマンドのエイリアスを作成できます。 例えば、git log --name-staus コマンドでは、各コミットで変更のあったファイルリストを git log に含めて表示してくれます。が、ちょっとコマンドが長いです。\n$ git config --global alias.logst \u0026#34;log --name-status\u0026#34; とすと、以降、git logst で git log --name-status と同じ結果を得られます。\nオプションとして --global を指定すると、現在のプロジェクトのリポジトリだけではなく、他のリポジトリで作業する際にも使用できます。 代わりに --local を指定すると、現在のリポジトリ内でのみ有効なエイリアスとなります。\n自分が設定しているエイリアスは下記です。\n$ git config --list --global | grep alias alias.st=status alias.logst=log --name-status alias.chpn=cherry-pick -n alias.br=branch alias.cho=checkout ちなみに設定しているエイリアスを表示するには git config --list | grep alias で確認できます。 ここでも --global --local オプションを指定することで、それぞれに設定されたエイリアスを確認できます。\nいろいろな差分表示 ### HEAD と 現在の作業内容 との差分 $ git diff ### 特定のコミット と 現在の作業内容 との差分 $ git diff aad2dba98344b10e745842ef3cbcbb34f855da6d ### HEADの一つ前のコミット と HEAD との差分 $ git diff HEAD^ ### 特定のコミット と HEAD との差分 $ git diff aad2dba98344b10e745842ef3cbcbb34f855da6d HEAD ### 特定のコミット同士の差分 $ git diff aad2dba98344b10e745842ef3cbcbb34f855da6d 9a571807c8b44829c04e484065aff8dd79582fdf 特定のコミットとの差分など、2つの引数を渡す場合、古いものと新しいものどちらが先でも出力はされますが、先に古いもの、あとに新しいものを指定したほうが自然な出力結果になります。\nまた、たまに差分をファイルで欲しいと言われる場合があるので、その時は下記コマンドで出力結果をファイルに出力できます。\n$ git diff \u0026gt;\u0026gt; /Users/hogehoge/Desktop/git_dif.diff これはgitコマンドに限らずLinuxコマンドの出力結果をファイルに出力する場合に使えます。\n直近のコミットログを編集する $ git commit --amend ファイル閲覧（主にログ）関係 ファイルの後ろの方だけ閲覧 ### 後ろ10行を出力 $ tail sample.log ### 後ろ100行を出力 $ tail -100 sample.log ファイルへの追記を監視する $ tail -f sample.log 上の tail コマンドのオプションですが、ログファイルのように末尾に追記されているのをリアルタイムで確認できます。 例えばLaravelプロジェクトであれば ./storage/logs/ にログが出力されるので、$ tail -f ./storage/logs/laravel-2017-12-08.log を叩いてから処理を実行すれば、処理中に埋め込んだログを確認しながら操作ができます。\nファイル全体の閲覧 ### 行数が少ないファイル向け $ cat sample.txt ### 行数が多いファイル向け $ cat sample.log | less $ cat sample.log | less -N #行数を表示したい場合 以上\n",
    "permalink": "https://michimani.net/post/develop-frequently-used-commands/",
    "title": "普段の開発・作業中にターミナルで使っているコマンドたち"
  },
  {
    "contents": "何番煎じかわかりませんが、今更ながらJenkinsを自分でインストールするところからやったので、そのメモです。\nMacにJenkinsインストール 環境 MacBook Air 13-inch, 2015 macOS Sierra 10.12.6 1.6GHz Intel Core i5 メモリ8GB Javaが入っているか Jenkinsを動かすにはJavaが必要なので、入っているか確認します。\n$ java -version java version \u0026#34;1.8.0_144\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_144-b01) Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode) Jenkinsインストール Homebrewでインストールします。\n$ brew install jenkins これでインストール自体は終わりです。\nJenkins起動・初期設定 起動 ターミナルから下記コマンドでJenkinsを起動します。\n$ jenkins すると、途中で下記のような出力があります。\n************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: 12345678abcdefgh12345678abcdefgh This may also be found at: /Users/hogehoge/.jenkins/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* この出力の後、数行の出力があってターミナルの動きが止まるので、ブラウザで http://localhost:8080 にアクセスします。 以下のような画面が表示されるので、ターミナルに出力された 12345678abcdefgh12345678abcdefgh を入力して continue を押します。\n以降の画面で初期設定をしていきます。\n初期設定 まず、プラグインをインストールします。推奨されるものをインストールするか、自分で選択してインストールするかを選べますが、とりあえず Install suggested plugins を選択して、Jenkinsコミュニティでよく使われているプラグインをインストールして進めます。\n続いてユーザの登録です。\n必要事項を記入して Save and Finish をクリック。\n無事におじさんに会うことができました。\n",
    "permalink": "https://michimani.net/post/develop-install-jenkins-to-mac/",
    "title": "MacにJenkinsをインストールして初期設定をするまで"
  },
  {
    "contents": "タイトルの通り、Laravelのバージョンを5.1から5.5へアップグレードしたので、その時のメモです。\nちなみにですが、5.1から5.5へのアップグレードなんてやるべきではないです。新バージョンが出るたびに細かくアップグレードしていくのがベストです。小規模なサービスですら対応範囲は多いので、大規模サービスとなるとかなりの工数がかかると思います。\nとりあえずバージョンを上げてみる 5.1から5.5となると変更点はかなりありますが、そこまで大きいプロジェクトでもなかったのでとりあえず5.5にバージョンアップしてみました。 Laravelのバージョンアップは、基本的には composer.json を書き換えて composer update を実行します。\ncomposer.json書き換え まずはcomposer.jsonの書き換えとなります。ほぼ全部書き換えです。 とりあえずアップグレード と言ってますが、5.5からはPHPの対応バージョンが7.0以上となっているので、サーバのPHPバージョンが5系のままだと、そもそもアップグレードができません。 今回、PHPのバージョンは7以上だったので、強行突破してます。\ndiff --git a/composer.json b/composer.json index cc83e39..14321dd 100644 --- a/composer.json +++ b/composer.json @@ -5,15 +5,22 @@ \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;project\u0026#34;, \u0026#34;require\u0026#34;: { - \u0026#34;php\u0026#34;: \u0026#34;\u0026gt;=5.5.9\u0026#34;, - \u0026#34;laravel/framework\u0026#34;: \u0026#34;5.1.*\u0026#34;, - \u0026#34;doctrine/dbal\u0026#34;: \u0026#34;^2.5\u0026#34; + \u0026#34;php\u0026#34;: \u0026#34;\u0026gt;=7.0.0\u0026#34;, + \u0026#34;fideloper/proxy\u0026#34;: \u0026#34;~3.3\u0026#34;, + \u0026#34;laravel/framework\u0026#34;: \u0026#34;5.5.*\u0026#34;, + \u0026#34;laravel/tinker\u0026#34;: \u0026#34;~1.0\u0026#34; }, \u0026#34;require-dev\u0026#34;: { + \u0026#34;filp/whoops\u0026#34;: \u0026#34;~2.0\u0026#34;, \u0026#34;fzaninotto/faker\u0026#34;: \u0026#34;~1.4\u0026#34;, \u0026#34;mockery/mockery\u0026#34;: \u0026#34;0.9.*\u0026#34;, - \u0026#34;phpunit/phpunit\u0026#34;: \u0026#34;~4.0\u0026#34;, - \u0026#34;phpspec/phpspec\u0026#34;: \u0026#34;~2.1\u0026#34; + \u0026#34;phpunit/phpunit\u0026#34;: \u0026#34;~6.0\u0026#34; + }, + \u0026#34;extra\u0026#34;: { + \u0026#34;laravel\u0026#34;: { + \u0026#34;dont-discover\u0026#34;: [ + ] + } }, \u0026#34;autoload\u0026#34;: { \u0026#34;classmap\u0026#34;: [ @@ -31,18 +38,14 @@ }, \u0026#34;scripts\u0026#34;: { \u0026#34;post-root-package-install\u0026#34;: [ - \u0026#34;php -r \\\u0026#34;copy(\u0026#39;.env.example\u0026#39;, \u0026#39;.env\u0026#39;);\\\u0026#34;\u0026#34; + \u0026#34;@php -r \\\u0026#34;file_exists(\u0026#39;.env\u0026#39;) || copy(\u0026#39;.env.example\u0026#39;, \u0026#39;.env\u0026#39;);\\\u0026#34;\u0026#34; ], \u0026#34;post-create-project-cmd\u0026#34;: [ - \u0026#34;php artisan key:generate\u0026#34; - ], - \u0026#34;post-install-cmd\u0026#34;: [ - \u0026#34;Illuminate\\\\Foundation\\\\ComposerScripts::postInstall\u0026#34;, - \u0026#34;php artisan optimize\u0026#34; + \u0026#34;@php artisan key:generate\u0026#34; ], - \u0026#34;post-update-cmd\u0026#34;: [ - \u0026#34;Illuminate\\\\Foundation\\\\ComposerScripts::postUpdate\u0026#34;, - \u0026#34;php artisan optimize\u0026#34; + \u0026#34;post-autoload-dump\u0026#34;: [ + \u0026#34;Illuminate\\\\Foundation\\\\ComposerScripts::postAutoloadDump\u0026#34;, + \u0026#34;@php artisan package:discover\u0026#34; ] }, \u0026#34;config\u0026#34;: { composer update composer.json を書き換えたら、composer update を実行します。\n$ composer update Loading composer repositories with package information Updating dependencies (including require-dev) ... ... ... Writing lock file Generating autoload files \u0026gt; Illuminate\\Foundation\\ComposerScripts::postAutoloadDump \u0026gt; @php artisan package:discover In ProviderRepository.php line 208: Class \u0026#39;Illuminate\\Routing\\ControllerServiceProvider\u0026#39; not found Script @php artisan package:discover handling the post-autoload-dump event returned with error code 1 これは、5.2へのアップグレード時に対応する内容です。\nconfig/app.php の中の 下記の記述を削除して、もう一度 composer update 実行。\nIlluminate\\Routing\\ControllerServiceProvider::class, $ composer update Loading composer repositories with package information Updating dependencies (including require-dev) Nothing to install or update Generating autoload files \u0026gt; Illuminate\\Foundation\\ComposerScripts::postAutoloadDump \u0026gt; @php artisan package:discover In EventServiceProvider.php line 8: Declaration of App\\Providers\\EventServiceProvider::boot(Illuminate\\Contract s\\Events\\Dispatcher $events) should be compatible with Illuminate\\Foundatio n\\Support\\Providers\\EventServiceProvider::boot() Script @php artisan package:discover handling the post-autoload-dump event returned with error code 1 またエラーが出ました。\nDeclaration of App\\Providers\\EventServiceProvider::boot(Illuminate\\Contract s\\Events\\Dispatcher $events) should be compatible with Illuminate\\Foundatio n\\Support\\Providers\\EventServiceProvider::boot() これは、5.3へアップグレードするときの対応内容です。\napp/Providers/ 以下の EventServiceProvider.php および RouteServiceProvider.php の boot()メソッドの引数を削除します。\n@@ -24,9 +24,9 @@ class EventServiceProvider extends ServiceProvider * @param \\Illuminate\\Contracts\\Events\\Dispatcher $events * @return void */ - public function boot(DispatcherContract $events) + public function boot() { - parent::boot($events); + parent::boot(); // } @@ -22,11 +22,11 @@ class RouteServiceProvider extends ServiceProvider * @param \\Illuminate\\Routing\\Router $router * @return void */ - public function boot(Router $router) + public function boot() { // - parent::boot($router); + parent::boot(); } /** 再度 composer update 実行。\n$ composer update Loading composer repositories with package information Updating dependencies (including require-dev) Nothing to install or update Generating autoload files \u0026gt; Illuminate\\Foundation\\ComposerScripts::postAutoloadDump \u0026gt; @php artisan package:discover Discovered Package: fideloper/proxy Discovered Package: laravel/tinker Package manifest generated successfully. 成功したようなので、バージョンを確認します。\n$ php artisan --version Laravel Framework 5.5.23 とりあえず動かしてみる Laravel自体のアップグレードはできたので、プロジェクトのほうを修正していきます。\nLaravelの公式ドキュメントは、各バージョンへのアップグレード時に何をする必要があるのかを書いてくれているので、今回のように一気にアップグレードではなく、その都度アップグレードしている場合にはこれに沿って対応してくのが基本です。\n5.1から5.2 アップグレードガイド 5.2 Laravel 5.2から5.3 アップグレードガイド 5.3 Laravel 5.3から5.4 アップグレードガイド 5.4 Laravel 5.4から5.5 アップグレードガイド 5.5 Laravel 今回は、とりあえず動かしてみて、エラーと戦ってきたいと思います。\nセッション Column not found: 1054 Unknown column 'user_id' in 'field list'\nセッションをDBに保持するようにしている場合、上記のようなエラーが出ます。\nこれは5.2へのアップグレード時に対応する内容で、ドキュメントには次のように書かれています。\nユーザID、IPアドレス、ユーザエージェントのような情報をより含む、新しいdatabaseセッションドライバが書かれました。古いドライバーを使い続けたい場合は、session.php設定ファイルへlegacy-databaseドライバを指定してください。 新しいドライバーを使用する場合、セッションのデータベーステーブルへ、user_id (NULL値を許す整数)、ip_address (NULL値を許す整数)、user_agent (テキスト)カラムを追加してください。\nということで、カラムを追加します。\n$ php artisan make:migration alter_add_session \u0026lt;?php /** * Run the migrations. * * @return void */ public function up() { Schema::table(\u0026#39;sessions\u0026#39;, function($t){ $t-\u0026gt;integer(\u0026#39;user_id\u0026#39;)-\u0026gt;nullable(); $t-\u0026gt;integer(\u0026#39;ip_address\u0026#39;)-\u0026gt;nullable(); $t-\u0026gt;string(\u0026#39;user_agent\u0026#39;); }); } /** * Reverse the migrations. * * @return void */ public function down() { Schema::table(\u0026#39;sessions\u0026#39;, function($t){ $t-\u0026gt;dropColumn([\u0026#39;user_id\u0026#39;, \u0026#39;ip_address\u0026#39;, \u0026#39;user_agent\u0026#39;]); }); } $ php artisan migrate ************************************** * Application In Production! * ************************************** Do you really wish to run this command? (yes/no) [no]: \u0026gt; アプリケーションが本番だけど、大丈夫？と聞かれてしまいました。.env では APP_ENV=local となっているのに、なぜ？\nこれも5.2で対応内容ですが、config/app.php に env のデフォルトオプションが追加されていました。\ndiff --git a/config/app.php b/config/app.php index f013735..3f597f9 100644 --- a/config/app.php +++ b/config/app.php @@ -2,6 +2,8 @@ return [ + \u0026#39;env\u0026#39; =\u0026gt; env(\u0026#39;APP_ENV\u0026#39;, \u0026#39;production\u0026#39;), これで local で確認無しでマイグレーションが通ります。\nルートフィルター Method [beforeFilter] does not exist on [App\\Http\\Controllers\\***\nこれは5.1の時点で非推奨、5.2で削除となったルートフィルターの使用によるエラーです。\nより好ましいミドルウェアにより、ルートフィルターは非推奨となりました。\nとあるように、ミドルウェアへの移行が必要です。まあ、5.1のうちに移行しておくべきだった内容ですが。\nミドルウェアの作成 今回の場合、以下のようにController内にフィルターを書いていました。\n\u0026lt;?php class HogeController extends Controller { public function __construct() { parent::__construct(); $this-\u0026gt;beforeFilter(\u0026#34;@filterHoge\u0026#34;); } public function filterHoge() { // 処理 } } これを、ミドルウェアに移行します。\n$ php artisan make:middleware HogeMiddleware \u0026lt;?php class HogeMiddleware { /** * Handle an incoming request. * * @param \\Illuminate\\Http\\Request $request * @param \\Closure $next * @return mixed */ public function handle($request, Closure $next) { // 処理 return $next($request); } } ヘルパ url() url()ヘルパ関数が、引数のパスの指定がない場合にIlluminate\\Routing\\UrlGeneratorを返すようになりました。\n例えばプロジェクトのルートパスが https://hogehoge.com だった場合、5.1では\n\u0026lt;?php $u = url(); echo $u; // https://hogehoge.com でしたが、5.2以降で同じような結果を得るためには\n\u0026lt;?php $u = url(\u0026#39;/\u0026#39;); echo $u; // https://hogehoge.com とする必要があります。 url() のまま放置して、その値に文字列結合をしている場合、Object of class Illuminate\\Routing\\UrlGenerator could not be converted to string と言われます。\nLaravelの恩恵を受けきれていない？ ざっと動かした感じでは致命的なものはこれくらいでした。 おそらく、フレームワークに依存してないのが良かったと思われます。 逆に言うと、Laravelの恩恵を受けきれていないのかもしれません。\n何かあれば追記していきます。\n",
    "permalink": "https://michimani.net/post/php-update-laravel-5.1-to-5.5/",
    "title": "Laravel5.1から5.5へアップグレード"
  },
  {
    "contents": "バージョンの値が正しいかどうかをチェックする正規表現です。\nまた、そもそもバージョンの付け方についても調べてみました。\nバージョンとは アプリやWebサービスをリリースするときに付与する値のことです。 例えば、2017年12月4日時点のChromeの最新バージョンは 62.0.3202.94 となっています。\nバージョニング方法 バージョンというのは以前のバージョンよりも小さい値を小さい値を設定してはいけません。 これは、ユーザの誤解を招くとともに、バージョン管理という概念が崩れてしまいます。 たとえ中身を前の状態に戻す場合であっても、バージョンの値は以前よりも大きい値を付ける必要があります。\nまた、バージョンの形式も同じである必要があります。 形式とは、ピリオドによる区切られ方 のことです。 例えば、現在のバージョンが 1.0.0 であるとき、次のバージョンは 1.0.1 や 2.0.0 のように、同じ形式である必要があります。2.0などにしてはいけません。\nこのあたりの話は、 セマンティックバージョニング と呼ばれるバージョンの付け方のドキュメントにわかりやすく書かれています。 セマンティック バージョニング 2.0.0 | Semantic Versioning 正規表現 バージョンの付け方には上記のセマンティックバージョニング以外にも色々あると思いますが、一般的には数値を幾つかのピリオドで区切った形になります。 区切られた値は0以上の整数値です。（NG: 1.001.1） ピリオドは先頭と末尾には使用できません。連続で使用することもできません。（NG: 1.2.3..4、2.2.5.） 上記を満たす正規表現が下記になります。\n/^([0-9]|[1-9][0-9]{1,})(\\.([0-9]|[1-9][0-9]{1,}))*$/ 前後関係のチェック 新しいバージョンが現在のバージョンより小さい値になっていないか、形式は同じか、をチェックしたいというときに使うスクリプトです。\nPHP版 \u0026lt;?php /** * @param string $new_version // 新しいバージョン * @param string $current_version // 現在のバージョン * @return bool */ public function checkVersion($new_version, $current_version) { // 形式チェック if (substr_count($new_version, \u0026#39;.\u0026#39;) != substr_count($current_version, \u0026#39;.\u0026#39;)) { echo \u0026#39;形式が違う\u0026#39;; return false; } // 前後関係チェック $new_version_arr = explode(\u0026#39;.\u0026#39;, $new_version); $current_version_arr = explode(\u0026#39;.\u0026#39;, $current_version); $chk_tmp_new = \u0026#34;\u0026#34;; $chk_tmp_curr = \u0026#34;\u0026#34;; for ($i = 0; $i \u0026lt; count($new_version_arr); $i++) { $chk_tmp_new = $new_version_arr[$i]; $chk_tmp_curr = $current_version_arr[$i]; if ($i === count($new_version_arr) - 1) { if ((int)$chk_tmp_new \u0026gt;= (int)$chk_tmp_curr) { return true; } } else { if ((int)$chk_tmp_new \u0026gt; (int)$chk_tmp_curr) { return true; } else if ((int)$chk_tmp_new \u0026lt; (int)$chk_tmp_curr) { echo \u0026#39;新しいバージョンのほうが小さい\u0026#39;; return false; } } } return false; } javascript版 /** * @param string new_version // 新しいバージョン * @param string current_version // 現在のバージョン * @return bool */ function CheckVersion(new_version, current_version) { // 形式チェック if (new_version.split(\u0026#34;.\u0026#34;).length != current_version.split(\u0026#34;.\u0026#34;).length) { console.log(\u0026#34;形式が違う\u0026#34;); return false; } // 前後関係チェック var new_version_arr = new_version.split(\u0026#34;.\u0026#34;); var current_version_arr = current_version.split(\u0026#34;.\u0026#34;); var chk_tmp_new = \u0026#34;\u0026#34;; var chk_tmp_curr = \u0026#34;\u0026#34;; for (var i = 0; i \u0026lt; new_version_arr.length; i++) { chk_tmp_new = parseInt(new_version_arr[i], 10); chk_tmp_curr = parseInt(current_version_arr[i], 10); if (i == new_version_arr.length - 1) { if (chk_tmp_new \u0026gt;= chk_tmp_curr) { return true; } } else { if (chk_tmp_new \u0026lt; chk_tmp_curr) { console.log(\u0026#34;新しいバージョンのほうが小さい\u0026#34;); return false; } else if (chk_tmp_new \u0026gt; chk_tmp_curr) { return true; } } } return false; } ",
    "permalink": "https://michimani.net/post/develop-how-to-decide-version/",
    "title": "バージョニング方法、バージョンの前後関係チェックについて"
  },
  {
    "contents": "PHP において strpos と preg_match のどちらが速いのか調べてみました。\n前置き PHPで、ある文字がある文字列の中に含まれているかどうかをチェックするときの話です。\n正規表現でチェックする場合、 preg_match() を使用しますが、単純な文字列をチェックする場合にも preg_match() を使っていました。\nが、preg_match()に関する 公式ドキュメント を見てみると\nヒント ある文字列が他の文字列内に含まれているかどうかを調べるためだけに preg_match() を使うのは避けた方が良いでしょう。 strpos() 関数を使うほうが速くなります。\nとあります。 ではどれだけ差が出るのか、試してみました。\nテスト用ソース \u0026lt;?php date_default_timezone_set(\u0026#39;Asia/Tokyo\u0026#39;); $exec_cnt = 100; echo sprintf(\u0026#39;試行回数%s回%s\u0026#39;, $exec_cnt, \u0026#34;\\n\\n\u0026#34;); $res = 0; $start = microtime(true); for ($n = 0; $n \u0026lt; $exec_cnt; $n++) { $str = sprintf(\u0026#39;今の数字は%sです\u0026#39;, $n); if (preg_match(\u0026#39;/55/\u0026#39;, $n)) { $res++; } } echo sprintf(\u0026#39;preg_match ... res = %s (%s)%s\u0026#39;, $res, sprintf(\u0026#39;%.10f\u0026#39;, (microtime(true) - $start)), \u0026#34;\\n\u0026#34;); $res = 0; $start = microtime(true); for ($n = 0; $n \u0026lt; $exec_cnt; $n++) { $str = sprintf(\u0026#39;今の数字は%sです\u0026#39;, $n); if (strpos($n, \u0026#39;55\u0026#39;) !== false) { $res++; } } echo sprintf(\u0026#39;strpos ... res = %s (%s)%s\u0026#39;, $res, sprintf(\u0026#39;%.10f\u0026#39;, (microtime(true) - $start)), \u0026#34;\\n\u0026#34;); 上の $exec_cnt でループ回数を指定して、かかった時間をそれぞれ計測してみます。 例えば上のソースの通り、100回ループした場合の出力結果は\n試行回数100回 preg_match ... res = 1 (0.0001780987) strpos ... res = 1 (0.0000598431) となり、100回の時点で strpos() のほうが preg_match() より約3倍速いです。以下、試行回数ごとの秒数です。 ※実行したのはMacのローカル環境です\n試行回数 preg_match()\u0026hellip;p strpos()\u0026hellip;s p/s 1000 0.0008649826 0.0005629063 1.53 10000 0.0074071884 0.0050859451 1.47 100000 0.0710110664 0.0543570518 1.31 1000000 0.6552450657 0.4806499481 1.36 \u0026hellip; 100000000 70.5298569202 47.3726339340 1.49 試行回数が少ないときのほうが差は大きくなるようですが、strpos() のほうが preg_match()よりも、平均して1.4倍ほど速いようです。\nやっぱり公式ドキュメントはしっかり読んだほうがいいですね。\n",
    "permalink": "https://michimani.net/post/php-conpare-strpos-pregmatch/",
    "title": "【PHP】strposはpreg_matchよりどれくらい速いのか"
  },
  {
    "contents": "PNG画像の透過部分以外を白単色で塗りつぶします。（塗りつぶしたように見える状態にします）\n前提 PHP5、PHP7 PHPの標準ライブラリ GD を使用 GDライブラリが使用可能かどうか GDライブラリが使用可能かどうかを調べるには、phpinfo内の GD Support の値を確認します。 enabled であれば使用可能です。\nまた、ターミナル等で以下のコマンドを実行することでも確認できます。\n$ php -m | grep gd $ gd ## 使用可能であれば出力される ※ちなみに、さくらレンタルサーバーではデフォルトで使用可能なようです\nGDライブラリを使用可能にする 上記の方法でGDライブラリが使用不可の状態である場合は、使用可能な状態にします。 実際に試したのは EC2（Amazon Linux）の場合のみなので、それ以外については PHPの公式ページ を確認してください。\nEC2（Amazon Linux）の場合 ## GDインストール (PHP7.1系の場合) $ sudo yum install -y php71-gd ## Apache再起動 $ sudo service httpd restart 塗りつぶす処理 \u0026lt;?php $file_path = \u0026#34;sample.png\u0026#34;; $after_file_path = \u0026#34;sample_after.png\u0026#34;; try { if (!file_exists($file_path)) { throw new Exception(\u0026#39;対象のファイルが存在しません。\u0026#39;); } $canvas = imagecreatefrompng($file_path); imagealphablending($canvas, false); imagesavealpha($canvas, true); if($canvas \u0026amp;\u0026amp; imagefilter($canvas, IMG_FILTER_BRIGHTNESS, 255)) { imagepng($canvas, $after_file_path); imagedestroy($canvas); $res = true; } else { throw new Exception(\u0026#39;失敗しました。\u0026#39;); } } catch (Exception $e) { echo $e-\u0026gt;getMessage(); } タイトルで （塗りつぶしたようにみえる） と書いているとおり、上記処理では、実際に白で塗りつぶしているわけではありません。 やっていることは、透過部分以外の色がついている部分に対して、 輝度を最大にしている ということです。\n処理で言うと\n\u0026lt;?php imagefilter($canvas, IMG_FILTER_BRIGHTNESS, 255) の部分です。\n任意の色で塗りつぶそうかと試行錯誤してみたのですが、どうもうまくいきませんでした。 今回の目的としては白一色にしたかったのでこれで大丈夫そうですが、できれば任意の色で塗りつぶしたいです。\n独り言 アドベントカレンダーの季節なので、今月は ひとりアドベントカレンダー と題して、細かいネタも含めてたくさん書いていこうと思います。\n",
    "permalink": "https://michimani.net/post/php-fill-alpha-channel-of-png/",
    "title": "【PHP】PNG画像の透過部分以外を白で塗りつぶす（塗りつぶしたように見える）"
  },
  {
    "contents": "Laravel プロジェクトで Google の 翻訳 API (Google Cloud Translation API) を実行したときのメモです。\n必要なもの GCP(Google Cloud Platform)で、Google Cloud Translation API用のAPIキーを発行しておきます。\nライブラリをインストール PHP用のライブラリが用意されているので、composerでインストールします。\n$ composer require google/cloud-translate 実装 Laravelで使用することを前提としたソースになりますが、使うライブラリは同じなので他のフレームワークでも同様になると思います。 インストールしたライブラリを使用するためのクラスを作る形で実装します。 .envにAPIキーを記載します。\nGOOGLE_TRANSLATION_API_KEY=ABCDEFGH12345678abcdefgh12345678 クラスを作ります。\n\u0026lt;?php namespace App\\Libs use Google\\Cloud\\Translate\\TranslateClient; // ライブラリの読み込み use Illuminate\\Support\\Facades\\Log; class GoogleTranslateManager { private api_key = \u0026#39;\u0026#39;; public function __construct() { $this-\u0026gt;api_key = env(\u0026#34;GOOGLE_TRANSLATION_API_KEY\u0026#34;); } /** * 単文翻訳 * @param string $text // 翻訳対象テキスト * @param string $lang_code // 翻訳後の言語コード */ public function executeTransrationSingle($text, $lang_code) { try { $translate = new TranslateClient([\u0026#39;key\u0026#39; =\u0026gt; $this-\u0026gt;api_key]); $result = $translate-\u0026gt;translate( $text, [\u0026#39;target\u0026#39; =\u0026gt; $lang_code] ); Log::debug(print_r($result,1)); } catch (\\Exception $e) { // エラー処理 } } /** * 複数文翻訳 * @param array $text_list // 翻訳対象テキストのリスト * @param string $lang_code // 翻訳後の言語コード */ public function executeTransrationMulti($text_list, $lang_code) { try { $translate = new TranslateClient([\u0026#39;key\u0026#39; =\u0026gt; $this-\u0026gt;api_key]); $result = $translate-\u0026gt;translateBatch( $text_list, [\u0026#39;target\u0026#39; =\u0026gt; $lang_code] ); Log::debug(print_r($result,1)); } catch (\\Exception $e) { // エラー処理 } } } 実行結果 \u0026lt;?php $gObj = new GoogleTranslateManager(); $gObj-\u0026gt;executeTransrationSingle(\u0026#39;今日はとても気分が悪いです。\u0026#39;, \u0026#39;pt\u0026#39;); // Array // ( // [source] =\u0026gt; ja // [input] =\u0026gt; 今日はとても気分が悪いです。 // [text] =\u0026gt; Estou hoje muito doente. // [model] =\u0026gt; // ) $gObj-\u0026gt;executeTransrationMulti( [ \u0026#39;昨日はとても気分が良かったです。\u0026#39;, \u0026#39;今日はとても気分が悪いです。\u0026#39;, \u0026#39;明日はいい気分になりたいです。\u0026#39; ], \u0026#39;es\u0026#39; ); // Array // ( // [0] =\u0026gt; Array // ( // [source] =\u0026gt; ja // [input] =\u0026gt; 今日はとても気分が悪いです。 // [text] =\u0026gt; Estoy muy enfermo hoy. // [model] =\u0026gt; // ) // // [1] =\u0026gt; Array // ( // [source] =\u0026gt; ja // [input] =\u0026gt; 明日はいい気分になりたいです。 // [text] =\u0026gt; Me gustaría sentirme bien mañana. // [model] =\u0026gt; // ) // // [2] =\u0026gt; Array // ( // [source] =\u0026gt; ja // [input] =\u0026gt; 昨日はとても気分が良かったです。 // [text] =\u0026gt; Me sentí muy bien ayer. // [model] =\u0026gt; // ) // ) 複数文翻訳の場合、指定できる配列の要素数は128が最大のようです。\n１つのリクエストで別々に翻訳することができるので、複数の単語を翻訳するときには便利かもしれません。\n",
    "permalink": "https://michimani.net/post/php-exec-google-cloud-translate-api/",
    "title": "PHP(Laravel5.x)でGoogle Cloud Translation APIを実行する（単文・複数文）"
  },
  {
    "contents": "LaravelでExcelファイルを簡単に操作できるライブラリ Laravel-Excel を使ってxlsxファイルのデータを読み込む処理を作ったときのメモです。\nLaravel-Excelが使える状態にするまでのことについては公式ページを参照してください。 Getting started - Laravel Excel Documentation - Maatwebsite 前提 formからpostされたxlsxファイルを読み込む 対象のデータが入っているのは一番左のシート xlsxファイルの中身は下記のようなデータ 読み込み処理 ※エラー処理は仮です\n\u0026lt;?php namespace App\\Http\\Controllers; use Illuminate\\Http\\Request; use App\\Http\\Controllers\\Controller; use Excel; class SampleController extends Controller { public function post(Request $request) { try { $reader = Excel::load($request-\u0026gt;file(\u0026#39;xlsx_file\u0026#39;)-\u0026gt;getRealPath()); if ($reader == null) { throw new \\Exception(\u0026#39;error.\u0026#39;); } // ファイル内のシートの枚数によって $reader-\u0026gt;all() が返すオブジェクトのクラスが異なる if (preg_match(\u0026#39;/SheetCollection$/\u0026#39;, get_class($reader-\u0026gt;all()))) { // シートが複数ある場合 $sheet = $reader-\u0026gt;first(); } else if (preg_match(\u0026#39;/RowCollection$/\u0026#39;, get_class($reader-\u0026gt;all()))) { // シートが1枚の場合 $sheet = $reader; } else { throw new \\Exception(\u0026#39;error.\u0026#39;); } $data = []; foreach ($sheet-\u0026gt;all() as $cells) { $data[] = $cells-\u0026gt;all(); } Log::debug(print_r($data,true)); } catch (\\Exception $e) { Log::error($e-\u0026gt;getMessage()); } } } ログには下記のように出力されます。\nlocal.DEBUG: Array ( [0] =\u0026gt; Array ( [route_name] =\u0026gt; 銀座線 [symbol] =\u0026gt; G [color] =\u0026gt; オレンジ ) [1] =\u0026gt; Array ( [route_name] =\u0026gt; 丸ノ内線 [symbol] =\u0026gt; M,Mb [color] =\u0026gt; レッド ) [2] =\u0026gt; Array ( [route_name] =\u0026gt; 日比谷線 [symbol] =\u0026gt; H [color] =\u0026gt; シルバー ) [3] =\u0026gt; Array ( [route_name] =\u0026gt; 東西線 [symbol] =\u0026gt; T [color] =\u0026gt; スカイブルー ) [4] =\u0026gt; Array ( [route_name] =\u0026gt; 千代田線 [symbol] =\u0026gt; C [color] =\u0026gt; グリーン ) [5] =\u0026gt; Array ( [route_name] =\u0026gt; 有楽町線 [symbol] =\u0026gt; Y [color] =\u0026gt; ゴールド ) [6] =\u0026gt; Array ( [route_name] =\u0026gt; 半蔵門線 [symbol] =\u0026gt; Z [color] =\u0026gt; パープル ) [7] =\u0026gt; Array ( [route_name] =\u0026gt; 南北線 [symbol] =\u0026gt; N [color] =\u0026gt; エメラルドグリーン ) [8] =\u0026gt; Array ( [route_name] =\u0026gt; 副都心線 [symbol] =\u0026gt; F [color] =\u0026gt; ブラウン ) ) ヘッダーに入力する文字の注意点 1行目はヘッダーとなり、連想配列のキーになります。ただし、全角文字、空白がヘッダー行のセルに含まれていた場合、キーとしては 0 にキャストされます。 例えば上の例で、ヘッダー行を 路線名、記号、 色 としていた場合のログ出力結果は次のようになります。\nlocal.DEBUG: Array ( [0] =\u0026gt; Array ( [0] =\u0026gt; オレンジ ) [1] =\u0026gt; Array ( [0] =\u0026gt; レッド ) [2] =\u0026gt; Array ( [0] =\u0026gt; シルバー ) [3] =\u0026gt; Array ( [0] =\u0026gt; スカイブルー ) [4] =\u0026gt; Array ( [0] =\u0026gt; グリーン ) [5] =\u0026gt; Array ( [0] =\u0026gt; ゴールド ) [6] =\u0026gt; Array ( [0] =\u0026gt; パープル ) [7] =\u0026gt; Array ( [0] =\u0026gt; エメラルドグリーン ) [8] =\u0026gt; Array ( [0] =\u0026gt; ブラウン ) ) -（半角ハイフン）については_（アンダースコア）に置き換えられるようです。\n※ヘッダー行のセル「route-name」→配列のキーroute_name 文字中に空白が含まれる、またはセル自体が空白の場合は 0 にキャストされます。\n※ヘッダー行のセル「route name」→配列のキー0\n※ヘッダー行のセル「」→配列のキー0 ",
    "permalink": "https://michimani.net/post/php-import-excel-in-laravel-project/",
    "title": "Laravel-Excelを使ってxlsxファイルのデータを読み込む"
  },
  {
    "contents": "Gitでソース管理をするにあたって、AWSにも CodeCommit というGitリポジトリを作れるサービスがあります。実際にリポジトリを作ってsshで接続できるようにするまでのメモです。\nAWS CodeCommit AWS CodeCommit は、完全マネージド型ソースコントロールサービスで、安全で非常にスケーラブルなプライベート Git リポジトリを簡単にホスティングできます。 Amazon CodeCommit（安全でスケーラブルなマネージド型ソースコントロールサービス） | AWS 料金 アクティブユーザが5人まで 無料 特典 無制限のリポジトリ 50 GB のストレージ/月 10,000 回 のGitリクエスト/月 アクティブユーザが6人以上 1 USD/月 特典 無制限のリポジトリ アクティブユーザーごとに 10 GB のストレージ/月 アクティブユーザーごとに 2000 回 の Git リクエスト/月 つまり、個人で作ったコードを管理する分には、ほぼ無料の範囲内で利用できます。月に10,000回もgitリクエストを投げることは無いと思うので..。\n仮に10,000回を超えたとしても、0.001 USD/Gitリクエスト なので大した料金にはならないでしょう。 ストレージについても、超過分については 0.06 USD/GB となっています。\n類似サービスとの比較 類似のGitホスティングサービスとしては、Web上で利用できる GitHub や、サーバにインストールして使用する Gogs などがありますが、それらとの比較を簡単に書きます。(個人で使用する前提です)\n料金がほぼ無料（ GitHubはプライベートリポジトリを作成する場合 $7/月 ）\n2019年1月から、 GitHub でも無料でプライベートリポジトリを無制限に作成できるようになりました。\nNew year, new GitHub: Announcing unlimited free private repos and unified Enterprise offering 導入が簡単（GitHubは説明がEnglish、Gogsはサーバ(PC)へのインストールが必要） プルリクエスト機能がない（GitHub、Gogs共に有り）\nプルリクエストの機能が使用可能になりました この他、Commit履歴の可視化や、タグ・ブランチの管理など、基本的な操作はGUIでできます。\nAWS特有の機能としては、ブランチへのpushなどのイベントをトリガーとして、アラートを飛ばしたり、Lambda関数を実行できたりします。\nリポジトリを作成してssh接続できるようにするまでの手順 今すぐ始める をクリック。\nリポジトリ名 と 説明 を入力して リポジトリの作成 をクリック。\nこれでリポジトリが作成されました。 次の画面では https 、 ssh それぞれでの接続方法が書かれているので、この通りに準備をしていきます。\nまず、ssh接続用のキーペアを生成します。既にキーペアを持っている場合は、新たに生成する必要はありません。 ここからはターミナルでの操作になります。(Macでの例です)\n$ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/Users/hogehoge/.ssh/id_rsa): /Users/hogehoge/.ssh/codecommit_rsa ;;;任意のパス、名前を指定します Enter passphrase (empty for no passphrase): ;;;任意のパスフレーズを入力します Enter same passphrase again: ;;;任意のパスフレーズを再度入力します Your identification has been saved in /Users/hogehoge/.ssh/codecommit_rsa. Your public key has been saved in /Users/hogehoge/.ssh/codecommit_rsa.pub. The key fingerprint is: SHA256:Ha6ETkhmC7muDZmmxzZk8DgRi7XZMpMU8khj4f3qzuQ hogehoge@HOGEHOGE.local The key\u0026#39;s randomart image is: +---[RSA 2048]----+ | o .+oo | | ..o..+. o | |.B.++ ..* | |O E..= + . | |.X... o S | |+. + o | | .B + . | |.o =oo | | .++ooo. | +----[SHA256]-----+ これでキーペアが生成されるので、パブリックキーの値をコピーします。\n$ cat ~/.ssh/codecommit_rsa.pub ssh-rsa AAAAB3NzaCino3r4aa2GpYkIpjzJAOOO8VcZw6eYfJr9RmdDNMS2/qcEk1yc2EAAAADAQABAAABAQDe0jBpJOHTSGuWRLmJT3s7RwGXw3rV56D4BDOasaCz24m3HxA8oXtRLsUZmiAf+LODBpoLPxWuBKfH/n1aseCByrmHV3FXhAHqH2v33EgrEeYm07xB1RifJHn5oXfCDpssfKVGd25FBslDgzjBElDjA+BRRGVI3lkitl2su19AtGa9b/jzj6UtdAc7msuR5ZM+MP/8jnq2zuc2YtMopt/YXdMAYuo1T0y6atVDmCqirSXE0+ITezFwzIEiximRkQHtO2B4CzJE9LF6kSD5RfLY9r0XHWcc+RpuO+QlD hogehoge@HOGEHOGE.local AWS マネジメントコンソールに戻って、IAMの画面を表示します。 既存(または新規)のIAMユーザに対して、CodeCommitへのSSH接続キーを設定します。\nIAM \u0026gt; ユーザー \u0026gt; {ユーザ選択} \u0026gt; 認証情報\n先ほどコピーした ~/.ssh/codecommit_rsa.pub の内容を貼り付けて、 SSH公開キーのアップロード をクリックします。\nすると、赤枠の部分に SSHキーID が表示されているので、これをコピーします。\nここから、再度ターミナルでの作業になります。\n$ vi ~/.ssh/config Host git-codecommit.*.amazonaws.com User AAAAAAAAAAAAAA # コピーした SSHキーID を記載 IdentityFile ~/.ssh/codecommit_rsa # 先ほど生成したプライベートキーへのパス ここまでで、sshの設定は完了です。あとは push するなり pull するなり clone するなり、普段通りGitコマンドを叩いてください。\n",
    "permalink": "https://michimani.net/post/aws-using-codecommit/",
    "title": "GitリポジトリとしてAWS CodeCommitを使う"
  },
  {
    "contents": "Amazon LinuxにPython3系をインストールした時のメモです。\n環境 Amazon Linux\nAmazon Linuxではインスタンス起動時点でPython2系が使用可能ですが、これから新しく何かを作ろうとするのであれば3系を使ったほうがよいので、デフォルトでPython3系が動くようにします。\npython3 コマンドで3系を動かすという手もありますが、今回は python コマンドで3系が動くようにします。\nPythonのバージョン管理 pyenv というコマンドラインツールを使用して、Pythonのバージョン管理を行います。\n今回の目的は3系を使用するようにすることですが、こちらのツールを使用すればいつでも簡単にデフォルトの2系、または他のバージョンに切り替えることができます。\n手順 1. pyenvを使用するために必要なライブラリのインストール $ sudo yum install gcc gcc-c++ make git openssl-devel bzip2-devel zlib-devel readline-devel sqlite-devel 2. pyenvのインストール、権限・パスの設定 $ sudo git clone https://github.com/yyuu/pyenv.git /usr/bin/.pyenv $ cd /usr/bin/.pyenv $ sudo mkdir shims $ sudo mkdir versions （任意）phpのexec()でpythonコマンドを実行する際権限のエラーとなったため、.pyenv以下の権限を変更\n$ sudo chown -R ec2-user:apache /usr/bin/.pyenv # あるいはapacheがインストールされていない場合は $ sudo chown -R ec2-user:ec2-user /usr/bin/.pyenv パス設定\n$ vi ~/.bashrc 下記を追記\nexport PYENV_ROOT=\u0026#34;/usr/bin/.pyenv\u0026#34; if [ -d \u0026#34;${PYENV_ROOT}\u0026#34; ]; then export PATH=${PYENV_ROOT}/bin:$PATH eval \u0026#34;$(pyenv init -)\u0026#34; fi $ source ~/.bashrc 3. pythonの任意のバージョンをインストール インストール可能なバージョンの確認\n$ pyenv install --list (...略...) 2.7.12 2.7.13 2.7.14rc1 3.0.1 3.1 3.1.1 3.1.2 3.1.3 3.1.4 3.1.5 (...中略...) 3.5.4 3.6.0 3.6-dev 3.6.1 3.6.2 3.7-dev (...略...) pythonインストール\n$ pyenv install 3.6.2 ※この時点ではまだデフォルトのバージョン $ python -V Python 2.7.12 メインで使用するバージョンの変更\n$ pyenv global 3.6.2 $ python -V Python 3.6.2 元に戻したい場合、他のバージョンを使いたい場合\n$ pyenv versions system * 3.6.2 (set by /usr/bin/.pyenv/version) $ pyenv global {使いたいバージョン(system)} ",
    "permalink": "https://michimani.net/post/aws-using-python3-on-amazonlinux/",
    "title": "Amazon LinuxでPython3系を使う"
  },
  {
    "contents": "EC2 で作った LAMP 環境 + Application Load Balancer で常時 SSL を実現しようとした際に、 http → https のリダイレクトがループしてしまったときの対処方法です。\n前提 SSL証明書はAWSのCertificate Managerで取得 取得したSSL証明書は、 ELB (Application Load Balancer) で使用 詰まった点 単純にhttp→httpsリダイレクトをすると無限ループに陥る 常時SSLにするためには、http→https のリダイレクトが必要ですが、上記の前提で実現しようとするとこのリダイレクトが無限ループとなってしまいます。\nAWSのELBを使用している場合、クライアントからサーバ(EC2インスタンス)へのアクセスの間にELBが入ります。 ここで注意が必要なのが、ELBからEC2への通信はhttpだということです。 つまり、クライアントがhttpsでアクセスしていても、ELBからEC2への通信はhttpとなります。その結果、Apache（EC2）でプロトコルのチェックをしても常にhttp通信だということになります。 そのため、「httpならhttpsへリダイレクト」という条件が無限ループとなってしまうわけです。\nこれを回避するため、常時SSLにしたいディレクトリの.htaccessは下記のように記述します。\nRewriteEngine On RewriteCond %{HTTP_USER_AGENT} !^ELB-HealthChecker RewriteCond %{HTTPS} !=on RewriteCond %{HTTP:X-Forwarded-Proto} !=https RewriteRule ^/?(.*) https://%{HTTP_HOST}/$1 [R=301,L] この記述では、以下の条件に当てはまる場合はhttpsでリダイレクトする という記述です。\nRewriteCond %{HTTP_USER_AGENT} !^ELB-HealthChecker ユーザーエージェントがELB-HealthCheckerから始まらない RewriteCond %{HTTPS} !=on 通信プロトコルがhttpsではない RewriteCond %{HTTP:X-Forwarded-Proto} !=https HTTPヘッダーX-Forwarded-Proto の値が httpsではない 1については後ほど書きます。\n2, 3が常時SSLに関わる部分です。 特に3つ目の条件は、AWSでELBを使用してhttps通信を実現する際の回避策としては必須となっているようです。(現状では) HTTPヘッダーX-Forwarded-Protoは、ELBからEC2へ通信する際に付与されるヘッダー情報で、クライアントからELBへの通信がhttpだった場合のみ、httpという値になる というものです。 この値を判定して、httpsへのリダイレクトを実現します。\n(2はELBを通している以上、不要？かもしれません)\nELBのヘルスチェックがエラーになる 単純にhttp→httpsのリダイレクトを実現するためには前項の2, 3のみでよいのですが、その状態で運用しているとELBのヘルスチェックがエラーとなってしまいます。\nELBのヘルスチェックとは、定期的にELBからEC2に通信を行い、返ってくるレスポンスコードによって正常かどうかを判定しています。 ヘルスチェックでは、返ってくるコードが200以外の場合はエラーとするようです。 前項で書いた1の条件が無いと、ヘルスチェックのための通信もhttp→httpsリダイレクトされ、ELBに返すコードとしては301になってしまいます。\n1は、これを回避するためにアクセス元のUserAgentを見て「ヘルスチェックの際のUserAgentであるELB-HealthCheckerの場合はリダイレクトをしない」という記述になります。\n参考URL AWSで構築されたサイトをSSL化しようとしたらリダイレクトループではまった - Qiita ",
    "permalink": "https://michimani.net/post/aws-always-ssl-on-ec2/",
    "title": "Amazon EC2で常時SSLを実現する際の注意点"
  },
  {
    "contents": "AWS EC2にLAMP環境を構築したときのメモです。DBも、RDSを使用せずにEC2に入れてます。\n作りたい環境 Amazon Linux Apache2.4 MySQL 5.6 PHP7 前提 EC2インスタンスが起動している ターミナルからSSHでログインできる 手順 必要なものをインストール ログイン\n~ $ ssh -i .ssh/your_key.pem ec2-user@{インスタンスのIP} yumアップデート\n[ec2-user@ip-***** ~]$ sudo yum update -y Apache、MySQL、PHP、MySQLドライバのインストール\n[ec2-user@ip-***** ~]$ sudo yum install -y httpd24 php70 mysql56-server php70-mysqlnd Apache、PHP、MySQLのバージョン確認\n# Apache [ec2-user@ip-***** ~]$ httpd -v Server version: Apache/2.4.25 (Amazon) Server built: Jan 19 2017 16:55:49 # PHP [ec2-user@ip-***** ~]$ php -v PHP 7.0.16 (cli) (built: Mar 6 2017 19:45:42) ( NTS ) Copyright (c) 1997-2017 The PHP Group Zend Engine v3.0.0, Copyright (c) 1998-2017 Zend Technologies # MySQL [ec2-user@ip-***** ~]$ sudo service mysqld start ... ... ... Starting mysqld: [ OK ] [ec2-user@ip-***** ~]$ mysql --version mysql Ver 14.14 Distrib 5.6.36, for Linux (x86_64) using EditLine wrapper Apacheの設定 起動・スタートページの表示 # サービス開始 (起動はするが ServerNameを設定してください と出る(一旦スルー)) [ec2-user@ip-***** ~]$ sudo service httpd start Starting httpd: AH00557: httpd: apr_sockaddr_info_get() failed for ip-****** AH00558: httpd: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 127.0.0.1. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message [ OK ] #ブラウザで EC2のIPアドレスにアクセスして、Apaceのスタートページが表示されることを確認する # ドキュメントルートの確認 [ec2-user@ip-***** ~]$ sudo cat /etc/httpd/conf/httpd.conf | less # 「/DocumentRoot」 で検索 DocumentRoot \u0026#34;/var/www/html\u0026#34; # 起動設定 [ec2-user@ip-***** ~]$ sudo chkconfig httpd on # 確認 (2, 3, 4, 5がONになっていればOK) [ec2-user@ip-***** ~]$ chkconfig httpd 0:off\t1:off\t2:on\t3:on\t4:on\t5:on\t6:off グループ設定 # ec2-user を apache グループに追加 [ec2-user@ip-***** ~]$ sudo usermod -a -G apache ec2-user # 一旦ログアウトして再度ログイン [ec2-user@ip-***** ~]$ groups ec2-user wheel apache # /var/www以下の権限確認 [ec2-user@ip-***** ~]$ ls -l /var/www/ total 20 drwxr-xr-x 2 root root 4096 Jan 19 16:56 cgi-bin drwxr-xr-x 3 root root 4096 Jun 26 06:18 error drwxr-xr-x 2 root root 4096 Jan 19 16:56 html drwxr-xr-x 3 root root 4096 Jun 26 06:18 icons drwxr-xr-x 2 root root 4096 Jun 26 06:18 noindex # apacheグループに /var/www 所有・書き込み権限付与 [ec2-user@ip-***** ~]$ sudo chown -R ec2-user:apache /var/www [ec2-user@ip-***** ~]$ sudo chmod 2775 /var/www/ # /var/www 以下のディレクトリの権限変更 [ec2-user@ip-***** ~]$ find /var/www -type d -exec sudo chmod 2775 {} \\; # /var/www 以下のファイルの権限変更 [ec2-user@ip-***** ~]$ find /var/www -type f -exec sudo chmod 0664 {} \\; httpd.confの設定 # サーバ管理者メールアドレス 変更 87行目あたり ServerAdmin your_email@example.com # サーバ名 (ドメイン設定してから) # クロスサイトトレーシング対策 追記 TraceEnable Off # ディレクトリ一覧を非表示 変更 145行目あたり Options -Indexes +FollowSymLinks # cgi-bin使用しない 248〜260行目あたり # ScriptAlias /cgi-bin/ \u0026#34;/var/www/cgi-bin/\u0026#34; #\u0026lt;Directory \u0026#34;/var/www/cgi-bin\u0026#34;\u0026gt; # AllowOverride None # Options None # Require all granted #\u0026lt;/Directory\u0026gt; security.confの設定(新規作成) ServerTokens Prod Header unset X-Powered-By # httpoxy 対策 RequestHeader unset Proxy # クリックジャッキング対策 Header append X-Frame-Options SAMEORIGIN # XSS対策 Header set X-XSS-Protection \u0026#34;1; mode=block\u0026#34; Header set X-Content-Type-Options nosniff # XST対策 TraceEnable Off \u0026lt;Directory /var/www/html\u0026gt; # .htaccess の有効化 AllowOverride All # ファイル一覧出力の禁止 Options -Indexes \u0026lt;/Directory\u0026gt; Welcomeページ非表示 /etc/httpd/conf.d/welcome.conf 中身をコメントアウト\nApache再起動\n[ec2-user@ip-***** ~]$ sudo service httpd restart phpinfoの設置 [ec2-user@ip-***** ~]$ vi /var/www/html/phpinfo.php \u0026lt;?php echo phpinfo(); # ブラウザで IPアドレス/phpinfo.php にアクセス # 確認後、phpinfo.phpを削除 [ec2-user@ip-***** ~]$ rm /var/www/html/phpinfo.php PHP設定 [ec2-user@ip-***** ~]$ sudo cp /etc/php.ini /etc/php.ini.org # タイムゾーン設定 - ;date.timezone = + date.timezone = Asia/Tokyo # 日本語設定 - ;mbstring.internal_encoding = + mbstring.internal_encoding = UTF-8 - ;mbstring.language = Japanese + mbstring.language = Japanese - ;mbstring.http_input = + mbstring.http_input = auto - ;mbstring.detect_order = auto + mbstring.detect_order = auto MySQLの設定 起動 [ec2-user@ip-***** ~]$ sudo /etc/init.d/mysqld start ... Starting mysqld: [ OK ] 起動設定 [ec2-user@ip-***** ~]$ sudo chkconfig mysqld on 初期設定 [ec2-user@ip-***** ~]$ sudo mysql_secure_installation ... ... Enter current password for root (enter for none): # Enterキー Set root password? [Y/n] # rootのパスワードを変更するか。 # Y New password: # 任意のパスワード入力 Re-enter new password: # もう一度入力 Password updated successfully! Remove anonymous users? [Y/n] # 匿名ユーザーを削除するかどうか。 # Y Disallow root login remotely? [Y/n] # rootユーザーのリモートホストからのログインを無効化するかどうか。 # Y Remove test database and access to it? [Y/n] # testデータベースを削除するか。 # Y Reload privilege tables now? [Y/n] # これらの変更を即座に反映するか。 # Y ... Thanks for using MySQL! Cleaning up... mysqlへログイン [ec2-user@ip-***** ~]$ mysql -u root -p Enter password: # 初期設定で設定したパスワード Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 14 Server version: 5.6.36 MySQL Community Server (GPL) Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. ... mysql\u0026gt; アプリケーション用データベース追加 # 現在の状態を確認 mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ # データベース「test_app_db」を追加 mysql\u0026gt; create database test_app_db character set utf8; # 確認 mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | test_app_db | +--------------------+ アプリケーションユーザ追加 # 現在のユーザ設定確認 mysql\u0026gt; select Host, User, Password from mysql.user; +-----------+------+-------------------------------------------+ | Host | User | Password | +-----------+------+-------------------------------------------+ | localhost | root | *1234567890ABCDEFGHIJ0987654321KLMNOPQRST| | 127.0.0.1 | root | *1234567890ABCDEFGHIJ0987654321KLMNOPQRST| | ::1 | root | *1234567890ABCDEFGHIJ0987654321KLMNOPQRST| +-----------+------+-------------------------------------------+ # ユーザ「app_user」を追加 (ユーザ名は16文字以内) mysql\u0026gt; create user \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;任意のパスワード\u0026#39;; # データベース「test_app_db」にのみアクセス可能 # 権限は、ALL mysql\u0026gt; grant ALL on test_app_db.* to \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39;; # 設定の反映 mysql\u0026gt; flush privileges; # 権限の確認 mysql\u0026gt; show grants for \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39;; +--------------------------------------------------------------------------------------------------------------------+ | Grants for app_user@localhost | +--------------------------------------------------------------------------------------------------------------------+ | GRANT USAGE ON *.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY PASSWORD \u0026#39;*ABCDEFGHIJ0987654321KLMNOPQRST1234567890\u0026#39; | | GRANT ALL PRIVILEGES ON `test_app_db`.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; | +--------------------------------------------------------------------------------------------------------------------+ # ログアウト mysql\u0026gt; quit Bye # アプリケーションユーザでログイン [ec2-user@ip-***** ~]$ mysql -u app_user -p # データベースの確認 mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | test_app_db | +--------------------+ mysql\u0026gt; quit phpMyAdminのインストール # 「EPEL」リポジトリ有効化 [ec2-user@ip-***** ~]$ sudo yum-config-manager --enable epel # phpMyAdminインストール [ec2-user@ip-***** ~]$ sudo yum install -y phpMyAdmin # エラーになった Error: php70-common conflicts with php-common-5.3.29-1.8.amzn1.x86_64 Error: php56-common conflicts with php-common-5.3.29-1.8.amzn1.x86_64 Error: php56-process conflicts with php-process-5.3.29-1.8.amzn1.x86_64 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest # 手動でインストール [ec2-user@ip-***** ~]$ cd /var/www/html [ec2-user@ip-***** html]$ sudo wget https://files.phpmyadmin.net/phpMyAdmin/4.6.6/phpMyAdmin-4.6.6-all-languages.tar.gz [ec2-user@ip-***** html]$ sudo tar xzvf phpMyAdmin-4.6.6-all-languages.tar.gz [ec2-user@ip-***** html]$ sudo mv phpMyAdmin-4.6.6-all-languages phpMyAdmin [ec2-user@ip-***** html]$ sudo rm phpMyAdmin-4.6.6-all-languages.tar.gz [ec2-user@ip-***** html]$ cd phpMyAdmin [ec2-user@ip-***** phpMyAdmin]$ sudo cp config.sample.inc.php config.inc.php # IPアドレス/phpMyAdmin でアクセス # PHPの「mbstring」拡張が無いというエラーが出るのでインストール [ec2-user@ip-***** ~]$ sudo yum install -y php70-mbstring # Apace再起動 [ec2-user@ip-***** ~]$ sudo service httpd restart # .htaccessでphpMyAdminへのアクセス制限 [ec2-user@ip-***** phpMyAdmin]$ sudo vi .htaccess order deny,allow deny from all allow from **.***.***.*** # ブラウザで [IPアドレス]/phpMyAdmin にアクセスしてログインできるか確認 phpMyAdminの環境保護領域設定 ブラウザでアクセスした際、画面下に「phpMyAdmin 環境保管領域が完全に設定されていないため、いくつかの拡張機能が無効になっています。」というメッセージが表示されていた場合、初期設定が必要。\nまず、phpMyAdminのディレクトリ内にある create_tables.sql を流す /phpMyAdmin/sql/create_tables.sql\n# アプリケーションユーザに phpMyAdminテーブルへのアクセス権を付与する [ec2-user@ip-***** ~]$ mysql -u root -p # 現在の権限確認 mysql\u0026gt; show grants for \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39;; +--------------------------------------------------------------------------------------------------------------------+ | Grants for app_user@localhost | +--------------------------------------------------------------------------------------------------------------------+ | GRANT USAGE ON *.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY PASSWORD \u0026#39;*ABCDEFGHIJ0987654321KLMNOPQRST1234567890\u0026#39; | | GRANT ALL PRIVILEGES ON `test_app_db`.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; | +--------------------------------------------------------------------------------------------------------------------+ 2 rows in set (0.00 sec) # select, insert, update ,deleteの権限を付与 mysql\u0026gt; grant select, insert, update, delete on phpmyadmin.* to \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39;; # 設定を反映 mysql\u0026gt; flush privileges; # 確認 mysql\u0026gt; show grants for \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39;; +--------------------------------------------------------------------------------------------------------------------+ | Grants for app_user@localhost | +--------------------------------------------------------------------------------------------------------------------+ | GRANT USAGE ON *.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY PASSWORD \u0026#39;*ABCDEFGHIJ0987654321KLMNOPQRST1234567890\u0026#39; | | GRANT SELECT, INSERT, UPDATE, DELETE ON `phpmyadmin`.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; | | GRANT ALL PRIVILEGES ON `test_app_db`.* TO \u0026#39;app_user\u0026#39;@\u0026#39;localhost\u0026#39; | +--------------------------------------------------------------------------------------------------------------------+ 3 rows in set (0.00 sec) # config.inc.php の編集 [ec2-user@ip-***** ~]$ sudo vi /var/www/html/phpMyAdmin/config.inc.php # 以下の記述のコメントアウトを解除する /* Storage database and tables */ // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;pmadb\u0026#39;] = \u0026#39;phpmyadmin\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;bookmarktable\u0026#39;] = \u0026#39;pma__bookmark\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;relation\u0026#39;] = \u0026#39;pma__relation\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;table_info\u0026#39;] = \u0026#39;pma__table_info\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;table_coords\u0026#39;] = \u0026#39;pma__table_coords\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;pdf_pages\u0026#39;] = \u0026#39;pma__pdf_pages\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;column_info\u0026#39;] = \u0026#39;pma__column_info\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;history\u0026#39;] = \u0026#39;pma__history\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;table_uiprefs\u0026#39;] = \u0026#39;pma__table_uiprefs\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;tracking\u0026#39;] = \u0026#39;pma__tracking\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;userconfig\u0026#39;] = \u0026#39;pma__userconfig\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;recent\u0026#39;] = \u0026#39;pma__recent\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;favorite\u0026#39;] = \u0026#39;pma__favorite\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;users\u0026#39;] = \u0026#39;pma__users\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;usergroups\u0026#39;] = \u0026#39;pma__usergroups\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;navigationhiding\u0026#39;] = \u0026#39;pma__navigationhiding\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;savedsearches\u0026#39;] = \u0026#39;pma__savedsearches\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;central_columns\u0026#39;] = \u0026#39;pma__central_columns\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;designer_settings\u0026#39;] = \u0026#39;pma__designer_settings\u0026#39;; // $cfg[\u0026#39;Servers\u0026#39;][$i][\u0026#39;export_templates\u0026#39;] = \u0026#39;pma__export_templates\u0026#39;; 設定ファイル用パスフレーズの設定 ログイン時に「設定ファイルに、暗号化 (blowfish_secret) 用の非公開パスフレーズの設定を必要とするようになりました。」というメッセージが表示されている場合、設定ファイルにパスフレーズを追記する必要がある。\n[ec2-user@ip-***** ~]$ sudo vi /var/www/html/phpMyAdmin/config.inc.php # 下記の部分に任意の文字列(32文字以上)を入力 /** * This is needed for cookie based authentication to encrypt password in * cookie. Needs to be 32 chars long. */ $cfg[\u0026#39;blowfish_secret\u0026#39;] = \u0026#39;\u0026#39;; /* YOU MUST FILL IN THIS FOR COOKIE AUTH! */ パスワード自動生成 その他 時刻設定 [ec2-user@ip-***** ~]$ sudo vi /etc/sysconfig/clock ZONE=\u0026#34;Asia/Tokyo\u0026#34; UTC=false [ec2-user@ip-***** ~]$ sudo cp /usr/share/zoneinfo/Japan /etc/localtime [ec2-user@ip-***** ~]$ sudo /etc/init.d/crond restart [ec2-user@ip-***** ~]$ date Tue Jun 27 10:20:25 JST 2017 git [ec2-user@ip-***** ~]$ sudo yum install -y git [ec2-user@ip-***** ~]$ git --version git version 2.7.5 composer [ec2-user@ip-***** ~]$ curl -sS https://getcomposer.org/installer | php All settings correct for using Composer Downloading... Composer (version 1.4.2) successfully installed to: /home/ec2-user/composer.phar Use it: php composer.phar [ec2-user@ip-***** ~]$ sudo mv composer.phar /usr/local/bin/composer [ec2-user@ip-***** ~]$ composer ______ / ____/___ ____ ___ ____ ____ ________ _____ / / / __ \\/ __ `__ \\/ __ \\/ __ \\/ ___/ _ \\/ ___/ / /___/ /_/ / / / / / / /_/ / /_/ (__ ) __/ / \\____/\\____/_/ /_/ /_/ .___/\\____/____/\\___/_/ /_/ Composer version 1.4.2 2017-05-17 08:17:52 参考URL Amazon Linux への LAMP ウェブサーバーのインストール EC2にMySQLインストールと設定確認 MySQL 5.6 リファレンスマニュアル :: 6.3.2 ユーザーアカウントの追加 [MySQL]データベースの作成、ユーザの追加と権限の設定 AWSのApache2.4の初期設定とセキュリティ設定 Apacheセキュリティ設定 phpMyAdminの使い方 ",
    "permalink": "https://michimani.net/post/aws-lamp-environment-on-ec2/",
    "title": "AWS EC2にLAMP環境を構築するまで"
  },
  {
    "contents": "「さくらレンタルサーバー　Laravel」で検索すると様々な内容がでてきますが、実際に自分でやった内容についてまとめておきたいと思います。\n前提 環境 さくらレンタルサーバー (スタンダードプラン) PHP5.6 (5.6以上であればOK) MySQL5.5 運用方法 ドメインを追加して、そこで運用\n具体的には、さくらのコントロールパネルから hogehoge.hoge というドメインを追加して、指定フォルダを /home/{アカウント名}/www/hoge/public/ にします。\nやったこと Laravelプロジェクトの作成 まずはLaravelプロジェクトの作成についてですが、これについては既に他の方々が書かれている内容の通りに進めれば大丈夫です。(ここは丸投げで)\n簡単な流れだけ書いておきます。\nさくらレンタルサーバーに composer をインストールする composer を使用して Laravelプロジェクト を作成する 以上です。(ざっくりと)\n僕が参考にさせていただいたのは下記の記事です。ありがとうございました。 さくらのレンタルサーバー Laravel5 インストール 上記記事では /home/{アカウント名}/ 配下にプロジェクトを作成していますが、前提条件で書きました通り、 /home/{アカウント名}/www/ 配下に hoge というプロジェクトを作るという形で話を進めます。\nということで、上記記事で書かれている「publicディレクトリのコピー」は不要です。\n.htaccessの内容変更 まず、よく書かれている public/.htaccess の修正内容です。 Laravelプロジェクトをインストールした状態からの差分は以下のとおりです。\n\u0026lt;IfModule mod_rewrite.c\u0026gt; \u0026lt;IfModule mod_negotiation.c\u0026gt; Options -MultiViews \u0026lt;/IfModule\u0026gt; RewriteEngine On # Redirect Trailing Slashes If Not A Folder... RewriteCond %{REQUEST_FILENAME} !-d RewriteRule ^(.*)/$ /$1 [L,R=301] + RewriteBase / # Handle Front Controller... RewriteCond %{REQUEST_FILENAME} !-d RewriteCond %{REQUEST_FILENAME} !-f RewriteRule ^ index.php [L] \u0026lt;/IfModule\u0026gt; さくらレンタルサーバーでLaravelを動かす記事でよく書かれているのが\nOptions -MultiViews 部分をコメントアウトする\nという内容ですが、今年の3月中旬以降、以下のようにさくらレンタルサーバーにてApacheのシステム設定変更があり、 .htaccess 内で、条件付きではありますが Options の指定が可能になっています。\n※1 .htaccess にて Options が設定可能になります。 自動インデックス機能がデフォルトで無効となります。 詳細な情報は後日公開いたします。 2017年02月28日追記 Options では、All,FollowSymLinks 以外の機能が設定可能です。\nメンテナンス・障害情報・機能追加｜さくらインターネット公式サポートサイト DB設定 基本的には .env の内容変更となります。\nAPP_ENV=local APP_DEBUG=true APP_KEY=*********************************** DB_CONNECTION=mysql - DB_HOST=127.0.0.1 + DB_HOST=mysql***.db.sakura.ne.jp // コントロールパネル \u0026gt; データベースの設定 で確認 - DB_DATABASE=homestead + DB_USERNAME=hogehogeid - DB_PASSWORD=homestead + DB_PASSWORD=hogehogepass CACHE_DRIVER=file SESSION_DRIVER=file QUEUE_DRIVER=sync REDIS_HOST=127.0.0.1 REDIS_PASSWORD=null REDIS_PORT=6379 MAIL_DRIVER=smtp MAIL_HOST=mailtrap.io MAIL_PORT=2525 MAIL_USERNAME=null MAIL_PASSWORD=null MAIL_ENCRYPTION=null .env の内容変更で接続自体は可能になりますが、php artisan migrate コマンドを実行した際に作成されるテーブルの照合順序を変更する場合は、config/database.php 内での指定が必要です。\n\u0026#39;mysql\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;host\u0026#39; =\u0026gt; env(\u0026#39;DB_HOST\u0026#39;, \u0026#39;localhost\u0026#39;), \u0026#39;database\u0026#39; =\u0026gt; env(\u0026#39;DB_DATABASE\u0026#39;, \u0026#39;forge\u0026#39;), \u0026#39;username\u0026#39; =\u0026gt; env(\u0026#39;DB_USERNAME\u0026#39;, \u0026#39;forge\u0026#39;), \u0026#39;password\u0026#39; =\u0026gt; env(\u0026#39;DB_PASSWORD\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;charset\u0026#39; =\u0026gt; \u0026#39;utf8\u0026#39;, - \u0026#39;collation\u0026#39; =\u0026gt; \u0026#39;utf8_unicode_ci\u0026#39;, + \u0026#39;collation\u0026#39; =\u0026gt; \u0026#39;utf8_general_ci\u0026#39;, \u0026#39;prefix\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;strict\u0026#39; =\u0026gt; false, ], assetsディレクトリ css や js などを格納する assets ディレクトリですが、デフォルトでは resources/assets に設置されています。 が、 blade に用意されている assets() メソッドで参照するのは public ディレクトリ配下ということなので、 resources/assets については使用せず、 public/assets を新たに作成して、そこに css や js を格納していきます。\nまとめ assets ディレクトリを新たに作成したこと以外は、インストール時のディレクトリ構造を変更する必要はありませんでした。 .htaccess についても、さくらレンタルサーバーのApacheシステム変更によりコメントアウトをする必要がなくなりました。追記は必要ですが。 DB設定についても既に多くの方が書いておられる内容、および公式ドキュメントに書かれている内容ですが、照合順序についてはマイグレーションでテーブルを作ったあとに発覚したので、もし変更される方は注意してください。\n",
    "permalink": "https://michimani.net/post/php-run-laravel-at-sakura/",
    "title": "さくらレンタルサーバーでLaravel5.1が動くようになるまでにしたこと"
  },
  {
    "contents": "リストやテーブルの用をの順番をドラッグ\u0026amp;ドロップで入れ替えるUIがよくありますが、その実装方法が思ったより簡単でした。\niPhone, iPadでも移動できます。\n必要なjsファイル jquery.js jquery-ui.js jqyery-ui.touch-punch.js 書き方(リストの場合) \u0026lt;ul id=\u0026#34;sortable-list\u0026#34;\u0026gt; \u0026lt;li\u0026gt;ドラッグ\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;アンド\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;ドロップ\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;移動できる\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;リストです\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; ulに指定するidは任意です。\n$(function() { $(\u0026#39;#sortable-list\u0026#39;).sortable({ items: \u0026#39;li\u0026#39;, cursor: \u0026#39;move\u0026#39;, opacity: 0.5 }); $(\u0026#39;#sortable-first\u0026#39;).disableSelection(); }); 各オプションの意味ですが、\nitems ・・・ 移動させる要素 cursor ・・・ 要素を掴んでいるときにアイコンのようなものを表示するかどうか opacity・・・掴んでいる要素を透過させるかどうか という感じです。\n書き方(テーブルの場合) \u0026lt;table id=\u0026#34;sortable-table\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;ドラッグ\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;アンド\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;ドロップ\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;移動できる\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;テーブルです\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; $(function() { $(\u0026#39;#sortable-table\u0026#39;).sortable({ items: \u0026#39;tr\u0026#39;, cursor: \u0026#39;move\u0026#39;, opacity: 0.5 }); $(\u0026#39;#sortable-first\u0026#39;).disableSelection(); }); iOSでのセレクトリストの代わりになる？ こういった項目の順序入れ替えにはセレクトリストを使用する場合が多いかと思いますが、iOSのSafariではセレクトリストの表示がロール式になっており、順序を入れ替えるという使い方では非常に見にくいことになります。\n項目の順序を入れ替えるという目的であれば、こちらで代用できそうです。\n参考にしたサイト 【jQuery】iPad・iPnoneにも対応！ドラッグ＆ドロップで移動できるリストを作る方法 ",
    "permalink": "https://michimani.net/post/javascript-drag-drop-list-items/",
    "title": "リスト、テーブルの要素をドラッグ\u0026ドロップで移動できるようにする"
  },
  {
    "contents": "タイトルの通りですが、bootstrap標準のmodal.jsを使用して表示したモーダル上で、bootstrap-datepicker.jsを使用した日付入力欄を配置した際に、テキストエリアにフォーカスしてもカレンダー(ぴろっと下に出てくるやつ)が出てこなかったので、その対処方法です。\nhtml \u0026lt;div class=\u0026#34;modal modal-warning fade\u0026#34; id=\u0026#34;modal-id\u0026#34; tabindex=\u0026#34;-1\u0026#34; role=\u0026#34;dialog\u0026#34; aria-labelledby=\u0026#34;modal-label\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-dialog\u0026#34; role=\u0026#34;document\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-content\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-header\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;close\u0026#34; data-dismiss=\u0026#34;modal\u0026#34; aria-label=\u0026#34;Close\u0026#34;\u0026gt; \u0026lt;span aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026amp;times;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;h4 class=\u0026#34;modal-title\u0026#34; id=\u0026#34;modal-label\u0026#34;\u0026gt;MODAL TITLE\u0026lt;/h4\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;form role=\u0026#34;form\u0026#34; action=\u0026#34;\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-body\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;label\u0026gt;DATE\u0026lt;/label\u0026gt; \u0026lt;div class=\u0026#34;input-group date\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;input-group-addon\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-calendar\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; class=\u0026#34;form-control pull-right datepicker-form\u0026#34; id=\u0026#34;datepicker\u0026#34; name=\u0026#34;input_date\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;modal-footer\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-outline pull-left\u0026#34; data-dismiss=\u0026#34;modal\u0026#34;\u0026gt;CANCEL\u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34; class=\u0026#34;btn btn-outline\u0026#34; disabled=\u0026#34;disabled\u0026#34; id=\u0026#34;cmp-btn\u0026#34;\u0026gt;OK\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; こんな感じで、モーダル内に日付入力用のテキストボックスを配置している状態でした。\nちなみに、datepickerのカレンダー部分のソースは以下(のような形)。\n\u0026lt;div class=\u0026#34;datepicker datepicker-dropdown dropdown-menu datepicker-orient-left datepicker-orient-top\u0026#34; style=\u0026#34;display: block; top: 160.333px; left: 417.167px;\u0026#34;\u0026gt; ... \u0026lt;/div\u0026gt; 原因 原因は、モーダル(.modal)のz-indexのほうが、datepickerのカレンダー(.datepicker.dropdown-menu)のz-indexよりも大きかったからです。\n.modal { z-index: 1050; } .datepicker.dropdown-menu { z-index: 1000; } 対策 元のcssは触りたくないので、無理やりですが以下のjsで対応しました。\n$(\u0026#39;.datepicker-form\u0026#39;).focus(function(){ $(\u0026#39;.datepicker-dropdown\u0026#39;).css(\u0026#34;z-index\u0026#34;, \u0026#34;1060\u0026#34;); }); ",
    "permalink": "https://michimani.net/post/css-bootstrap-datepicker/",
    "title": "bootstrapのmodalでdatepickerのカレンダーが出ない"
  },
  {
    "contents": "cakePHP のプロジェクト内で Twitter API を使用できるようにしたときのメモです。\n準備 フレームワークはcakePHPを使用します。 TwitterAPIのライブラリとして、 OAuth consumers for CakePHP を使用します。 こちら からzipファイルをダウンロードして、中にある OAuthフォルダを**app/Vender/**に配置します。\n構成 あくまでも例ですが。\nOAuth認証も含めて、その他TwitterAPIで使用する機能(ツイート、タイムライン取得など)のメソッドをTwitterCompornent.phpに記載して、各コントローラーからそれを使うというイメージです。\n実装例 TwitterCompornent.php \u0026lt;?php // OAuth consumersの読み込み App::import(\u0026#39;Vendor\u0026#39;, \u0026#39;OAuth/OAuthClient\u0026#39;); class TwitterComponent extends Component { public function initialize( Controller $controller ) { $this-\u0026gt;Controller = $controller; } /** アプリケーションのConsumer Key (API Key) */ const TWITTER_CK = \u0026#39;CK1234567890\u0026#39;; /** アプリケーションのConsumer Secret (API Secret) */ const TWITTER_CS = \u0026#39;CS1234567890\u0026#39;; /** アプリケーションのCallback URL */ const TWITTER_CALLBACK_URL = \u0026#39;http://www.example.com/test/callback\u0026#39;; /** アクセストークン取得 **/ const URL_OAUTH_ACCESS_TOKEN = \u0026#39;https://api.twitter.com/oauth/access_token\u0026#39;; /** リクエストトークン取得 **/ const URL_OAUTH_REQUEST_TOKEN = \u0026#39;https://api.twitter.com/oauth/request_token\u0026#39;; /** * ユーザー認証を行う * return array */ public function authorizeUser() { // Twitterオブジェクト作成 $twitterOauthObj = $this-\u0026gt;__createClient( self::TWITTER_CK, self::TWITTER_CS ); $twitterOauthToken = $twitterOauthObj-\u0026gt;post( self::TWITTER_AT, self::TWITTER_AS, self::URL_OAUTH_REQUEST_TOKEN, array( \u0026#39;oauth_callback\u0026#39; =\u0026gt; rawurldecode( self::TWITTER_CALLBACK_URL ) ) ); if ( $twitterOauthToken-\u0026gt;headers[ \u0026#39;status\u0026#39; ] != \u0026#39;200 OK\u0026#39; ) { return false; } else { $return_params = explode( \u0026#39;\u0026amp;\u0026#39;, $twitterOauthToken-\u0026gt;body ); $oauth_info = array(); foreach ( $return_params as $p ) { $p_tmp = explode( \u0026#39;=\u0026#39;, $p ); if ( $p_tmp[ 0 ] == \u0026#39;oauth_callback_confirmed\u0026#39; ) { $oauth_info[ $p_tmp[ 0 ] ] = ( $p_tmp[ 1 ] == \u0026#39;true\u0026#39; ) ? true : false; } else { $oauth_info[ $p_tmp[ 0 ] ] = $p_tmp[ 1 ]; } } } // 認証用URL生成 $authenticate_url = sprintf( \u0026#39;https://api.twitter.com/oauth/authenticate?oauth_token=%s\u0026#39;, $oauth_info[ \u0026#39;oauth_token\u0026#39; ] ); $reutrn_array = array( \u0026#39;authenticate_url\u0026#39; =\u0026gt; $authenticate_url, \u0026#39;user_oauth_session\u0026#39; =\u0026gt; $oauth_info ); return $reutrn_array; } /** * アクセストークンを取得する * @param String $request_token * @param String $request_token_secret * @param String $oauth_verifier * @return array */ public function criateAccessToken( $request_token, $request_token_secret, $oauth_verifier ) { // Twitterオブジェクト作成 $twitterOauthObj = $this-\u0026gt;__createClient( self::TWITTER_CK, self::TWITTER_CS ); $res = $twitterOauthObj-\u0026gt;post( $request_token, $request_token_secret, self::URL_OAUTH_ACCESS_TOKEN, array( \u0026#39;oauth_verifier\u0026#39; =\u0026gt; $oauth_verifier ) ); // レスポンスを配列に変換 $return_params = explode( \u0026#39;\u0026amp;\u0026#39;, $res-\u0026gt;body ); $oauth_info = array(); foreach ( $return_params as $p ) { $p_tmp = explode( \u0026#39;=\u0026#39;, $p ); $oauth_info[ $p_tmp[ 0 ] ] = $p_tmp[ 1 ]; } return $oauth_info; } /** * インスタンス作成 * @param String $ck * @param String $cs * @return OAuthClient */ protected function _createClient( $ck, $cs ) { return new OAuthClient( $ck, //Consumer key $cs //Consumer secret ); } } TestController.php \u0026lt;?php class TestController extends AppController { public $components = array( \u0026#39;Twitter\u0026#39; ); function __construct ( $request, $response ) { parent::__construct( $request, $response ); } /** * ログイン */ public function login () { $this-\u0026gt;autoRender = false; // ユーザー認証 $res = $this-\u0026gt;Twitter-\u0026gt;authorizeUser(); if ( $res == false ) { $this-\u0026gt;redirect( \u0026#39;/\u0026#39; ); } else { // セッション登録 $this-\u0026gt;Session-\u0026gt;write( \u0026#39;user_oauth\u0026#39;, $res[ \u0026#39;user_oauth_session\u0026#39; ] ); // 認証ページヘリダイレクト $this-\u0026gt;redirect( $res[ \u0026#39;authenticate_url\u0026#39; ] ); } } /** * コールバック */ public function callback () { $this-\u0026gt;autoRender = false; try { // 認証キャンセルの場合はセッションを削除してトップページへ if ( isset( $this-\u0026gt;params-\u0026gt;query[ \u0026#39;denied\u0026#39; ] ) ) { if ( $this-\u0026gt;Session-\u0026gt;check( \u0026#39;user_oauth\u0026#39; ) ) { $this-\u0026gt;Session-\u0026gt;delete( \u0026#39;user_oauth\u0026#39; ); } $this-\u0026gt;redirect( \u0026#39;/\u0026#39; ); } // セッション情報取得 if ( !$this-\u0026gt;Session-\u0026gt;check( \u0026#39;user_oauth\u0026#39; ) ) { throw new Exception( \u0026#39;セッション情報を取得できませんでした。\u0026#39; ); } $user_oauth_session = $this-\u0026gt;Session-\u0026gt;read( \u0026#39;user_oauth\u0026#39; ); // Twitterから返却されたOAuthトークンとセッションに保存されたOAuthトークンを比較 $retutn_oauth_token = ( isset( $this-\u0026gt;params-\u0026gt;query[ \u0026#39;oauth_token\u0026#39; ] ) ) ? $this-\u0026gt;params-\u0026gt;query[ \u0026#39;oauth_token\u0026#39; ] : null; if ( $retutn_oauth_token != $user_oauth_session[ \u0026#39;oauth_token\u0026#39; ] ) { // セッション削除 $this-\u0026gt;Session-\u0026gt;delete( \u0026#39;user_oauth\u0026#39; ); throw new Exception( \u0026#39;OAuthトークンが無効です。\u0026#39; ); } // アクセストークンを取得する $access_token = $this-\u0026gt;Twitter-\u0026gt;criateAccessToken( $user_oauth_session[ \u0026#39;oauth_token\u0026#39; ], $user_oauth_session[ \u0026#39;oauth_token_secret\u0026#39; ], $this-\u0026gt;params-\u0026gt;query[ \u0026#39;oauth_verifier\u0026#39; ] ); if ( !$access_token ) { // セッション削除 $this-\u0026gt;Session-\u0026gt;delete( \u0026#39;user_oauth\u0026#39; ); throw new Exception( \u0026#39;アクセストークンが取得できませんでした。\u0026#39; ); } // セッションに保存 $user_oauth_session[ \u0026#39;access_token\u0026#39; ] = $access_token; $this-\u0026gt;Session-\u0026gt;write( \u0026#39;user_oauth\u0026#39;, $user_oauth_session ); // トップページヘリダイレクト $this-\u0026gt;redirect( \u0026#39;/\u0026#39; ); } catch ( Exception $e ) { $this-\u0026gt;log( $e-\u0026gt;getMessage() ); } } } 認証処理が成功すると、セッション情報として以下の形で oauth_token、 oauth_token_secretがそれぞれ取得できます。\n\u0026lt;?php $user_oauth_session = array( \u0026#39;access_token\u0026#39; =\u0026gt; array( \u0026#39;oauth_token\u0026#39; =\u0026gt; \u0026#39;OT1234567890\u0026#39;, \u0026#39;oauth_token_secret\u0026#39; =\u0026gt; \u0026#39;OTS1234567890\u0026#39; ) ); あとはこの２つの値を使ってTwitterAPIを利用することになります。\nちなみに、この方法で実装したアプリケーションがこちらです。 ただ「やっほ〜！」とつぶやくだけ 文字通り、ただやっほ〜とつぶやくだけです。一応つぶやいた数をカウントしているので、どんどんつぶやいてみてください。\n",
    "permalink": "https://michimani.net/post/php-twitter-oauth-cakephp/",
    "title": "cakePHPでTwitterAPIのOAuth認証を実装してみる"
  },
  {
    "contents": "ページ内にある画像を全て保存したいけど いちいち右クリック\u0026gt;名前をつけて保存 とか画像をドラッグアンドドロップでフォルダに移動 とかするのが面倒なときに、対象ページのURLを渡せばページ内の画像を全て保存してくれる処理を考えてみました。\n改良版を作りました 動作環境 ・Mac OS X ・MAMP\nソース ライブラリは使わずに、純粋にPHPだけです。\n\u0026lt;?php /** * @param String $targetUrl */ function getAllImgByUrl( $targetUrl ) { // 画像保存先 $savePath = \u0026#39;/Users/hogehoge/Desktop/savePath/\u0026#39;; //任意の場所 // htmlソース取得 $htmlSource = @file_get_contents( $targetUrl ); if ( $htmlSource ) { // 画像ファイルのURL取得 preg_match_all( \u0026#39;/src=\u0026#34;(.*?(\\.jpg|\\.jpeg|\\.gif|\\.png))\u0026#34;/i\u0026#39;, $htmlSource, $res ); // (\\.jpg|\\.jpeg|\\.gif|\\.png) ここは必要に応じて変更 if ( isset( $res[ 1 ] ) ) { foreach( $res[ 1 ] as $targetImgUrl ) { // ファイル名生成 $fileNameTmp = explode( \u0026#39;/\u0026#39;, $targetImgUrl ); $fileNameTmp = array_reverse( $fileNameTmp ); $fileName = $fileNameTmp[ 0 ]; // 画像保存 $imgData = @file_get_contents( $targetImgUrl ); if ( $imgData ) { @file_put_contents( self::IMG_SAVE_PATH . $fileName, $imgData ); } } } } } ",
    "permalink": "https://michimani.net/post/php-save-all-image-in-a-page/",
    "title": "PHPでWEBページ内の画像ファイルを全て保存してみる"
  },
  {
    "contents": "ちょっと凝ったSQLを作ろうとするときに使う SUM() とか CASE() ですが、いざ使おうとするときにちょっと書き方を忘れたりするので簡単な例で書き留めておきます。\n目次 目次 使うデータ SUM() を使って各商品の合計金額を出してみる SUM() と CASE() を組み合わせてみる 使うデータ 例として使うのは、売上の情報を持っている salesテーブルにあるデータです。\nmysql\u0026gt; desc sales; +---------------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +---------------+--------------+------+-----+---------+----------------+ | id | int(11) | NO | PRI | NULL | auto_increment | | item_name | varchar(100) | NO | | NULL | | | item_quantity | int(11) | NO | | 0 | | | total_price | int(11) | NO | | 0 | | | purchase_date | date | YES | | NULL | | +---------------+--------------+------+-----+---------+----------------+ mysql\u0026gt; SELECT * FROM sales; +----+-------------+---------------+-------------+---------------+ | id | item_name | item_quantity | total_price | purchase_date | +----+-------------+---------------+-------------+---------------+ | 1 | apple | 1 | 100 | 2016-05-02 | | 2 | orange | 10 | 300 | 2016-05-03 | | 3 | watermelon | 2 | 600 | 2016-05-04 | | 4 | apple | 3 | 300 | 2016-05-06 | | 5 | apple | 4 | 400 | 2016-05-06 | | 6 | orange | 5 | 150 | 2016-05-06 | | 7 | watermelon | 2 | 600 | 2016-05-08 | | 8 | strawberry | 10 | 2000 | 2016-05-09 | | 9 | orange | 23 | 690 | 2016-05-10 | | 10 | watermelon | 5 | 1500 | 2016-05-11 | +----+-------------+---------------+-------------+---------------+ SUM() を使って各商品の合計金額を出してみる SUM() を使うときは、 GROUP BY 句もセットで使います。今回は各商品での合計を出すので、 item_name でグループ化します。\nSELECT item_name, SUM(total_price) AS item_total_price FROM sales GROUP BY item_name; +-------------+------------------+ | item_name | item_total_price | +-------------+------------------+ | apple | 800 | | orange | 1140 | | water melon | 2700 | | strawberry | 2000 | +-------------+------------------+ 各日に売れた合計金額を出したい場合は次のようになります。\nSELECT purchase_date, SUM(total_price) FROM sales GROUP BY purchase_date; +---------------+------------------+ | purchase_date | SUM(total_price) | +---------------+------------------+ | 2016-05-02 | 100 | | 2016-05-03 | 300 | | 2016-05-04 | 600 | | 2016-05-06 | 850 | | 2016-05-08 | 600 | | 2016-05-09 | 2000 | | 2016-05-10 | 690 | | 2016-05-11 | 1500 | +---------------+------------------+ SUM() と CASE() を組み合わせてみる 取得するときに条件をつけて取得できる CASE() ですが、**SUM()**と組み合わせるとデータを取得する段階で集計ができるので、取得後の処理が少なくなります。 今回の例では、2016年5月6日の前と後それぞれの売上合計金額を出したいと思います。まず取得結果から見てみます。\n+--------------------------------+-------------------------------+ | sum of total before 2016/05/06 | sum of total after 2016/05/07 | +--------------------------------+-------------------------------+ | 1850 | 4790 | +--------------------------------+-------------------------------+ この結果が得られるSQLは以下のようになります。\nSELECT SUM( CASE WHEN purchase_date \u0026lt;= \u0026#34;2016-05-06\u0026#34; THEN total_price ELSE 0 END ) AS \u0026#34;sum of total before 2016/05/06\u0026#34;, SUM( CASE WHEN purchase_date \u0026gt;= \u0026#34;2016-05-07\u0026#34; THEN total_price ELSE 0 END ) AS \u0026#34;sum of total after 2016/05/07\u0026#34; FROM sales; CASE() の構文は\nCASE WHEN A=B THEN C ELSE D END という形で、A=Bなら C, それ以外の場合は D という意味です。\nなので、上記の例では、 「purchase_date が \u0026ldquo;2016-05-06\u0026rdquo; 以前の場合は total_price, それ以外の場合は 0」 を SUM() の中に入れることで、purchase_date が \u0026ldquo;2016-05-06\u0026quot;以前のtotal_priceの合計 を出すことができます。\nこの方法を使えば、あるフラグを持つ場合のみ加算する といった使い方もできます。\n",
    "permalink": "https://michimani.net/post/database-using-sum-case-in-mysql/",
    "title": "MySQL で SUM() と CASE() を組み合わせてデータを取得する"
  },
  {
    "contents": "さくらレンタルサーバー上にGitリポジトリを作成して色々ファイルを置いてますが、そのGitリポジトリを作るまでの手順をメモ程度に残しておきます。\nやりたいこと さくらレンタルサーバーの初期ドメイン hogehoge.sakura.ne.jp のルートディレクトリは /home/hogehoge/www/ です。(ユーザーIDが hogehoge の場合) 今回、例としてそのルートディレクトリ直下にある /home/hogehoge/www/git_test をGitでバージョン管理したいと思います。\nGitリポジトリ作成手順 1. Git管理したいディレクトリへ移動。初期化。 今回は git_test 以下をGit管理したいので git_test へ移動。\n$ cd /home/hogehoge/www/git_test 移動したら次のコマンドで初期化します。\n$ git init 初期化が完了すると下記メッセージが表示され、ディレクトリ内に .git ディレクトリが作成されます。\nInitialized empty Git repository in /home/hogehoge/www/git_test/.git/ 2. ステータス確認、初回コミット Git管理したいディレクトリの初期化が終わったら、適当なファイルを作ってコミットしてみます。\n$ touch example.txt $ git status On branch master Initial commit Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) example.txt nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) $ git add .　(全てのファイルをインデックスへ追加する) $ git commit -m \u0026#34;first commit\u0026#34;　(コメントは任意) 3. リモートリポジトリを作成 続いてリモートリポジトリの作成ですが、さくらレンタルサーバーの場合は /home/hogehoge/git/ 以下にリモートリポジトリを作ります。\n$ cd /home/hogehoge/git $ mkdir git_test.git $ cd git_test.git $ git init --bare 上記で作成するディレクトリ名は任意ですが、実際にGit管理したいディレクトリと合わせたほうが良いと思います。任意ではありますが、語尾には .git を付けます。\n4. リモートリポジトリへの最初のpush 3で作ったリモートリポジトリと、1のディレクトリを紐付けます。\n$ cd /home/hogehoge/www/git_test $ git remote add origin ssh://hogehoge@hogehoge.sakura.ne.jp/home/hogehoge/git/git_test.git $ git remote -v (設定が出来ているか確認) pushします。\n$ git push origin master:master これで一通りの流れが終わったので、あとはGitのコマンドを駆使してcommit、pushしていきます。\n番外編. リモートリポジトリからcloneする リモートリポジトリからソースをcloneするときは下記コマンドで。\n$ cd /home/hogehoge/www $ git clone /home/hogehoge/git/git_test.git git_test_clone これで /home/hogehoge/www/git_test_clone にソースが落ちてきます。\n",
    "permalink": "https://michimani.net/post/sakura-create-git-repo/",
    "title": "さくらレンタルサーバーにgitリポジトリを作る"
  },
  {
    "contents": "だいぶ前にTwitterAPI1.1で画像付きのツイートをする方法について個人のブログに書きましたが、ほとんど反応がなかったのでこちらに書いてみます。\n実装環境 まず、以下の条件下での方法ですので、ご確認ください。\nフレームワークは CakePHP 2.3.10 APIとのやりとりにはOAuth consumers for CakePHPを使用 OAuth consumers for CakePHPでは基本的に以下のような形でAPIとやりとりします。\n\u0026lt;?php $client = new OAuthClient( $ck, // アプリケーションのConsumer key $cs // アプリケーションのConsumer secret ); $client-\u0026gt;post( $at, // アプリケーションのaccess token $ats, // アプリケーションのaccess token secret $api_url, // 各種APIのリソースURL $params // 各種APIに必要なパラメータ ); 例えば、普通にツイートするときは次のようになります。\n\u0026lt;?php $client-\u0026gt;post( $at, // アプリケーションのaccess token $ats, // アプリケーションのaccess token secret \u0026#39;https://api.twitter.com/1.1/statuses/update.json\u0026#39;, array( \u0026#39;status\u0026#39; =\u0026gt; \u0026#39;Tweet内容\u0026#39; ) ); 画像を1枚添付してツイートする では、画像を１枚添付する場合は、リソースURLが https://api.twitter.com/1.1/statuses/update_with_media.json ですので、以下のようにします。\n\u0026lt;?php $client-\u0026gt;postMultipartFormData( $at, // アプリケーションのaccess token $ats, // アプリケーションのaccess token secret \u0026#39;https://api.twitter.com/1.1/statuses/update_with_media.json\u0026#39;, array( \u0026#39;media[]\u0026#39; =\u0026gt; \u0026#39;添付する画像へのパス\u0026#39; ), array( \u0026#39;status\u0026#39; =\u0026gt; \u0026#39;Tweet内容\u0026#39; ), ); 注意したいのが、普通にツイートする場合と違って、 OAuthClient の post ではなく postMultipartFormData を使うという点です。 ここに気付かなくて大分苦労しました。\n※追記 コメント欄にてご指摘いただきました通り、 TwitterAPIの公式リファレンス POST statuses/update_with_media (deprecated) | Twitter Developers では、update_with_mediaのAPIがdeprecated(非推奨)となっていますので、画像1枚の場合も後述する方法を用いたほうが良さそうです。 @riocamposさん、ありがとうございました。\n画像を複数枚(最大4枚)添付してツイートする １枚の画像がこれでいけるなら、複数画像の場合はarray()の中に追加すればいいんじゃないかと思うところですが、残念ながらそれはできません。。 なので、画像を複数添付する場合は、別の方法を使います。\n簡単に手順を書くと、\nTwitterに画像をアップロードして、その画像固有のIDを取得する 取得したIDをカンマ(,)で結合する 通常のツイートのパラメータに、結合した文字列を渡す となります。 1.Twitterに画像をアップロードして、その画像固有のIDを取得する では、まず手順１。 画像アップロードのリソースURLは https://upload.twitter.com/1.1/media/upload.json です。\n\u0026lt;?php $res = $client-\u0026gt;postMultipartFormData( $at, // アプリケーションのaccess token $ats, // アプリケーションのaccess token secret \u0026#39;https://upload.twitter.com/1.1/media/upload.json\u0026#39;, array( \u0026#39;media\u0026#39; =\u0026gt; \u0026#39;添付する画像へのパス\u0026#39; ), ); ここで注意したいのは、先程と同様に post ではなく postMultipartFormData を使うという点と、 パラメータの名前が media[] ではなく media であるという点です。\n2.取得したIDをカンマ(,)で結合する 正常にアップロードが完了すると、json形式でレスポンスが取れるので、その中からmedia_idを取得します。\n\u0026lt;?php $res_obj = json_decode( $res ); $media_id = $res_obj-\u0026gt;media_id; これを画像の枚数分繰り返して、取得したmedia_idをカンマ(,)で結合するのが手順２となります。(現状、4枚まで添付可能です。)\n3.通常のツイートのパラメータに、結合した文字列を渡す あとは、一番最初に紹介した普通にツイートする際のパラメータに、結合した文字列を追加するだけです。\n\u0026lt;?php $client-\u0026gt;post( $at, // アプリケーションのaccess token $ats, // アプリケーションのaccess token secret \u0026#39;https://api.twitter.com/1.1/statuses/update.json\u0026#39;, array( \u0026#39;status\u0026#39; =\u0026gt; \u0026#39;Tweet内容\u0026#39;, \u0026#39;media_ids\u0026#39; =\u0026gt; \u0026#39;mediaID01,mediaID02,mediaID03\u0026#39; ) ); 以上が、Twitter API1.1画像付きツイートする方法でした。\n",
    "permalink": "https://michimani.net/post/twitter-tweet-with-images/",
    "title": "Twitter API1.1で画像付きツイートしてみる"
  },
  {
    "contents": "書いてる人 michimani\n書くこと 技術的なこと ガジェットのこと 読んだ本のこと などなどを書きます。\nSNS Twitter - @michimani210 Medium - @michimani GitHub - @michimani Speaker Deck - @michimani お問い合わせ お問い合わせは、下記メールアドレス または Twitter へのリプライ／DM または 各記事のコメント欄にてお願いいたします。\nmail: michimani210[a]gmail.com その他 広告の配信について michimani.net では第三者配信の広告サービス「Google Adsense グーグルアドセンス」を利用しています。\n広告配信事業者は、ユーザーの興味に応じた広告を表示するために Cookie を使用することがあります。\nCookie を無効にする設定および Google アドセンスに関する詳細は 広告 – ポリシーと規約 – Google をご覧ください。\nまた、michimani.net は、Amazon.co.jpを宣伝しリンクすることによってサイトが紹介料を獲得できる手段を提供することを目的に設定されたアフィリエイトプログラムである、 Amazon アソシエイト・プログラムの参加者です。\n第三者がコンテンツおよび宣伝を提供し、訪問者から直接情報を収集し、訪問者のブラウザに Cookie を設定したりこれを認識したりする場合があります。\nアクセス解析ツールについて michimani.net では、 Google によるアクセス解析ツール「Google アナリティクス」を利用しています。\nこの Google アナリティクスはトラフィックデータの収集のために Cookie を使用しています。このトラフィックデータは匿名で収集されており、個人を特定するものではありません。この機能は Cookie を無効にすることで収集を拒否することが出来ますので、お使いのブラウザの設定をご確認ください。この規約に関して、詳しくは Google アナリティクス利用規約 を参照してください。\n免責事項 michimani.net (以下、当ブログ) で掲載している画像の著作権・肖像権等は各権利所有者に帰属致します。権利を侵害する目的ではございません。記事の内容や掲載画像等に問題がございましたら、各権利所有者様本人が直接メールでご連絡下さい。確認後、対応させて頂きます。\n当ブログからリンクやバナーなどによって他のサイトに移動された場合、移動先サイトで提供される情報、サービス等について一切の責任を負いません。\n当ブログのコンテンツ・情報につきまして、可能な限り正確な情報を掲載するよう努めておりますが、誤情報が入り込んだり、情報が古くなっていることもございます。また、当ブログで掲載されている情報に関しては告知なしに情報を変更・削除することがあります。\n当ブログに掲載された内容によって生じた損害等の一切の責任を負いかねますのでご了承ください。\n",
    "permalink": "https://michimani.net/about/",
    "title": "About"
  }
]